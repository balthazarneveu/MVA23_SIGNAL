{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c235e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torchmetrics\n",
    "import h5py # pour gérer les formats de données utilisés ici \n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa84c0",
   "metadata": {},
   "source": [
    "# TP3: Reconnaissance de signaux de communication par apprentissage profond \n",
    "\n",
    "<div class=consignes> Listez les noms des étudiants (2 au maximum) ayant participé à ce notebook dans la cellule suivante (prénom, nom, affectation).<br/>\n",
    "Au moment du rendu, le notebook doit être nommé nom1_nom2_dlts_tp3.ipynb \n",
    "\n",
    "3 séances de TP sur ce sujet : le 15 novembre (1h00), le 22 novembre (3h) et le 29 novembre (3h).<br> \n",
    "Deadline : 6 décembre 2023, 11h59, par mail à deepetsignal.mva@gmail.com <br> \n",
    "\n",
    "Pour installer les paquets nécessaires à la réalisation de ce TP vous pouvez utiliser dans le notebook \n",
    "    \n",
    "```\n",
    "!pip install \\< nom_du_paquet \\>\n",
    "```\n",
    "merci de regrouper toutes les installations dans la première cellule du notebook. \n",
    "Essayez de faire en sorte que votre notebook puisse se lire comme un compte rendu, évitez de laisser du code mort et prennez le temps de commenter vos observations et résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747407a",
   "metadata": {},
   "source": [
    "## Problématique\n",
    "\n",
    "On cherche à identifier un type d'émetteur de communication à partir de l'observation d'un signal provenant de l'émetteur \n",
    "de 2048 échantillons IQ (In Phase / Quadrature) ie le signal prend des valeurs complexes. On représente la partie \n",
    "réelle et la partie imaginaire du signal par deux canaux réel d'un signal multivarié. \n",
    "\n",
    "L'émetteur peut provenir de 6 catégories différentes. \n",
    "Les paramètres différenciant les différentes catégories sont \n",
    "- le type de modulation \n",
    "- la présence ou non de séquences pilotes et le cas échéant la structure de trame pilotes / données \n",
    "- le débit de la transmission \n",
    "\n",
    "Les signaux se propagent en champs libre et sont enregistrés par une antenne. Le signal reçu est transposé en bande de base c'est à dire que si le signal est transmis autour d'une fréquence centrale f0, une première étape de traitement du signal à la réception recentre le signal autour de la fréquence 0. \n",
    "\n",
    "\n",
    "Les différents signaux observés dans ce TP sont entachés de différentes erreurs caractéristiques de la propagation \n",
    "électromagnétiques comme : \n",
    "- modification aléatoire de la phase du signal lors de la transmission\n",
    "- imperfection de la transposition en bande de base qui laisse le signal transposé à une fréquence df0 << f0\n",
    "- présence d'interférence entre les symboles transmis (dûes par exemple à plusieurs chemins de propagation)\n",
    "- présence d'un bruit blanc additif gaussien\n",
    "\n",
    "Le niveau du bruit relativement à celui du signal utile est décrit par le SNR (Signal to Noise Ratio) et exprimé en dB. On suppose que le SNR est connu lors de l'acquisition d'un signal. Lors de ce TP nous rencontrerons 4 niveaux de SNR: 30 dB (facile), 20 dB, 10 dB et 0 dB (en espérant qu'on puisse faire quelque chose de ces données). \n",
    "Un de nos objectifs sera de qualifier la performance des algorithmes mis en place en fonction du SNR.\n",
    "\n",
    "Les objectifs de ce TP sont: \n",
    "1/ Définir une ou plusieurs architectures de réseaux de neurones profonds et les implémenter en PyTorch\n",
    "2/ Entrainer ces architectures, la fonction de perte employée pourra être la log vraisemblance négative: https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html. \n",
    "3/ Qualifier les performances de votre réseau de neurones sur l'ensemble de test via: \n",
    "   - Le calcul de l'accuracy implémentée par exemple dans le package TorchMetrics (https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html)\n",
    "   - La réalisation d'un graphique accuracy vs SNR \n",
    "   - La réalisation des matrices de confusion entre les différentes classes pour les différents SNR (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix)\n",
    "\n",
    "Durant l'entraînement on observera l'évolution de la fonction de perte et de l'accuracy sur l'ensemble d'entraînement et sur l'ensemble de validation. \n",
    "\n",
    "\n",
    "Les 4 premières parties sont un échauffement sur lequel vous pouvez passer vite si vous êtes à l'aise avec le sujet. \n",
    "Le gros du travail est dans la partie 5 \"Entraînemenent d'un réseau de neurones\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4865e6",
   "metadata": {},
   "source": [
    "## Chargement des données en numpy\n",
    "\n",
    "Le TP est composé de trois jeux de données : \n",
    "- train.hdf5 destiné à nourrir l'entrainement de réseaux de neurones \n",
    "- test.hdf5 destiné à évaluer les algorithmes après entrainement\n",
    "- samples.hdf5 qui est beaucoup plus petit que train.hdf5 et destiné à servir de modèle de données dans une phase de prototypage \n",
    "des algorithmes et de la pipeline d'entrainement\n",
    "\n",
    "Les trois jeux de données sont au format hdf5 qui peut être manipulé via l'API Python h5py https://docs.h5py.org/en/stable/quick.html.\n",
    "Un fichier hdf5 est consitué d'une arborescence de datasets et de groups. Un dataset hdf5 représente un tenseur n dimensionnel. Un dataset se convertit très facilement en numpy array.\n",
    "\n",
    "Par exemple vous pouvez charger les données samples selon la cellule suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "803ab62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path(\"data\")\n",
    "data_path = DATA/\"samples.hdf5\"\n",
    "assert data_path.exists()\n",
    "\n",
    "data = h5py.File(data_path , 'r')\n",
    "\n",
    "signals = np.array(data['signaux'])\n",
    "snr =  np.array(data['snr'])\n",
    "labels_id = np.array(data['labels'])\n",
    "\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66630a9e",
   "metadata": {},
   "source": [
    "Vous pouvez récupérer le nom de la correspondance entre un label et le nom du standard d'émetteur correspondant via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecab0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(open_h5_file): \n",
    "    return {\n",
    "        open_h5_file['label_name'].attrs[k] : k\n",
    "        for k in open_h5_file['label_name'].attrs.keys()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c61acbe",
   "metadata": {},
   "source": [
    "### Visualisation des données \n",
    "\n",
    "Commencez par étudier les données: \n",
    "\n",
    "    - observez leur taille \n",
    "    - la distribution des différentes classes et des différents SNR dans l'ensemble d'entrainement \n",
    "    - visualisez quelques signaux bien choisis selon une ou des représentations que vous choisirez "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f0a2f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des signaux : (200, 2048, 2)\n",
      "Shape SNR :  (200,)\n",
      "Shape SNR :  (200,)\n",
      "(200,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7150fb052d6e4eb99b02ffe1cb7cad9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='idx', max=199), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize(idx)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensions\n",
    "print(f\"Taille des signaux : {signals.shape}\")\n",
    "print(\"Shape SNR : \", snr.shape)\n",
    "print(\"Shape SNR : \", snr.shape)\n",
    "print(labels_id.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(labels_id)\n",
    "plt.title(\"Répartition des classes\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(snr)\n",
    "plt.title(\"Répartition des SNR\")\n",
    "plt.show()\n",
    "\n",
    "# Visualisation des données dans le repère IQ\n",
    "def visualize(idx):\n",
    "    plt.figure()\n",
    "    plt.plot(signals[idx, :, 0], signals[idx, :, 1], '.')\n",
    "    modulation = get_labels(h5py.File(data_path, 'r'))[labels_id[idx]]\n",
    "    plt.title(f'Signal {idx} de snr {snr[idx]} (mod {modulation})')\n",
    "    plt.grid()\n",
    "    plt.xlabel('I')\n",
    "    plt.ylabel('Q')\n",
    "    plt.show()\n",
    "\n",
    "interact(visualize, idx=IntSlider(min=0, max=signals.shape[0] - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque : \n",
    "Dans l'ensemble samples, il y a 20 signaux de 2048 échantillons sur 2 canaux (I et Q)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9fa901",
   "metadata": {},
   "source": [
    "## Chargement des données en Pytorch\n",
    "\n",
    "Pour entrainer des réseaux de neurones profond sur nos données nous allons utiliser le framework Pytorch. \n",
    "Une première étape va consister à transférer les données de numpy à PyTorch, cela passe par deux objets : \n",
    "    - un Dataset qui modélise le dataset à haut niveau dans la mémoire de l'ordinateur\n",
    "    - un Dataloader qui permet d'échantillonner le Dataset Pytorch dans les itérations de l'optimisation du réseau de neurones \n",
    "    \n",
    "Un dataset prend la forme \n",
    "```python\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path_to_data):\n",
    "        ...\n",
    "    def __len__(self): #retourne le nombre de données dans le dataset\n",
    "        ...\n",
    "    def __getitem__(self,i): #retourne pour chaque indice i un couple (data_i, lablel_i), data_i étant un signal et label_i le label associé au signal\n",
    "        ...\n",
    "```\n",
    "\n",
    "Implémentez une classe Dataset pour le dataset considéré ici "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "53406fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(torch.utils.data.Dataset) : \n",
    "    def __init__(self, path_to_data : Path) :\n",
    "        torch.utils.data.Dataset.__init__(self)\n",
    "        assert path_to_data.exists()\n",
    "        data = h5py.File(path_to_data , 'r')\n",
    "\n",
    "        self.signals = np.array(data['signaux'])\n",
    "        self.snr =  np.array(data['snr'])\n",
    "        self.labels_id = np.array(data['labels'])\n",
    "\n",
    "        data.close()\n",
    "    \n",
    "    def __len__(self) :\n",
    "        \"\"\"Retourne la taille du dataset\"\"\"\n",
    "        return self.signals.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i : int) :\n",
    "        \"\"\"retourne pour chaque indice i un couple (data_i, lablel_i),\n",
    "        data_i étant un signal et label_i le label associé au signal\"\"\"\n",
    "        return((self.signals[i], int(self.labels_id[i])))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69069805",
   "metadata": {},
   "source": [
    "Instanciez un objet dataset et testez le sur les données samples\n",
    "```python\n",
    "dataset = MyDataset(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "144c1e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dataset : 200\n",
      "Element 3 du dataset : \n",
      "signal [[ 0.01738485  0.4679469 ]\n",
      " [-0.02483279  0.3787216 ]\n",
      " [-0.22873749  0.1594786 ]\n",
      " ...\n",
      " [-0.33821508  1.5790684 ]\n",
      " [-0.50299037  1.0259963 ]\n",
      " [-0.6004157  -0.8126683 ]],\n",
      " label 3\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataSet(data_path)\n",
    "print(f\"Taille du dataset : {len(dataset)}\")\n",
    "print(f\"Element 3 du dataset : \\nsignal {dataset[3][0]},\\n label {dataset[3][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6532b5",
   "metadata": {},
   "source": [
    "Pytorch propose une classe Dataloader qui permet d'échantillonner des batchs de taille fixe à partir d'un dataset. \n",
    "La cellule suivante donne un exemple d'utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3044a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=10, \n",
    "                        shuffle=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f1b6e",
   "metadata": {},
   "source": [
    "Testez le dataloader pour différentes valeurs de batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "965210fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch de 10 : torch.Size([10, 2048, 2]) torch.Size([10])\n",
      "1er élément :  tensor([[ 0.2033,  0.6199],\n",
      "        [-0.1835,  0.4715],\n",
      "        [-0.7318,  0.5121],\n",
      "        ...,\n",
      "        [ 0.6348,  0.8279],\n",
      "        [ 0.4756,  0.4679],\n",
      "        [-0.0154, -0.6884]])\n",
      "batch de 30 : torch.Size([30, 2048, 2]) torch.Size([30])\n",
      "1er élément :  tensor([[-0.9284,  0.0633],\n",
      "        [-0.7979,  0.0506],\n",
      "        [ 0.4080,  0.0400],\n",
      "        ...,\n",
      "        [-0.6053, -1.8346],\n",
      "        [ 0.1402, -1.0426],\n",
      "        [-0.6492, -0.6717]])\n",
      "<class 'torch.utils.data.dataloader.DataLoader'> <class 'torch.Tensor'> <class 'torch.Tensor'> tensor([1, 0, 3, 3, 0, 3, 4, 5, 0, 5, 1, 0, 3, 4, 0, 5, 4, 1, 4, 2, 0, 1, 0, 4,\n",
      "        1, 2, 3, 2, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=10, \n",
    "                        shuffle=True\n",
    "                       )\n",
    "signals_set, labels_set = next(iter(dataloader))\n",
    "print(\"batch de 10 :\", signals_set.size(), labels_set.size())\n",
    "print(\"1er élément : \", signals_set[0])\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=30, \n",
    "                        shuffle=True\n",
    "                       )\n",
    "signals_set, labels_set = next(iter(dataloader))\n",
    "print(\"batch de 30 :\", signals_set.size(), labels_set.size())\n",
    "print(\"1er élément : \", signals_set[0])\n",
    "\n",
    "print(type(dataloader), type(signals_set), type(labels_set), labels_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La taille du batch_size permet de choisir le compromis entre un batch_size faible qui donne un gradient très bruité, et un batch_size élevé pour lequel le gradient est peu bruité mais qui est alors plus facilement \"bloqué\" sur un minima local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le DataLoader crée un tenseur avec une taille de batch donnée, les éléments étant tirés aléatoirement dans la base de données initiale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7a7c17",
   "metadata": {},
   "source": [
    "## Mise en place d'un réseau \"dumb\" pour tester la pipeline d'entrainement\n",
    "\n",
    "Définissez un premier modèle Pytorch qui prend en entrée un batch de données (tenseur de dimensions [B , C, T] avec B la taille du batch, C le nombre de canaux des signaux et T le nombre d'échantillons dans les signaux) et renvoie un batch de vecteur de probabilités (ou de log probabilités si vous préférez) (tenseur de dimensions [B,N] où N est le nombre de classe à identifier). \n",
    "\n",
    "Ce Modèle doit être très simple, il doit être rapide à exécuter, il servira à tester et éventuellement débugger la pipeline d'entrainement que vous mettrez en place un peu plus loin. Un template d'implémentation d'une classe Model se trouve dans les diapositives du cours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8710cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DumbModel(torch.nn.Module) :\n",
    "    def __init__(self, chan_size, time_size, nb_class : int) :\n",
    "        torch.nn.Module.__init__(self)\n",
    "\n",
    "        self.nb_class = nb_class\n",
    "        self.chan_size = chan_size\n",
    "        self.time_size = time_size\n",
    "        self.avgpooling = torch.nn.AvgPool1d(10)\n",
    "        self.fc = torch.nn.Linear(chan_size * int(time_size / 10), self.nb_class)\n",
    "        \n",
    "    \n",
    "    def forward(self, x : torch.Tensor) :\n",
    "        \"\"\" \n",
    "            Input :\n",
    "                x a batch of size [B, C, T]\n",
    "            Output : \n",
    "                a batch of size [B, self.nb_class]\n",
    "        \"\"\"\n",
    "        assert x.size(1) == self.chan_size\n",
    "        assert x.size(2) == self.time_size\n",
    "        x_sub = self.avgpooling(x)\n",
    "        x_sub = x_sub.reshape(x_sub.size(0), x_sub.size(1) * x_sub.size(2)) #[B, C * T] (flatten)\n",
    "        logits = self.fc(x_sub)\n",
    "        # return torch.nn.functional.softmax(logits, dim = 1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Si le dumb model est trop long à éxécuter car ici on a CxTxnb_class = 2x2048x6 poids, on peut décimer le signal d'entrée (par ex. 1points sur 10) avant la couche fully connected). On renvoie les logits sans softmax par praticité pour les fonctions de loss utilisées ensuite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8767db4",
   "metadata": {},
   "source": [
    "Instanciez votre modèle et testez la consistence de ses entrées / sorties vis à vis des données étudiées "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "48d3766f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrée :  torch.Size([30, 2, 2048])\n",
      "Sortie :  torch.Size([30, 6]) de 1er elem :  tensor([-0.1820,  0.0078,  0.1366,  0.0404,  0.0628,  0.1122],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(-0.4639, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True\n",
    "                       )\n",
    "signals_set, labels_set = next(iter(dataloader)) #dim [B, T, C] and [B, T]\n",
    "# Swap dimensions to have a tensor of dim [B, C, T] rather than [B, T, C]\n",
    "signals_set_bct = signals_set.permute(0, 2, 1) # dim [B, C, T]\n",
    "print(\"Entrée : \", signals_set_bct.size())\n",
    "\n",
    "dumbModel = DumbModel(signals_set_bct.size(1), signals_set_bct.size(2), 6)\n",
    "y = dumbModel(signals_set_bct) # forward\n",
    "\n",
    "print(\"Sortie : \", y.size(), \"de 1er elem : \", y[0])\n",
    "print(y[20].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617dcb82",
   "metadata": {},
   "source": [
    "## Mise en place de la pipeline d'entraînement\n",
    "\n",
    "La pipeline d'entrainement consiste à \n",
    "- charger les données \n",
    "- les batcher \n",
    "- réaliser des itération (epochs) de descente de gradient pour optimiser les paramètres d'un algorithme selon une fonction de perte (loss)\n",
    "- logger l'évolution au fil des epochs  de la loss sur l'ensemble train et l'ensemble de validation et éventuellement de métriques complémentaires \n",
    "\n",
    "Un cavnevas d'implémentation pourrait être:\n",
    "\n",
    "```python\n",
    "device = 'cpu' # set so 'cuda:xx' if you have a GPU, xx is GPU index. L'entraînement des réseaux de neurones est grandement accéléré par l'utilisation d'un GPU \n",
    "\n",
    "model = ...  # vous instanciez ici votre modèle\n",
    "\n",
    "loss = .... # définissez la fonction de perte selon laquelle le modèle sera optimisé\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters()) # en pratique on utilise pas une simple descente de gradient mais une procédure d'optimisation plus sophistiquée qui est implémentée sous la forme d'un objet Optimizer. Il en existe beaucoup d'optimizers différents, vous pouvez en tester différents, je vous propose d'utiliser en premier lieu l'algorithme Adam\n",
    "\n",
    "n_epochs = ... # le nombre d'itérations dans l'entrainement \n",
    "\n",
    "chemin_vers_sauvegarde_model = # chemin vers un fichier où vous sauvegarderez votre modèle après optimisation pour le réutiliser plus tard. \n",
    "\n",
    "model.to(device) # on place le modèle dans le GPU si nécessaire\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for batch_x,batch_y in dataloader_train:\n",
    "        \n",
    "        batch_x.to(device)\n",
    "        batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_y_predicted = model(batch_x)\n",
    "        \n",
    "        l = loss(batch_y_predicted, batch_y)\n",
    "        # loggez la loss sur le batch d'entraînement\n",
    "        \n",
    "        l.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    for batch_x,batch_y in dataloader_valid:\n",
    "        \n",
    "        batch_x.to(device)\n",
    "        batch_y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_y_predicted = model(batch_x)  \n",
    "            \n",
    "        # loggez la loss et les métriques sur le batch de validation\n",
    "\n",
    "torch.save(model, chemin_vers_sauvegarde_model)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c67127",
   "metadata": {},
   "source": [
    "Mettez en place votre pipeline et testez là sur votre modèle dumb. Faites en sorte que votre façon de logger les loss et les métriques vous permette de visualiser l'évolution de ces différents indicateurs sur l'ensemble d'entrainement et de validation au fil des epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parametres and datasets\n",
    "batch_size = 20\n",
    "DATA = Path(\"data\")\n",
    "dataset_valid = MyDataSet(DATA/\"samples.hdf5\")\n",
    "dataset_train = MyDataSet(DATA/\"train.hdf5\")\n",
    "dataset_validation = MyDataSet(DATA/\"validation.hdf5\")\n",
    "\n",
    "\n",
    "# Dataloaders\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True\n",
    "                       )\n",
    "dataloader_valid = DataLoader(dataset_validation, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True\n",
    "                       )\n",
    "print(len(dataloader_train))\n",
    "print(len(dataloader_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6d8ffa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0 batch 0 with loss 1.7644121646881104, accuracy 0.28333336114883423.\n",
      "Training epoch 0 batch 1 with loss 1.7718942165374756, accuracy 0.3499999940395355.\n",
      "Training epoch 0 batch 2 with loss 1.7532899379730225, accuracy 0.2658730149269104.\n",
      "Training epoch 0 batch 3 with loss 1.7764561176300049, accuracy 0.2638888955116272.\n",
      "Training epoch 0 batch 4 with loss 1.8629167079925537, accuracy 0.0625.\n",
      "Training epoch 0 batch 5 with loss 1.8399890661239624, accuracy 0.2611111104488373.\n",
      "Training epoch 0 batch 6 with loss 1.8183246850967407, accuracy 0.21111111342906952.\n",
      "Training epoch 0 batch 7 with loss 1.8412437438964844, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 8 with loss 1.7633650302886963, accuracy 0.3253968358039856.\n",
      "Training epoch 0 batch 9 with loss 1.7276073694229126, accuracy 0.3055555522441864.\n",
      "Training epoch 0 batch 10 with loss 1.8205829858779907, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 11 with loss 1.8228204250335693, accuracy 0.23333333432674408.\n",
      "Training epoch 0 batch 12 with loss 1.7812858819961548, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 13 with loss 1.8187570571899414, accuracy 0.15555556118488312.\n",
      "Training epoch 0 batch 14 with loss 1.8443609476089478, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 15 with loss 1.8734687566757202, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 16 with loss 1.7630043029785156, accuracy 0.222222238779068.\n",
      "Training epoch 0 batch 17 with loss 1.787536859512329, accuracy 0.2916666567325592.\n",
      "Training epoch 0 batch 18 with loss 1.7988624572753906, accuracy 0.1626984179019928.\n",
      "Training epoch 0 batch 19 with loss 1.778071403503418, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 20 with loss 1.7574306726455688, accuracy 0.222222238779068.\n",
      "Training epoch 0 batch 21 with loss 1.7824596166610718, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 22 with loss 1.8930832147598267, accuracy 0.0.\n",
      "Training epoch 0 batch 23 with loss 1.8629624843597412, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 24 with loss 1.8515746593475342, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 25 with loss 1.8957853317260742, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 26 with loss 1.8337376117706299, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 27 with loss 1.6886541843414307, accuracy 0.22499999403953552.\n",
      "Training epoch 0 batch 28 with loss 1.7434041500091553, accuracy 0.2944444417953491.\n",
      "Training epoch 0 batch 29 with loss 1.8603107929229736, accuracy 0.0.\n",
      "Training epoch 0 batch 30 with loss 1.850773572921753, accuracy 0.0793650820851326.\n",
      "Training epoch 0 batch 31 with loss 1.8352725505828857, accuracy 0.13611111044883728.\n",
      "Training epoch 0 batch 32 with loss 1.7331609725952148, accuracy 0.1924603283405304.\n",
      "Training epoch 0 batch 33 with loss 1.8513469696044922, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 34 with loss 1.862932562828064, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 35 with loss 1.8479697704315186, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 36 with loss 1.8420441150665283, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 37 with loss 1.826696753501892, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 38 with loss 1.880671739578247, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 39 with loss 1.7435810565948486, accuracy 0.255952388048172.\n",
      "Training epoch 0 batch 40 with loss 1.7706775665283203, accuracy 0.1488095223903656.\n",
      "Training epoch 0 batch 41 with loss 1.7829678058624268, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 42 with loss 1.8072690963745117, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 43 with loss 1.7695214748382568, accuracy 0.12037037312984467.\n",
      "Training epoch 0 batch 44 with loss 1.8283876180648804, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 45 with loss 1.7795391082763672, accuracy 0.2916666567325592.\n",
      "Training epoch 0 batch 46 with loss 1.8440570831298828, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 47 with loss 1.8753929138183594, accuracy 0.125.\n",
      "Training epoch 0 batch 48 with loss 1.8641183376312256, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 49 with loss 1.7727041244506836, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 50 with loss 1.7741779088974, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 51 with loss 1.8839870691299438, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 52 with loss 1.8348569869995117, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 53 with loss 1.798383355140686, accuracy 0.20000000298023224.\n",
      "Training epoch 0 batch 54 with loss 1.8910690546035767, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 55 with loss 1.8013637065887451, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 56 with loss 1.8136920928955078, accuracy 0.2666666805744171.\n",
      "Training epoch 0 batch 57 with loss 1.8384889364242554, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 58 with loss 1.7911603450775146, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 59 with loss 1.8317201137542725, accuracy 0.236111119389534.\n",
      "Training epoch 0 batch 60 with loss 1.836666464805603, accuracy 0.0763888880610466.\n",
      "Training epoch 0 batch 61 with loss 1.8218612670898438, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 62 with loss 1.8511638641357422, accuracy 0.25555557012557983.\n",
      "Training epoch 0 batch 63 with loss 1.8053582906723022, accuracy 0.19305555522441864.\n",
      "Training epoch 0 batch 64 with loss 1.7541484832763672, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 65 with loss 1.7929506301879883, accuracy 0.125.\n",
      "Training epoch 0 batch 66 with loss 1.7417047023773193, accuracy 0.2916666567325592.\n",
      "Training epoch 0 batch 67 with loss 1.825646996498108, accuracy 0.125.\n",
      "Training epoch 0 batch 68 with loss 1.8159904479980469, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 69 with loss 1.782305121421814, accuracy 0.22777777910232544.\n",
      "Training epoch 0 batch 70 with loss 1.8149750232696533, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 71 with loss 1.6990134716033936, accuracy 0.3698412775993347.\n",
      "Training epoch 0 batch 72 with loss 1.7693758010864258, accuracy 0.39444446563720703.\n",
      "Training epoch 0 batch 73 with loss 1.7857666015625, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 74 with loss 1.8246132135391235, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 75 with loss 1.7890344858169556, accuracy 0.3055555522441864.\n",
      "Training epoch 0 batch 76 with loss 1.8102232217788696, accuracy 0.23055556416511536.\n",
      "Training epoch 0 batch 77 with loss 1.7855656147003174, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 78 with loss 1.8598581552505493, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 79 with loss 1.7954269647598267, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 80 with loss 1.7540127038955688, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 81 with loss 1.7630869150161743, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 82 with loss 1.8533881902694702, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 83 with loss 1.7588714361190796, accuracy 0.25.\n",
      "Training epoch 0 batch 84 with loss 1.7725101709365845, accuracy 0.18611110746860504.\n",
      "Training epoch 0 batch 85 with loss 1.7539455890655518, accuracy 0.26547619700431824.\n",
      "Training epoch 0 batch 86 with loss 1.7992980480194092, accuracy 0.23333334922790527.\n",
      "Training epoch 0 batch 87 with loss 1.853527307510376, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 88 with loss 1.820842981338501, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 89 with loss 1.8203455209732056, accuracy 0.0476190485060215.\n",
      "Training epoch 0 batch 90 with loss 1.8352622985839844, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 91 with loss 1.8495540618896484, accuracy 0.0694444477558136.\n",
      "Training epoch 0 batch 92 with loss 1.845499038696289, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 93 with loss 1.7760223150253296, accuracy 0.25.\n",
      "Training epoch 0 batch 94 with loss 1.8570435047149658, accuracy 0.2361111342906952.\n",
      "Training epoch 0 batch 95 with loss 1.8423608541488647, accuracy 0.1587301641702652.\n",
      "Training epoch 0 batch 96 with loss 1.8203684091567993, accuracy 0.0.\n",
      "Training epoch 0 batch 97 with loss 1.8689558506011963, accuracy 0.0.\n",
      "Training epoch 0 batch 98 with loss 1.8386884927749634, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 99 with loss 1.7783457040786743, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 100 with loss 1.8352530002593994, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 101 with loss 1.799986481666565, accuracy 0.26944443583488464.\n",
      "Training epoch 0 batch 102 with loss 1.8326257467269897, accuracy 0.118055559694767.\n",
      "Training epoch 0 batch 103 with loss 1.7922121286392212, accuracy 0.24166665971279144.\n",
      "Training epoch 0 batch 104 with loss 1.7886826992034912, accuracy 0.0694444477558136.\n",
      "Training epoch 0 batch 105 with loss 1.764556646347046, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 106 with loss 1.8100944757461548, accuracy 0.236111119389534.\n",
      "Training epoch 0 batch 107 with loss 1.7848867177963257, accuracy 0.17777778208255768.\n",
      "Training epoch 0 batch 108 with loss 1.850809097290039, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 109 with loss 1.8039324283599854, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 110 with loss 1.8052442073822021, accuracy 0.06111111491918564.\n",
      "Training epoch 0 batch 111 with loss 1.7362728118896484, accuracy 0.10833333432674408.\n",
      "Training epoch 0 batch 112 with loss 1.6917407512664795, accuracy 0.3472222089767456.\n",
      "Training epoch 0 batch 113 with loss 1.7454853057861328, accuracy 0.1875.\n",
      "Training epoch 0 batch 114 with loss 1.7431857585906982, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 115 with loss 1.7882511615753174, accuracy 0.2777777910232544.\n",
      "Training epoch 0 batch 116 with loss 1.7637081146240234, accuracy 0.2750000059604645.\n",
      "Training epoch 0 batch 117 with loss 1.7569319009780884, accuracy 0.2083333283662796.\n",
      "Training epoch 0 batch 118 with loss 1.7303060293197632, accuracy 0.23333333432674408.\n",
      "Training epoch 0 batch 119 with loss 1.7289679050445557, accuracy 0.2666666805744171.\n",
      "Training epoch 0 batch 120 with loss 1.7694194316864014, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 121 with loss 1.7745301723480225, accuracy 0.2638888955116272.\n",
      "Training epoch 0 batch 122 with loss 1.7778654098510742, accuracy 0.2738095223903656.\n",
      "Training epoch 0 batch 123 with loss 1.7352664470672607, accuracy 0.3392857313156128.\n",
      "Training epoch 0 batch 124 with loss 1.8250617980957031, accuracy 0.0.\n",
      "Training epoch 0 batch 125 with loss 1.712436318397522, accuracy 0.4464285969734192.\n",
      "Training epoch 0 batch 126 with loss 1.761386513710022, accuracy 0.15138888359069824.\n",
      "Training epoch 0 batch 127 with loss 1.8109474182128906, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 128 with loss 1.8235594034194946, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 129 with loss 1.877928376197815, accuracy 0.02380952425301075.\n",
      "Training epoch 0 batch 130 with loss 1.793317437171936, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 131 with loss 1.815966010093689, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 132 with loss 1.8452131748199463, accuracy 0.1597222238779068.\n",
      "Training epoch 0 batch 133 with loss 1.8733513355255127, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 134 with loss 1.8079595565795898, accuracy 0.18611110746860504.\n",
      "Training epoch 0 batch 135 with loss 1.8095016479492188, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 136 with loss 1.821977972984314, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 137 with loss 1.7827894687652588, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 138 with loss 1.871638298034668, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 139 with loss 1.787542700767517, accuracy 0.12222222983837128.\n",
      "Training epoch 0 batch 140 with loss 1.7747538089752197, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 141 with loss 1.7924751043319702, accuracy 0.1597222238779068.\n",
      "Training epoch 0 batch 142 with loss 1.829292893409729, accuracy 0.17777778208255768.\n",
      "Training epoch 0 batch 143 with loss 1.8259950876235962, accuracy 0.2888889014720917.\n",
      "Training epoch 0 batch 144 with loss 1.780578374862671, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 145 with loss 1.8708279132843018, accuracy 0.1587301641702652.\n",
      "Training epoch 0 batch 146 with loss 1.7621402740478516, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 147 with loss 1.8074910640716553, accuracy 0.16388890147209167.\n",
      "Training epoch 0 batch 148 with loss 1.7767549753189087, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 149 with loss 1.7992546558380127, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 150 with loss 1.739336371421814, accuracy 0.3916666805744171.\n",
      "Training epoch 0 batch 151 with loss 1.8454920053482056, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 152 with loss 1.8688535690307617, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 153 with loss 1.835963249206543, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 154 with loss 1.7712268829345703, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 155 with loss 1.7967379093170166, accuracy 0.20000001788139343.\n",
      "Training epoch 0 batch 156 with loss 1.7856372594833374, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 157 with loss 1.8552557229995728, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 158 with loss 1.819267988204956, accuracy 0.3055555522441864.\n",
      "Training epoch 0 batch 159 with loss 1.8184343576431274, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 160 with loss 1.8123432397842407, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 161 with loss 1.74505615234375, accuracy 0.15740741789340973.\n",
      "Training epoch 0 batch 162 with loss 1.72379469871521, accuracy 0.4305555820465088.\n",
      "Training epoch 0 batch 163 with loss 1.7824218273162842, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 164 with loss 1.795585036277771, accuracy 0.0892857164144516.\n",
      "Training epoch 0 batch 165 with loss 1.7726027965545654, accuracy 0.15833334624767303.\n",
      "Training epoch 0 batch 166 with loss 1.8595526218414307, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 167 with loss 1.8421815633773804, accuracy 0.10000000894069672.\n",
      "Training epoch 0 batch 168 with loss 1.717312216758728, accuracy 0.3611111342906952.\n",
      "Training epoch 0 batch 169 with loss 1.8153040409088135, accuracy 0.0.\n",
      "Training epoch 0 batch 170 with loss 1.8102285861968994, accuracy 0.2777777910232544.\n",
      "Training epoch 0 batch 171 with loss 1.8495839834213257, accuracy 0.25.\n",
      "Training epoch 0 batch 172 with loss 1.8107936382293701, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 173 with loss 1.824498176574707, accuracy 0.2420634925365448.\n",
      "Training epoch 0 batch 174 with loss 1.8396084308624268, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 175 with loss 1.8402572870254517, accuracy 0.25.\n",
      "Training epoch 0 batch 176 with loss 1.8775148391723633, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 177 with loss 1.8549988269805908, accuracy 0.02380952425301075.\n",
      "Training epoch 0 batch 178 with loss 1.8044252395629883, accuracy 0.1875.\n",
      "Training epoch 0 batch 179 with loss 1.7616901397705078, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 180 with loss 1.8197768926620483, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 181 with loss 1.8656971454620361, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 182 with loss 1.839525818824768, accuracy 0.19722223281860352.\n",
      "Training epoch 0 batch 183 with loss 1.8812042474746704, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 184 with loss 1.8181250095367432, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 185 with loss 1.770668625831604, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 186 with loss 1.8855396509170532, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 187 with loss 1.8192672729492188, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 188 with loss 1.7768828868865967, accuracy 0.1319444477558136.\n",
      "Training epoch 0 batch 189 with loss 1.800523042678833, accuracy 0.14047619700431824.\n",
      "Training epoch 0 batch 190 with loss 1.8160209655761719, accuracy 0.06018518656492233.\n",
      "Training epoch 0 batch 191 with loss 1.8640899658203125, accuracy 0.12222222983837128.\n",
      "Training epoch 0 batch 192 with loss 1.7732808589935303, accuracy 0.3214285969734192.\n",
      "Training epoch 0 batch 193 with loss 1.8499797582626343, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 194 with loss 1.8295323848724365, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 195 with loss 1.7581069469451904, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 196 with loss 1.8770482540130615, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 197 with loss 1.7540467977523804, accuracy 0.31666669249534607.\n",
      "Training epoch 0 batch 198 with loss 1.7808377742767334, accuracy 0.3055555522441864.\n",
      "Training epoch 0 batch 199 with loss 1.8832454681396484, accuracy 0.0.\n",
      "Training epoch 0 batch 200 with loss 1.7703797817230225, accuracy 0.15833333134651184.\n",
      "Training epoch 0 batch 201 with loss 1.812986135482788, accuracy 0.2281746119260788.\n",
      "Training epoch 0 batch 202 with loss 1.7919046878814697, accuracy 0.14166666567325592.\n",
      "Training epoch 0 batch 203 with loss 1.7791773080825806, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 204 with loss 1.7927916049957275, accuracy 0.2321428656578064.\n",
      "Training epoch 0 batch 205 with loss 1.8159675598144531, accuracy 0.09444444626569748.\n",
      "Training epoch 0 batch 206 with loss 1.748899221420288, accuracy 0.23333334922790527.\n",
      "Training epoch 0 batch 207 with loss 1.7736743688583374, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 208 with loss 1.8176987171173096, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 209 with loss 1.848825216293335, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 210 with loss 1.7418766021728516, accuracy 0.21111111342906952.\n",
      "Training epoch 0 batch 211 with loss 1.752168893814087, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 212 with loss 1.7660070657730103, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 213 with loss 1.7656307220458984, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 214 with loss 1.8453586101531982, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 215 with loss 1.8335498571395874, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 216 with loss 1.8301429748535156, accuracy 0.25.\n",
      "Training epoch 0 batch 217 with loss 1.8889938592910767, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 218 with loss 1.7730077505111694, accuracy 0.39444446563720703.\n",
      "Training epoch 0 batch 219 with loss 1.7643330097198486, accuracy 0.0892857164144516.\n",
      "Training epoch 0 batch 220 with loss 1.827361822128296, accuracy 0.10833333432674408.\n",
      "Training epoch 0 batch 221 with loss 1.7922840118408203, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 222 with loss 1.7678219079971313, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 223 with loss 1.877232551574707, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 224 with loss 1.7780863046646118, accuracy 0.2986111342906952.\n",
      "Training epoch 0 batch 225 with loss 1.87200927734375, accuracy 0.0.\n",
      "Training epoch 0 batch 226 with loss 1.8251698017120361, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 227 with loss 1.8290445804595947, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 228 with loss 1.8313812017440796, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 229 with loss 1.782578706741333, accuracy 0.39444443583488464.\n",
      "Training epoch 0 batch 230 with loss 1.8385165929794312, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 231 with loss 1.8476762771606445, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 232 with loss 1.8456350564956665, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 233 with loss 1.8276395797729492, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 234 with loss 1.75875723361969, accuracy 0.19166666269302368.\n",
      "Training epoch 0 batch 235 with loss 1.8218247890472412, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 236 with loss 1.807233214378357, accuracy 0.1865079402923584.\n",
      "Training epoch 0 batch 237 with loss 1.8353261947631836, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 238 with loss 1.7746318578720093, accuracy 0.14047619700431824.\n",
      "Training epoch 0 batch 239 with loss 1.834202527999878, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 240 with loss 1.847288727760315, accuracy 0.1626984179019928.\n",
      "Training epoch 0 batch 241 with loss 1.8654682636260986, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 242 with loss 1.8525959253311157, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 243 with loss 1.7790775299072266, accuracy 0.17777778208255768.\n",
      "Training epoch 0 batch 244 with loss 1.8072121143341064, accuracy 0.22777777910232544.\n",
      "Training epoch 0 batch 245 with loss 1.795530915260315, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 246 with loss 1.7870914936065674, accuracy 0.1865079402923584.\n",
      "Training epoch 0 batch 247 with loss 1.7851173877716064, accuracy 0.16428571939468384.\n",
      "Training epoch 0 batch 248 with loss 1.8031177520751953, accuracy 0.2529762089252472.\n",
      "Training epoch 0 batch 249 with loss 1.7431761026382446, accuracy 0.21388889849185944.\n",
      "Training epoch 0 batch 250 with loss 1.7344887256622314, accuracy 0.3027777671813965.\n",
      "Training epoch 0 batch 251 with loss 1.7964961528778076, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 252 with loss 1.8728649616241455, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 253 with loss 1.7935947179794312, accuracy 0.2777777910232544.\n",
      "Training epoch 0 batch 254 with loss 1.7910315990447998, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 255 with loss 1.8816629648208618, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 256 with loss 1.9260591268539429, accuracy 0.0.\n",
      "Training epoch 0 batch 257 with loss 1.8078362941741943, accuracy 0.15833334624767303.\n",
      "Training epoch 0 batch 258 with loss 1.809844732284546, accuracy 0.28333336114883423.\n",
      "Training epoch 0 batch 259 with loss 1.805566430091858, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 260 with loss 1.8514158725738525, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 261 with loss 1.8303123712539673, accuracy 0.2638888955116272.\n",
      "Training epoch 0 batch 262 with loss 1.8838961124420166, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 263 with loss 1.7857450246810913, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 264 with loss 1.8457969427108765, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 265 with loss 1.8204532861709595, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 266 with loss 1.7795377969741821, accuracy 0.28333336114883423.\n",
      "Training epoch 0 batch 267 with loss 1.787261724472046, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 268 with loss 1.803445816040039, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 269 with loss 1.7641748189926147, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 270 with loss 1.8097177743911743, accuracy 0.15833333134651184.\n",
      "Training epoch 0 batch 271 with loss 1.8295619487762451, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 272 with loss 1.802303671836853, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 273 with loss 1.8345153331756592, accuracy 0.3861111104488373.\n",
      "Training epoch 0 batch 274 with loss 1.7792799472808838, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 275 with loss 1.8278820514678955, accuracy 0.02380952425301075.\n",
      "Training epoch 0 batch 276 with loss 1.8385343551635742, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 277 with loss 1.8144636154174805, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 278 with loss 1.8000602722167969, accuracy 0.2777777910232544.\n",
      "Training epoch 0 batch 279 with loss 1.7888797521591187, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 280 with loss 1.8344223499298096, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 281 with loss 1.831561803817749, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 282 with loss 1.789707899093628, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 283 with loss 1.754159688949585, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 284 with loss 1.8553516864776611, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 285 with loss 1.8282064199447632, accuracy 0.125.\n",
      "Training epoch 0 batch 286 with loss 1.8385988473892212, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 287 with loss 1.7762725353240967, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 288 with loss 1.8279787302017212, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 289 with loss 1.7270129919052124, accuracy 0.4682539999485016.\n",
      "Training epoch 0 batch 290 with loss 1.7900972366333008, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 291 with loss 1.8110370635986328, accuracy 0.0.\n",
      "Training epoch 0 batch 292 with loss 1.7656524181365967, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 293 with loss 1.7736765146255493, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 294 with loss 1.843503713607788, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 295 with loss 1.8081293106079102, accuracy 0.25.\n",
      "Training epoch 0 batch 296 with loss 1.8431199789047241, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 297 with loss 1.7079299688339233, accuracy 0.40833336114883423.\n",
      "Training epoch 0 batch 298 with loss 1.7331231832504272, accuracy 0.3888888955116272.\n",
      "Training epoch 0 batch 299 with loss 1.7669309377670288, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 300 with loss 1.8320735692977905, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 301 with loss 1.8859975337982178, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 302 with loss 1.7922899723052979, accuracy 0.24166665971279144.\n",
      "Training epoch 0 batch 303 with loss 1.873168706893921, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 304 with loss 1.8067411184310913, accuracy 0.1626984179019928.\n",
      "Training epoch 0 batch 305 with loss 1.7682262659072876, accuracy 0.22500000894069672.\n",
      "Training epoch 0 batch 306 with loss 1.804054617881775, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 307 with loss 1.8176918029785156, accuracy 0.17777778208255768.\n",
      "Training epoch 0 batch 308 with loss 1.7294561862945557, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 309 with loss 1.7848917245864868, accuracy 0.2750000059604645.\n",
      "Training epoch 0 batch 310 with loss 1.793735146522522, accuracy 0.14166668057441711.\n",
      "Training epoch 0 batch 311 with loss 1.8395732641220093, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 312 with loss 1.7776883840560913, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 313 with loss 1.8176931142807007, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 314 with loss 1.830582618713379, accuracy 0.10000000894069672.\n",
      "Training epoch 0 batch 315 with loss 1.9091230630874634, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 316 with loss 1.790773630142212, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 317 with loss 1.7152299880981445, accuracy 0.3819444477558136.\n",
      "Training epoch 0 batch 318 with loss 1.7357791662216187, accuracy 0.2666666805744171.\n",
      "Training epoch 0 batch 319 with loss 1.786604881286621, accuracy 0.24722221493721008.\n",
      "Training epoch 0 batch 320 with loss 1.8474290370941162, accuracy 0.25555557012557983.\n",
      "Training epoch 0 batch 321 with loss 1.8451493978500366, accuracy 0.1349206417798996.\n",
      "Training epoch 0 batch 322 with loss 1.843051552772522, accuracy 0.1031746044754982.\n",
      "Training epoch 0 batch 323 with loss 1.8788032531738281, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 324 with loss 1.7328345775604248, accuracy 0.2083333283662796.\n",
      "Training epoch 0 batch 325 with loss 1.6802129745483398, accuracy 0.4166666865348816.\n",
      "Training epoch 0 batch 326 with loss 1.877863883972168, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 327 with loss 1.845293402671814, accuracy 0.2361111044883728.\n",
      "Training epoch 0 batch 328 with loss 1.731209397315979, accuracy 0.3611111044883728.\n",
      "Training epoch 0 batch 329 with loss 1.778807282447815, accuracy 0.25925925374031067.\n",
      "Training epoch 0 batch 330 with loss 1.8003345727920532, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 331 with loss 1.8063243627548218, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 332 with loss 1.893762230873108, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 333 with loss 1.8246381282806396, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 334 with loss 1.8236134052276611, accuracy 0.2430555522441864.\n",
      "Training epoch 0 batch 335 with loss 1.7886991500854492, accuracy 0.26944446563720703.\n",
      "Training epoch 0 batch 336 with loss 1.7835054397583008, accuracy 0.19166666269302368.\n",
      "Training epoch 0 batch 337 with loss 1.8485488891601562, accuracy 0.0793650820851326.\n",
      "Training epoch 0 batch 338 with loss 1.7106497287750244, accuracy 0.2083333283662796.\n",
      "Training epoch 0 batch 339 with loss 1.7988243103027344, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 340 with loss 1.7755550146102905, accuracy 0.40833333134651184.\n",
      "Training epoch 0 batch 341 with loss 1.8368237018585205, accuracy 0.16388888657093048.\n",
      "Training epoch 0 batch 342 with loss 1.8101574182510376, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 343 with loss 1.740204095840454, accuracy 0.2013888955116272.\n",
      "Training epoch 0 batch 344 with loss 1.8225282430648804, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 345 with loss 1.762494683265686, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 346 with loss 1.8553129434585571, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 347 with loss 1.794999122619629, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 348 with loss 1.8304309844970703, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 349 with loss 1.786812424659729, accuracy 0.22499999403953552.\n",
      "Training epoch 0 batch 350 with loss 1.7692241668701172, accuracy 0.12222222983837128.\n",
      "Training epoch 0 batch 351 with loss 1.8905775547027588, accuracy 0.02083333395421505.\n",
      "Training epoch 0 batch 352 with loss 1.7511394023895264, accuracy 0.10000000894069672.\n",
      "Training epoch 0 batch 353 with loss 1.7478080987930298, accuracy 0.2777777910232544.\n",
      "Training epoch 0 batch 354 with loss 1.8206641674041748, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 355 with loss 1.7410684823989868, accuracy 0.17777778208255768.\n",
      "Training epoch 0 batch 356 with loss 1.8100534677505493, accuracy 0.19722221791744232.\n",
      "Training epoch 0 batch 357 with loss 1.839845895767212, accuracy 0.1805555671453476.\n",
      "Training epoch 0 batch 358 with loss 1.7943729162216187, accuracy 0.2777777910232544.\n",
      "Training epoch 0 batch 359 with loss 1.8028209209442139, accuracy 0.1180555522441864.\n",
      "Training epoch 0 batch 360 with loss 1.8202276229858398, accuracy 0.3888888955116272.\n",
      "Training epoch 0 batch 361 with loss 1.8155676126480103, accuracy 0.10833333432674408.\n",
      "Training epoch 0 batch 362 with loss 1.7884538173675537, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 363 with loss 1.7880640029907227, accuracy 0.20370370149612427.\n",
      "Training epoch 0 batch 364 with loss 1.8126767873764038, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 365 with loss 1.8585017919540405, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 366 with loss 1.8862812519073486, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 367 with loss 1.830969214439392, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 368 with loss 1.778677225112915, accuracy 0.15833334624767303.\n",
      "Training epoch 0 batch 369 with loss 1.8484294414520264, accuracy 0.0476190485060215.\n",
      "Training epoch 0 batch 370 with loss 1.737649917602539, accuracy 0.3948412835597992.\n",
      "Training epoch 0 batch 371 with loss 1.821368932723999, accuracy 0.1547619104385376.\n",
      "Training epoch 0 batch 372 with loss 1.791712760925293, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 373 with loss 1.8698184490203857, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 374 with loss 1.7676969766616821, accuracy 0.2571428716182709.\n",
      "Training epoch 0 batch 375 with loss 1.7568790912628174, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 376 with loss 1.8692152500152588, accuracy 0.12777778506278992.\n",
      "Training epoch 0 batch 377 with loss 1.7716470956802368, accuracy 0.24166667461395264.\n",
      "Training epoch 0 batch 378 with loss 1.8363134860992432, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 379 with loss 1.8294975757598877, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 380 with loss 1.858095407485962, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 381 with loss 1.8095802068710327, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 382 with loss 1.830283522605896, accuracy 0.20000000298023224.\n",
      "Training epoch 0 batch 383 with loss 1.8389896154403687, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 384 with loss 1.8655669689178467, accuracy 0.12222222983837128.\n",
      "Training epoch 0 batch 385 with loss 1.8395411968231201, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 386 with loss 1.7731269598007202, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 387 with loss 1.7645890712738037, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 388 with loss 1.8192265033721924, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 389 with loss 1.8845993280410767, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 390 with loss 1.8310638666152954, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 391 with loss 1.8133354187011719, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 392 with loss 1.8081512451171875, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 393 with loss 1.8151435852050781, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 394 with loss 1.807135820388794, accuracy 0.2420634925365448.\n",
      "Training epoch 0 batch 395 with loss 1.7993866205215454, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 396 with loss 1.7801042795181274, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 397 with loss 1.8666563034057617, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 398 with loss 1.8997619152069092, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 399 with loss 1.7755053043365479, accuracy 0.130952388048172.\n",
      "Training epoch 0 batch 400 with loss 1.7782275676727295, accuracy 0.23333333432674408.\n",
      "Training epoch 0 batch 401 with loss 1.7783209085464478, accuracy 0.23333334922790527.\n",
      "Training epoch 0 batch 402 with loss 1.819502592086792, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 403 with loss 1.8649200201034546, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 404 with loss 1.7982828617095947, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 405 with loss 1.8144420385360718, accuracy 0.18333333730697632.\n",
      "Training epoch 0 batch 406 with loss 1.7624149322509766, accuracy 0.31111112236976624.\n",
      "Training epoch 0 batch 407 with loss 1.7874794006347656, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 408 with loss 1.7372617721557617, accuracy 0.4226190447807312.\n",
      "Training epoch 0 batch 409 with loss 1.810351014137268, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 410 with loss 1.7376188039779663, accuracy 0.347222238779068.\n",
      "Training epoch 0 batch 411 with loss 1.9185819625854492, accuracy 0.09880952537059784.\n",
      "Training epoch 0 batch 412 with loss 1.8836301565170288, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 413 with loss 1.806488275527954, accuracy 0.13611111044883728.\n",
      "Training epoch 0 batch 414 with loss 1.8266303539276123, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 415 with loss 1.7449411153793335, accuracy 0.23333334922790527.\n",
      "Training epoch 0 batch 416 with loss 1.8979841470718384, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 417 with loss 1.8111391067504883, accuracy 0.3194444477558136.\n",
      "Training epoch 0 batch 418 with loss 1.741250991821289, accuracy 0.2888889014720917.\n",
      "Training epoch 0 batch 419 with loss 1.7731736898422241, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 420 with loss 1.7309789657592773, accuracy 0.408730149269104.\n",
      "Training epoch 0 batch 421 with loss 1.772228479385376, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 422 with loss 1.7368295192718506, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 423 with loss 1.821284294128418, accuracy 0.1626984179019928.\n",
      "Training epoch 0 batch 424 with loss 1.8165686130523682, accuracy 0.2805555760860443.\n",
      "Training epoch 0 batch 425 with loss 1.8146858215332031, accuracy 0.10000000894069672.\n",
      "Training epoch 0 batch 426 with loss 1.8096389770507812, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 427 with loss 1.7698838710784912, accuracy 0.22777777910232544.\n",
      "Training epoch 0 batch 428 with loss 1.789412260055542, accuracy 0.33888891339302063.\n",
      "Training epoch 0 batch 429 with loss 1.8273048400878906, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 430 with loss 1.7532167434692383, accuracy 0.236111119389534.\n",
      "Training epoch 0 batch 431 with loss 1.757474660873413, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 432 with loss 1.7739540338516235, accuracy 0.3611111342906952.\n",
      "Training epoch 0 batch 433 with loss 1.7097795009613037, accuracy 0.2666666805744171.\n",
      "Training epoch 0 batch 434 with loss 1.7827380895614624, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 435 with loss 1.8477102518081665, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 436 with loss 1.7643804550170898, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 437 with loss 1.7679866552352905, accuracy 0.26944443583488464.\n",
      "Training epoch 0 batch 438 with loss 1.8530124425888062, accuracy 0.10833333432674408.\n",
      "Training epoch 0 batch 439 with loss 1.7868156433105469, accuracy 0.14166666567325592.\n",
      "Training epoch 0 batch 440 with loss 1.8058799505233765, accuracy 0.20000000298023224.\n",
      "Training epoch 0 batch 441 with loss 1.7578303813934326, accuracy 0.15555556118488312.\n",
      "Training epoch 0 batch 442 with loss 1.8175327777862549, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 443 with loss 1.751582384109497, accuracy 0.2361111044883728.\n",
      "Training epoch 0 batch 444 with loss 1.8209950923919678, accuracy 0.25555557012557983.\n",
      "Training epoch 0 batch 445 with loss 1.8012971878051758, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 446 with loss 1.791834831237793, accuracy 0.20000000298023224.\n",
      "Training epoch 0 batch 447 with loss 1.7769782543182373, accuracy 0.34583336114883423.\n",
      "Training epoch 0 batch 448 with loss 1.7893234491348267, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 449 with loss 1.781546950340271, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 450 with loss 1.8292871713638306, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 451 with loss 1.8666260242462158, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 452 with loss 1.7454551458358765, accuracy 0.1031746044754982.\n",
      "Training epoch 0 batch 453 with loss 1.7679980993270874, accuracy 0.1547619104385376.\n",
      "Training epoch 0 batch 454 with loss 1.798871397972107, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 455 with loss 1.8593244552612305, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 456 with loss 1.8511260747909546, accuracy 0.30158731341362.\n",
      "Training epoch 0 batch 457 with loss 1.7781951427459717, accuracy 0.4642857313156128.\n",
      "Training epoch 0 batch 458 with loss 1.8220069408416748, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 459 with loss 1.867595911026001, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 460 with loss 1.7599916458129883, accuracy 0.2777777910232544.\n",
      "Training epoch 0 batch 461 with loss 1.8634685277938843, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 462 with loss 1.8012332916259766, accuracy 0.3611111044883728.\n",
      "Training epoch 0 batch 463 with loss 1.783407211303711, accuracy 0.125.\n",
      "Training epoch 0 batch 464 with loss 1.8030964136123657, accuracy 0.125.\n",
      "Training epoch 0 batch 465 with loss 1.8179517984390259, accuracy 0.1031746044754982.\n",
      "Training epoch 0 batch 466 with loss 1.8553683757781982, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 467 with loss 1.8272632360458374, accuracy 0.09444444626569748.\n",
      "Training epoch 0 batch 468 with loss 1.7698068618774414, accuracy 0.28333336114883423.\n",
      "Training epoch 0 batch 469 with loss 1.892245888710022, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 470 with loss 1.8375561237335205, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 471 with loss 1.8294366598129272, accuracy 0.09047619253396988.\n",
      "Training epoch 0 batch 472 with loss 1.862352728843689, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 473 with loss 1.7814468145370483, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 474 with loss 1.763525366783142, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 475 with loss 1.75161612033844, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 476 with loss 1.8593307733535767, accuracy 0.0.\n",
      "Training epoch 0 batch 477 with loss 1.8561935424804688, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 478 with loss 1.851824164390564, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 479 with loss 1.7930355072021484, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 480 with loss 1.812042474746704, accuracy 0.065476194024086.\n",
      "Training epoch 0 batch 481 with loss 1.7580137252807617, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 482 with loss 1.7903941869735718, accuracy 0.2361111044883728.\n",
      "Training epoch 0 batch 483 with loss 1.7805302143096924, accuracy 0.125.\n",
      "Training epoch 0 batch 484 with loss 1.7974164485931396, accuracy 0.31111112236976624.\n",
      "Training epoch 0 batch 485 with loss 1.8431918621063232, accuracy 0.25.\n",
      "Training epoch 0 batch 486 with loss 1.8271108865737915, accuracy 0.1071428582072258.\n",
      "Training epoch 0 batch 487 with loss 1.8367588520050049, accuracy 0.1031746044754982.\n",
      "Training epoch 0 batch 488 with loss 1.8251960277557373, accuracy 0.14047619700431824.\n",
      "Training epoch 0 batch 489 with loss 1.8518590927124023, accuracy 0.0476190485060215.\n",
      "Training epoch 0 batch 490 with loss 1.7929013967514038, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 491 with loss 1.7750184535980225, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 492 with loss 1.8338991403579712, accuracy 0.2638888955116272.\n",
      "Training epoch 0 batch 493 with loss 1.7814048528671265, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 494 with loss 1.8252254724502563, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 495 with loss 1.7872002124786377, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 496 with loss 1.7613540887832642, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 497 with loss 1.8351844549179077, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 498 with loss 1.8378522396087646, accuracy 0.0.\n",
      "Training epoch 0 batch 499 with loss 1.8811115026474, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 500 with loss 1.8709388971328735, accuracy 0.0.\n",
      "Training epoch 0 batch 501 with loss 1.8563690185546875, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 502 with loss 1.7687351703643799, accuracy 0.12222222983837128.\n",
      "Training epoch 0 batch 503 with loss 1.8491655588150024, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 504 with loss 1.783334732055664, accuracy 0.1269841343164444.\n",
      "Training epoch 0 batch 505 with loss 1.7842490673065186, accuracy 0.21388889849185944.\n",
      "Training epoch 0 batch 506 with loss 1.7476139068603516, accuracy 0.4555555582046509.\n",
      "Training epoch 0 batch 507 with loss 1.8471386432647705, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 508 with loss 1.8347498178482056, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 509 with loss 1.9053453207015991, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 510 with loss 1.7875525951385498, accuracy 0.130952388048172.\n",
      "Training epoch 0 batch 511 with loss 1.8172309398651123, accuracy 0.2083333283662796.\n",
      "Training epoch 0 batch 512 with loss 1.7272363901138306, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 513 with loss 1.785627007484436, accuracy 0.10833333432674408.\n",
      "Training epoch 0 batch 514 with loss 1.7744299173355103, accuracy 0.0515873022377491.\n",
      "Training epoch 0 batch 515 with loss 1.798993468284607, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 516 with loss 1.7935107946395874, accuracy 0.02380952425301075.\n",
      "Training epoch 0 batch 517 with loss 1.7473094463348389, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 518 with loss 1.8313671350479126, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 519 with loss 1.8346967697143555, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 520 with loss 1.7860052585601807, accuracy 0.20000000298023224.\n",
      "Training epoch 0 batch 521 with loss 1.8015130758285522, accuracy 0.3194444477558136.\n",
      "Training epoch 0 batch 522 with loss 1.7502241134643555, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 523 with loss 1.754517912864685, accuracy 0.22777777910232544.\n",
      "Training epoch 0 batch 524 with loss 1.8096708059310913, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 525 with loss 1.7623977661132812, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 526 with loss 1.9065825939178467, accuracy 0.125.\n",
      "Training epoch 0 batch 527 with loss 1.811851143836975, accuracy 0.15833334624767303.\n",
      "Training epoch 0 batch 528 with loss 1.842222809791565, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 529 with loss 1.8459984064102173, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 530 with loss 1.7476097345352173, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 531 with loss 1.8152105808258057, accuracy 0.1805555671453476.\n",
      "Training epoch 0 batch 532 with loss 1.8717739582061768, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 533 with loss 1.826276183128357, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 534 with loss 1.7801592350006104, accuracy 0.125.\n",
      "Training epoch 0 batch 535 with loss 1.776065468788147, accuracy 0.21944445371627808.\n",
      "Training epoch 0 batch 536 with loss 1.81302011013031, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 537 with loss 1.8082830905914307, accuracy 0.22777777910232544.\n",
      "Training epoch 0 batch 538 with loss 1.8070310354232788, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 539 with loss 1.8017089366912842, accuracy 0.16388890147209167.\n",
      "Training epoch 0 batch 540 with loss 1.8367809057235718, accuracy 0.21944445371627808.\n",
      "Training epoch 0 batch 541 with loss 1.7732270956039429, accuracy 0.1349206417798996.\n",
      "Training epoch 0 batch 542 with loss 1.7685678005218506, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 543 with loss 1.745466947555542, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 544 with loss 1.7923460006713867, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 545 with loss 1.8935257196426392, accuracy 0.0793650820851326.\n",
      "Training epoch 0 batch 546 with loss 1.7829818725585938, accuracy 0.4000000059604645.\n",
      "Training epoch 0 batch 547 with loss 1.881691575050354, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 548 with loss 1.7083781957626343, accuracy 0.4305555820465088.\n",
      "Training epoch 0 batch 549 with loss 1.7695953845977783, accuracy 0.4027777910232544.\n",
      "Training epoch 0 batch 550 with loss 1.888840675354004, accuracy 0.1805555671453476.\n",
      "Training epoch 0 batch 551 with loss 1.863003134727478, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 552 with loss 1.7864078283309937, accuracy 0.0992063507437706.\n",
      "Training epoch 0 batch 553 with loss 1.762830376625061, accuracy 0.20000001788139343.\n",
      "Training epoch 0 batch 554 with loss 1.8366594314575195, accuracy 0.3194444477558136.\n",
      "Training epoch 0 batch 555 with loss 1.7747882604599, accuracy 0.16428571939468384.\n",
      "Training epoch 0 batch 556 with loss 1.7704546451568604, accuracy 0.25555557012557983.\n",
      "Training epoch 0 batch 557 with loss 1.8517948389053345, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 558 with loss 1.798771619796753, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 559 with loss 1.880007028579712, accuracy 0.02083333395421505.\n",
      "Training epoch 0 batch 560 with loss 1.832641363143921, accuracy 0.2777777910232544.\n",
      "Training epoch 0 batch 561 with loss 1.8101011514663696, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 562 with loss 1.8443177938461304, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 563 with loss 1.7842891216278076, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 564 with loss 1.8403984308242798, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 565 with loss 1.8160772323608398, accuracy 0.15833333134651184.\n",
      "Training epoch 0 batch 566 with loss 1.785416841506958, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 567 with loss 1.792011022567749, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 568 with loss 1.8437178134918213, accuracy 0.065476194024086.\n",
      "Training epoch 0 batch 569 with loss 1.8162769079208374, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 570 with loss 1.8200029134750366, accuracy 0.125.\n",
      "Training epoch 0 batch 571 with loss 1.8430248498916626, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 572 with loss 1.8199214935302734, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 573 with loss 1.7903445959091187, accuracy 0.15833333134651184.\n",
      "Training epoch 0 batch 574 with loss 1.850293755531311, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 575 with loss 1.7963529825210571, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 576 with loss 1.8043241500854492, accuracy 0.05714286118745804.\n",
      "Training epoch 0 batch 577 with loss 1.789525032043457, accuracy 0.2809523940086365.\n",
      "Training epoch 0 batch 578 with loss 1.781961441040039, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 579 with loss 1.8380439281463623, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 580 with loss 1.7957597970962524, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 581 with loss 1.7860921621322632, accuracy 0.2638888955116272.\n",
      "Training epoch 0 batch 582 with loss 1.7741382122039795, accuracy 0.2797619104385376.\n",
      "Training epoch 0 batch 583 with loss 1.8633270263671875, accuracy 0.0793650820851326.\n",
      "Training epoch 0 batch 584 with loss 1.8380159139633179, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 585 with loss 1.932173490524292, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 586 with loss 1.8409416675567627, accuracy 0.222222238779068.\n",
      "Training epoch 0 batch 587 with loss 1.7824089527130127, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 588 with loss 1.7945823669433594, accuracy 0.2083333283662796.\n",
      "Training epoch 0 batch 589 with loss 1.8444483280181885, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 590 with loss 1.8188260793685913, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 591 with loss 1.8020706176757812, accuracy 0.19305557012557983.\n",
      "Training epoch 0 batch 592 with loss 1.7995325326919556, accuracy 0.3333333432674408.\n",
      "Training epoch 0 batch 593 with loss 1.8203470706939697, accuracy 0.1597222238779068.\n",
      "Training epoch 0 batch 594 with loss 1.8046071529388428, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 595 with loss 1.7633222341537476, accuracy 0.3333333432674408.\n",
      "Training epoch 0 batch 596 with loss 1.809737205505371, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 597 with loss 1.7585694789886475, accuracy 0.23333334922790527.\n",
      "Training epoch 0 batch 598 with loss 1.7698614597320557, accuracy 0.03703703731298447.\n",
      "Training epoch 0 batch 599 with loss 1.8810478448867798, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 600 with loss 1.7975279092788696, accuracy 0.144841268658638.\n",
      "Training epoch 0 batch 601 with loss 1.7977502346038818, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 602 with loss 1.7613980770111084, accuracy 0.2888889014720917.\n",
      "Training epoch 0 batch 603 with loss 1.8190122842788696, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 604 with loss 1.808774709701538, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 605 with loss 1.8192636966705322, accuracy 0.10833333432674408.\n",
      "Training epoch 0 batch 606 with loss 1.8216612339019775, accuracy 0.19722221791744232.\n",
      "Training epoch 0 batch 607 with loss 1.7335565090179443, accuracy 0.2013888955116272.\n",
      "Training epoch 0 batch 608 with loss 1.771155595779419, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 609 with loss 1.8643944263458252, accuracy 0.0694444477558136.\n",
      "Training epoch 0 batch 610 with loss 1.865451455116272, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 611 with loss 1.8111690282821655, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 612 with loss 1.761805772781372, accuracy 0.27222222089767456.\n",
      "Training epoch 0 batch 613 with loss 1.7843759059906006, accuracy 0.2777777910232544.\n",
      "Training epoch 0 batch 614 with loss 1.8671098947525024, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 615 with loss 1.7725064754486084, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 616 with loss 1.8184118270874023, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 617 with loss 1.834916353225708, accuracy 0.19603174924850464.\n",
      "Training epoch 0 batch 618 with loss 1.8227440118789673, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 619 with loss 1.8886295557022095, accuracy 0.05416666716337204.\n",
      "Training epoch 0 batch 620 with loss 1.7924606800079346, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 621 with loss 1.7772451639175415, accuracy 0.0714285746216774.\n",
      "Training epoch 0 batch 622 with loss 1.8291151523590088, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 623 with loss 1.8219763040542603, accuracy 0.12222222983837128.\n",
      "Training epoch 0 batch 624 with loss 1.7593990564346313, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 625 with loss 1.869213342666626, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 626 with loss 1.844038724899292, accuracy 0.0.\n",
      "Training epoch 0 batch 627 with loss 1.8036750555038452, accuracy 0.19166666269302368.\n",
      "Training epoch 0 batch 628 with loss 1.829129934310913, accuracy 0.0694444477558136.\n",
      "Training epoch 0 batch 629 with loss 1.7884094715118408, accuracy 0.28333336114883423.\n",
      "Training epoch 0 batch 630 with loss 1.7059240341186523, accuracy 0.3492063581943512.\n",
      "Training epoch 0 batch 631 with loss 1.7733428478240967, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 632 with loss 1.8167190551757812, accuracy 0.02380952425301075.\n",
      "Training epoch 0 batch 633 with loss 1.8419620990753174, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 634 with loss 1.7009881734848022, accuracy 0.23888888955116272.\n",
      "Training epoch 0 batch 635 with loss 1.7734664678573608, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 636 with loss 1.7953563928604126, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 637 with loss 1.8369314670562744, accuracy 0.06111111491918564.\n",
      "Training epoch 0 batch 638 with loss 1.8265222311019897, accuracy 0.18333333730697632.\n",
      "Training epoch 0 batch 639 with loss 1.792445421218872, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 640 with loss 1.8165696859359741, accuracy 0.1865079402923584.\n",
      "Training epoch 0 batch 641 with loss 1.840924859046936, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 642 with loss 1.7969110012054443, accuracy 0.2182539701461792.\n",
      "Training epoch 0 batch 643 with loss 1.7455867528915405, accuracy 0.3333333432674408.\n",
      "Training epoch 0 batch 644 with loss 1.8252918720245361, accuracy 0.15833334624767303.\n",
      "Training epoch 0 batch 645 with loss 1.8480157852172852, accuracy 0.2083333283662796.\n",
      "Training epoch 0 batch 646 with loss 1.8061583042144775, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 647 with loss 1.7997066974639893, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 648 with loss 1.8531780242919922, accuracy 0.125.\n",
      "Training epoch 0 batch 649 with loss 1.7576669454574585, accuracy 0.2750000059604645.\n",
      "Training epoch 0 batch 650 with loss 1.8104438781738281, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 651 with loss 1.834808111190796, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 652 with loss 1.8024587631225586, accuracy 0.2916666567325592.\n",
      "Training epoch 0 batch 653 with loss 1.7885316610336304, accuracy 0.15833333134651184.\n",
      "Training epoch 0 batch 654 with loss 1.7946265935897827, accuracy 0.13333334028720856.\n",
      "Training epoch 0 batch 655 with loss 1.83917236328125, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 656 with loss 1.9151897430419922, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 657 with loss 1.7603133916854858, accuracy 0.17658731341362.\n",
      "Training epoch 0 batch 658 with loss 1.8542578220367432, accuracy 0.31111112236976624.\n",
      "Training epoch 0 batch 659 with loss 1.7766971588134766, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 660 with loss 1.7689651250839233, accuracy 0.13809524476528168.\n",
      "Training epoch 0 batch 661 with loss 1.8089584112167358, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 662 with loss 1.8480151891708374, accuracy 0.06111111491918564.\n",
      "Training epoch 0 batch 663 with loss 1.7742693424224854, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 664 with loss 1.8479814529418945, accuracy 0.1626984179019928.\n",
      "Training epoch 0 batch 665 with loss 1.8129533529281616, accuracy 0.222222238779068.\n",
      "Training epoch 0 batch 666 with loss 1.8109709024429321, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 667 with loss 1.7528760433197021, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 668 with loss 1.747580885887146, accuracy 0.16388890147209167.\n",
      "Training epoch 0 batch 669 with loss 1.7710660696029663, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 670 with loss 1.726787805557251, accuracy 0.3777777850627899.\n",
      "Training epoch 0 batch 671 with loss 1.8565126657485962, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 672 with loss 1.8446353673934937, accuracy 0.26851850748062134.\n",
      "Training epoch 0 batch 673 with loss 1.8744678497314453, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 674 with loss 1.7486894130706787, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 675 with loss 1.7000707387924194, accuracy 0.319444477558136.\n",
      "Training epoch 0 batch 676 with loss 1.8389383554458618, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 677 with loss 1.8493444919586182, accuracy 0.125.\n",
      "Training epoch 0 batch 678 with loss 1.780451774597168, accuracy 0.1805555671453476.\n",
      "Training epoch 0 batch 679 with loss 1.8021163940429688, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 680 with loss 1.776521921157837, accuracy 0.125.\n",
      "Training epoch 0 batch 681 with loss 1.8182251453399658, accuracy 0.20000000298023224.\n",
      "Training epoch 0 batch 682 with loss 1.768681526184082, accuracy 0.20555555820465088.\n",
      "Training epoch 0 batch 683 with loss 1.843971610069275, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 684 with loss 1.8960342407226562, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 685 with loss 1.7659683227539062, accuracy 0.2916666567325592.\n",
      "Training epoch 0 batch 686 with loss 1.8542566299438477, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 687 with loss 1.8211357593536377, accuracy 0.18333333730697632.\n",
      "Training epoch 0 batch 688 with loss 1.839859962463379, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 689 with loss 1.823359727859497, accuracy 0.1349206417798996.\n",
      "Training epoch 0 batch 690 with loss 1.7856853008270264, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 691 with loss 1.7735071182250977, accuracy 0.2341269850730896.\n",
      "Training epoch 0 batch 692 with loss 1.7778003215789795, accuracy 0.25.\n",
      "Training epoch 0 batch 693 with loss 1.8131706714630127, accuracy 0.09259259700775146.\n",
      "Training epoch 0 batch 694 with loss 1.8269309997558594, accuracy 0.190476194024086.\n",
      "Training epoch 0 batch 695 with loss 1.7594658136367798, accuracy 0.18611112236976624.\n",
      "Training epoch 0 batch 696 with loss 1.8175296783447266, accuracy 0.15833333134651184.\n",
      "Training epoch 0 batch 697 with loss 1.8104794025421143, accuracy 0.19166666269302368.\n",
      "Training epoch 0 batch 698 with loss 1.8672033548355103, accuracy 0.0.\n",
      "Training epoch 0 batch 699 with loss 1.8028570413589478, accuracy 0.1458333283662796.\n",
      "Training epoch 0 batch 700 with loss 1.824695348739624, accuracy 0.17142857611179352.\n",
      "Training epoch 0 batch 701 with loss 1.8329346179962158, accuracy 0.02380952425301075.\n",
      "Training epoch 0 batch 702 with loss 1.840644121170044, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 703 with loss 1.718285322189331, accuracy 0.3365079462528229.\n",
      "Training epoch 0 batch 704 with loss 1.8528985977172852, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 705 with loss 1.8335167169570923, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 706 with loss 1.8578563928604126, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 707 with loss 1.7677786350250244, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 708 with loss 1.8375288248062134, accuracy 0.222222238779068.\n",
      "Training epoch 0 batch 709 with loss 1.7887042760849, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 710 with loss 1.8088032007217407, accuracy 0.25.\n",
      "Training epoch 0 batch 711 with loss 1.8459742069244385, accuracy 0.15833334624767303.\n",
      "Training epoch 0 batch 712 with loss 1.7486528158187866, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 713 with loss 1.815988302230835, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 714 with loss 1.9130618572235107, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 715 with loss 1.8243745565414429, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 716 with loss 1.9059867858886719, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 717 with loss 1.8608672618865967, accuracy 0.05833333730697632.\n",
      "Training epoch 0 batch 718 with loss 1.755904197692871, accuracy 0.2361111044883728.\n",
      "Training epoch 0 batch 719 with loss 1.7926356792449951, accuracy 0.11269841343164444.\n",
      "Training epoch 0 batch 720 with loss 1.827411413192749, accuracy 0.17777778208255768.\n",
      "Training epoch 0 batch 721 with loss 1.775743842124939, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 722 with loss 1.874637246131897, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 723 with loss 1.888615608215332, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 724 with loss 1.8184807300567627, accuracy 0.0.\n",
      "Training epoch 0 batch 725 with loss 1.8388007879257202, accuracy 0.20555555820465088.\n",
      "Training epoch 0 batch 726 with loss 1.7367957830429077, accuracy 0.23888888955116272.\n",
      "Training epoch 0 batch 727 with loss 1.8172438144683838, accuracy 0.0.\n",
      "Training epoch 0 batch 728 with loss 1.8258663415908813, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 729 with loss 1.721683144569397, accuracy 0.375.\n",
      "Training epoch 0 batch 730 with loss 1.7450670003890991, accuracy 0.26944443583488464.\n",
      "Training epoch 0 batch 731 with loss 1.8240175247192383, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 732 with loss 1.836594581604004, accuracy 0.3611111342906952.\n",
      "Training epoch 0 batch 733 with loss 1.8257211446762085, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 734 with loss 1.8239004611968994, accuracy 0.10833333432674408.\n",
      "Training epoch 0 batch 735 with loss 1.8366725444793701, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 736 with loss 1.7733691930770874, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 737 with loss 1.7724412679672241, accuracy 0.16388890147209167.\n",
      "Training epoch 0 batch 738 with loss 1.7890136241912842, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 739 with loss 1.7540903091430664, accuracy 0.1899999976158142.\n",
      "Training epoch 0 batch 740 with loss 1.774806261062622, accuracy 0.08095238357782364.\n",
      "Training epoch 0 batch 741 with loss 1.8536593914031982, accuracy 0.10476191341876984.\n",
      "Training epoch 0 batch 742 with loss 1.8235180377960205, accuracy 0.130952388048172.\n",
      "Training epoch 0 batch 743 with loss 1.8112132549285889, accuracy 0.3154761791229248.\n",
      "Training epoch 0 batch 744 with loss 1.819978952407837, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 745 with loss 1.8026046752929688, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 746 with loss 1.8374710083007812, accuracy 0.25555557012557983.\n",
      "Training epoch 0 batch 747 with loss 1.8553320169448853, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 748 with loss 1.800727128982544, accuracy 0.1527777910232544.\n",
      "Training epoch 0 batch 749 with loss 1.8519700765609741, accuracy 0.125.\n",
      "Training epoch 0 batch 750 with loss 1.8560816049575806, accuracy 0.22777777910232544.\n",
      "Training epoch 0 batch 751 with loss 1.8172906637191772, accuracy 0.18518519401550293.\n",
      "Training epoch 0 batch 752 with loss 1.8371635675430298, accuracy 0.17777778208255768.\n",
      "Training epoch 0 batch 753 with loss 1.8910824060440063, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 754 with loss 1.7671403884887695, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 755 with loss 1.8289234638214111, accuracy 0.07407407462596893.\n",
      "Training epoch 0 batch 756 with loss 1.7972028255462646, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 757 with loss 1.8068307638168335, accuracy 0.0694444477558136.\n",
      "Training epoch 0 batch 758 with loss 1.8446424007415771, accuracy 0.10277777910232544.\n",
      "Training epoch 0 batch 759 with loss 1.825487732887268, accuracy 0.20555555820465088.\n",
      "Training epoch 0 batch 760 with loss 1.7334187030792236, accuracy 0.25.\n",
      "Training epoch 0 batch 761 with loss 1.822353720664978, accuracy 0.19166666269302368.\n",
      "Training epoch 0 batch 762 with loss 1.7712481021881104, accuracy 0.125.\n",
      "Training epoch 0 batch 763 with loss 1.7551982402801514, accuracy 0.17777778208255768.\n",
      "Training epoch 0 batch 764 with loss 1.7583520412445068, accuracy 0.18888890743255615.\n",
      "Training epoch 0 batch 765 with loss 1.7824453115463257, accuracy 0.23888888955116272.\n",
      "Training epoch 0 batch 766 with loss 1.779732346534729, accuracy 0.125.\n",
      "Training epoch 0 batch 767 with loss 1.8295879364013672, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 768 with loss 1.8077516555786133, accuracy 0.21388888359069824.\n",
      "Training epoch 0 batch 769 with loss 1.7856369018554688, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 770 with loss 1.7876707315444946, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 771 with loss 1.7985475063323975, accuracy 0.3222222328186035.\n",
      "Training epoch 0 batch 772 with loss 1.867023229598999, accuracy 0.0.\n",
      "Training epoch 0 batch 773 with loss 1.7924789190292358, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 774 with loss 1.741429328918457, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 775 with loss 1.8030818700790405, accuracy 0.17499999701976776.\n",
      "Training epoch 0 batch 776 with loss 1.8242114782333374, accuracy 0.15555556118488312.\n",
      "Training epoch 0 batch 777 with loss 1.792372465133667, accuracy 0.15555556118488312.\n",
      "Training epoch 0 batch 778 with loss 1.873272180557251, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 779 with loss 1.7995309829711914, accuracy 0.09259259700775146.\n",
      "Training epoch 0 batch 780 with loss 1.7588409185409546, accuracy 0.2611111104488373.\n",
      "Training epoch 0 batch 781 with loss 1.8071372509002686, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 782 with loss 1.7855209112167358, accuracy 0.25.\n",
      "Training epoch 0 batch 783 with loss 1.7808740139007568, accuracy 0.2888889014720917.\n",
      "Training epoch 0 batch 784 with loss 1.8688089847564697, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 785 with loss 1.872939109802246, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 786 with loss 1.8879587650299072, accuracy 0.125.\n",
      "Training epoch 0 batch 787 with loss 1.767481803894043, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 788 with loss 1.7713453769683838, accuracy 0.25555557012557983.\n",
      "Training epoch 0 batch 789 with loss 1.7712678909301758, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 790 with loss 1.7229562997817993, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 791 with loss 1.7699867486953735, accuracy 0.111111119389534.\n",
      "Training epoch 0 batch 792 with loss 1.8529021739959717, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 793 with loss 1.8296455144882202, accuracy 0.125.\n",
      "Training epoch 0 batch 794 with loss 1.7348235845565796, accuracy 0.42222222685813904.\n",
      "Training epoch 0 batch 795 with loss 1.7781648635864258, accuracy 0.18333333730697632.\n",
      "Training epoch 0 batch 796 with loss 1.831642746925354, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 797 with loss 1.8515942096710205, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 798 with loss 1.8352735042572021, accuracy 0.18333333730697632.\n",
      "Training epoch 0 batch 799 with loss 1.7624785900115967, accuracy 0.29722222685813904.\n",
      "Training epoch 0 batch 800 with loss 1.7206261157989502, accuracy 0.22499999403953552.\n",
      "Training epoch 0 batch 801 with loss 1.8309097290039062, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 802 with loss 1.8144031763076782, accuracy 0.1071428582072258.\n",
      "Training epoch 0 batch 803 with loss 1.7563854455947876, accuracy 0.24166667461395264.\n",
      "Training epoch 0 batch 804 with loss 1.816164255142212, accuracy 0.21944443881511688.\n",
      "Training epoch 0 batch 805 with loss 1.8384764194488525, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 806 with loss 1.8148696422576904, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 807 with loss 1.7371904850006104, accuracy 0.222222238779068.\n",
      "Training epoch 0 batch 808 with loss 1.793666124343872, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 809 with loss 1.7876312732696533, accuracy 0.28333333134651184.\n",
      "Training epoch 0 batch 810 with loss 1.797243356704712, accuracy 0.11428572237491608.\n",
      "Training epoch 0 batch 811 with loss 1.8447093963623047, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 812 with loss 1.7896416187286377, accuracy 0.17777778208255768.\n",
      "Training epoch 0 batch 813 with loss 1.8263849020004272, accuracy 0.0793650820851326.\n",
      "Training epoch 0 batch 814 with loss 1.7752211093902588, accuracy 0.2777777910232544.\n",
      "Training epoch 0 batch 815 with loss 1.8150184154510498, accuracy 0.125.\n",
      "Training epoch 0 batch 816 with loss 1.8014466762542725, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 817 with loss 1.7960350513458252, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 818 with loss 1.8297207355499268, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 819 with loss 1.754128098487854, accuracy 0.02380952425301075.\n",
      "Training epoch 0 batch 820 with loss 1.798972725868225, accuracy 0.0694444477558136.\n",
      "Training epoch 0 batch 821 with loss 1.8620792627334595, accuracy 0.22500000894069672.\n",
      "Training epoch 0 batch 822 with loss 1.7416061162948608, accuracy 0.3055555522441864.\n",
      "Training epoch 0 batch 823 with loss 1.7981517314910889, accuracy 0.24722224473953247.\n",
      "Training epoch 0 batch 824 with loss 1.8445688486099243, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 825 with loss 1.8460056781768799, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 826 with loss 1.7648836374282837, accuracy 0.18888889253139496.\n",
      "Training epoch 0 batch 827 with loss 1.7379423379898071, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 828 with loss 1.7985016107559204, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 829 with loss 1.8712037801742554, accuracy 0.25555557012557983.\n",
      "Training epoch 0 batch 830 with loss 1.7820684909820557, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 831 with loss 1.7900069952011108, accuracy 0.13611111044883728.\n",
      "Training epoch 0 batch 832 with loss 1.8858652114868164, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 833 with loss 1.832480788230896, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 834 with loss 1.8081655502319336, accuracy 0.1527777910232544.\n",
      "Training epoch 0 batch 835 with loss 1.8655202388763428, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 836 with loss 1.8351539373397827, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 837 with loss 1.8104976415634155, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 838 with loss 1.8428747653961182, accuracy 0.20000000298023224.\n",
      "Training epoch 0 batch 839 with loss 1.7660125494003296, accuracy 0.1269841343164444.\n",
      "Training epoch 0 batch 840 with loss 1.77765691280365, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 841 with loss 1.7817094326019287, accuracy 0.2361111044883728.\n",
      "Training epoch 0 batch 842 with loss 1.7978357076644897, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 843 with loss 1.790729284286499, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 844 with loss 1.7930212020874023, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 845 with loss 1.8613779544830322, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 846 with loss 1.7563444375991821, accuracy 0.19206349551677704.\n",
      "Training epoch 0 batch 847 with loss 1.804039716720581, accuracy 0.25.\n",
      "Training epoch 0 batch 848 with loss 1.81089186668396, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 849 with loss 1.835376501083374, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 850 with loss 1.826671838760376, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 851 with loss 1.823748230934143, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 852 with loss 1.774496078491211, accuracy 0.222222238779068.\n",
      "Training epoch 0 batch 853 with loss 1.7756344079971313, accuracy 0.222222238779068.\n",
      "Training epoch 0 batch 854 with loss 1.8291349411010742, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 855 with loss 1.8898184299468994, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 856 with loss 1.793828010559082, accuracy 0.15833333134651184.\n",
      "Training epoch 0 batch 857 with loss 1.8174407482147217, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 858 with loss 1.8420522212982178, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 859 with loss 1.7449356317520142, accuracy 0.2133333384990692.\n",
      "Training epoch 0 batch 860 with loss 1.7806622982025146, accuracy 0.236111119389534.\n",
      "Training epoch 0 batch 861 with loss 1.8014118671417236, accuracy 0.1071428582072258.\n",
      "Training epoch 0 batch 862 with loss 1.7933673858642578, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 863 with loss 1.7400926351547241, accuracy 0.4583333730697632.\n",
      "Training epoch 0 batch 864 with loss 1.7918192148208618, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 865 with loss 1.7614328861236572, accuracy 0.2611111104488373.\n",
      "Training epoch 0 batch 866 with loss 1.8419069051742554, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 867 with loss 1.8869785070419312, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 868 with loss 1.7630094289779663, accuracy 0.26944443583488464.\n",
      "Training epoch 0 batch 869 with loss 1.7794582843780518, accuracy 0.25158730149269104.\n",
      "Training epoch 0 batch 870 with loss 1.8513844013214111, accuracy 0.236111119389534.\n",
      "Training epoch 0 batch 871 with loss 1.8051083087921143, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 872 with loss 1.8623838424682617, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 873 with loss 1.7921785116195679, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 874 with loss 1.8186664581298828, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 875 with loss 1.7625176906585693, accuracy 0.2944444417953491.\n",
      "Training epoch 0 batch 876 with loss 1.837527871131897, accuracy 0.02380952425301075.\n",
      "Training epoch 0 batch 877 with loss 1.8034242391586304, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 878 with loss 1.8317804336547852, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 879 with loss 1.810819387435913, accuracy 0.2698412835597992.\n",
      "Training epoch 0 batch 880 with loss 1.8236020803451538, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 881 with loss 1.855320692062378, accuracy 0.125.\n",
      "Training epoch 0 batch 882 with loss 1.842934012413025, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 883 with loss 1.7718045711517334, accuracy 0.21944445371627808.\n",
      "Training epoch 0 batch 884 with loss 1.8498519659042358, accuracy 0.065476194024086.\n",
      "Training epoch 0 batch 885 with loss 1.8304097652435303, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 886 with loss 1.811394453048706, accuracy 0.1587301641702652.\n",
      "Training epoch 0 batch 887 with loss 1.7978519201278687, accuracy 0.11666666716337204.\n",
      "Training epoch 0 batch 888 with loss 1.8207248449325562, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 889 with loss 1.74136221408844, accuracy 0.2638888955116272.\n",
      "Training epoch 0 batch 890 with loss 1.7678172588348389, accuracy 0.23888888955116272.\n",
      "Training epoch 0 batch 891 with loss 1.7446991205215454, accuracy 0.33888888359069824.\n",
      "Training epoch 0 batch 892 with loss 1.8052723407745361, accuracy 0.222222238779068.\n",
      "Training epoch 0 batch 893 with loss 1.8214296102523804, accuracy 0.15833334624767303.\n",
      "Training epoch 0 batch 894 with loss 1.8082424402236938, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 895 with loss 1.8165490627288818, accuracy 0.2083333283662796.\n",
      "Training epoch 0 batch 896 with loss 1.8304640054702759, accuracy 0.20555555820465088.\n",
      "Training epoch 0 batch 897 with loss 1.841217041015625, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 898 with loss 1.8038461208343506, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 899 with loss 1.8521531820297241, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 900 with loss 1.8085273504257202, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 901 with loss 1.8867371082305908, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 902 with loss 1.7943557500839233, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 903 with loss 1.8151874542236328, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 904 with loss 1.8251798152923584, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 905 with loss 1.8622033596038818, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 906 with loss 1.788193941116333, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 907 with loss 1.8906456232070923, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 908 with loss 1.7491028308868408, accuracy 0.24166667461395264.\n",
      "Training epoch 0 batch 909 with loss 1.732556939125061, accuracy 0.3027777671813965.\n",
      "Training epoch 0 batch 910 with loss 1.7762954235076904, accuracy 0.20000000298023224.\n",
      "Training epoch 0 batch 911 with loss 1.8411951065063477, accuracy 0.24166667461395264.\n",
      "Training epoch 0 batch 912 with loss 1.8232940435409546, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 913 with loss 1.8095648288726807, accuracy 0.2142857313156128.\n",
      "Training epoch 0 batch 914 with loss 1.8161245584487915, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 915 with loss 1.8017637729644775, accuracy 0.33888888359069824.\n",
      "Training epoch 0 batch 916 with loss 1.8128753900527954, accuracy 0.20000001788139343.\n",
      "Training epoch 0 batch 917 with loss 1.8080129623413086, accuracy 0.0694444477558136.\n",
      "Training epoch 0 batch 918 with loss 1.788773536682129, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 919 with loss 1.7490828037261963, accuracy 0.39444446563720703.\n",
      "Training epoch 0 batch 920 with loss 1.8049561977386475, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 921 with loss 1.7673193216323853, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 922 with loss 1.790264368057251, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 923 with loss 1.8460090160369873, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 924 with loss 1.8058841228485107, accuracy 0.24444445967674255.\n",
      "Training epoch 0 batch 925 with loss 1.8062458038330078, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 926 with loss 1.7884308099746704, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 927 with loss 1.7755804061889648, accuracy 0.2638888955116272.\n",
      "Training epoch 0 batch 928 with loss 1.7118269205093384, accuracy 0.4000000059604645.\n",
      "Training epoch 0 batch 929 with loss 1.7376604080200195, accuracy 0.2666666805744171.\n",
      "Training epoch 0 batch 930 with loss 1.9083328247070312, accuracy 0.0.\n",
      "Training epoch 0 batch 931 with loss 1.7342722415924072, accuracy 0.4027777910232544.\n",
      "Training epoch 0 batch 932 with loss 1.748821496963501, accuracy 0.16388888657093048.\n",
      "Training epoch 0 batch 933 with loss 1.7891960144042969, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 934 with loss 1.838422179222107, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 935 with loss 1.801888108253479, accuracy 0.21111111342906952.\n",
      "Training epoch 0 batch 936 with loss 1.7166025638580322, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 937 with loss 1.8740400075912476, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 938 with loss 1.828698754310608, accuracy 0.3055555522441864.\n",
      "Training epoch 0 batch 939 with loss 1.7559804916381836, accuracy 0.22777777910232544.\n",
      "Training epoch 0 batch 940 with loss 1.829064130783081, accuracy 0.0793650820851326.\n",
      "Training epoch 0 batch 941 with loss 1.7889823913574219, accuracy 0.1130952388048172.\n",
      "Training epoch 0 batch 942 with loss 1.7466026544570923, accuracy 0.2666666805744171.\n",
      "Training epoch 0 batch 943 with loss 1.8056132793426514, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 944 with loss 1.776949167251587, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 945 with loss 1.7856165170669556, accuracy 0.0892857164144516.\n",
      "Training epoch 0 batch 946 with loss 1.8736377954483032, accuracy 0.12222222983837128.\n",
      "Training epoch 0 batch 947 with loss 1.780034065246582, accuracy 0.10833333432674408.\n",
      "Training epoch 0 batch 948 with loss 1.7974815368652344, accuracy 0.25.\n",
      "Training epoch 0 batch 949 with loss 1.745619535446167, accuracy 0.34259262681007385.\n",
      "Training epoch 0 batch 950 with loss 1.8093820810317993, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 951 with loss 1.8806369304656982, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 952 with loss 1.7922611236572266, accuracy 0.222222238779068.\n",
      "Training epoch 0 batch 953 with loss 1.8142932653427124, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 954 with loss 1.8481636047363281, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 955 with loss 1.8361011743545532, accuracy 0.26111114025115967.\n",
      "Training epoch 0 batch 956 with loss 1.8289077281951904, accuracy 0.0.\n",
      "Training epoch 0 batch 957 with loss 1.853846788406372, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 958 with loss 1.8297197818756104, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 959 with loss 1.7552850246429443, accuracy 0.18611110746860504.\n",
      "Training epoch 0 batch 960 with loss 1.82118821144104, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 961 with loss 1.7584333419799805, accuracy 0.2638888955116272.\n",
      "Training epoch 0 batch 962 with loss 1.7544755935668945, accuracy 0.3861111104488373.\n",
      "Training epoch 0 batch 963 with loss 1.7905782461166382, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 964 with loss 1.799020767211914, accuracy 0.15833334624767303.\n",
      "Training epoch 0 batch 965 with loss 1.8059675693511963, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 966 with loss 1.816764235496521, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 967 with loss 1.7648966312408447, accuracy 0.23333334922790527.\n",
      "Training epoch 0 batch 968 with loss 1.7456400394439697, accuracy 0.2916666567325592.\n",
      "Training epoch 0 batch 969 with loss 1.794105887413025, accuracy 0.125.\n",
      "Training epoch 0 batch 970 with loss 1.7896239757537842, accuracy 0.125.\n",
      "Training epoch 0 batch 971 with loss 1.7945091724395752, accuracy 0.13611111044883728.\n",
      "Training epoch 0 batch 972 with loss 1.803518295288086, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 973 with loss 1.9242913722991943, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 974 with loss 1.8609997034072876, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 975 with loss 1.8152878284454346, accuracy 0.17777778208255768.\n",
      "Training epoch 0 batch 976 with loss 1.8604774475097656, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 977 with loss 1.813233733177185, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 978 with loss 1.7585090398788452, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 979 with loss 1.869466781616211, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 980 with loss 1.7884933948516846, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 981 with loss 1.7825037240982056, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 982 with loss 1.7801144123077393, accuracy 0.19365079700946808.\n",
      "Training epoch 0 batch 983 with loss 1.8380613327026367, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 984 with loss 1.882598638534546, accuracy 0.125.\n",
      "Training epoch 0 batch 985 with loss 1.7662270069122314, accuracy 0.20555555820465088.\n",
      "Training epoch 0 batch 986 with loss 1.8205350637435913, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 987 with loss 1.779914140701294, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 988 with loss 1.8254207372665405, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 989 with loss 1.7252511978149414, accuracy 0.2976190447807312.\n",
      "Training epoch 0 batch 990 with loss 1.8324973583221436, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 991 with loss 1.708380103111267, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 992 with loss 1.7731845378875732, accuracy 0.2361111044883728.\n",
      "Training epoch 0 batch 993 with loss 1.8402519226074219, accuracy 0.36666667461395264.\n",
      "Training epoch 0 batch 994 with loss 1.8249393701553345, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 995 with loss 1.8520698547363281, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 996 with loss 1.7440780401229858, accuracy 0.2976190447807312.\n",
      "Training epoch 0 batch 997 with loss 1.944650650024414, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 998 with loss 1.8297345638275146, accuracy 0.10000000894069672.\n",
      "Training epoch 0 batch 999 with loss 1.7601877450942993, accuracy 0.3472222089767456.\n",
      "Training epoch 0 batch 1000 with loss 1.8331987857818604, accuracy 0.2638888955116272.\n",
      "Training epoch 0 batch 1001 with loss 1.8128273487091064, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 1002 with loss 1.8343957662582397, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 1003 with loss 1.8722679615020752, accuracy 0.125.\n",
      "Training epoch 0 batch 1004 with loss 1.8347820043563843, accuracy 0.23333333432674408.\n",
      "Training epoch 0 batch 1005 with loss 1.8427053689956665, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 1006 with loss 1.8503811359405518, accuracy 0.1031746044754982.\n",
      "Training epoch 0 batch 1007 with loss 1.7773568630218506, accuracy 0.25.\n",
      "Training epoch 0 batch 1008 with loss 1.864911675453186, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 1009 with loss 1.7809959650039673, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 1010 with loss 1.7349252700805664, accuracy 0.2750000059604645.\n",
      "Training epoch 0 batch 1011 with loss 1.819750428199768, accuracy 0.111111119389534.\n",
      "Training epoch 0 batch 1012 with loss 1.7620553970336914, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 1013 with loss 1.839496374130249, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 1014 with loss 1.790355920791626, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 1015 with loss 1.8160431385040283, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 1016 with loss 1.7292072772979736, accuracy 0.3392857015132904.\n",
      "Training epoch 0 batch 1017 with loss 1.741634726524353, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 1018 with loss 1.811828851699829, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 1019 with loss 1.792001485824585, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 1020 with loss 1.8860162496566772, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 1021 with loss 1.7113879919052124, accuracy 0.3444444537162781.\n",
      "Training epoch 0 batch 1022 with loss 1.7659742832183838, accuracy 0.19166666269302368.\n",
      "Training epoch 0 batch 1023 with loss 1.8051340579986572, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 1024 with loss 1.8294575214385986, accuracy 0.20555555820465088.\n",
      "Training epoch 0 batch 1025 with loss 1.8765251636505127, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 1026 with loss 1.8259899616241455, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 1027 with loss 1.7234834432601929, accuracy 0.3055555522441864.\n",
      "Training epoch 0 batch 1028 with loss 1.7719438076019287, accuracy 0.0694444477558136.\n",
      "Training epoch 0 batch 1029 with loss 1.752164602279663, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1030 with loss 1.7382886409759521, accuracy 0.42222222685813904.\n",
      "Training epoch 0 batch 1031 with loss 1.7803685665130615, accuracy 0.16388890147209167.\n",
      "Training epoch 0 batch 1032 with loss 1.8216365575790405, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 1033 with loss 1.7864011526107788, accuracy 0.28333333134651184.\n",
      "Training epoch 0 batch 1034 with loss 1.833645224571228, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 1035 with loss 1.8827733993530273, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1036 with loss 1.826864242553711, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 1037 with loss 1.873687982559204, accuracy 0.16388888657093048.\n",
      "Training epoch 0 batch 1038 with loss 1.8453575372695923, accuracy 0.19722223281860352.\n",
      "Training epoch 0 batch 1039 with loss 1.845874547958374, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1040 with loss 1.8898341655731201, accuracy 0.2430555522441864.\n",
      "Training epoch 0 batch 1041 with loss 1.765458106994629, accuracy 0.0793650820851326.\n",
      "Training epoch 0 batch 1042 with loss 1.7558590173721313, accuracy 0.2291666716337204.\n",
      "Training epoch 0 batch 1043 with loss 1.8580154180526733, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 1044 with loss 1.7806974649429321, accuracy 0.3055555522441864.\n",
      "Training epoch 0 batch 1045 with loss 1.7956234216690063, accuracy 0.15833334624767303.\n",
      "Training epoch 0 batch 1046 with loss 1.835060715675354, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 1047 with loss 1.7825456857681274, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 1048 with loss 1.7489150762557983, accuracy 0.19722223281860352.\n",
      "Training epoch 0 batch 1049 with loss 1.8862974643707275, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1050 with loss 1.8389527797698975, accuracy 0.1686507910490036.\n",
      "Training epoch 0 batch 1051 with loss 1.7762807607650757, accuracy 0.28333333134651184.\n",
      "Training epoch 0 batch 1052 with loss 1.7533948421478271, accuracy 0.3611111044883728.\n",
      "Training epoch 0 batch 1053 with loss 1.8477327823638916, accuracy 0.2777777910232544.\n",
      "Training epoch 0 batch 1054 with loss 1.9178924560546875, accuracy 0.2291666716337204.\n",
      "Training epoch 0 batch 1055 with loss 1.803900957107544, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 1056 with loss 1.8048979043960571, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 1057 with loss 1.8131163120269775, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 1058 with loss 1.8208776712417603, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 1059 with loss 1.7356258630752563, accuracy 0.222222238779068.\n",
      "Training epoch 0 batch 1060 with loss 1.8554588556289673, accuracy 0.125.\n",
      "Training epoch 0 batch 1061 with loss 1.8389661312103271, accuracy 0.02380952425301075.\n",
      "Training epoch 0 batch 1062 with loss 1.8580522537231445, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1063 with loss 1.9227590560913086, accuracy 0.0.\n",
      "Training epoch 0 batch 1064 with loss 1.791330337524414, accuracy 0.11666666716337204.\n",
      "Training epoch 0 batch 1065 with loss 1.810850739479065, accuracy 0.10277777910232544.\n",
      "Training epoch 0 batch 1066 with loss 1.7325503826141357, accuracy 0.19722223281860352.\n",
      "Training epoch 0 batch 1067 with loss 1.831911325454712, accuracy 0.0.\n",
      "Training epoch 0 batch 1068 with loss 1.8175395727157593, accuracy 0.1133333295583725.\n",
      "Training epoch 0 batch 1069 with loss 1.8729499578475952, accuracy 0.20000000298023224.\n",
      "Training epoch 0 batch 1070 with loss 1.7814468145370483, accuracy 0.4444444477558136.\n",
      "Training epoch 0 batch 1071 with loss 1.8260027170181274, accuracy 0.0694444477558136.\n",
      "Training epoch 0 batch 1072 with loss 1.7798395156860352, accuracy 0.2083333283662796.\n",
      "Training epoch 0 batch 1073 with loss 1.8585678339004517, accuracy 0.1488095223903656.\n",
      "Training epoch 0 batch 1074 with loss 1.774446725845337, accuracy 0.277777761220932.\n",
      "Training epoch 0 batch 1075 with loss 1.765077829360962, accuracy 0.20000000298023224.\n",
      "Training epoch 0 batch 1076 with loss 1.7902768850326538, accuracy 0.236111119389534.\n",
      "Training epoch 0 batch 1077 with loss 1.7445383071899414, accuracy 0.21388889849185944.\n",
      "Training epoch 0 batch 1078 with loss 1.8004834651947021, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 1079 with loss 1.7769927978515625, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1080 with loss 1.854219675064087, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 1081 with loss 1.862027883529663, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 1082 with loss 1.7550264596939087, accuracy 0.2777777910232544.\n",
      "Training epoch 0 batch 1083 with loss 1.8273038864135742, accuracy 0.09259259700775146.\n",
      "Training epoch 0 batch 1084 with loss 1.8728389739990234, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 1085 with loss 1.7515805959701538, accuracy 0.4027778208255768.\n",
      "Training epoch 0 batch 1086 with loss 1.8516743183135986, accuracy 0.2708333432674408.\n",
      "Training epoch 0 batch 1087 with loss 1.8609535694122314, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1088 with loss 1.7921336889266968, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 1089 with loss 1.7895419597625732, accuracy 0.2638888955116272.\n",
      "Training epoch 0 batch 1090 with loss 1.7752043008804321, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 1091 with loss 1.7773854732513428, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1092 with loss 1.7571595907211304, accuracy 0.29722222685813904.\n",
      "Training epoch 0 batch 1093 with loss 1.8519489765167236, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 1094 with loss 1.8451484441757202, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 1095 with loss 1.8532111644744873, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 1096 with loss 1.7975702285766602, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 1097 with loss 1.7990391254425049, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 1098 with loss 1.8283878564834595, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 1099 with loss 1.7968813180923462, accuracy 0.222222238779068.\n",
      "Training epoch 0 batch 1100 with loss 1.8070122003555298, accuracy 0.12222222983837128.\n",
      "Training epoch 0 batch 1101 with loss 1.8003036975860596, accuracy 0.16388888657093048.\n",
      "Training epoch 0 batch 1102 with loss 1.7720184326171875, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 1103 with loss 1.7975409030914307, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 1104 with loss 1.7979161739349365, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 1105 with loss 1.7710444927215576, accuracy 0.22777777910232544.\n",
      "Training epoch 0 batch 1106 with loss 1.7954879999160767, accuracy 0.19722223281860352.\n",
      "Training epoch 0 batch 1107 with loss 1.770374059677124, accuracy 0.19722223281860352.\n",
      "Training epoch 0 batch 1108 with loss 1.83095383644104, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 1109 with loss 1.8895078897476196, accuracy 0.1071428582072258.\n",
      "Training epoch 0 batch 1110 with loss 1.8784713745117188, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 1111 with loss 1.7434593439102173, accuracy 0.17777778208255768.\n",
      "Training epoch 0 batch 1112 with loss 1.781604528427124, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 1113 with loss 1.7700204849243164, accuracy 0.19722223281860352.\n",
      "Training epoch 0 batch 1114 with loss 1.9255950450897217, accuracy 0.0.\n",
      "Training epoch 0 batch 1115 with loss 1.7499920129776, accuracy 0.255952388048172.\n",
      "Training epoch 0 batch 1116 with loss 1.8468959331512451, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 1117 with loss 1.8664398193359375, accuracy 0.17083334922790527.\n",
      "Training epoch 0 batch 1118 with loss 1.803802728652954, accuracy 0.18333333730697632.\n",
      "Training epoch 0 batch 1119 with loss 1.803481101989746, accuracy 0.190476194024086.\n",
      "Training epoch 0 batch 1120 with loss 1.829694390296936, accuracy 0.1805555671453476.\n",
      "Training epoch 0 batch 1121 with loss 1.7794177532196045, accuracy 0.13333334028720856.\n",
      "Training epoch 0 batch 1122 with loss 1.7898393869400024, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 1123 with loss 1.848698616027832, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 1124 with loss 1.7853376865386963, accuracy 0.16388890147209167.\n",
      "Training epoch 0 batch 1125 with loss 1.7957195043563843, accuracy 0.06111111491918564.\n",
      "Training epoch 0 batch 1126 with loss 1.8049967288970947, accuracy 0.17500001192092896.\n",
      "Training epoch 0 batch 1127 with loss 1.771634817123413, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 1128 with loss 1.7841358184814453, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 1129 with loss 1.870685338973999, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 1130 with loss 1.7897441387176514, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 1131 with loss 1.780591368675232, accuracy 0.1269841343164444.\n",
      "Training epoch 0 batch 1132 with loss 1.8693605661392212, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 1133 with loss 1.7732341289520264, accuracy 0.16388888657093048.\n",
      "Training epoch 0 batch 1134 with loss 1.83431077003479, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 1135 with loss 1.7475776672363281, accuracy 0.17777778208255768.\n",
      "Training epoch 0 batch 1136 with loss 1.8006350994110107, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 1137 with loss 1.8286454677581787, accuracy 0.0694444477558136.\n",
      "Training epoch 0 batch 1138 with loss 1.7981913089752197, accuracy 0.065476194024086.\n",
      "Training epoch 0 batch 1139 with loss 1.8577560186386108, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 1140 with loss 1.8094098567962646, accuracy 0.3333333432674408.\n",
      "Training epoch 0 batch 1141 with loss 1.794790506362915, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 1142 with loss 1.8284403085708618, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 1143 with loss 1.7915070056915283, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1144 with loss 1.8883211612701416, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 1145 with loss 1.7701208591461182, accuracy 0.2638888955116272.\n",
      "Training epoch 0 batch 1146 with loss 1.7650384902954102, accuracy 0.125.\n",
      "Training epoch 0 batch 1147 with loss 1.8523063659667969, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 1148 with loss 1.7854158878326416, accuracy 0.2611111104488373.\n",
      "Training epoch 0 batch 1149 with loss 1.832764983177185, accuracy 0.1805555671453476.\n",
      "Training epoch 0 batch 1150 with loss 1.8130178451538086, accuracy 0.0694444477558136.\n",
      "Training epoch 0 batch 1151 with loss 1.8267329931259155, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 1152 with loss 1.8212015628814697, accuracy 0.18518519401550293.\n",
      "Training epoch 0 batch 1153 with loss 1.758230447769165, accuracy 0.2916666865348816.\n",
      "Training epoch 0 batch 1154 with loss 1.8086074590682983, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 1155 with loss 1.7743936777114868, accuracy 0.1269841343164444.\n",
      "Training epoch 0 batch 1156 with loss 1.8681074380874634, accuracy 0.0.\n",
      "Training epoch 0 batch 1157 with loss 1.9291826486587524, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1158 with loss 1.798243522644043, accuracy 0.16388890147209167.\n",
      "Training epoch 0 batch 1159 with loss 1.810640573501587, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 1160 with loss 1.8279287815093994, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1161 with loss 1.8096109628677368, accuracy 0.2750000059604645.\n",
      "Training epoch 0 batch 1162 with loss 1.8437697887420654, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 1163 with loss 1.8666174411773682, accuracy 0.18000000715255737.\n",
      "Training epoch 0 batch 1164 with loss 1.7758735418319702, accuracy 0.31111112236976624.\n",
      "Training epoch 0 batch 1165 with loss 1.793077826499939, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1166 with loss 1.8444820642471313, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 1167 with loss 1.8025658130645752, accuracy 0.3472222089767456.\n",
      "Training epoch 0 batch 1168 with loss 1.7589969635009766, accuracy 0.2666666805744171.\n",
      "Training epoch 0 batch 1169 with loss 1.8355363607406616, accuracy 0.0.\n",
      "Training epoch 0 batch 1170 with loss 1.686687707901001, accuracy 0.3722222149372101.\n",
      "Training epoch 0 batch 1171 with loss 1.7835094928741455, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 1172 with loss 1.8390299081802368, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 1173 with loss 1.7988107204437256, accuracy 0.10000000894069672.\n",
      "Training epoch 0 batch 1174 with loss 1.8057098388671875, accuracy 0.20000000298023224.\n",
      "Training epoch 0 batch 1175 with loss 1.8374007940292358, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 1176 with loss 1.8427059650421143, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1177 with loss 1.8694978952407837, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 1178 with loss 1.8656399250030518, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 1179 with loss 1.7170312404632568, accuracy 0.329365074634552.\n",
      "Training epoch 0 batch 1180 with loss 1.7374025583267212, accuracy 0.34166669845581055.\n",
      "Training epoch 0 batch 1181 with loss 1.7997989654541016, accuracy 0.22777777910232544.\n",
      "Training epoch 0 batch 1182 with loss 1.744124412536621, accuracy 0.30000001192092896.\n",
      "Training epoch 0 batch 1183 with loss 1.8007166385650635, accuracy 0.07500000298023224.\n",
      "Training epoch 0 batch 1184 with loss 1.807247519493103, accuracy 0.45000001788139343.\n",
      "Training epoch 0 batch 1185 with loss 1.773851752281189, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 1186 with loss 1.7933413982391357, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 1187 with loss 1.8613855838775635, accuracy 0.14166668057441711.\n",
      "Training epoch 0 batch 1188 with loss 1.8163492679595947, accuracy 0.20000001788139343.\n",
      "Training epoch 0 batch 1189 with loss 1.816476583480835, accuracy 0.130952388048172.\n",
      "Training epoch 0 batch 1190 with loss 1.871045470237732, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1191 with loss 1.7760978937149048, accuracy 0.3055555522441864.\n",
      "Training epoch 0 batch 1192 with loss 1.8249881267547607, accuracy 0.10277777910232544.\n",
      "Training epoch 0 batch 1193 with loss 1.7824881076812744, accuracy 0.13333334028720856.\n",
      "Training epoch 0 batch 1194 with loss 1.8444945812225342, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 1195 with loss 1.7507362365722656, accuracy 0.2638888955116272.\n",
      "Training epoch 0 batch 1196 with loss 1.846747636795044, accuracy 0.18888889253139496.\n",
      "Training epoch 0 batch 1197 with loss 1.8316739797592163, accuracy 0.02380952425301075.\n",
      "Training epoch 0 batch 1198 with loss 1.876413345336914, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 1199 with loss 1.8156588077545166, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 1200 with loss 1.8338485956192017, accuracy 0.0793650820851326.\n",
      "Training epoch 0 batch 1201 with loss 1.817384123802185, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1202 with loss 1.8421785831451416, accuracy 0.2361111044883728.\n",
      "Training epoch 0 batch 1203 with loss 1.8575178384780884, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 1204 with loss 1.8642265796661377, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 1205 with loss 1.83944833278656, accuracy 0.15833334624767303.\n",
      "Training epoch 0 batch 1206 with loss 1.7627546787261963, accuracy 0.25357142090797424.\n",
      "Training epoch 0 batch 1207 with loss 1.8003383874893188, accuracy 0.2777777910232544.\n",
      "Training epoch 0 batch 1208 with loss 1.754783034324646, accuracy 0.4138889014720917.\n",
      "Training epoch 0 batch 1209 with loss 1.78720223903656, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1210 with loss 1.7846193313598633, accuracy 0.17976191639900208.\n",
      "Training epoch 0 batch 1211 with loss 1.8067715167999268, accuracy 0.18611112236976624.\n",
      "Training epoch 0 batch 1212 with loss 1.8303731679916382, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1213 with loss 1.7764695882797241, accuracy 0.33888891339302063.\n",
      "Training epoch 0 batch 1214 with loss 1.776383638381958, accuracy 0.2777777910232544.\n",
      "Training epoch 0 batch 1215 with loss 1.8045419454574585, accuracy 0.17777778208255768.\n",
      "Training epoch 0 batch 1216 with loss 1.8063783645629883, accuracy 0.15833333134651184.\n",
      "Training epoch 0 batch 1217 with loss 1.7675952911376953, accuracy 0.2777777910232544.\n",
      "Training epoch 0 batch 1218 with loss 1.7271220684051514, accuracy 0.35277777910232544.\n",
      "Training epoch 0 batch 1219 with loss 1.7770137786865234, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 1220 with loss 1.8186943531036377, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 1221 with loss 1.8099734783172607, accuracy 0.2611111104488373.\n",
      "Training epoch 0 batch 1222 with loss 1.7584874629974365, accuracy 0.2777777910232544.\n",
      "Training epoch 0 batch 1223 with loss 1.8725197315216064, accuracy 0.0.\n",
      "Training epoch 0 batch 1224 with loss 1.7374528646469116, accuracy 0.14166668057441711.\n",
      "Training epoch 0 batch 1225 with loss 1.7615089416503906, accuracy 0.18333333730697632.\n",
      "Training epoch 0 batch 1226 with loss 1.7553231716156006, accuracy 0.25555557012557983.\n",
      "Training epoch 0 batch 1227 with loss 1.8616011142730713, accuracy 0.0.\n",
      "Training epoch 0 batch 1228 with loss 1.8010690212249756, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 1229 with loss 1.7846238613128662, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 1230 with loss 1.789078712463379, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1231 with loss 1.778029441833496, accuracy 0.20555555820465088.\n",
      "Training epoch 0 batch 1232 with loss 1.7839893102645874, accuracy 0.17592592537403107.\n",
      "Training epoch 0 batch 1233 with loss 1.7715089321136475, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 1234 with loss 1.8610187768936157, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1235 with loss 1.817948579788208, accuracy 0.21666666865348816.\n",
      "Training epoch 0 batch 1236 with loss 1.8202917575836182, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 1237 with loss 1.8179304599761963, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1238 with loss 1.8701038360595703, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1239 with loss 1.869376540184021, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 1240 with loss 1.7925233840942383, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 1241 with loss 1.846448540687561, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 1242 with loss 1.74161696434021, accuracy 0.2698412835597992.\n",
      "Training epoch 0 batch 1243 with loss 1.7495014667510986, accuracy 0.3611111342906952.\n",
      "Training epoch 0 batch 1244 with loss 1.7778794765472412, accuracy 0.2916666865348816.\n",
      "Training epoch 0 batch 1245 with loss 1.782545804977417, accuracy 0.1210317462682724.\n",
      "Training epoch 0 batch 1246 with loss 1.827327013015747, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 1247 with loss 1.7943572998046875, accuracy 0.3333333432674408.\n",
      "Training epoch 0 batch 1248 with loss 1.8667207956314087, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 1249 with loss 1.7815574407577515, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 1250 with loss 1.8793909549713135, accuracy 0.09047619253396988.\n",
      "Training epoch 0 batch 1251 with loss 1.7775646448135376, accuracy 0.1349206417798996.\n",
      "Training epoch 0 batch 1252 with loss 1.8052139282226562, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 1253 with loss 1.7793277502059937, accuracy 0.2142857164144516.\n",
      "Training epoch 0 batch 1254 with loss 1.7428483963012695, accuracy 0.2152777761220932.\n",
      "Training epoch 0 batch 1255 with loss 1.8579528331756592, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1256 with loss 1.9304277896881104, accuracy 0.06111111491918564.\n",
      "Training epoch 0 batch 1257 with loss 1.869469404220581, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 1258 with loss 1.7556822299957275, accuracy 0.25999999046325684.\n",
      "Training epoch 0 batch 1259 with loss 1.7479547262191772, accuracy 0.4166666865348816.\n",
      "Training epoch 0 batch 1260 with loss 1.77872633934021, accuracy 0.2750000059604645.\n",
      "Training epoch 0 batch 1261 with loss 1.7996854782104492, accuracy 0.15833334624767303.\n",
      "Training epoch 0 batch 1262 with loss 1.7714948654174805, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 1263 with loss 1.7799829244613647, accuracy 0.3194444477558136.\n",
      "Training epoch 0 batch 1264 with loss 1.8068033456802368, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 1265 with loss 1.8639647960662842, accuracy 0.125.\n",
      "Training epoch 0 batch 1266 with loss 1.8418245315551758, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1267 with loss 1.817003607749939, accuracy 0.125.\n",
      "Training epoch 0 batch 1268 with loss 1.8107397556304932, accuracy 0.22380954027175903.\n",
      "Training epoch 0 batch 1269 with loss 1.8809674978256226, accuracy 0.0.\n",
      "Training epoch 0 batch 1270 with loss 1.847648024559021, accuracy 0.130952388048172.\n",
      "Training epoch 0 batch 1271 with loss 1.7966482639312744, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 1272 with loss 1.7812559604644775, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 1273 with loss 1.8019192218780518, accuracy 0.40833333134651184.\n",
      "Training epoch 0 batch 1274 with loss 1.7616335153579712, accuracy 0.2944444715976715.\n",
      "Training epoch 0 batch 1275 with loss 1.8123266696929932, accuracy 0.284722238779068.\n",
      "Training epoch 0 batch 1276 with loss 1.8009006977081299, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 1277 with loss 1.850826621055603, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 1278 with loss 1.8146787881851196, accuracy 0.33888891339302063.\n",
      "Training epoch 0 batch 1279 with loss 1.848118782043457, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 1280 with loss 1.8426620960235596, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 1281 with loss 1.7728649377822876, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 1282 with loss 1.8127435445785522, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 1283 with loss 1.7652181386947632, accuracy 0.1626984179019928.\n",
      "Training epoch 0 batch 1284 with loss 1.8943555355072021, accuracy 0.1319444477558136.\n",
      "Training epoch 0 batch 1285 with loss 1.7897599935531616, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 1286 with loss 1.8269624710083008, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 1287 with loss 1.897386908531189, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1288 with loss 1.8204578161239624, accuracy 0.125.\n",
      "Training epoch 0 batch 1289 with loss 1.7439721822738647, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 1290 with loss 1.79366135597229, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 1291 with loss 1.7946885824203491, accuracy 0.12222222983837128.\n",
      "Training epoch 0 batch 1292 with loss 1.8645236492156982, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1293 with loss 1.7934421300888062, accuracy 0.1527777910232544.\n",
      "Training epoch 0 batch 1294 with loss 1.8487908840179443, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1295 with loss 1.836028814315796, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 1296 with loss 1.7888450622558594, accuracy 0.1349206417798996.\n",
      "Training epoch 0 batch 1297 with loss 1.7642395496368408, accuracy 0.3055555820465088.\n",
      "Training epoch 0 batch 1298 with loss 1.8052679300308228, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 1299 with loss 1.7560150623321533, accuracy 0.2611111104488373.\n",
      "Training epoch 0 batch 1300 with loss 1.818730354309082, accuracy 0.10972222685813904.\n",
      "Training epoch 0 batch 1301 with loss 1.7574456930160522, accuracy 0.2916666865348816.\n",
      "Training epoch 0 batch 1302 with loss 1.8106130361557007, accuracy 0.125.\n",
      "Training epoch 0 batch 1303 with loss 1.8403065204620361, accuracy 0.1349206417798996.\n",
      "Training epoch 0 batch 1304 with loss 1.8530648946762085, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1305 with loss 1.765859842300415, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 1306 with loss 1.8411108255386353, accuracy 0.19166666269302368.\n",
      "Training epoch 0 batch 1307 with loss 1.8492100238800049, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1308 with loss 1.7945539951324463, accuracy 0.1805555671453476.\n",
      "Training epoch 0 batch 1309 with loss 1.8091070652008057, accuracy 0.0625.\n",
      "Training epoch 0 batch 1310 with loss 1.8228012323379517, accuracy 0.125.\n",
      "Training epoch 0 batch 1311 with loss 1.7902475595474243, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 1312 with loss 1.773098349571228, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 1313 with loss 1.8019616603851318, accuracy 0.09259259700775146.\n",
      "Training epoch 0 batch 1314 with loss 1.7514413595199585, accuracy 0.1825396865606308.\n",
      "Training epoch 0 batch 1315 with loss 1.8037649393081665, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 1316 with loss 1.8371562957763672, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 1317 with loss 1.8025867938995361, accuracy 0.2847222089767456.\n",
      "Training epoch 0 batch 1318 with loss 1.7520678043365479, accuracy 0.24444445967674255.\n",
      "Training epoch 0 batch 1319 with loss 1.8034406900405884, accuracy 0.10277777910232544.\n",
      "Training epoch 0 batch 1320 with loss 1.7960231304168701, accuracy 0.31666669249534607.\n",
      "Training epoch 0 batch 1321 with loss 1.8459842205047607, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 1322 with loss 1.8853368759155273, accuracy 0.375.\n",
      "Training epoch 0 batch 1323 with loss 1.7865060567855835, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 1324 with loss 1.8378652334213257, accuracy 0.2888889014720917.\n",
      "Training epoch 0 batch 1325 with loss 1.8081566095352173, accuracy 0.08095238357782364.\n",
      "Training epoch 0 batch 1326 with loss 1.9070587158203125, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 1327 with loss 1.805180549621582, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 1328 with loss 1.7530157566070557, accuracy 0.25.\n",
      "Training epoch 0 batch 1329 with loss 1.7806425094604492, accuracy 0.05416666716337204.\n",
      "Training epoch 0 batch 1330 with loss 1.7966668605804443, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 1331 with loss 1.8333183526992798, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 1332 with loss 1.8128883838653564, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 1333 with loss 1.8354032039642334, accuracy 0.20000001788139343.\n",
      "Training epoch 0 batch 1334 with loss 1.8143781423568726, accuracy 0.02083333395421505.\n",
      "Training epoch 0 batch 1335 with loss 1.8278230428695679, accuracy 0.10277777910232544.\n",
      "Training epoch 0 batch 1336 with loss 1.805957555770874, accuracy 0.125.\n",
      "Training epoch 0 batch 1337 with loss 1.7997801303863525, accuracy 0.0694444477558136.\n",
      "Training epoch 0 batch 1338 with loss 1.8280649185180664, accuracy 0.3333333432674408.\n",
      "Training epoch 0 batch 1339 with loss 1.8256399631500244, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 1340 with loss 1.8176345825195312, accuracy 0.23333334922790527.\n",
      "Training epoch 0 batch 1341 with loss 1.819535255432129, accuracy 0.065476194024086.\n",
      "Training epoch 0 batch 1342 with loss 1.8072702884674072, accuracy 0.2182539701461792.\n",
      "Training epoch 0 batch 1343 with loss 1.6905651092529297, accuracy 0.35555556416511536.\n",
      "Training epoch 0 batch 1344 with loss 1.8261067867279053, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 1345 with loss 1.7946466207504272, accuracy 0.2083333283662796.\n",
      "Training epoch 0 batch 1346 with loss 1.7832345962524414, accuracy 0.24761904776096344.\n",
      "Training epoch 0 batch 1347 with loss 1.8419477939605713, accuracy 0.02083333395421505.\n",
      "Training epoch 0 batch 1348 with loss 1.791512131690979, accuracy 0.17777778208255768.\n",
      "Training epoch 0 batch 1349 with loss 1.872576117515564, accuracy 0.0.\n",
      "Training epoch 0 batch 1350 with loss 1.8263742923736572, accuracy 0.25555557012557983.\n",
      "Training epoch 0 batch 1351 with loss 1.7649204730987549, accuracy 0.1805555671453476.\n",
      "Training epoch 0 batch 1352 with loss 1.8867677450180054, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 1353 with loss 1.8072532415390015, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 1354 with loss 1.826509714126587, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 1355 with loss 1.7908179759979248, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 1356 with loss 1.7657067775726318, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 1357 with loss 1.8002973794937134, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 1358 with loss 1.7767837047576904, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 1359 with loss 1.8551998138427734, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 1360 with loss 1.8345565795898438, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 1361 with loss 1.8076257705688477, accuracy 0.0793650820851326.\n",
      "Training epoch 0 batch 1362 with loss 1.7530295848846436, accuracy 0.27222222089767456.\n",
      "Training epoch 0 batch 1363 with loss 1.7691431045532227, accuracy 0.2361111044883728.\n",
      "Training epoch 0 batch 1364 with loss 1.7657638788223267, accuracy 0.0555555559694767.\n",
      "Training epoch 0 batch 1365 with loss 1.9508438110351562, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 1366 with loss 1.779646635055542, accuracy 0.1726190447807312.\n",
      "Training epoch 0 batch 1367 with loss 1.8076541423797607, accuracy 0.22499999403953552.\n",
      "Training epoch 0 batch 1368 with loss 1.8239269256591797, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 1369 with loss 1.8224961757659912, accuracy 0.2916666567325592.\n",
      "Training epoch 0 batch 1370 with loss 1.8038628101348877, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 1371 with loss 1.758967638015747, accuracy 0.24166667461395264.\n",
      "Training epoch 0 batch 1372 with loss 1.7963014841079712, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 1373 with loss 1.7806313037872314, accuracy 0.15833334624767303.\n",
      "Training epoch 0 batch 1374 with loss 1.7550668716430664, accuracy 0.2361111044883728.\n",
      "Training epoch 0 batch 1375 with loss 1.8086216449737549, accuracy 0.2638888955116272.\n",
      "Training epoch 0 batch 1376 with loss 1.8942406177520752, accuracy 0.125.\n",
      "Training epoch 0 batch 1377 with loss 1.763121247291565, accuracy 0.125.\n",
      "Training epoch 0 batch 1378 with loss 1.7984020709991455, accuracy 0.38055557012557983.\n",
      "Training epoch 0 batch 1379 with loss 1.804552435874939, accuracy 0.2142857313156128.\n",
      "Training epoch 0 batch 1380 with loss 1.867363691329956, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 1381 with loss 1.7576555013656616, accuracy 0.4194444715976715.\n",
      "Training epoch 0 batch 1382 with loss 1.8342924118041992, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 1383 with loss 1.7778412103652954, accuracy 0.06111111491918564.\n",
      "Training epoch 0 batch 1384 with loss 1.832782506942749, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 1385 with loss 1.8572019338607788, accuracy 0.2638888955116272.\n",
      "Training epoch 0 batch 1386 with loss 1.8345305919647217, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 1387 with loss 1.837519645690918, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 1388 with loss 1.7984809875488281, accuracy 0.2182539701461792.\n",
      "Training epoch 0 batch 1389 with loss 1.8324798345565796, accuracy 0.22777777910232544.\n",
      "Training epoch 0 batch 1390 with loss 1.8530571460723877, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 1391 with loss 1.7739064693450928, accuracy 0.2611111104488373.\n",
      "Training epoch 0 batch 1392 with loss 1.8071534633636475, accuracy 0.222222238779068.\n",
      "Training epoch 0 batch 1393 with loss 1.8110816478729248, accuracy 0.13214285671710968.\n",
      "Training epoch 0 batch 1394 with loss 1.8269436359405518, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 1395 with loss 1.7972456216812134, accuracy 0.12777778506278992.\n",
      "Training epoch 0 batch 1396 with loss 1.7842410802841187, accuracy 0.1527777910232544.\n",
      "Training epoch 0 batch 1397 with loss 1.7195045948028564, accuracy 0.29722222685813904.\n",
      "Training epoch 0 batch 1398 with loss 1.8711334466934204, accuracy 0.08888889104127884.\n",
      "Training epoch 0 batch 1399 with loss 1.7635139226913452, accuracy 0.2976190447807312.\n",
      "Training epoch 0 batch 1400 with loss 1.857804298400879, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1401 with loss 1.764640212059021, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 1402 with loss 1.7346007823944092, accuracy 0.21388888359069824.\n",
      "Training epoch 0 batch 1403 with loss 1.7853965759277344, accuracy 0.380952388048172.\n",
      "Training epoch 0 batch 1404 with loss 1.7657543420791626, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 1405 with loss 1.800188660621643, accuracy 0.3055555820465088.\n",
      "Training epoch 0 batch 1406 with loss 1.7977800369262695, accuracy 0.24166667461395264.\n",
      "Training epoch 0 batch 1407 with loss 1.8557889461517334, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 1408 with loss 1.8823182582855225, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 1409 with loss 1.8595263957977295, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1410 with loss 1.8338680267333984, accuracy 0.2750000059604645.\n",
      "Training epoch 0 batch 1411 with loss 1.8086779117584229, accuracy 0.0476190485060215.\n",
      "Training epoch 0 batch 1412 with loss 1.8283298015594482, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 1413 with loss 1.8157230615615845, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 1414 with loss 1.859217643737793, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 1415 with loss 1.7420727014541626, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 1416 with loss 1.7672691345214844, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 1417 with loss 1.795000433921814, accuracy 0.20555555820465088.\n",
      "Training epoch 0 batch 1418 with loss 1.7833884954452515, accuracy 0.1111111119389534.\n",
      "Training epoch 0 batch 1419 with loss 1.7757329940795898, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 1420 with loss 1.7738012075424194, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 1421 with loss 1.8049770593643188, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 1422 with loss 1.7482181787490845, accuracy 0.0714285746216774.\n",
      "Training epoch 0 batch 1423 with loss 1.7700624465942383, accuracy 0.21944445371627808.\n",
      "Training epoch 0 batch 1424 with loss 1.8829190731048584, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1425 with loss 1.7281965017318726, accuracy 0.1547619104385376.\n",
      "Training epoch 0 batch 1426 with loss 1.7905315160751343, accuracy 0.2361111044883728.\n",
      "Training epoch 0 batch 1427 with loss 1.8072216510772705, accuracy 0.2291666567325592.\n",
      "Training epoch 0 batch 1428 with loss 1.7517017126083374, accuracy 0.15555556118488312.\n",
      "Training epoch 0 batch 1429 with loss 1.6672035455703735, accuracy 0.37222224473953247.\n",
      "Training epoch 0 batch 1430 with loss 1.8422677516937256, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 1431 with loss 1.8191890716552734, accuracy 0.0763888880610466.\n",
      "Training epoch 0 batch 1432 with loss 1.816881537437439, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 1433 with loss 1.8353445529937744, accuracy 0.11666667461395264.\n",
      "Training epoch 0 batch 1434 with loss 1.8491863012313843, accuracy 0.0.\n",
      "Training epoch 0 batch 1435 with loss 1.847780466079712, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1436 with loss 1.8293399810791016, accuracy 0.0.\n",
      "Training epoch 0 batch 1437 with loss 1.7701950073242188, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 1438 with loss 1.8102750778198242, accuracy 0.13055555522441864.\n",
      "Training epoch 0 batch 1439 with loss 1.8948167562484741, accuracy 0.1071428656578064.\n",
      "Training epoch 0 batch 1440 with loss 1.8168392181396484, accuracy 0.12037037312984467.\n",
      "Training epoch 0 batch 1441 with loss 1.8223440647125244, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1442 with loss 1.8171476125717163, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1443 with loss 1.7549711465835571, accuracy 0.21388888359069824.\n",
      "Training epoch 0 batch 1444 with loss 1.7790663242340088, accuracy 0.23333334922790527.\n",
      "Training epoch 0 batch 1445 with loss 1.8472524881362915, accuracy 0.1765872985124588.\n",
      "Training epoch 0 batch 1446 with loss 1.834550142288208, accuracy 0.125.\n",
      "Training epoch 0 batch 1447 with loss 1.8419126272201538, accuracy 0.0476190485060215.\n",
      "Training epoch 0 batch 1448 with loss 1.8090474605560303, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 1449 with loss 1.8294925689697266, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 1450 with loss 1.8076059818267822, accuracy 0.14444445073604584.\n",
      "Training epoch 0 batch 1451 with loss 1.8086611032485962, accuracy 0.15833334624767303.\n",
      "Training epoch 0 batch 1452 with loss 1.7846323251724243, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1453 with loss 1.7753750085830688, accuracy 0.15833333134651184.\n",
      "Training epoch 0 batch 1454 with loss 1.837633728981018, accuracy 0.02777777798473835.\n",
      "Training epoch 0 batch 1455 with loss 1.8525688648223877, accuracy 0.10277777910232544.\n",
      "Training epoch 0 batch 1456 with loss 1.8017152547836304, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1457 with loss 1.7774690389633179, accuracy 0.06666667014360428.\n",
      "Training epoch 0 batch 1458 with loss 1.8668773174285889, accuracy 0.2142857313156128.\n",
      "Training epoch 0 batch 1459 with loss 1.7700319290161133, accuracy 0.1527777761220932.\n",
      "Training epoch 0 batch 1460 with loss 1.8533052206039429, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1461 with loss 1.8225549459457397, accuracy 0.14047619700431824.\n",
      "Training epoch 0 batch 1462 with loss 1.8704570531845093, accuracy 0.03703703731298447.\n",
      "Training epoch 0 batch 1463 with loss 1.8235441446304321, accuracy 0.125.\n",
      "Training epoch 0 batch 1464 with loss 1.8795686960220337, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 1465 with loss 1.766897439956665, accuracy 0.23333333432674408.\n",
      "Training epoch 0 batch 1466 with loss 1.8234870433807373, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 1467 with loss 1.8957583904266357, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 1468 with loss 1.7613683938980103, accuracy 0.1944444477558136.\n",
      "Training epoch 0 batch 1469 with loss 1.75801682472229, accuracy 0.190476194024086.\n",
      "Training epoch 0 batch 1470 with loss 1.8068548440933228, accuracy 0.18611110746860504.\n",
      "Training epoch 0 batch 1471 with loss 1.8371622562408447, accuracy 0.236111119389534.\n",
      "Training epoch 0 batch 1472 with loss 1.7191375494003296, accuracy 0.43888890743255615.\n",
      "Training epoch 0 batch 1473 with loss 1.6981174945831299, accuracy 0.2916666567325592.\n",
      "Training epoch 0 batch 1474 with loss 1.7489252090454102, accuracy 0.2083333432674408.\n",
      "Training epoch 0 batch 1475 with loss 1.757906198501587, accuracy 0.3305555582046509.\n",
      "Training epoch 0 batch 1476 with loss 1.8290750980377197, accuracy 0.20000000298023224.\n",
      "Training epoch 0 batch 1477 with loss 1.7933971881866455, accuracy 0.1587301641702652.\n",
      "Training epoch 0 batch 1478 with loss 1.8272340297698975, accuracy 0.16388888657093048.\n",
      "Training epoch 0 batch 1479 with loss 1.7258555889129639, accuracy 0.17222222685813904.\n",
      "Training epoch 0 batch 1480 with loss 1.8594582080841064, accuracy 0.1805555522441864.\n",
      "Training epoch 0 batch 1481 with loss 1.8486038446426392, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1482 with loss 1.834002137184143, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 1483 with loss 1.904147744178772, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 1484 with loss 1.7590233087539673, accuracy 0.3166666626930237.\n",
      "Training epoch 0 batch 1485 with loss 1.8244202136993408, accuracy 0.1666666716337204.\n",
      "Training epoch 0 batch 1486 with loss 1.772855520248413, accuracy 0.20000000298023224.\n",
      "Training epoch 0 batch 1487 with loss 1.819394826889038, accuracy 0.15000000596046448.\n",
      "Training epoch 0 batch 1488 with loss 1.7656570672988892, accuracy 0.4027777910232544.\n",
      "Training epoch 0 batch 1489 with loss 1.85500967502594, accuracy 0.03333333507180214.\n",
      "Training epoch 0 batch 1490 with loss 1.8372554779052734, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1491 with loss 1.799560785293579, accuracy 0.3571428656578064.\n",
      "Training epoch 0 batch 1492 with loss 1.7928088903427124, accuracy 0.26250001788139343.\n",
      "Training epoch 0 batch 1493 with loss 1.8284753561019897, accuracy 0.2222222238779068.\n",
      "Training epoch 0 batch 1494 with loss 1.864020586013794, accuracy 0.0416666679084301.\n",
      "Training epoch 0 batch 1495 with loss 1.7712723016738892, accuracy 0.2658730149269104.\n",
      "Training epoch 0 batch 1496 with loss 1.855015754699707, accuracy 0.0833333358168602.\n",
      "Training epoch 0 batch 1497 with loss 1.7876861095428467, accuracy 0.1388888955116272.\n",
      "Training epoch 0 batch 1498 with loss 1.7733843326568604, accuracy 0.0972222238779068.\n",
      "Training epoch 0 batch 1499 with loss 1.8116947412490845, accuracy 0.19166667759418488.\n",
      "Test batch 0 with loss 1.7848104238510132 and accuracy 0.1666666716337204.\n",
      "Test batch 1 with loss 1.827755331993103 and accuracy 0.0555555559694767.\n",
      "Test batch 2 with loss 1.8014791011810303 and accuracy 0.190476194024086.\n",
      "Test batch 3 with loss 1.8805853128433228 and accuracy 0.125.\n",
      "Test batch 4 with loss 1.797955870628357 and accuracy 0.3152381181716919.\n",
      "Test batch 5 with loss 1.8263896703720093 and accuracy 0.06111111491918564.\n",
      "Test batch 6 with loss 1.784099817276001 and accuracy 0.2638888955116272.\n",
      "Test batch 7 with loss 1.8331018686294556 and accuracy 0.0555555559694767.\n",
      "Test batch 8 with loss 1.8252513408660889 and accuracy 0.1666666716337204.\n",
      "Test batch 9 with loss 1.7615697383880615 and accuracy 0.15000000596046448.\n",
      "Test batch 10 with loss 1.8330360651016235 and accuracy 0.0892857164144516.\n",
      "Test batch 11 with loss 1.7806570529937744 and accuracy 0.28333333134651184.\n",
      "Test batch 12 with loss 1.900958776473999 and accuracy 0.0.\n",
      "Test batch 13 with loss 1.7133573293685913 and accuracy 0.4833333492279053.\n",
      "Test batch 14 with loss 1.7007620334625244 and accuracy 0.45000001788139343.\n",
      "Test batch 15 with loss 1.868338942527771 and accuracy 0.0555555559694767.\n",
      "Test batch 16 with loss 1.8099887371063232 and accuracy 0.11666667461395264.\n",
      "Test batch 17 with loss 1.8387720584869385 and accuracy 0.07500000298023224.\n",
      "Test batch 18 with loss 1.8358234167099 and accuracy 0.0833333358168602.\n",
      "Test batch 19 with loss 1.7972583770751953 and accuracy 0.0555555559694767.\n",
      "Test batch 20 with loss 1.8680683374404907 and accuracy 0.0416666679084301.\n",
      "Test batch 21 with loss 1.728546380996704 and accuracy 0.18333333730697632.\n",
      "Test batch 22 with loss 1.8489660024642944 and accuracy 0.0555555559694767.\n",
      "Test batch 23 with loss 1.8289779424667358 and accuracy 0.0416666679084301.\n",
      "Test batch 24 with loss 1.8554996252059937 and accuracy 0.26944443583488464.\n",
      "Test batch 25 with loss 1.7705446481704712 and accuracy 0.22500000894069672.\n",
      "Test batch 26 with loss 1.760568618774414 and accuracy 0.4738095700740814.\n",
      "Test batch 27 with loss 1.809874176979065 and accuracy 0.0833333358168602.\n",
      "Test batch 28 with loss 1.8201459646224976 and accuracy 0.1666666716337204.\n",
      "Test batch 29 with loss 1.789696455001831 and accuracy 0.17777778208255768.\n",
      "Test batch 30 with loss 1.8676538467407227 and accuracy 0.24166667461395264.\n",
      "Test batch 31 with loss 1.8175723552703857 and accuracy 0.0972222238779068.\n",
      "Test batch 32 with loss 1.7688567638397217 and accuracy 0.2888889014720917.\n",
      "Test batch 33 with loss 1.8163425922393799 and accuracy 0.17222222685813904.\n",
      "Test batch 34 with loss 1.8042404651641846 and accuracy 0.1805555671453476.\n",
      "Test batch 35 with loss 1.8298072814941406 and accuracy 0.11666667461395264.\n",
      "Test batch 36 with loss 1.869686484336853 and accuracy 0.07500000298023224.\n",
      "Test batch 37 with loss 1.7411434650421143 and accuracy 0.1388888955116272.\n",
      "Test batch 38 with loss 1.7358052730560303 and accuracy 0.2638888955116272.\n",
      "Test batch 39 with loss 1.8042408227920532 and accuracy 0.1388888955116272.\n",
      "Test batch 40 with loss 1.7648134231567383 and accuracy 0.12222222983837128.\n",
      "Test batch 41 with loss 1.8093230724334717 and accuracy 0.25555557012557983.\n",
      "Test batch 42 with loss 1.7964359521865845 and accuracy 0.1626984179019928.\n",
      "Test batch 43 with loss 1.833547830581665 and accuracy 0.1111111119389534.\n",
      "Test batch 44 with loss 1.8486617803573608 and accuracy 0.2380952537059784.\n",
      "Test batch 45 with loss 1.8702293634414673 and accuracy 0.0416666679084301.\n",
      "Test batch 46 with loss 1.8482954502105713 and accuracy 0.12222222983837128.\n",
      "Test batch 47 with loss 1.7520869970321655 and accuracy 0.2666666805744171.\n",
      "Test batch 48 with loss 1.7954938411712646 and accuracy 0.17222222685813904.\n",
      "Test batch 49 with loss 1.8075306415557861 and accuracy 0.1388888955116272.\n",
      "Test batch 50 with loss 1.8407526016235352 and accuracy 0.32499998807907104.\n",
      "Test batch 51 with loss 1.804814338684082 and accuracy 0.125.\n",
      "Test batch 52 with loss 1.7901378870010376 and accuracy 0.23888888955116272.\n",
      "Test batch 53 with loss 1.8526538610458374 and accuracy 0.11269842088222504.\n",
      "Test batch 54 with loss 1.794752836227417 and accuracy 0.08888889104127884.\n",
      "Test batch 55 with loss 1.8164567947387695 and accuracy 0.11666667461395264.\n",
      "Test batch 56 with loss 1.8784916400909424 and accuracy 0.1111111119389534.\n",
      "Test batch 57 with loss 1.7689707279205322 and accuracy 0.2142857164144516.\n",
      "Test batch 58 with loss 1.7491633892059326 and accuracy 0.1944444477558136.\n",
      "Test batch 59 with loss 1.7666174173355103 and accuracy 0.21388888359069824.\n",
      "Test batch 60 with loss 1.7382522821426392 and accuracy 0.1547619104385376.\n",
      "Test batch 61 with loss 1.8741708993911743 and accuracy 0.25.\n",
      "Test batch 62 with loss 1.7667146921157837 and accuracy 0.3194444477558136.\n",
      "Test batch 63 with loss 1.8076412677764893 and accuracy 0.20000000298023224.\n",
      "Test batch 64 with loss 1.8545411825180054 and accuracy 0.1666666716337204.\n",
      "Test batch 65 with loss 1.783412218093872 and accuracy 0.2083333283662796.\n",
      "Test batch 66 with loss 1.9138610363006592 and accuracy 0.0.\n",
      "Test batch 67 with loss 1.76377272605896 and accuracy 0.25.\n",
      "Test batch 68 with loss 1.7370798587799072 and accuracy 0.25.\n",
      "Test batch 69 with loss 1.81243097782135 and accuracy 0.03333333507180214.\n",
      "Test batch 70 with loss 1.7916233539581299 and accuracy 0.3166666626930237.\n",
      "Test batch 71 with loss 1.8761358261108398 and accuracy 0.1666666716337204.\n",
      "Test batch 72 with loss 1.868683099746704 and accuracy 0.0555555559694767.\n",
      "Test batch 73 with loss 1.8392537832260132 and accuracy 0.1944444477558136.\n",
      "Test batch 74 with loss 1.7909190654754639 and accuracy 0.2222222238779068.\n",
      "Test batch 75 with loss 1.760662317276001 and accuracy 0.347222238779068.\n",
      "Test batch 76 with loss 1.821488380432129 and accuracy 0.02380952425301075.\n",
      "Test batch 77 with loss 1.8003852367401123 and accuracy 0.16388888657093048.\n",
      "Test batch 78 with loss 1.8219887018203735 and accuracy 0.17222222685813904.\n",
      "Test batch 79 with loss 1.8360437154769897 and accuracy 0.21111111342906952.\n",
      "Test batch 80 with loss 1.8782380819320679 and accuracy 0.0972222238779068.\n",
      "Test batch 81 with loss 1.8166649341583252 and accuracy 0.26944443583488464.\n",
      "Test batch 82 with loss 1.8151839971542358 and accuracy 0.10833333432674408.\n",
      "Test batch 83 with loss 1.805118203163147 and accuracy 0.12222222983837128.\n",
      "Test batch 84 with loss 1.7982889413833618 and accuracy 0.20555555820465088.\n",
      "Test batch 85 with loss 1.7855182886123657 and accuracy 0.2777777910232544.\n",
      "Test batch 86 with loss 1.8524229526519775 and accuracy 0.1527777761220932.\n",
      "Test batch 87 with loss 1.7731120586395264 and accuracy 0.14444445073604584.\n",
      "Test batch 88 with loss 1.7696120738983154 and accuracy 0.2658730149269104.\n",
      "Test batch 89 with loss 1.864143967628479 and accuracy 0.222222238779068.\n",
      "Test batch 90 with loss 1.8168026208877563 and accuracy 0.0833333358168602.\n",
      "Test batch 91 with loss 1.7481809854507446 and accuracy 0.19166666269302368.\n",
      "Test batch 92 with loss 1.921019196510315 and accuracy 0.0.\n",
      "Test batch 93 with loss 1.8365005254745483 and accuracy 0.0972222238779068.\n",
      "Test batch 94 with loss 1.788373351097107 and accuracy 0.24166667461395264.\n",
      "Test batch 95 with loss 1.754293441772461 and accuracy 0.2666666805744171.\n",
      "Test batch 96 with loss 1.8181371688842773 and accuracy 0.03333333507180214.\n",
      "Test batch 97 with loss 1.7582826614379883 and accuracy 0.20555555820465088.\n",
      "Test batch 98 with loss 1.7726949453353882 and accuracy 0.0972222238779068.\n",
      "Test batch 99 with loss 1.7869527339935303 and accuracy 0.30158731341362.\n",
      "Test batch 100 with loss 1.8172972202301025 and accuracy 0.13333334028720856.\n",
      "Test batch 101 with loss 1.790759801864624 and accuracy 0.17222222685813904.\n",
      "Test batch 102 with loss 1.810378074645996 and accuracy 0.1547619104385376.\n",
      "Test batch 103 with loss 1.8153241872787476 and accuracy 0.23333334922790527.\n",
      "Test batch 104 with loss 1.7618694305419922 and accuracy 0.13055555522441864.\n",
      "Test batch 105 with loss 1.8034721612930298 and accuracy 0.1944444477558136.\n",
      "Test batch 106 with loss 1.8571317195892334 and accuracy 0.0833333358168602.\n",
      "Test batch 107 with loss 1.7837114334106445 and accuracy 0.1666666716337204.\n",
      "Test batch 108 with loss 1.7344341278076172 and accuracy 0.236111119389534.\n",
      "Test batch 109 with loss 1.7807868719100952 and accuracy 0.2083333432674408.\n",
      "Test batch 110 with loss 1.818145990371704 and accuracy 0.1111111119389534.\n",
      "Test batch 111 with loss 1.8595041036605835 and accuracy 0.0555555559694767.\n",
      "Test batch 112 with loss 1.8127533197402954 and accuracy 0.1944444477558136.\n",
      "Test batch 113 with loss 1.838997483253479 and accuracy 0.0416666679084301.\n",
      "Test batch 114 with loss 1.8554550409317017 and accuracy 0.1388888955116272.\n",
      "Test batch 115 with loss 1.8498985767364502 and accuracy 0.1388888955116272.\n",
      "Test batch 116 with loss 1.8826663494110107 and accuracy 0.1111111119389534.\n",
      "Test batch 117 with loss 1.8505456447601318 and accuracy 0.1111111119389534.\n",
      "Test batch 118 with loss 1.7402238845825195 and accuracy 0.20555555820465088.\n",
      "Test batch 119 with loss 1.8402822017669678 and accuracy 0.2083333283662796.\n",
      "Test batch 120 with loss 1.7859060764312744 and accuracy 0.2083333432674408.\n",
      "Test batch 121 with loss 1.854386568069458 and accuracy 0.14444445073604584.\n",
      "Test batch 122 with loss 1.8987411260604858 and accuracy 0.0793650820851326.\n",
      "Test batch 123 with loss 1.7876371145248413 and accuracy 0.07500000298023224.\n",
      "Test batch 124 with loss 1.770423173904419 and accuracy 0.125.\n",
      "Test batch 125 with loss 1.8483922481536865 and accuracy 0.3055555522441864.\n",
      "Test batch 126 with loss 1.8238321542739868 and accuracy 0.0555555559694767.\n",
      "Test batch 127 with loss 1.806809663772583 and accuracy 0.222222238779068.\n",
      "Test batch 128 with loss 1.7882080078125 and accuracy 0.111111119389534.\n",
      "Test batch 129 with loss 1.8156957626342773 and accuracy 0.08888889104127884.\n",
      "Test batch 130 with loss 1.900679588317871 and accuracy 0.10833333432674408.\n",
      "Test batch 131 with loss 1.8543856143951416 and accuracy 0.1527777761220932.\n",
      "Test batch 132 with loss 1.7814242839813232 and accuracy 0.44166669249534607.\n",
      "Test batch 133 with loss 1.874993085861206 and accuracy 0.125.\n",
      "Test batch 134 with loss 1.8264315128326416 and accuracy 0.2888889014720917.\n",
      "Test batch 135 with loss 1.7989444732666016 and accuracy 0.2777777910232544.\n",
      "Test batch 136 with loss 1.731711983680725 and accuracy 0.2638888955116272.\n",
      "Test batch 137 with loss 1.8564846515655518 and accuracy 0.15000000596046448.\n",
      "Test batch 138 with loss 1.819659948348999 and accuracy 0.09880952537059784.\n",
      "Test batch 139 with loss 1.7323592901229858 and accuracy 0.15555556118488312.\n",
      "Test batch 140 with loss 1.8695471286773682 and accuracy 0.10833333432674408.\n",
      "Test batch 141 with loss 1.7980034351348877 and accuracy 0.1944444477558136.\n",
      "Test batch 142 with loss 1.7712090015411377 and accuracy 0.1388888955116272.\n",
      "Test batch 143 with loss 1.7648903131484985 and accuracy 0.0972222238779068.\n",
      "Test batch 144 with loss 1.7934612035751343 and accuracy 0.2083333432674408.\n",
      "Test batch 145 with loss 1.8058162927627563 and accuracy 0.204365074634552.\n",
      "Test batch 146 with loss 1.8481025695800781 and accuracy 0.1527777761220932.\n",
      "Test batch 147 with loss 1.751528024673462 and accuracy 0.3777777850627899.\n",
      "Test batch 148 with loss 1.8077151775360107 and accuracy 0.03333333507180214.\n",
      "Test batch 149 with loss 1.7625081539154053 and accuracy 0.1527777761220932.\n",
      "Test batch 150 with loss 1.7977895736694336 and accuracy 0.1666666716337204.\n",
      "Test batch 151 with loss 1.8476940393447876 and accuracy 0.03333333507180214.\n",
      "Test batch 152 with loss 1.8274962902069092 and accuracy 0.10476191341876984.\n",
      "Test batch 153 with loss 1.849982500076294 and accuracy 0.0416666679084301.\n",
      "Test batch 154 with loss 1.887255311012268 and accuracy 0.1666666716337204.\n",
      "Test batch 155 with loss 1.7890727519989014 and accuracy 0.1865079402923584.\n",
      "Test batch 156 with loss 1.783263921737671 and accuracy 0.14444445073604584.\n",
      "Test batch 157 with loss 1.7966535091400146 and accuracy 0.11666667461395264.\n",
      "Test batch 158 with loss 1.843819260597229 and accuracy 0.1875.\n",
      "Test batch 159 with loss 1.7900936603546143 and accuracy 0.0476190485060215.\n",
      "Test batch 160 with loss 1.87494695186615 and accuracy 0.17222222685813904.\n",
      "Test batch 161 with loss 1.8389047384262085 and accuracy 0.0.\n",
      "Test batch 162 with loss 1.7663328647613525 and accuracy 0.14047619700431824.\n",
      "Test batch 163 with loss 1.7418549060821533 and accuracy 0.125.\n",
      "Test batch 164 with loss 1.7721296548843384 and accuracy 0.2986111342906952.\n",
      "Test batch 165 with loss 1.8328959941864014 and accuracy 0.1388888955116272.\n",
      "Test batch 166 with loss 1.8990014791488647 and accuracy 0.1388888955116272.\n",
      "Test batch 167 with loss 1.7961828708648682 and accuracy 0.11666667461395264.\n",
      "Test batch 168 with loss 1.8381303548812866 and accuracy 0.1111111119389534.\n",
      "Test batch 169 with loss 1.793336272239685 and accuracy 0.144841268658638.\n",
      "Test batch 170 with loss 1.8235819339752197 and accuracy 0.0714285746216774.\n",
      "Test batch 171 with loss 1.8899726867675781 and accuracy 0.1111111119389534.\n",
      "Test batch 172 with loss 1.8031717538833618 and accuracy 0.11666667461395264.\n",
      "Test batch 173 with loss 1.8540427684783936 and accuracy 0.125.\n",
      "Test batch 174 with loss 1.8320083618164062 and accuracy 0.1944444477558136.\n",
      "Test batch 175 with loss 1.8026612997055054 and accuracy 0.05714286118745804.\n",
      "Test batch 176 with loss 1.7917296886444092 and accuracy 0.1944444477558136.\n",
      "Test batch 177 with loss 1.8372383117675781 and accuracy 0.08888889104127884.\n",
      "Test batch 178 with loss 1.811455488204956 and accuracy 0.3055555820465088.\n",
      "Test batch 179 with loss 1.817583441734314 and accuracy 0.31111112236976624.\n",
      "Test batch 180 with loss 1.8617805242538452 and accuracy 0.125.\n",
      "Test batch 181 with loss 1.7288545370101929 and accuracy 0.2888889014720917.\n",
      "Test batch 182 with loss 1.8256862163543701 and accuracy 0.2611111104488373.\n",
      "Test batch 183 with loss 1.802354097366333 and accuracy 0.125.\n",
      "Test batch 184 with loss 1.8093541860580444 and accuracy 0.20000000298023224.\n",
      "Test batch 185 with loss 1.8158975839614868 and accuracy 0.0625.\n",
      "Test batch 186 with loss 1.8285363912582397 and accuracy 0.25.\n",
      "Test batch 187 with loss 1.7510372400283813 and accuracy 0.11666667461395264.\n",
      "Test batch 188 with loss 1.8544342517852783 and accuracy 0.2083333283662796.\n",
      "Test batch 189 with loss 1.8284152746200562 and accuracy 0.11666667461395264.\n",
      "Test batch 190 with loss 1.8536819219589233 and accuracy 0.0833333358168602.\n",
      "Test batch 191 with loss 1.7911131381988525 and accuracy 0.25.\n",
      "Test batch 192 with loss 1.8266162872314453 and accuracy 0.2222222238779068.\n",
      "Test batch 193 with loss 1.8083126544952393 and accuracy 0.18611110746860504.\n",
      "Test batch 194 with loss 1.7742424011230469 and accuracy 0.1666666716337204.\n",
      "Test batch 195 with loss 1.8029975891113281 and accuracy 0.2611111104488373.\n",
      "Test batch 196 with loss 1.8573920726776123 and accuracy 0.14444445073604584.\n",
      "Test batch 197 with loss 1.876936674118042 and accuracy 0.33888888359069824.\n",
      "Test batch 198 with loss 1.734440803527832 and accuracy 0.2750000059604645.\n",
      "Test batch 199 with loss 1.8162513971328735 and accuracy 0.17777778208255768.\n",
      "Test batch 200 with loss 1.8603923320770264 and accuracy 0.11666667461395264.\n",
      "Test batch 201 with loss 1.8320367336273193 and accuracy 0.1111111119389534.\n",
      "Test batch 202 with loss 1.8183488845825195 and accuracy 0.0416666679084301.\n",
      "Test batch 203 with loss 1.8595192432403564 and accuracy 0.1111111119389534.\n",
      "Test batch 204 with loss 1.8526338338851929 and accuracy 0.0833333358168602.\n",
      "Test batch 205 with loss 1.804626703262329 and accuracy 0.3253968358039856.\n",
      "Test batch 206 with loss 1.8307167291641235 and accuracy 0.0555555559694767.\n",
      "Test batch 207 with loss 1.779517412185669 and accuracy 0.17222222685813904.\n",
      "Test batch 208 with loss 1.858173131942749 and accuracy 0.08888889104127884.\n",
      "Test batch 209 with loss 1.8407175540924072 and accuracy 0.1071428582072258.\n",
      "Test batch 210 with loss 1.8269764184951782 and accuracy 0.1349206417798996.\n",
      "Test batch 211 with loss 1.7919553518295288 and accuracy 0.1388888955116272.\n",
      "Test batch 212 with loss 1.7859156131744385 and accuracy 0.2916666865348816.\n",
      "Test batch 213 with loss 1.7898931503295898 and accuracy 0.3638888895511627.\n",
      "Test batch 214 with loss 1.8252214193344116 and accuracy 0.1111111119389534.\n",
      "Test batch 215 with loss 1.8085769414901733 and accuracy 0.07500000298023224.\n",
      "Test batch 216 with loss 1.7974116802215576 and accuracy 0.3253968358039856.\n",
      "Test batch 217 with loss 1.7621530294418335 and accuracy 0.2750000059604645.\n",
      "Test batch 218 with loss 1.7804391384124756 and accuracy 0.1666666716337204.\n",
      "Test batch 219 with loss 1.7608554363250732 and accuracy 0.3083333373069763.\n",
      "Test batch 220 with loss 1.8063653707504272 and accuracy 0.0972222238779068.\n",
      "Test batch 221 with loss 1.8169349431991577 and accuracy 0.03333333507180214.\n",
      "Test batch 222 with loss 1.7814600467681885 and accuracy 0.1805555522441864.\n",
      "Test batch 223 with loss 1.7609418630599976 and accuracy 0.375.\n",
      "Test batch 224 with loss 1.7939742803573608 and accuracy 0.07500000298023224.\n",
      "Test batch 225 with loss 1.7556235790252686 and accuracy 0.347222238779068.\n",
      "Test batch 226 with loss 1.7953336238861084 and accuracy 0.2888889014720917.\n",
      "Test batch 227 with loss 1.8488986492156982 and accuracy 0.3194444477558136.\n",
      "Test batch 228 with loss 1.7059376239776611 and accuracy 0.3222222328186035.\n",
      "Test batch 229 with loss 1.7554607391357422 and accuracy 0.15555556118488312.\n",
      "Test batch 230 with loss 1.7491190433502197 and accuracy 0.13055555522441864.\n",
      "Test batch 231 with loss 1.8409370183944702 and accuracy 0.03333333507180214.\n",
      "Test batch 232 with loss 1.7936958074569702 and accuracy 0.07500000298023224.\n",
      "Test batch 233 with loss 1.7268307209014893 and accuracy 0.2944444417953491.\n",
      "Test batch 234 with loss 1.8036473989486694 and accuracy 0.03333333507180214.\n",
      "Test batch 235 with loss 1.8291313648223877 and accuracy 0.13333334028720856.\n",
      "Test batch 236 with loss 1.8359651565551758 and accuracy 0.1488095223903656.\n",
      "Test batch 237 with loss 1.842529535293579 and accuracy 0.1944444477558136.\n",
      "Test batch 238 with loss 1.7595525979995728 and accuracy 0.19166666269302368.\n",
      "Test batch 239 with loss 1.869154930114746 and accuracy 0.1666666716337204.\n",
      "Test batch 240 with loss 1.8172388076782227 and accuracy 0.1111111119389534.\n",
      "Test batch 241 with loss 1.8050861358642578 and accuracy 0.2805555462837219.\n",
      "Test batch 242 with loss 1.8646835088729858 and accuracy 0.11666667461395264.\n",
      "Test batch 243 with loss 1.8160403966903687 and accuracy 0.1944444477558136.\n",
      "Test batch 244 with loss 1.8272144794464111 and accuracy 0.1388888955116272.\n",
      "Test batch 245 with loss 1.7895265817642212 and accuracy 0.17222222685813904.\n",
      "Test batch 246 with loss 1.7909950017929077 and accuracy 0.0416666679084301.\n",
      "Test batch 247 with loss 1.7478420734405518 and accuracy 0.2777777910232544.\n",
      "Test batch 248 with loss 1.8465105295181274 and accuracy 0.12222222983837128.\n",
      "Test batch 249 with loss 1.8323018550872803 and accuracy 0.0416666679084301.\n",
      "Test batch 250 with loss 1.718431830406189 and accuracy 0.1944444477558136.\n",
      "Test batch 251 with loss 1.7390124797821045 and accuracy 0.2936508059501648.\n",
      "Test batch 252 with loss 1.7810834646224976 and accuracy 0.1388888955116272.\n",
      "Test batch 253 with loss 1.7955433130264282 and accuracy 0.1319444477558136.\n",
      "Test batch 254 with loss 1.8380029201507568 and accuracy 0.1805555522441864.\n",
      "Test batch 255 with loss 1.8360735177993774 and accuracy 0.0714285746216774.\n",
      "Test batch 256 with loss 1.8501403331756592 and accuracy 0.0416666679084301.\n",
      "Test batch 257 with loss 1.82399582862854 and accuracy 0.3194444477558136.\n",
      "Test batch 258 with loss 1.7657444477081299 and accuracy 0.0833333358168602.\n",
      "Test batch 259 with loss 1.7903163433074951 and accuracy 0.23333334922790527.\n",
      "Test batch 260 with loss 1.8150523900985718 and accuracy 0.1111111119389534.\n",
      "Test batch 261 with loss 1.7356884479522705 and accuracy 0.25555557012557983.\n",
      "Test batch 262 with loss 1.778977394104004 and accuracy 0.22499999403953552.\n",
      "Test batch 263 with loss 1.780036211013794 and accuracy 0.3055555522441864.\n",
      "Test batch 264 with loss 1.8586984872817993 and accuracy 0.1388888955116272.\n",
      "Test batch 265 with loss 1.8542311191558838 and accuracy 0.125.\n",
      "Test batch 266 with loss 1.8149585723876953 and accuracy 0.125.\n",
      "Test batch 267 with loss 1.8119924068450928 and accuracy 0.18611110746860504.\n",
      "Test batch 268 with loss 1.7858203649520874 and accuracy 0.255952388048172.\n",
      "Test batch 269 with loss 1.8570894002914429 and accuracy 0.12222222983837128.\n",
      "Test batch 270 with loss 1.8233642578125 and accuracy 0.0972222238779068.\n",
      "Test batch 271 with loss 1.793251395225525 and accuracy 0.15000000596046448.\n",
      "Test batch 272 with loss 1.8128407001495361 and accuracy 0.25.\n",
      "Test batch 273 with loss 1.7731813192367554 and accuracy 0.1944444477558136.\n",
      "Test batch 274 with loss 1.8180370330810547 and accuracy 0.111111119389534.\n",
      "Test batch 275 with loss 1.8309078216552734 and accuracy 0.0.\n",
      "Test batch 276 with loss 1.8605687618255615 and accuracy 0.10000000149011612.\n",
      "Test batch 277 with loss 1.8027210235595703 and accuracy 0.20000000298023224.\n",
      "Test batch 278 with loss 1.8391281366348267 and accuracy 0.1111111119389534.\n",
      "Test batch 279 with loss 1.7797110080718994 and accuracy 0.15833333134651184.\n",
      "Test batch 280 with loss 1.8558237552642822 and accuracy 0.0.\n",
      "Test batch 281 with loss 1.8409732580184937 and accuracy 0.21547618508338928.\n",
      "Test batch 282 with loss 1.7702512741088867 and accuracy 0.2433333396911621.\n",
      "Test batch 283 with loss 1.8882147073745728 and accuracy 0.12222222983837128.\n",
      "Test batch 284 with loss 1.8079713582992554 and accuracy 0.07500000298023224.\n",
      "Test batch 285 with loss 1.700905442237854 and accuracy 0.2916666567325592.\n",
      "Test batch 286 with loss 1.8091495037078857 and accuracy 0.12222222983837128.\n",
      "Test batch 287 with loss 1.8034318685531616 and accuracy 0.10277777910232544.\n",
      "Test batch 288 with loss 1.7962125539779663 and accuracy 0.3194444477558136.\n",
      "Test batch 289 with loss 1.8650108575820923 and accuracy 0.12777778506278992.\n",
      "Test batch 290 with loss 1.8094148635864258 and accuracy 0.2083333283662796.\n",
      "Test batch 291 with loss 1.7643696069717407 and accuracy 0.1388888955116272.\n",
      "Test batch 292 with loss 1.7778419256210327 and accuracy 0.3083333373069763.\n",
      "Test batch 293 with loss 1.7840526103973389 and accuracy 0.15833333134651184.\n",
      "Test batch 294 with loss 1.8483883142471313 and accuracy 0.1111111119389534.\n",
      "Test batch 295 with loss 1.8108152151107788 and accuracy 0.0833333358168602.\n",
      "Test batch 296 with loss 1.7696319818496704 and accuracy 0.1944444477558136.\n",
      "Test batch 297 with loss 1.8086698055267334 and accuracy 0.12222222983837128.\n",
      "Test batch 298 with loss 1.8385528326034546 and accuracy 0.2083333432674408.\n",
      "Test batch 299 with loss 1.77370285987854 and accuracy 0.25555557012557983.\n",
      "Test batch 300 with loss 1.787670373916626 and accuracy 0.33095240592956543.\n",
      "Test batch 301 with loss 1.887476921081543 and accuracy 0.1111111119389534.\n",
      "Test batch 302 with loss 1.7943086624145508 and accuracy 0.15000000596046448.\n",
      "Test batch 303 with loss 1.8299472332000732 and accuracy 0.14444445073604584.\n",
      "Test batch 304 with loss 1.8116466999053955 and accuracy 0.18333333730697632.\n",
      "Test batch 305 with loss 1.7764155864715576 and accuracy 0.25555557012557983.\n",
      "Test batch 306 with loss 1.7958015203475952 and accuracy 0.20000000298023224.\n",
      "Test batch 307 with loss 1.8547855615615845 and accuracy 0.0555555559694767.\n",
      "Test batch 308 with loss 1.8102048635482788 and accuracy 0.1071428582072258.\n",
      "Test batch 309 with loss 1.858539342880249 and accuracy 0.10833333432674408.\n",
      "Test batch 310 with loss 1.802477478981018 and accuracy 0.17500001192092896.\n",
      "Test batch 311 with loss 1.8197658061981201 and accuracy 0.144841268658638.\n",
      "Test batch 312 with loss 1.7923953533172607 and accuracy 0.09444444626569748.\n",
      "Test batch 313 with loss 1.7976512908935547 and accuracy 0.20000000298023224.\n",
      "Test batch 314 with loss 1.8738161325454712 and accuracy 0.0555555559694767.\n",
      "Test batch 315 with loss 1.8244943618774414 and accuracy 0.0555555559694767.\n",
      "Test batch 316 with loss 1.7960134744644165 and accuracy 0.2430555522441864.\n",
      "Test batch 317 with loss 1.8407728672027588 and accuracy 0.0793650820851326.\n",
      "Test batch 318 with loss 1.8282358646392822 and accuracy 0.0833333358168602.\n",
      "Test batch 319 with loss 1.794846773147583 and accuracy 0.0833333358168602.\n",
      "Test batch 320 with loss 1.719914436340332 and accuracy 0.17222222685813904.\n",
      "Test batch 321 with loss 1.7856934070587158 and accuracy 0.3888888955116272.\n",
      "Test batch 322 with loss 1.8577125072479248 and accuracy 0.1111111119389534.\n",
      "Test batch 323 with loss 1.7765756845474243 and accuracy 0.2142857164144516.\n",
      "Test batch 324 with loss 1.7865642309188843 and accuracy 0.236111119389534.\n",
      "Test batch 325 with loss 1.8206701278686523 and accuracy 0.2013888955116272.\n",
      "Test batch 326 with loss 1.8420133590698242 and accuracy 0.0555555559694767.\n",
      "Test batch 327 with loss 1.7781126499176025 and accuracy 0.20000000298023224.\n",
      "Test batch 328 with loss 1.7273294925689697 and accuracy 0.1527777761220932.\n",
      "Test batch 329 with loss 1.810847520828247 and accuracy 0.1388888955116272.\n",
      "Test batch 330 with loss 1.7474792003631592 and accuracy 0.21666666865348816.\n",
      "Test batch 331 with loss 1.764779806137085 and accuracy 0.08888889104127884.\n",
      "Test batch 332 with loss 1.788414716720581 and accuracy 0.1488095223903656.\n",
      "Test batch 333 with loss 1.818176507949829 and accuracy 0.08888889104127884.\n",
      "Test batch 334 with loss 1.8652385473251343 and accuracy 0.0.\n",
      "Test batch 335 with loss 1.847190260887146 and accuracy 0.0694444477558136.\n",
      "Test batch 336 with loss 1.8384908437728882 and accuracy 0.125.\n",
      "Test batch 337 with loss 1.7688096761703491 and accuracy 0.24722221493721008.\n",
      "Test batch 338 with loss 1.8473478555679321 and accuracy 0.11666667461395264.\n",
      "Test batch 339 with loss 1.7985156774520874 and accuracy 0.20000001788139343.\n",
      "Test batch 340 with loss 1.8221919536590576 and accuracy 0.1666666716337204.\n",
      "Test batch 341 with loss 1.847322702407837 and accuracy 0.0694444477558136.\n",
      "Test batch 342 with loss 1.7808010578155518 and accuracy 0.20000000298023224.\n",
      "Test batch 343 with loss 1.8124611377716064 and accuracy 0.03333333507180214.\n",
      "Test batch 344 with loss 1.7967302799224854 and accuracy 0.11666667461395264.\n",
      "Test batch 345 with loss 1.8320289850234985 and accuracy 0.0833333358168602.\n",
      "Test batch 346 with loss 1.8495018482208252 and accuracy 0.13750000298023224.\n",
      "Test batch 347 with loss 1.7417802810668945 and accuracy 0.1527777761220932.\n",
      "Test batch 348 with loss 1.8156557083129883 and accuracy 0.1944444477558136.\n",
      "Test batch 349 with loss 1.8073463439941406 and accuracy 0.222222238779068.\n",
      "Test batch 350 with loss 1.7990076541900635 and accuracy 0.0833333358168602.\n",
      "Test batch 351 with loss 1.7848880290985107 and accuracy 0.07500000298023224.\n",
      "Test batch 352 with loss 1.852097511291504 and accuracy 0.0.\n",
      "Test batch 353 with loss 1.7321685552597046 and accuracy 0.3154761791229248.\n",
      "Test batch 354 with loss 1.8320367336273193 and accuracy 0.19166666269302368.\n",
      "Test batch 355 with loss 1.781549096107483 and accuracy 0.2638888955116272.\n",
      "Test batch 356 with loss 1.7852249145507812 and accuracy 0.12037037312984467.\n",
      "Test batch 357 with loss 1.8739084005355835 and accuracy 0.0.\n",
      "Test batch 358 with loss 1.7719930410385132 and accuracy 0.1944444477558136.\n",
      "Test batch 359 with loss 1.7347462177276611 and accuracy 0.2611111104488373.\n",
      "Test batch 360 with loss 1.813720464706421 and accuracy 0.0972222238779068.\n",
      "Test batch 361 with loss 1.8250057697296143 and accuracy 0.1805555522441864.\n",
      "Test batch 362 with loss 1.792534589767456 and accuracy 0.15555556118488312.\n",
      "Test batch 363 with loss 1.7902063131332397 and accuracy 0.125.\n",
      "Test batch 364 with loss 1.8139928579330444 and accuracy 0.17777778208255768.\n",
      "Test batch 365 with loss 1.7488939762115479 and accuracy 0.22777777910232544.\n",
      "Test batch 366 with loss 1.821266531944275 and accuracy 0.2611111104488373.\n",
      "Test batch 367 with loss 1.8080689907073975 and accuracy 0.1666666716337204.\n",
      "Test batch 368 with loss 1.7760913372039795 and accuracy 0.5333333611488342.\n",
      "Test batch 369 with loss 1.7786003351211548 and accuracy 0.17777778208255768.\n",
      "Test batch 370 with loss 1.8390843868255615 and accuracy 0.11666667461395264.\n",
      "Test batch 371 with loss 1.8561939001083374 and accuracy 0.1111111119389534.\n",
      "Test batch 372 with loss 1.7441215515136719 and accuracy 0.32500001788139343.\n",
      "Test batch 373 with loss 1.7813661098480225 and accuracy 0.2361111044883728.\n",
      "Test batch 374 with loss 1.8050572872161865 and accuracy 0.2777777910232544.\n",
      "Test batch 375 with loss 1.7597906589508057 and accuracy 0.0972222238779068.\n",
      "Test batch 376 with loss 1.8257172107696533 and accuracy 0.0833333358168602.\n",
      "Test batch 377 with loss 1.7808034420013428 and accuracy 0.32499998807907104.\n",
      "Test batch 378 with loss 1.7385120391845703 and accuracy 0.2571428716182709.\n",
      "Test batch 379 with loss 1.8491548299789429 and accuracy 0.2083333432674408.\n",
      "Test batch 380 with loss 1.8502836227416992 and accuracy 0.02083333395421505.\n",
      "Test batch 381 with loss 1.8139352798461914 and accuracy 0.065476194024086.\n",
      "Test batch 382 with loss 1.799055814743042 and accuracy 0.1587301641702652.\n",
      "Test batch 383 with loss 1.8312610387802124 and accuracy 0.11666667461395264.\n",
      "Test batch 384 with loss 1.8969933986663818 and accuracy 0.0416666679084301.\n",
      "Test batch 385 with loss 1.8359581232070923 and accuracy 0.2916666865348816.\n",
      "Test batch 386 with loss 1.813701868057251 and accuracy 0.15000000596046448.\n",
      "Test batch 387 with loss 1.8434998989105225 and accuracy 0.18611112236976624.\n",
      "Test batch 388 with loss 1.7849632501602173 and accuracy 0.1666666716337204.\n",
      "Test batch 389 with loss 1.8112701177597046 and accuracy 0.0833333358168602.\n",
      "Test batch 390 with loss 1.8193445205688477 and accuracy 0.23888888955116272.\n",
      "Test batch 391 with loss 1.7886838912963867 and accuracy 0.2083333432674408.\n",
      "Test batch 392 with loss 1.797114610671997 and accuracy 0.10000000149011612.\n",
      "Test batch 393 with loss 1.75234854221344 and accuracy 0.1666666716337204.\n",
      "Test batch 394 with loss 1.8887802362442017 and accuracy 0.07500000298023224.\n",
      "Test batch 395 with loss 1.8029676675796509 and accuracy 0.3055555522441864.\n",
      "Test batch 396 with loss 1.8281691074371338 and accuracy 0.08888889104127884.\n",
      "Test batch 397 with loss 1.8725147247314453 and accuracy 0.0.\n",
      "Test batch 398 with loss 1.808121681213379 and accuracy 0.0694444477558136.\n",
      "Test batch 399 with loss 1.8145601749420166 and accuracy 0.17222222685813904.\n",
      "Test batch 400 with loss 1.8371890783309937 and accuracy 0.25.\n",
      "Test batch 401 with loss 1.8086585998535156 and accuracy 0.1388888955116272.\n",
      "Test batch 402 with loss 1.8022483587265015 and accuracy 0.1666666716337204.\n",
      "Test batch 403 with loss 1.8810532093048096 and accuracy 0.03333333507180214.\n",
      "Test batch 404 with loss 1.7871723175048828 and accuracy 0.33888891339302063.\n",
      "Test batch 405 with loss 1.857391357421875 and accuracy 0.2750000059604645.\n",
      "Test batch 406 with loss 1.8265033960342407 and accuracy 0.1388888955116272.\n",
      "Test batch 407 with loss 1.761526107788086 and accuracy 0.09444445371627808.\n",
      "Test batch 408 with loss 1.8285661935806274 and accuracy 0.125.\n",
      "Test batch 409 with loss 1.8744665384292603 and accuracy 0.0555555559694767.\n",
      "Test batch 410 with loss 1.8745315074920654 and accuracy 0.0555555559694767.\n",
      "Test batch 411 with loss 1.8600547313690186 and accuracy 0.0833333358168602.\n",
      "Test batch 412 with loss 1.8407100439071655 and accuracy 0.0833333358168602.\n",
      "Test batch 413 with loss 1.7940056324005127 and accuracy 0.2111111283302307.\n",
      "Test batch 414 with loss 1.7845094203948975 and accuracy 0.11666667461395264.\n",
      "Test batch 415 with loss 1.8433797359466553 and accuracy 0.06111111491918564.\n",
      "Test batch 416 with loss 1.816784143447876 and accuracy 0.1388888955116272.\n",
      "Test batch 417 with loss 1.8064720630645752 and accuracy 0.2222222238779068.\n",
      "Test batch 418 with loss 1.7090972661972046 and accuracy 0.3472222089767456.\n",
      "Test batch 419 with loss 1.7652854919433594 and accuracy 0.14166668057441711.\n",
      "Test batch 420 with loss 1.7228657007217407 and accuracy 0.1388888955116272.\n",
      "Test batch 421 with loss 1.8008466958999634 and accuracy 0.2083333432674408.\n",
      "Test batch 422 with loss 1.9466243982315063 and accuracy 0.1666666716337204.\n",
      "Test batch 423 with loss 1.8126194477081299 and accuracy 0.02380952425301075.\n",
      "Test batch 424 with loss 1.769073724746704 and accuracy 0.1111111119389534.\n",
      "Test batch 425 with loss 1.8480689525604248 and accuracy 0.1319444477558136.\n",
      "Test batch 426 with loss 1.7617874145507812 and accuracy 0.2083333432674408.\n",
      "Test batch 427 with loss 1.8757259845733643 and accuracy 0.03333333507180214.\n",
      "Test batch 428 with loss 1.8299283981323242 and accuracy 0.11666667461395264.\n",
      "Test batch 429 with loss 1.7729520797729492 and accuracy 0.23333333432674408.\n",
      "Test batch 430 with loss 1.7908319234848022 and accuracy 0.18333333730697632.\n",
      "Test batch 431 with loss 1.7916713953018188 and accuracy 0.1944444477558136.\n",
      "Test batch 432 with loss 1.8061094284057617 and accuracy 0.0694444477558136.\n",
      "Test batch 433 with loss 1.7764413356781006 and accuracy 0.15833333134651184.\n",
      "Test batch 434 with loss 1.7910372018814087 and accuracy 0.1111111119389534.\n",
      "Test batch 435 with loss 1.7833654880523682 and accuracy 0.30317461490631104.\n",
      "Test batch 436 with loss 1.824683427810669 and accuracy 0.065476194024086.\n",
      "Test batch 437 with loss 1.8190906047821045 and accuracy 0.2182539701461792.\n",
      "Test batch 438 with loss 1.8145214319229126 and accuracy 0.15833333134651184.\n",
      "Test batch 439 with loss 1.7336785793304443 and accuracy 0.5722222328186035.\n",
      "Test batch 440 with loss 1.8795578479766846 and accuracy 0.0.\n",
      "Test batch 441 with loss 1.7602574825286865 and accuracy 0.2222222238779068.\n",
      "Test batch 442 with loss 1.7412954568862915 and accuracy 0.19761905074119568.\n",
      "Test batch 443 with loss 1.7837730646133423 and accuracy 0.32499998807907104.\n",
      "Test batch 444 with loss 1.7991548776626587 and accuracy 0.24166667461395264.\n",
      "Test batch 445 with loss 1.7826709747314453 and accuracy 0.125.\n",
      "Test batch 446 with loss 1.8299452066421509 and accuracy 0.3472222089767456.\n",
      "Test batch 447 with loss 1.8039896488189697 and accuracy 0.15000000596046448.\n",
      "Test batch 448 with loss 1.8213446140289307 and accuracy 0.20555555820465088.\n",
      "Test batch 449 with loss 1.8138630390167236 and accuracy 0.1319444477558136.\n",
      "Test batch 450 with loss 1.8461040258407593 and accuracy 0.0833333358168602.\n",
      "Test batch 451 with loss 1.7764934301376343 and accuracy 0.27936509251594543.\n",
      "Test batch 452 with loss 1.7911456823349 and accuracy 0.0793650820851326.\n",
      "Test batch 453 with loss 1.789674997329712 and accuracy 0.3750000298023224.\n",
      "Test batch 454 with loss 1.783071517944336 and accuracy 0.2222222238779068.\n",
      "Test batch 455 with loss 1.8034394979476929 and accuracy 0.12222222983837128.\n",
      "Test batch 456 with loss 1.7958561182022095 and accuracy 0.1388888955116272.\n",
      "Test batch 457 with loss 1.769230842590332 and accuracy 0.25.\n",
      "Test batch 458 with loss 1.795445442199707 and accuracy 0.1666666716337204.\n",
      "Test batch 459 with loss 1.8020470142364502 and accuracy 0.12222222983837128.\n",
      "Test batch 460 with loss 1.8360456228256226 and accuracy 0.14444445073604584.\n",
      "Test batch 461 with loss 1.803228735923767 and accuracy 0.15000000596046448.\n",
      "Test batch 462 with loss 1.7725555896759033 and accuracy 0.3611111044883728.\n",
      "Test batch 463 with loss 1.826148271560669 and accuracy 0.07500000298023224.\n",
      "Test batch 464 with loss 1.7871803045272827 and accuracy 0.20000000298023224.\n",
      "Test batch 465 with loss 1.8331778049468994 and accuracy 0.125.\n",
      "Test batch 466 with loss 1.7765676975250244 and accuracy 0.1666666716337204.\n",
      "Test batch 467 with loss 1.7806810140609741 and accuracy 0.13333334028720856.\n",
      "Test batch 468 with loss 1.7696235179901123 and accuracy 0.2361111342906952.\n",
      "Test batch 469 with loss 1.8507839441299438 and accuracy 0.06666667014360428.\n",
      "Test batch 470 with loss 1.8390891551971436 and accuracy 0.0416666679084301.\n",
      "Test batch 471 with loss 1.7559581995010376 and accuracy 0.2888889014720917.\n",
      "Test batch 472 with loss 1.8611047267913818 and accuracy 0.08888889104127884.\n",
      "Test batch 473 with loss 1.854372262954712 and accuracy 0.0833333358168602.\n",
      "Test batch 474 with loss 1.8202358484268188 and accuracy 0.17777778208255768.\n",
      "Test batch 475 with loss 1.7738134860992432 and accuracy 0.1388888955116272.\n",
      "Test batch 476 with loss 1.7983163595199585 and accuracy 0.1666666716337204.\n",
      "Test batch 477 with loss 1.8550916910171509 and accuracy 0.06111111491918564.\n",
      "Test batch 478 with loss 1.7417628765106201 and accuracy 0.14444445073604584.\n",
      "Test batch 479 with loss 1.8072636127471924 and accuracy 0.125.\n",
      "Test batch 480 with loss 1.8457978963851929 and accuracy 0.1666666716337204.\n",
      "Test batch 481 with loss 1.7399545907974243 and accuracy 0.44166669249534607.\n",
      "Test batch 482 with loss 1.8393644094467163 and accuracy 0.0694444477558136.\n",
      "Test batch 483 with loss 1.7663764953613281 and accuracy 0.1597222238779068.\n",
      "Test batch 484 with loss 1.874612808227539 and accuracy 0.1388888955116272.\n",
      "Test batch 485 with loss 1.7134631872177124 and accuracy 0.21388888359069824.\n",
      "Test batch 486 with loss 1.7691023349761963 and accuracy 0.25.\n",
      "Test batch 487 with loss 1.8183298110961914 and accuracy 0.0.\n",
      "Test batch 488 with loss 1.7898622751235962 and accuracy 0.1805555522441864.\n",
      "Test batch 489 with loss 1.797115683555603 and accuracy 0.06666667014360428.\n",
      "Test batch 490 with loss 1.8057546615600586 and accuracy 0.1388888955116272.\n",
      "Test batch 491 with loss 1.808452844619751 and accuracy 0.0625.\n",
      "Test batch 492 with loss 1.9267780780792236 and accuracy 0.07500000298023224.\n",
      "Test batch 493 with loss 1.77742600440979 and accuracy 0.11666667461395264.\n",
      "Test batch 494 with loss 1.8670024871826172 and accuracy 0.18333333730697632.\n",
      "Test batch 495 with loss 1.8814127445220947 and accuracy 0.0833333358168602.\n",
      "Test batch 496 with loss 1.8080142736434937 and accuracy 0.2460317611694336.\n",
      "Test batch 497 with loss 1.7660459280014038 and accuracy 0.25555557012557983.\n",
      "Test batch 498 with loss 1.787715196609497 and accuracy 0.25.\n",
      "Test batch 499 with loss 1.8099896907806396 and accuracy 0.125.\n",
      "Training epoch 1 batch 0 with loss 1.7126582860946655, accuracy 0.20000001788139343.\n",
      "Training epoch 1 batch 1 with loss 1.7341808080673218, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 2 with loss 1.7761650085449219, accuracy 0.28333333134651184.\n",
      "Training epoch 1 batch 3 with loss 1.7288105487823486, accuracy 0.1488095223903656.\n",
      "Training epoch 1 batch 4 with loss 1.7182767391204834, accuracy 0.222222238779068.\n",
      "Training epoch 1 batch 5 with loss 1.7268425226211548, accuracy 0.2916666567325592.\n",
      "Training epoch 1 batch 6 with loss 1.809436559677124, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 7 with loss 1.765594720840454, accuracy 0.130952388048172.\n",
      "Training epoch 1 batch 8 with loss 1.7913881540298462, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 9 with loss 1.7684189081192017, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 10 with loss 1.808717131614685, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 11 with loss 1.7136480808258057, accuracy 0.2361111044883728.\n",
      "Training epoch 1 batch 12 with loss 1.8388049602508545, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 13 with loss 1.7483177185058594, accuracy 0.277777761220932.\n",
      "Training epoch 1 batch 14 with loss 1.7191070318222046, accuracy 0.2666666805744171.\n",
      "Training epoch 1 batch 15 with loss 1.7551158666610718, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 16 with loss 1.7898976802825928, accuracy 0.2698412835597992.\n",
      "Training epoch 1 batch 17 with loss 1.7889273166656494, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 18 with loss 1.786902666091919, accuracy 0.20555555820465088.\n",
      "Training epoch 1 batch 19 with loss 1.78034245967865, accuracy 0.49166667461395264.\n",
      "Training epoch 1 batch 20 with loss 1.725342035293579, accuracy 0.23888888955116272.\n",
      "Training epoch 1 batch 21 with loss 1.8396755456924438, accuracy 0.1031746044754982.\n",
      "Training epoch 1 batch 22 with loss 1.883518934249878, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 23 with loss 1.7805557250976562, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 24 with loss 1.7250988483428955, accuracy 0.347222238779068.\n",
      "Training epoch 1 batch 25 with loss 1.7848665714263916, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 26 with loss 1.7538055181503296, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 27 with loss 1.7398090362548828, accuracy 0.24722222983837128.\n",
      "Training epoch 1 batch 28 with loss 1.8265283107757568, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 29 with loss 1.7749885320663452, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 30 with loss 1.768186330795288, accuracy 0.28333333134651184.\n",
      "Training epoch 1 batch 31 with loss 1.7861108779907227, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 32 with loss 1.7065153121948242, accuracy 0.25.\n",
      "Training epoch 1 batch 33 with loss 1.756387710571289, accuracy 0.018518518656492233.\n",
      "Training epoch 1 batch 34 with loss 1.7232255935668945, accuracy 0.3333333432674408.\n",
      "Training epoch 1 batch 35 with loss 1.7474634647369385, accuracy 0.25.\n",
      "Training epoch 1 batch 36 with loss 1.7042204141616821, accuracy 0.3055555522441864.\n",
      "Training epoch 1 batch 37 with loss 1.7981643676757812, accuracy 0.14166668057441711.\n",
      "Training epoch 1 batch 38 with loss 1.7326319217681885, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 39 with loss 1.7776159048080444, accuracy 0.3499999940395355.\n",
      "Training epoch 1 batch 40 with loss 1.706958532333374, accuracy 0.5083333849906921.\n",
      "Training epoch 1 batch 41 with loss 1.7575805187225342, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 42 with loss 1.7675491571426392, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 43 with loss 1.7948964834213257, accuracy 0.18333333730697632.\n",
      "Training epoch 1 batch 44 with loss 1.6959012746810913, accuracy 0.17380952835083008.\n",
      "Training epoch 1 batch 45 with loss 1.7499592304229736, accuracy 0.3888889253139496.\n",
      "Training epoch 1 batch 46 with loss 1.862052321434021, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 47 with loss 1.7776024341583252, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 48 with loss 1.748772382736206, accuracy 0.222222238779068.\n",
      "Training epoch 1 batch 49 with loss 1.7891772985458374, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 50 with loss 1.8319631814956665, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 51 with loss 1.749925971031189, accuracy 0.1865079402923584.\n",
      "Training epoch 1 batch 52 with loss 1.8324239253997803, accuracy 0.3194444477558136.\n",
      "Training epoch 1 batch 53 with loss 1.7278006076812744, accuracy 0.125.\n",
      "Training epoch 1 batch 54 with loss 1.7900302410125732, accuracy 0.14166666567325592.\n",
      "Training epoch 1 batch 55 with loss 1.7319867610931396, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 56 with loss 1.771178960800171, accuracy 0.16388890147209167.\n",
      "Training epoch 1 batch 57 with loss 1.7352946996688843, accuracy 0.2698412835597992.\n",
      "Training epoch 1 batch 58 with loss 1.8241527080535889, accuracy 0.1488095223903656.\n",
      "Training epoch 1 batch 59 with loss 1.8308016061782837, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 60 with loss 1.7915531396865845, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 61 with loss 1.7838687896728516, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 62 with loss 1.808203935623169, accuracy 0.130952388048172.\n",
      "Training epoch 1 batch 63 with loss 1.7437210083007812, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 64 with loss 1.7820885181427002, accuracy 0.125.\n",
      "Training epoch 1 batch 65 with loss 1.7638435363769531, accuracy 0.4361111521720886.\n",
      "Training epoch 1 batch 66 with loss 1.7929872274398804, accuracy 0.0793650820851326.\n",
      "Training epoch 1 batch 67 with loss 1.8132314682006836, accuracy 0.1071428582072258.\n",
      "Training epoch 1 batch 68 with loss 1.768634557723999, accuracy 0.2420634925365448.\n",
      "Training epoch 1 batch 69 with loss 1.7497278451919556, accuracy 0.23333333432674408.\n",
      "Training epoch 1 batch 70 with loss 1.7547881603240967, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 71 with loss 1.81392502784729, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 72 with loss 1.7248016595840454, accuracy 0.18333333730697632.\n",
      "Training epoch 1 batch 73 with loss 1.76558518409729, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 74 with loss 1.79340398311615, accuracy 0.24166667461395264.\n",
      "Training epoch 1 batch 75 with loss 1.8018745183944702, accuracy 0.19761905074119568.\n",
      "Training epoch 1 batch 76 with loss 1.8748363256454468, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 77 with loss 1.7345008850097656, accuracy 0.10000000149011612.\n",
      "Training epoch 1 batch 78 with loss 1.7601654529571533, accuracy 0.31111112236976624.\n",
      "Training epoch 1 batch 79 with loss 1.7777820825576782, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 80 with loss 1.6640218496322632, accuracy 0.2666666805744171.\n",
      "Training epoch 1 batch 81 with loss 1.7516348361968994, accuracy 0.25.\n",
      "Training epoch 1 batch 82 with loss 1.7617270946502686, accuracy 0.20555555820465088.\n",
      "Training epoch 1 batch 83 with loss 1.7617552280426025, accuracy 0.31111112236976624.\n",
      "Training epoch 1 batch 84 with loss 1.7522341012954712, accuracy 0.1349206417798996.\n",
      "Training epoch 1 batch 85 with loss 1.8225475549697876, accuracy 0.13333334028720856.\n",
      "Training epoch 1 batch 86 with loss 1.7457844018936157, accuracy 0.125.\n",
      "Training epoch 1 batch 87 with loss 1.758236289024353, accuracy 0.3055555820465088.\n",
      "Training epoch 1 batch 88 with loss 1.8185657262802124, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 89 with loss 1.7499945163726807, accuracy 0.2888889014720917.\n",
      "Training epoch 1 batch 90 with loss 1.714827537536621, accuracy 0.22499999403953552.\n",
      "Training epoch 1 batch 91 with loss 1.7484443187713623, accuracy 0.1805555671453476.\n",
      "Training epoch 1 batch 92 with loss 1.6812080144882202, accuracy 0.1785714328289032.\n",
      "Training epoch 1 batch 93 with loss 1.797555923461914, accuracy 0.1349206417798996.\n",
      "Training epoch 1 batch 94 with loss 1.7553284168243408, accuracy 0.29722222685813904.\n",
      "Training epoch 1 batch 95 with loss 1.7571876049041748, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 96 with loss 1.777064323425293, accuracy 0.18611112236976624.\n",
      "Training epoch 1 batch 97 with loss 1.7572208642959595, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 98 with loss 1.775477409362793, accuracy 0.24722222983837128.\n",
      "Training epoch 1 batch 99 with loss 1.7348592281341553, accuracy 0.255952388048172.\n",
      "Training epoch 1 batch 100 with loss 1.7774667739868164, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 101 with loss 1.7750084400177002, accuracy 0.10476191341876984.\n",
      "Training epoch 1 batch 102 with loss 1.8051656484603882, accuracy 0.3194444477558136.\n",
      "Training epoch 1 batch 103 with loss 1.8254029750823975, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 104 with loss 1.7489490509033203, accuracy 0.3293651044368744.\n",
      "Training epoch 1 batch 105 with loss 1.8634897470474243, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 106 with loss 1.737396478652954, accuracy 0.2599206566810608.\n",
      "Training epoch 1 batch 107 with loss 1.7909218072891235, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 108 with loss 1.7984552383422852, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 109 with loss 1.7288974523544312, accuracy 0.1805555671453476.\n",
      "Training epoch 1 batch 110 with loss 1.7141220569610596, accuracy 0.4166666865348816.\n",
      "Training epoch 1 batch 111 with loss 1.7616007328033447, accuracy 0.32777777314186096.\n",
      "Training epoch 1 batch 112 with loss 1.7199938297271729, accuracy 0.3166666626930237.\n",
      "Training epoch 1 batch 113 with loss 1.8342643976211548, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 114 with loss 1.810145616531372, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 115 with loss 1.79620361328125, accuracy 0.130952388048172.\n",
      "Training epoch 1 batch 116 with loss 1.774545431137085, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 117 with loss 1.7196872234344482, accuracy 0.1865079402923584.\n",
      "Training epoch 1 batch 118 with loss 1.7443287372589111, accuracy 0.2361111044883728.\n",
      "Training epoch 1 batch 119 with loss 1.7509324550628662, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 120 with loss 1.8396971225738525, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 121 with loss 1.755204439163208, accuracy 0.4166666865348816.\n",
      "Training epoch 1 batch 122 with loss 1.711717963218689, accuracy 0.26944443583488464.\n",
      "Training epoch 1 batch 123 with loss 1.7913175821304321, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 124 with loss 1.7443675994873047, accuracy 0.3583333492279053.\n",
      "Training epoch 1 batch 125 with loss 1.7372055053710938, accuracy 0.3194444477558136.\n",
      "Training epoch 1 batch 126 with loss 1.766103744506836, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 127 with loss 1.8089956045150757, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 128 with loss 1.8391048908233643, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 129 with loss 1.8186441659927368, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 130 with loss 1.7774722576141357, accuracy 0.25555557012557983.\n",
      "Training epoch 1 batch 131 with loss 1.7645080089569092, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 132 with loss 1.8035017251968384, accuracy 0.1626984179019928.\n",
      "Training epoch 1 batch 133 with loss 1.7687742710113525, accuracy 0.15833333134651184.\n",
      "Training epoch 1 batch 134 with loss 1.683107614517212, accuracy 0.444444477558136.\n",
      "Training epoch 1 batch 135 with loss 1.8065963983535767, accuracy 0.10476191341876984.\n",
      "Training epoch 1 batch 136 with loss 1.7595065832138062, accuracy 0.3083333373069763.\n",
      "Training epoch 1 batch 137 with loss 1.8283706903457642, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 138 with loss 1.7563707828521729, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 139 with loss 1.7990903854370117, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 140 with loss 1.7890713214874268, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 141 with loss 1.7402387857437134, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 142 with loss 1.7535905838012695, accuracy 0.21111111342906952.\n",
      "Training epoch 1 batch 143 with loss 1.7895421981811523, accuracy 0.2777777910232544.\n",
      "Training epoch 1 batch 144 with loss 1.779361367225647, accuracy 0.21944445371627808.\n",
      "Training epoch 1 batch 145 with loss 1.7827517986297607, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 146 with loss 1.752153754234314, accuracy 0.3194444477558136.\n",
      "Training epoch 1 batch 147 with loss 1.74140202999115, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 148 with loss 1.786928415298462, accuracy 0.2916666567325592.\n",
      "Training epoch 1 batch 149 with loss 1.806478500366211, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 150 with loss 1.811216115951538, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 151 with loss 1.7671241760253906, accuracy 0.3571428656578064.\n",
      "Training epoch 1 batch 152 with loss 1.7832485437393188, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 153 with loss 1.6799211502075195, accuracy 0.3055555522441864.\n",
      "Training epoch 1 batch 154 with loss 1.7486375570297241, accuracy 0.1865079402923584.\n",
      "Training epoch 1 batch 155 with loss 1.6992387771606445, accuracy 0.2611111104488373.\n",
      "Training epoch 1 batch 156 with loss 1.761776328086853, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 157 with loss 1.8145395517349243, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 158 with loss 1.7737226486206055, accuracy 0.16825397312641144.\n",
      "Training epoch 1 batch 159 with loss 1.7767117023468018, accuracy 0.25.\n",
      "Training epoch 1 batch 160 with loss 1.780076026916504, accuracy 0.3333333432674408.\n",
      "Training epoch 1 batch 161 with loss 1.7502301931381226, accuracy 0.25555557012557983.\n",
      "Training epoch 1 batch 162 with loss 1.734869360923767, accuracy 0.3444444537162781.\n",
      "Training epoch 1 batch 163 with loss 1.7934259176254272, accuracy 0.0793650820851326.\n",
      "Training epoch 1 batch 164 with loss 1.7601921558380127, accuracy 0.3361110985279083.\n",
      "Training epoch 1 batch 165 with loss 1.8386558294296265, accuracy 0.1349206417798996.\n",
      "Training epoch 1 batch 166 with loss 1.790572166442871, accuracy 0.21111111342906952.\n",
      "Training epoch 1 batch 167 with loss 1.8877757787704468, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 168 with loss 1.734506368637085, accuracy 0.38333335518836975.\n",
      "Training epoch 1 batch 169 with loss 1.75299870967865, accuracy 0.210317462682724.\n",
      "Training epoch 1 batch 170 with loss 1.7168277502059937, accuracy 0.2916666865348816.\n",
      "Training epoch 1 batch 171 with loss 1.7389100790023804, accuracy 0.25555557012557983.\n",
      "Training epoch 1 batch 172 with loss 1.709324836730957, accuracy 0.39444443583488464.\n",
      "Training epoch 1 batch 173 with loss 1.7767889499664307, accuracy 0.28333333134651184.\n",
      "Training epoch 1 batch 174 with loss 1.8192999362945557, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 175 with loss 1.7691290378570557, accuracy 0.17777778208255768.\n",
      "Training epoch 1 batch 176 with loss 1.7973800897598267, accuracy 0.29722222685813904.\n",
      "Training epoch 1 batch 177 with loss 1.7724031209945679, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 178 with loss 1.795654535293579, accuracy 0.12037037312984467.\n",
      "Training epoch 1 batch 179 with loss 1.8525779247283936, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 180 with loss 1.7274805307388306, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 181 with loss 1.7884438037872314, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 182 with loss 1.754577398300171, accuracy 0.23888888955116272.\n",
      "Training epoch 1 batch 183 with loss 1.8340898752212524, accuracy 0.25.\n",
      "Training epoch 1 batch 184 with loss 1.7402846813201904, accuracy 0.222222238779068.\n",
      "Training epoch 1 batch 185 with loss 1.7728391885757446, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 186 with loss 1.780504584312439, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 187 with loss 1.8326088190078735, accuracy 0.125.\n",
      "Training epoch 1 batch 188 with loss 1.7101901769638062, accuracy 0.4166666865348816.\n",
      "Training epoch 1 batch 189 with loss 1.7563930749893188, accuracy 0.25555557012557983.\n",
      "Training epoch 1 batch 190 with loss 1.8225713968276978, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 191 with loss 1.7044376134872437, accuracy 0.31666669249534607.\n",
      "Training epoch 1 batch 192 with loss 1.8063322305679321, accuracy 0.125.\n",
      "Training epoch 1 batch 193 with loss 1.776904821395874, accuracy 0.1210317462682724.\n",
      "Training epoch 1 batch 194 with loss 1.814084768295288, accuracy 0.3055555820465088.\n",
      "Training epoch 1 batch 195 with loss 1.7636207342147827, accuracy 0.32500001788139343.\n",
      "Training epoch 1 batch 196 with loss 1.7638053894042969, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 197 with loss 1.7176545858383179, accuracy 0.38952383399009705.\n",
      "Training epoch 1 batch 198 with loss 1.8377240896224976, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 199 with loss 1.7906150817871094, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 200 with loss 1.7816016674041748, accuracy 0.0.\n",
      "Training epoch 1 batch 201 with loss 1.7591533660888672, accuracy 0.125.\n",
      "Training epoch 1 batch 202 with loss 1.7086089849472046, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 203 with loss 1.828121542930603, accuracy 0.0625.\n",
      "Training epoch 1 batch 204 with loss 1.769984245300293, accuracy 0.24166667461395264.\n",
      "Training epoch 1 batch 205 with loss 1.7958987951278687, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 206 with loss 1.7906970977783203, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 207 with loss 1.7605581283569336, accuracy 0.222222238779068.\n",
      "Training epoch 1 batch 208 with loss 1.7488105297088623, accuracy 0.28333333134651184.\n",
      "Training epoch 1 batch 209 with loss 1.8548028469085693, accuracy 0.02777777798473835.\n",
      "Training epoch 1 batch 210 with loss 1.8164981603622437, accuracy 0.3055555522441864.\n",
      "Training epoch 1 batch 211 with loss 1.7291179895401, accuracy 0.4988095462322235.\n",
      "Training epoch 1 batch 212 with loss 1.7461204528808594, accuracy 0.25.\n",
      "Training epoch 1 batch 213 with loss 1.8358023166656494, accuracy 0.22777777910232544.\n",
      "Training epoch 1 batch 214 with loss 1.778140664100647, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 215 with loss 1.7700344324111938, accuracy 0.1865079402923584.\n",
      "Training epoch 1 batch 216 with loss 1.6734071969985962, accuracy 0.5031746029853821.\n",
      "Training epoch 1 batch 217 with loss 1.8276838064193726, accuracy 0.09444444626569748.\n",
      "Training epoch 1 batch 218 with loss 1.7573429346084595, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 219 with loss 1.7342529296875, accuracy 0.24166667461395264.\n",
      "Training epoch 1 batch 220 with loss 1.852063536643982, accuracy 0.2916666865348816.\n",
      "Training epoch 1 batch 221 with loss 1.7662246227264404, accuracy 0.2916666865348816.\n",
      "Training epoch 1 batch 222 with loss 1.759732961654663, accuracy 0.23333334922790527.\n",
      "Training epoch 1 batch 223 with loss 1.8290321826934814, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 224 with loss 1.7671806812286377, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 225 with loss 1.7549467086791992, accuracy 0.18333333730697632.\n",
      "Training epoch 1 batch 226 with loss 1.7382357120513916, accuracy 0.2611111104488373.\n",
      "Training epoch 1 batch 227 with loss 1.8150975704193115, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 228 with loss 1.7709392309188843, accuracy 0.2708333432674408.\n",
      "Training epoch 1 batch 229 with loss 1.7199513912200928, accuracy 0.2666666805744171.\n",
      "Training epoch 1 batch 230 with loss 1.6963077783584595, accuracy 0.25555557012557983.\n",
      "Training epoch 1 batch 231 with loss 1.8250703811645508, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 232 with loss 1.8099746704101562, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 233 with loss 1.8098255395889282, accuracy 0.1458333432674408.\n",
      "Training epoch 1 batch 234 with loss 1.7004029750823975, accuracy 0.125.\n",
      "Training epoch 1 batch 235 with loss 1.8167310953140259, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 236 with loss 1.856114149093628, accuracy 0.0.\n",
      "Training epoch 1 batch 237 with loss 1.7495574951171875, accuracy 0.23888888955116272.\n",
      "Training epoch 1 batch 238 with loss 1.675585389137268, accuracy 0.2888889014720917.\n",
      "Training epoch 1 batch 239 with loss 1.759469985961914, accuracy 0.2361111044883728.\n",
      "Training epoch 1 batch 240 with loss 1.8007562160491943, accuracy 0.24722221493721008.\n",
      "Training epoch 1 batch 241 with loss 1.7044193744659424, accuracy 0.37222224473953247.\n",
      "Training epoch 1 batch 242 with loss 1.6921100616455078, accuracy 0.20555555820465088.\n",
      "Training epoch 1 batch 243 with loss 1.8578211069107056, accuracy 0.17777778208255768.\n",
      "Training epoch 1 batch 244 with loss 1.7581478357315063, accuracy 0.25.\n",
      "Training epoch 1 batch 245 with loss 1.813269019126892, accuracy 0.347222238779068.\n",
      "Training epoch 1 batch 246 with loss 1.8291953802108765, accuracy 0.24166667461395264.\n",
      "Training epoch 1 batch 247 with loss 1.7707719802856445, accuracy 0.21984127163887024.\n",
      "Training epoch 1 batch 248 with loss 1.7212289571762085, accuracy 0.33888888359069824.\n",
      "Training epoch 1 batch 249 with loss 1.7985057830810547, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 250 with loss 1.778334617614746, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 251 with loss 1.7790409326553345, accuracy 0.204365074634552.\n",
      "Training epoch 1 batch 252 with loss 1.707105278968811, accuracy 0.2202380895614624.\n",
      "Training epoch 1 batch 253 with loss 1.742891550064087, accuracy 0.4722222089767456.\n",
      "Training epoch 1 batch 254 with loss 1.7184633016586304, accuracy 0.2809523940086365.\n",
      "Training epoch 1 batch 255 with loss 1.8416354656219482, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 256 with loss 1.7836710214614868, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 257 with loss 1.7624919414520264, accuracy 0.3253968358039856.\n",
      "Training epoch 1 batch 258 with loss 1.8181556463241577, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 259 with loss 1.778266191482544, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 260 with loss 1.8493547439575195, accuracy 0.125.\n",
      "Training epoch 1 batch 261 with loss 1.6278482675552368, accuracy 0.4000000059604645.\n",
      "Training epoch 1 batch 262 with loss 1.7713558673858643, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 263 with loss 1.7833753824234009, accuracy 0.222222238779068.\n",
      "Training epoch 1 batch 264 with loss 1.800154447555542, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 265 with loss 1.7076536417007446, accuracy 0.25.\n",
      "Training epoch 1 batch 266 with loss 1.7845289707183838, accuracy 0.472222238779068.\n",
      "Training epoch 1 batch 267 with loss 1.725757360458374, accuracy 0.3055555522441864.\n",
      "Training epoch 1 batch 268 with loss 1.7721096277236938, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 269 with loss 1.6632000207901, accuracy 0.2944444417953491.\n",
      "Training epoch 1 batch 270 with loss 1.7711023092269897, accuracy 0.09047619253396988.\n",
      "Training epoch 1 batch 271 with loss 1.8638741970062256, accuracy 0.18518519401550293.\n",
      "Training epoch 1 batch 272 with loss 1.7850490808486938, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 273 with loss 1.8162893056869507, accuracy 0.3305555582046509.\n",
      "Training epoch 1 batch 274 with loss 1.7262777090072632, accuracy 0.1458333432674408.\n",
      "Training epoch 1 batch 275 with loss 1.7740228176116943, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 276 with loss 1.754485845565796, accuracy 0.32698413729667664.\n",
      "Training epoch 1 batch 277 with loss 1.7376060485839844, accuracy 0.15555556118488312.\n",
      "Training epoch 1 batch 278 with loss 1.7782407999038696, accuracy 0.2083333283662796.\n",
      "Training epoch 1 batch 279 with loss 1.7358906269073486, accuracy 0.2611111104488373.\n",
      "Training epoch 1 batch 280 with loss 1.7764155864715576, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 281 with loss 1.7420988082885742, accuracy 0.23055556416511536.\n",
      "Training epoch 1 batch 282 with loss 1.8803274631500244, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 283 with loss 1.760353446006775, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 284 with loss 1.7556883096694946, accuracy 0.3680555820465088.\n",
      "Training epoch 1 batch 285 with loss 1.762097954750061, accuracy 0.375.\n",
      "Training epoch 1 batch 286 with loss 1.7980937957763672, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 287 with loss 1.7721647024154663, accuracy 0.19166667759418488.\n",
      "Training epoch 1 batch 288 with loss 1.7705646753311157, accuracy 0.22777777910232544.\n",
      "Training epoch 1 batch 289 with loss 1.7620508670806885, accuracy 0.236111119389534.\n",
      "Training epoch 1 batch 290 with loss 1.7142484188079834, accuracy 0.35555556416511536.\n",
      "Training epoch 1 batch 291 with loss 1.7797445058822632, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 292 with loss 1.7628587484359741, accuracy 0.3769841194152832.\n",
      "Training epoch 1 batch 293 with loss 1.7189843654632568, accuracy 0.33888888359069824.\n",
      "Training epoch 1 batch 294 with loss 1.8222707509994507, accuracy 0.236111119389534.\n",
      "Training epoch 1 batch 295 with loss 1.7152044773101807, accuracy 0.25.\n",
      "Training epoch 1 batch 296 with loss 1.773374319076538, accuracy 0.2361111044883728.\n",
      "Training epoch 1 batch 297 with loss 1.775835394859314, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 298 with loss 1.7590858936309814, accuracy 0.144841268658638.\n",
      "Training epoch 1 batch 299 with loss 1.900684118270874, accuracy 0.1875.\n",
      "Training epoch 1 batch 300 with loss 1.7323055267333984, accuracy 0.5416666865348816.\n",
      "Training epoch 1 batch 301 with loss 1.7894176244735718, accuracy 0.125.\n",
      "Training epoch 1 batch 302 with loss 1.761765718460083, accuracy 0.3055555522441864.\n",
      "Training epoch 1 batch 303 with loss 1.7000484466552734, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 304 with loss 1.762616753578186, accuracy 0.375.\n",
      "Training epoch 1 batch 305 with loss 1.7611805200576782, accuracy 0.3472222089767456.\n",
      "Training epoch 1 batch 306 with loss 1.728882074356079, accuracy 0.21388888359069824.\n",
      "Training epoch 1 batch 307 with loss 1.8031011819839478, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 308 with loss 1.776394248008728, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 309 with loss 1.8497438430786133, accuracy 0.125.\n",
      "Training epoch 1 batch 310 with loss 1.7092506885528564, accuracy 0.5.\n",
      "Training epoch 1 batch 311 with loss 1.815718412399292, accuracy 0.125.\n",
      "Training epoch 1 batch 312 with loss 1.7610950469970703, accuracy 0.31111112236976624.\n",
      "Training epoch 1 batch 313 with loss 1.8362400531768799, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 314 with loss 1.8644564151763916, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 315 with loss 1.727209448814392, accuracy 0.15833333134651184.\n",
      "Training epoch 1 batch 316 with loss 1.743564248085022, accuracy 0.2611111104488373.\n",
      "Training epoch 1 batch 317 with loss 1.8414480686187744, accuracy 0.20000001788139343.\n",
      "Training epoch 1 batch 318 with loss 1.8111613988876343, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 319 with loss 1.8543736934661865, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 320 with loss 1.804604172706604, accuracy 0.06111111491918564.\n",
      "Training epoch 1 batch 321 with loss 1.7274739742279053, accuracy 0.20555555820465088.\n",
      "Training epoch 1 batch 322 with loss 1.7975397109985352, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 323 with loss 1.8962396383285522, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 324 with loss 1.7734663486480713, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 325 with loss 1.7056957483291626, accuracy 0.3888888955116272.\n",
      "Training epoch 1 batch 326 with loss 1.7942298650741577, accuracy 0.2361111044883728.\n",
      "Training epoch 1 batch 327 with loss 1.7414153814315796, accuracy 0.2611111104488373.\n",
      "Training epoch 1 batch 328 with loss 1.7516247034072876, accuracy 0.25833335518836975.\n",
      "Training epoch 1 batch 329 with loss 1.7869571447372437, accuracy 0.3115079402923584.\n",
      "Training epoch 1 batch 330 with loss 1.813399076461792, accuracy 0.2916666865348816.\n",
      "Training epoch 1 batch 331 with loss 1.7262938022613525, accuracy 0.4166666865348816.\n",
      "Training epoch 1 batch 332 with loss 1.6585865020751953, accuracy 0.25555557012557983.\n",
      "Training epoch 1 batch 333 with loss 1.7517402172088623, accuracy 0.20000001788139343.\n",
      "Training epoch 1 batch 334 with loss 1.8473745584487915, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 335 with loss 1.7729332447052002, accuracy 0.1805555671453476.\n",
      "Training epoch 1 batch 336 with loss 1.7000267505645752, accuracy 0.4555555582046509.\n",
      "Training epoch 1 batch 337 with loss 1.7189433574676514, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 338 with loss 1.7406575679779053, accuracy 0.3571428656578064.\n",
      "Training epoch 1 batch 339 with loss 1.7808315753936768, accuracy 0.2888889014720917.\n",
      "Training epoch 1 batch 340 with loss 1.7694374322891235, accuracy 0.21666666865348816.\n",
      "Training epoch 1 batch 341 with loss 1.842765212059021, accuracy 0.125.\n",
      "Training epoch 1 batch 342 with loss 1.8575494289398193, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 343 with loss 1.9075400829315186, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 344 with loss 1.7475175857543945, accuracy 0.24166665971279144.\n",
      "Training epoch 1 batch 345 with loss 1.8466870784759521, accuracy 0.20555555820465088.\n",
      "Training epoch 1 batch 346 with loss 1.7701780796051025, accuracy 0.23333334922790527.\n",
      "Training epoch 1 batch 347 with loss 1.8105192184448242, accuracy 0.0476190485060215.\n",
      "Training epoch 1 batch 348 with loss 1.744309425354004, accuracy 0.31666669249534607.\n",
      "Training epoch 1 batch 349 with loss 1.8188276290893555, accuracy 0.2777777910232544.\n",
      "Training epoch 1 batch 350 with loss 1.6885912418365479, accuracy 0.19722223281860352.\n",
      "Training epoch 1 batch 351 with loss 1.8537328243255615, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 352 with loss 1.779919981956482, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 353 with loss 1.7716236114501953, accuracy 0.19166666269302368.\n",
      "Training epoch 1 batch 354 with loss 1.7530357837677002, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 355 with loss 1.8204967975616455, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 356 with loss 1.7999035120010376, accuracy 0.10833333432674408.\n",
      "Training epoch 1 batch 357 with loss 1.803531289100647, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 358 with loss 1.7957041263580322, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 359 with loss 1.7925786972045898, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 360 with loss 1.7050883769989014, accuracy 0.3611111044883728.\n",
      "Training epoch 1 batch 361 with loss 1.765641450881958, accuracy 0.2916666567325592.\n",
      "Training epoch 1 batch 362 with loss 1.7490050792694092, accuracy 0.15833333134651184.\n",
      "Training epoch 1 batch 363 with loss 1.794719934463501, accuracy 0.10277777910232544.\n",
      "Training epoch 1 batch 364 with loss 1.7888435125350952, accuracy 0.10833333432674408.\n",
      "Training epoch 1 batch 365 with loss 1.7240772247314453, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 366 with loss 1.744428277015686, accuracy 0.3444444537162781.\n",
      "Training epoch 1 batch 367 with loss 1.827242136001587, accuracy 0.08095238357782364.\n",
      "Training epoch 1 batch 368 with loss 1.668280839920044, accuracy 0.3174603283405304.\n",
      "Training epoch 1 batch 369 with loss 1.7454252243041992, accuracy 0.25.\n",
      "Training epoch 1 batch 370 with loss 1.7718651294708252, accuracy 0.347222238779068.\n",
      "Training epoch 1 batch 371 with loss 1.7516340017318726, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 372 with loss 1.655487060546875, accuracy 0.5138888955116272.\n",
      "Training epoch 1 batch 373 with loss 1.7365026473999023, accuracy 0.18333333730697632.\n",
      "Training epoch 1 batch 374 with loss 1.667557716369629, accuracy 0.2805555462837219.\n",
      "Training epoch 1 batch 375 with loss 1.8074254989624023, accuracy 0.25555557012557983.\n",
      "Training epoch 1 batch 376 with loss 1.7681773900985718, accuracy 0.190476194024086.\n",
      "Training epoch 1 batch 377 with loss 1.7132762670516968, accuracy 0.36666667461395264.\n",
      "Training epoch 1 batch 378 with loss 1.7697808742523193, accuracy 0.19761905074119568.\n",
      "Training epoch 1 batch 379 with loss 1.7666791677474976, accuracy 0.2916666865348816.\n",
      "Training epoch 1 batch 380 with loss 1.79754638671875, accuracy 0.19166666269302368.\n",
      "Training epoch 1 batch 381 with loss 1.7667055130004883, accuracy 0.2916666865348816.\n",
      "Training epoch 1 batch 382 with loss 1.7802398204803467, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 383 with loss 1.7500312328338623, accuracy 0.3305555582046509.\n",
      "Training epoch 1 batch 384 with loss 1.7261215448379517, accuracy 0.3750000298023224.\n",
      "Training epoch 1 batch 385 with loss 1.7917760610580444, accuracy 0.1865079402923584.\n",
      "Training epoch 1 batch 386 with loss 1.8605419397354126, accuracy 0.13055555522441864.\n",
      "Training epoch 1 batch 387 with loss 1.7009432315826416, accuracy 0.27222222089767456.\n",
      "Training epoch 1 batch 388 with loss 1.867401361465454, accuracy 0.19166666269302368.\n",
      "Training epoch 1 batch 389 with loss 1.754404067993164, accuracy 0.2182539701461792.\n",
      "Training epoch 1 batch 390 with loss 1.8452354669570923, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 391 with loss 1.776698112487793, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 392 with loss 1.8534437417984009, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 393 with loss 1.7575010061264038, accuracy 0.190476194024086.\n",
      "Training epoch 1 batch 394 with loss 1.7913767099380493, accuracy 0.35277777910232544.\n",
      "Training epoch 1 batch 395 with loss 1.749297857284546, accuracy 0.16388890147209167.\n",
      "Training epoch 1 batch 396 with loss 1.7192249298095703, accuracy 0.347222238779068.\n",
      "Training epoch 1 batch 397 with loss 1.728079080581665, accuracy 0.2460317611694336.\n",
      "Training epoch 1 batch 398 with loss 1.8383843898773193, accuracy 0.0694444477558136.\n",
      "Training epoch 1 batch 399 with loss 1.6731064319610596, accuracy 0.25.\n",
      "Training epoch 1 batch 400 with loss 1.7855005264282227, accuracy 0.3611111044883728.\n",
      "Training epoch 1 batch 401 with loss 1.7391971349716187, accuracy 0.2944444417953491.\n",
      "Training epoch 1 batch 402 with loss 1.7924152612686157, accuracy 0.2361111044883728.\n",
      "Training epoch 1 batch 403 with loss 1.8191741704940796, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 404 with loss 1.817460298538208, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 405 with loss 1.723472237586975, accuracy 0.30277779698371887.\n",
      "Training epoch 1 batch 406 with loss 1.808707594871521, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 407 with loss 1.802083969116211, accuracy 0.1865079402923584.\n",
      "Training epoch 1 batch 408 with loss 1.7009328603744507, accuracy 0.4833333492279053.\n",
      "Training epoch 1 batch 409 with loss 1.7506908178329468, accuracy 0.2111111283302307.\n",
      "Training epoch 1 batch 410 with loss 1.7849490642547607, accuracy 0.2777777910232544.\n",
      "Training epoch 1 batch 411 with loss 1.6991198062896729, accuracy 0.4000000059604645.\n",
      "Training epoch 1 batch 412 with loss 1.8422105312347412, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 413 with loss 1.7472000122070312, accuracy 0.25.\n",
      "Training epoch 1 batch 414 with loss 1.6588528156280518, accuracy 0.4833333194255829.\n",
      "Training epoch 1 batch 415 with loss 1.7623897790908813, accuracy 0.13750000298023224.\n",
      "Training epoch 1 batch 416 with loss 1.6936308145523071, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 417 with loss 1.7235758304595947, accuracy 0.190476194024086.\n",
      "Training epoch 1 batch 418 with loss 1.8735332489013672, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 419 with loss 1.7719370126724243, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 420 with loss 1.798584222793579, accuracy 0.02083333395421505.\n",
      "Training epoch 1 batch 421 with loss 1.8395713567733765, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 422 with loss 1.6852372884750366, accuracy 0.2888889014720917.\n",
      "Training epoch 1 batch 423 with loss 1.8182356357574463, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 424 with loss 1.7847007513046265, accuracy 0.23888888955116272.\n",
      "Training epoch 1 batch 425 with loss 1.7714595794677734, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 426 with loss 1.7438026666641235, accuracy 0.3861111104488373.\n",
      "Training epoch 1 batch 427 with loss 1.7705469131469727, accuracy 0.3194444477558136.\n",
      "Training epoch 1 batch 428 with loss 1.7645437717437744, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 429 with loss 1.7476389408111572, accuracy 0.25.\n",
      "Training epoch 1 batch 430 with loss 1.8118892908096313, accuracy 0.2916666865348816.\n",
      "Training epoch 1 batch 431 with loss 1.68595290184021, accuracy 0.36944445967674255.\n",
      "Training epoch 1 batch 432 with loss 1.8136436939239502, accuracy 0.0694444477558136.\n",
      "Training epoch 1 batch 433 with loss 1.8542912006378174, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 434 with loss 1.9200506210327148, accuracy 0.02380952425301075.\n",
      "Training epoch 1 batch 435 with loss 1.8030105829238892, accuracy 0.10000000149011612.\n",
      "Training epoch 1 batch 436 with loss 1.7086915969848633, accuracy 0.36666667461395264.\n",
      "Training epoch 1 batch 437 with loss 1.7891967296600342, accuracy 0.26944446563720703.\n",
      "Training epoch 1 batch 438 with loss 1.8237688541412354, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 439 with loss 1.7476470470428467, accuracy 0.20158730447292328.\n",
      "Training epoch 1 batch 440 with loss 1.7211380004882812, accuracy 0.2666666805744171.\n",
      "Training epoch 1 batch 441 with loss 1.8015018701553345, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 442 with loss 1.8382031917572021, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 443 with loss 1.8130242824554443, accuracy 0.22777777910232544.\n",
      "Training epoch 1 batch 444 with loss 1.727662444114685, accuracy 0.222222238779068.\n",
      "Training epoch 1 batch 445 with loss 1.745819091796875, accuracy 0.24166667461395264.\n",
      "Training epoch 1 batch 446 with loss 1.7865549325942993, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 447 with loss 1.7562177181243896, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 448 with loss 1.7945696115493774, accuracy 0.23333334922790527.\n",
      "Training epoch 1 batch 449 with loss 1.8037726879119873, accuracy 0.24166667461395264.\n",
      "Training epoch 1 batch 450 with loss 1.890740990638733, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 451 with loss 1.8977811336517334, accuracy 0.10277777910232544.\n",
      "Training epoch 1 batch 452 with loss 1.9039843082427979, accuracy 0.1041666716337204.\n",
      "Training epoch 1 batch 453 with loss 1.8437280654907227, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 454 with loss 1.6662927865982056, accuracy 0.5000000596046448.\n",
      "Training epoch 1 batch 455 with loss 1.8317883014678955, accuracy 0.0892857164144516.\n",
      "Training epoch 1 batch 456 with loss 1.7993876934051514, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 457 with loss 1.731231927871704, accuracy 0.2805555462837219.\n",
      "Training epoch 1 batch 458 with loss 1.7235187292099, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 459 with loss 1.79581618309021, accuracy 0.255952388048172.\n",
      "Training epoch 1 batch 460 with loss 1.8441715240478516, accuracy 0.21944445371627808.\n",
      "Training epoch 1 batch 461 with loss 1.8352504968643188, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 462 with loss 1.7894794940948486, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 463 with loss 1.7968820333480835, accuracy 0.2750000059604645.\n",
      "Training epoch 1 batch 464 with loss 1.7140781879425049, accuracy 0.32500001788139343.\n",
      "Training epoch 1 batch 465 with loss 1.769025444984436, accuracy 0.2182539701461792.\n",
      "Training epoch 1 batch 466 with loss 1.8217636346817017, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 467 with loss 1.845266580581665, accuracy 0.08095238357782364.\n",
      "Training epoch 1 batch 468 with loss 1.872290849685669, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 469 with loss 1.7523517608642578, accuracy 0.3055555522441864.\n",
      "Training epoch 1 batch 470 with loss 1.8248275518417358, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 471 with loss 1.8136132955551147, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 472 with loss 1.7334299087524414, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 473 with loss 1.7666435241699219, accuracy 0.3333333432674408.\n",
      "Training epoch 1 batch 474 with loss 1.7532905340194702, accuracy 0.12962962687015533.\n",
      "Training epoch 1 batch 475 with loss 1.7907030582427979, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 476 with loss 1.7089128494262695, accuracy 0.1805555671453476.\n",
      "Training epoch 1 batch 477 with loss 1.8347432613372803, accuracy 0.125.\n",
      "Training epoch 1 batch 478 with loss 1.7812201976776123, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 479 with loss 1.8723933696746826, accuracy 0.25.\n",
      "Training epoch 1 batch 480 with loss 1.7731624841690063, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 481 with loss 1.8228187561035156, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 482 with loss 1.8282665014266968, accuracy 0.0892857164144516.\n",
      "Training epoch 1 batch 483 with loss 1.802109956741333, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 484 with loss 1.7058340311050415, accuracy 0.4305555820465088.\n",
      "Training epoch 1 batch 485 with loss 1.7957671880722046, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 486 with loss 1.7906181812286377, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 487 with loss 1.790657639503479, accuracy 0.14166666567325592.\n",
      "Training epoch 1 batch 488 with loss 1.8095964193344116, accuracy 0.0.\n",
      "Training epoch 1 batch 489 with loss 1.7602310180664062, accuracy 0.12380952388048172.\n",
      "Training epoch 1 batch 490 with loss 1.7272913455963135, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 491 with loss 1.7546545267105103, accuracy 0.130952388048172.\n",
      "Training epoch 1 batch 492 with loss 1.694383978843689, accuracy 0.13055555522441864.\n",
      "Training epoch 1 batch 493 with loss 1.7473781108856201, accuracy 0.29722222685813904.\n",
      "Training epoch 1 batch 494 with loss 1.756224274635315, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 495 with loss 1.7515325546264648, accuracy 0.13333334028720856.\n",
      "Training epoch 1 batch 496 with loss 1.8280165195465088, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 497 with loss 1.8463373184204102, accuracy 0.125.\n",
      "Training epoch 1 batch 498 with loss 1.793827772140503, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 499 with loss 1.8478214740753174, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 500 with loss 1.8191181421279907, accuracy 0.125.\n",
      "Training epoch 1 batch 501 with loss 1.9123690128326416, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 502 with loss 1.7243105173110962, accuracy 0.19166666269302368.\n",
      "Training epoch 1 batch 503 with loss 1.826664686203003, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 504 with loss 1.6710960865020752, accuracy 0.35277777910232544.\n",
      "Training epoch 1 batch 505 with loss 1.7339274883270264, accuracy 0.20317460596561432.\n",
      "Training epoch 1 batch 506 with loss 1.9003143310546875, accuracy 0.2916666865348816.\n",
      "Training epoch 1 batch 507 with loss 1.7606958150863647, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 508 with loss 1.7669578790664673, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 509 with loss 1.7708985805511475, accuracy 0.2750000059604645.\n",
      "Training epoch 1 batch 510 with loss 1.8337169885635376, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 511 with loss 1.8388217687606812, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 512 with loss 1.7130882740020752, accuracy 0.2777777910232544.\n",
      "Training epoch 1 batch 513 with loss 1.825990080833435, accuracy 0.125.\n",
      "Training epoch 1 batch 514 with loss 1.7836740016937256, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 515 with loss 1.7881160974502563, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 516 with loss 1.790474534034729, accuracy 0.25.\n",
      "Training epoch 1 batch 517 with loss 1.7770605087280273, accuracy 0.3583333492279053.\n",
      "Training epoch 1 batch 518 with loss 1.8238751888275146, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 519 with loss 1.7884588241577148, accuracy 0.2083333283662796.\n",
      "Training epoch 1 batch 520 with loss 1.7739454507827759, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 521 with loss 1.7464096546173096, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 522 with loss 1.8088557720184326, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 523 with loss 1.7608953714370728, accuracy 0.25555557012557983.\n",
      "Training epoch 1 batch 524 with loss 1.7682583332061768, accuracy 0.236111119389534.\n",
      "Training epoch 1 batch 525 with loss 1.6967003345489502, accuracy 0.190476194024086.\n",
      "Training epoch 1 batch 526 with loss 1.7606351375579834, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 527 with loss 1.871422529220581, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 528 with loss 1.7961841821670532, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 529 with loss 1.7857965230941772, accuracy 0.1726190447807312.\n",
      "Training epoch 1 batch 530 with loss 1.7579662799835205, accuracy 0.3194444477558136.\n",
      "Training epoch 1 batch 531 with loss 1.8966131210327148, accuracy 0.02083333395421505.\n",
      "Training epoch 1 batch 532 with loss 1.8354088068008423, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 533 with loss 1.8534669876098633, accuracy 0.11666666716337204.\n",
      "Training epoch 1 batch 534 with loss 1.8080819845199585, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 535 with loss 1.8666174411773682, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 536 with loss 1.812491774559021, accuracy 0.25555554032325745.\n",
      "Training epoch 1 batch 537 with loss 1.7948100566864014, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 538 with loss 1.7936546802520752, accuracy 0.2182539701461792.\n",
      "Training epoch 1 batch 539 with loss 1.7897627353668213, accuracy 0.2182539701461792.\n",
      "Training epoch 1 batch 540 with loss 1.8249276876449585, accuracy 0.13055555522441864.\n",
      "Training epoch 1 batch 541 with loss 1.7155414819717407, accuracy 0.2361111044883728.\n",
      "Training epoch 1 batch 542 with loss 1.8194830417633057, accuracy 0.4027777910232544.\n",
      "Training epoch 1 batch 543 with loss 1.756485939025879, accuracy 0.13333334028720856.\n",
      "Training epoch 1 batch 544 with loss 1.8162696361541748, accuracy 0.0694444477558136.\n",
      "Training epoch 1 batch 545 with loss 1.7347345352172852, accuracy 0.19722223281860352.\n",
      "Training epoch 1 batch 546 with loss 1.7424938678741455, accuracy 0.4583333432674408.\n",
      "Training epoch 1 batch 547 with loss 1.8767445087432861, accuracy 0.0.\n",
      "Training epoch 1 batch 548 with loss 1.825840711593628, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 549 with loss 1.789594054222107, accuracy 0.1587301641702652.\n",
      "Training epoch 1 batch 550 with loss 1.6974388360977173, accuracy 0.2726190686225891.\n",
      "Training epoch 1 batch 551 with loss 1.7249829769134521, accuracy 0.21944445371627808.\n",
      "Training epoch 1 batch 552 with loss 1.7304102182388306, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 553 with loss 1.733402967453003, accuracy 0.2777777910232544.\n",
      "Training epoch 1 batch 554 with loss 1.7834402322769165, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 555 with loss 1.7289655208587646, accuracy 0.3583333492279053.\n",
      "Training epoch 1 batch 556 with loss 1.7766635417938232, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 557 with loss 1.8102480173110962, accuracy 0.2611111104488373.\n",
      "Training epoch 1 batch 558 with loss 1.78347909450531, accuracy 0.13333334028720856.\n",
      "Training epoch 1 batch 559 with loss 1.7793874740600586, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 560 with loss 1.8623555898666382, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 561 with loss 1.7758233547210693, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 562 with loss 1.8158800601959229, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 563 with loss 1.7986230850219727, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 564 with loss 1.771593451499939, accuracy 0.21111111342906952.\n",
      "Training epoch 1 batch 565 with loss 1.818621039390564, accuracy 0.125.\n",
      "Training epoch 1 batch 566 with loss 1.8090747594833374, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 567 with loss 1.7935197353363037, accuracy 0.29722222685813904.\n",
      "Training epoch 1 batch 568 with loss 1.7595189809799194, accuracy 0.36666667461395264.\n",
      "Training epoch 1 batch 569 with loss 1.708544373512268, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 570 with loss 1.747462272644043, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 571 with loss 1.7340024709701538, accuracy 0.3194444477558136.\n",
      "Training epoch 1 batch 572 with loss 1.723981499671936, accuracy 0.2291666716337204.\n",
      "Training epoch 1 batch 573 with loss 1.7413930892944336, accuracy 0.23888888955116272.\n",
      "Training epoch 1 batch 574 with loss 1.8017833232879639, accuracy 0.15833333134651184.\n",
      "Training epoch 1 batch 575 with loss 1.7803919315338135, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 576 with loss 1.8068926334381104, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 577 with loss 1.7935653924942017, accuracy 0.31111112236976624.\n",
      "Training epoch 1 batch 578 with loss 1.8495981693267822, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 579 with loss 1.7820020914077759, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 580 with loss 1.7565962076187134, accuracy 0.17777778208255768.\n",
      "Training epoch 1 batch 581 with loss 1.676587700843811, accuracy 0.347222238779068.\n",
      "Training epoch 1 batch 582 with loss 1.788450002670288, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 583 with loss 1.721864938735962, accuracy 0.0714285746216774.\n",
      "Training epoch 1 batch 584 with loss 1.7984740734100342, accuracy 0.2809523940086365.\n",
      "Training epoch 1 batch 585 with loss 1.8499510288238525, accuracy 0.10833333432674408.\n",
      "Training epoch 1 batch 586 with loss 1.781529188156128, accuracy 0.25555557012557983.\n",
      "Training epoch 1 batch 587 with loss 1.8077268600463867, accuracy 0.125.\n",
      "Training epoch 1 batch 588 with loss 1.7609704732894897, accuracy 0.25.\n",
      "Training epoch 1 batch 589 with loss 1.802551031112671, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 590 with loss 1.7048813104629517, accuracy 0.23333333432674408.\n",
      "Training epoch 1 batch 591 with loss 1.8108364343643188, accuracy 0.17777778208255768.\n",
      "Training epoch 1 batch 592 with loss 1.8684056997299194, accuracy 0.02380952425301075.\n",
      "Training epoch 1 batch 593 with loss 1.702582597732544, accuracy 0.20555555820465088.\n",
      "Training epoch 1 batch 594 with loss 1.798150658607483, accuracy 0.0625.\n",
      "Training epoch 1 batch 595 with loss 1.809851050376892, accuracy 0.2750000059604645.\n",
      "Training epoch 1 batch 596 with loss 1.7554051876068115, accuracy 0.3055555820465088.\n",
      "Training epoch 1 batch 597 with loss 1.7887470722198486, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 598 with loss 1.780418038368225, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 599 with loss 1.7754144668579102, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 600 with loss 1.8373092412948608, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 601 with loss 1.8514868021011353, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 602 with loss 1.795715570449829, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 603 with loss 1.8174524307250977, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 604 with loss 1.7932685613632202, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 605 with loss 1.7863447666168213, accuracy 0.23333334922790527.\n",
      "Training epoch 1 batch 606 with loss 1.7804514169692993, accuracy 0.24166665971279144.\n",
      "Training epoch 1 batch 607 with loss 1.763030767440796, accuracy 0.25.\n",
      "Training epoch 1 batch 608 with loss 1.706171989440918, accuracy 0.3305555582046509.\n",
      "Training epoch 1 batch 609 with loss 1.8380868434906006, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 610 with loss 1.756155014038086, accuracy 0.18611110746860504.\n",
      "Training epoch 1 batch 611 with loss 1.791990041732788, accuracy 0.2777777910232544.\n",
      "Training epoch 1 batch 612 with loss 1.8085275888442993, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 613 with loss 1.7920353412628174, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 614 with loss 1.7684987783432007, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 615 with loss 1.8379030227661133, accuracy 0.17777778208255768.\n",
      "Training epoch 1 batch 616 with loss 1.7851145267486572, accuracy 0.20555555820465088.\n",
      "Training epoch 1 batch 617 with loss 1.8251512050628662, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 618 with loss 1.7797095775604248, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 619 with loss 1.824609398841858, accuracy 0.25555557012557983.\n",
      "Training epoch 1 batch 620 with loss 1.8397353887557983, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 621 with loss 1.7693687677383423, accuracy 0.2777777910232544.\n",
      "Training epoch 1 batch 622 with loss 1.7317674160003662, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 623 with loss 1.8702017068862915, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 624 with loss 1.7651857137680054, accuracy 0.236111119389534.\n",
      "Training epoch 1 batch 625 with loss 1.7641916275024414, accuracy 0.2777777910232544.\n",
      "Training epoch 1 batch 626 with loss 1.7412736415863037, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 627 with loss 1.7024800777435303, accuracy 0.2111111283302307.\n",
      "Training epoch 1 batch 628 with loss 1.8128767013549805, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 629 with loss 1.8747276067733765, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 630 with loss 1.7431846857070923, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 631 with loss 1.797888994216919, accuracy 0.125.\n",
      "Training epoch 1 batch 632 with loss 1.8334248065948486, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 633 with loss 1.7607409954071045, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 634 with loss 1.7517801523208618, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 635 with loss 1.8533449172973633, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 636 with loss 1.816357970237732, accuracy 0.222222238779068.\n",
      "Training epoch 1 batch 637 with loss 1.7647985219955444, accuracy 0.25.\n",
      "Training epoch 1 batch 638 with loss 1.7721145153045654, accuracy 0.20555555820465088.\n",
      "Training epoch 1 batch 639 with loss 1.7621288299560547, accuracy 0.25.\n",
      "Training epoch 1 batch 640 with loss 1.8863317966461182, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 641 with loss 1.8832120895385742, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 642 with loss 1.7554121017456055, accuracy 0.41111111640930176.\n",
      "Training epoch 1 batch 643 with loss 1.8010215759277344, accuracy 0.1071428582072258.\n",
      "Training epoch 1 batch 644 with loss 1.827613115310669, accuracy 0.1458333432674408.\n",
      "Training epoch 1 batch 645 with loss 1.7331016063690186, accuracy 0.2142857164144516.\n",
      "Training epoch 1 batch 646 with loss 1.7721807956695557, accuracy 0.444444477558136.\n",
      "Training epoch 1 batch 647 with loss 1.8749005794525146, accuracy 0.10000000149011612.\n",
      "Training epoch 1 batch 648 with loss 1.8920732736587524, accuracy 0.0763888880610466.\n",
      "Training epoch 1 batch 649 with loss 1.7870218753814697, accuracy 0.23888888955116272.\n",
      "Training epoch 1 batch 650 with loss 1.791033387184143, accuracy 0.236111119389534.\n",
      "Training epoch 1 batch 651 with loss 1.8394908905029297, accuracy 0.31111112236976624.\n",
      "Training epoch 1 batch 652 with loss 1.8741823434829712, accuracy 0.0.\n",
      "Training epoch 1 batch 653 with loss 1.8673816919326782, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 654 with loss 1.7433007955551147, accuracy 0.3611111044883728.\n",
      "Training epoch 1 batch 655 with loss 1.7444446086883545, accuracy 0.17777778208255768.\n",
      "Training epoch 1 batch 656 with loss 1.792414665222168, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 657 with loss 1.8986446857452393, accuracy 0.111111119389534.\n",
      "Training epoch 1 batch 658 with loss 1.8228861093521118, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 659 with loss 1.7098588943481445, accuracy 0.28333333134651184.\n",
      "Training epoch 1 batch 660 with loss 1.745631456375122, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 661 with loss 1.7852857112884521, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 662 with loss 1.7639968395233154, accuracy 0.2888889014720917.\n",
      "Training epoch 1 batch 663 with loss 1.7112443447113037, accuracy 0.5111111402511597.\n",
      "Training epoch 1 batch 664 with loss 1.765883445739746, accuracy 0.3174603283405304.\n",
      "Training epoch 1 batch 665 with loss 1.9032227993011475, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 666 with loss 1.7979004383087158, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 667 with loss 1.8341073989868164, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 668 with loss 1.8243646621704102, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 669 with loss 1.7787086963653564, accuracy 0.3333333432674408.\n",
      "Training epoch 1 batch 670 with loss 1.7873023748397827, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 671 with loss 1.8343368768692017, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 672 with loss 1.801804780960083, accuracy 0.2003968209028244.\n",
      "Training epoch 1 batch 673 with loss 1.6918150186538696, accuracy 0.45000001788139343.\n",
      "Training epoch 1 batch 674 with loss 1.742682695388794, accuracy 0.4722222089767456.\n",
      "Training epoch 1 batch 675 with loss 1.712620496749878, accuracy 0.22777777910232544.\n",
      "Training epoch 1 batch 676 with loss 1.8441107273101807, accuracy 0.21388889849185944.\n",
      "Training epoch 1 batch 677 with loss 1.881252646446228, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 678 with loss 1.752124547958374, accuracy 0.19166667759418488.\n",
      "Training epoch 1 batch 679 with loss 1.8078505992889404, accuracy 0.02777777798473835.\n",
      "Training epoch 1 batch 680 with loss 1.7509084939956665, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 681 with loss 1.711507797241211, accuracy 0.33194446563720703.\n",
      "Training epoch 1 batch 682 with loss 1.8404242992401123, accuracy 0.2750000059604645.\n",
      "Training epoch 1 batch 683 with loss 1.792525291442871, accuracy 0.19761905074119568.\n",
      "Training epoch 1 batch 684 with loss 1.7924623489379883, accuracy 0.125.\n",
      "Training epoch 1 batch 685 with loss 1.7715009450912476, accuracy 0.28333333134651184.\n",
      "Training epoch 1 batch 686 with loss 1.7038733959197998, accuracy 0.39722222089767456.\n",
      "Training epoch 1 batch 687 with loss 1.8331304788589478, accuracy 0.18333333730697632.\n",
      "Training epoch 1 batch 688 with loss 1.7733581066131592, accuracy 0.2281746119260788.\n",
      "Training epoch 1 batch 689 with loss 1.8169448375701904, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 690 with loss 1.7704737186431885, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 691 with loss 1.7664836645126343, accuracy 0.283730149269104.\n",
      "Training epoch 1 batch 692 with loss 1.75348699092865, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 693 with loss 1.947161316871643, accuracy 0.02083333395421505.\n",
      "Training epoch 1 batch 694 with loss 1.7718757390975952, accuracy 0.2777777910232544.\n",
      "Training epoch 1 batch 695 with loss 1.7161853313446045, accuracy 0.17777778208255768.\n",
      "Training epoch 1 batch 696 with loss 1.7960216999053955, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 697 with loss 1.6862589120864868, accuracy 0.2738095223903656.\n",
      "Training epoch 1 batch 698 with loss 1.7651455402374268, accuracy 0.3222222328186035.\n",
      "Training epoch 1 batch 699 with loss 1.732936143875122, accuracy 0.24722224473953247.\n",
      "Training epoch 1 batch 700 with loss 1.7816689014434814, accuracy 0.125.\n",
      "Training epoch 1 batch 701 with loss 1.7534347772598267, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 702 with loss 1.8336756229400635, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 703 with loss 1.769878625869751, accuracy 0.17500001192092896.\n",
      "Training epoch 1 batch 704 with loss 1.7678442001342773, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 705 with loss 1.7523391246795654, accuracy 0.2797619104385376.\n",
      "Training epoch 1 batch 706 with loss 1.786767601966858, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 707 with loss 1.8562259674072266, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 708 with loss 1.7202335596084595, accuracy 0.347222238779068.\n",
      "Training epoch 1 batch 709 with loss 1.846573829650879, accuracy 0.125.\n",
      "Training epoch 1 batch 710 with loss 1.7466892004013062, accuracy 0.17777778208255768.\n",
      "Training epoch 1 batch 711 with loss 1.8547697067260742, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 712 with loss 1.8546193838119507, accuracy 0.28333333134651184.\n",
      "Training epoch 1 batch 713 with loss 1.8685636520385742, accuracy 0.2916666865348816.\n",
      "Training epoch 1 batch 714 with loss 1.8099285364151, accuracy 0.18214286863803864.\n",
      "Training epoch 1 batch 715 with loss 1.8820003271102905, accuracy 0.0694444477558136.\n",
      "Training epoch 1 batch 716 with loss 1.801628828048706, accuracy 0.18611112236976624.\n",
      "Training epoch 1 batch 717 with loss 1.899836540222168, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 718 with loss 1.855102300643921, accuracy 0.125.\n",
      "Training epoch 1 batch 719 with loss 1.8082497119903564, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 720 with loss 1.8431755304336548, accuracy 0.15555556118488312.\n",
      "Training epoch 1 batch 721 with loss 1.7739648818969727, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 722 with loss 1.7406803369522095, accuracy 0.18888890743255615.\n",
      "Training epoch 1 batch 723 with loss 1.9139105081558228, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 724 with loss 1.6776975393295288, accuracy 0.5099206566810608.\n",
      "Training epoch 1 batch 725 with loss 1.775244951248169, accuracy 0.2611111104488373.\n",
      "Training epoch 1 batch 726 with loss 1.7632694244384766, accuracy 0.444444477558136.\n",
      "Training epoch 1 batch 727 with loss 1.8239967823028564, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 728 with loss 1.8152170181274414, accuracy 0.18611110746860504.\n",
      "Training epoch 1 batch 729 with loss 1.7838796377182007, accuracy 0.28333336114883423.\n",
      "Training epoch 1 batch 730 with loss 1.771503210067749, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 731 with loss 1.8450435400009155, accuracy 0.065476194024086.\n",
      "Training epoch 1 batch 732 with loss 1.8695173263549805, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 733 with loss 1.7342300415039062, accuracy 0.3361111283302307.\n",
      "Training epoch 1 batch 734 with loss 1.8008010387420654, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 735 with loss 1.8052587509155273, accuracy 0.02777777798473835.\n",
      "Training epoch 1 batch 736 with loss 1.7514787912368774, accuracy 0.25.\n",
      "Training epoch 1 batch 737 with loss 1.8068664073944092, accuracy 0.1805555671453476.\n",
      "Training epoch 1 batch 738 with loss 1.8375955820083618, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 739 with loss 1.7797386646270752, accuracy 0.24166667461395264.\n",
      "Training epoch 1 batch 740 with loss 1.777708649635315, accuracy 0.24074074625968933.\n",
      "Training epoch 1 batch 741 with loss 1.8038780689239502, accuracy 0.15833333134651184.\n",
      "Training epoch 1 batch 742 with loss 1.7043176889419556, accuracy 0.3333333432674408.\n",
      "Training epoch 1 batch 743 with loss 1.6970384120941162, accuracy 0.472222238779068.\n",
      "Training epoch 1 batch 744 with loss 1.8750625848770142, accuracy 0.2083333283662796.\n",
      "Training epoch 1 batch 745 with loss 1.6673002243041992, accuracy 0.222222238779068.\n",
      "Training epoch 1 batch 746 with loss 1.8093353509902954, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 747 with loss 1.7208595275878906, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 748 with loss 1.925722360610962, accuracy 0.18333333730697632.\n",
      "Training epoch 1 batch 749 with loss 1.7295198440551758, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 750 with loss 1.8115031719207764, accuracy 0.20555555820465088.\n",
      "Training epoch 1 batch 751 with loss 1.8286654949188232, accuracy 0.2658730447292328.\n",
      "Training epoch 1 batch 752 with loss 1.7899879217147827, accuracy 0.2085714340209961.\n",
      "Training epoch 1 batch 753 with loss 1.6898705959320068, accuracy 0.2738095223903656.\n",
      "Training epoch 1 batch 754 with loss 1.8846321105957031, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 755 with loss 1.771707534790039, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 756 with loss 1.7877223491668701, accuracy 0.24722224473953247.\n",
      "Training epoch 1 batch 757 with loss 1.7815182209014893, accuracy 0.25833335518836975.\n",
      "Training epoch 1 batch 758 with loss 1.7335717678070068, accuracy 0.15555556118488312.\n",
      "Training epoch 1 batch 759 with loss 1.8416579961776733, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 760 with loss 1.7023189067840576, accuracy 0.15833334624767303.\n",
      "Training epoch 1 batch 761 with loss 1.730441689491272, accuracy 0.15833333134651184.\n",
      "Training epoch 1 batch 762 with loss 1.8214762210845947, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 763 with loss 1.8770067691802979, accuracy 0.0.\n",
      "Training epoch 1 batch 764 with loss 1.765329122543335, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 765 with loss 1.8318496942520142, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 766 with loss 1.8260825872421265, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 767 with loss 1.6704559326171875, accuracy 0.3682539761066437.\n",
      "Training epoch 1 batch 768 with loss 1.7778041362762451, accuracy 0.3333333432674408.\n",
      "Training epoch 1 batch 769 with loss 1.8690006732940674, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 770 with loss 1.8419487476348877, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 771 with loss 1.8004112243652344, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 772 with loss 1.815352439880371, accuracy 0.20000001788139343.\n",
      "Training epoch 1 batch 773 with loss 1.7264890670776367, accuracy 0.125.\n",
      "Training epoch 1 batch 774 with loss 1.7832059860229492, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 775 with loss 1.7752653360366821, accuracy 0.3083333373069763.\n",
      "Training epoch 1 batch 776 with loss 1.7392451763153076, accuracy 0.3591269850730896.\n",
      "Training epoch 1 batch 777 with loss 1.7566614151000977, accuracy 0.1805555671453476.\n",
      "Training epoch 1 batch 778 with loss 1.7678642272949219, accuracy 0.130952388048172.\n",
      "Training epoch 1 batch 779 with loss 1.8058645725250244, accuracy 0.3055555522441864.\n",
      "Training epoch 1 batch 780 with loss 1.8368650674819946, accuracy 0.13055555522441864.\n",
      "Training epoch 1 batch 781 with loss 1.8839099407196045, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 782 with loss 1.7239128351211548, accuracy 0.18333333730697632.\n",
      "Training epoch 1 batch 783 with loss 1.6574375629425049, accuracy 0.4444444477558136.\n",
      "Training epoch 1 batch 784 with loss 1.8047155141830444, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 785 with loss 1.897080659866333, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 786 with loss 1.8704462051391602, accuracy 0.2916666865348816.\n",
      "Training epoch 1 batch 787 with loss 1.7967170476913452, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 788 with loss 1.9022953510284424, accuracy 0.2083333283662796.\n",
      "Training epoch 1 batch 789 with loss 1.8575729131698608, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 790 with loss 1.8290073871612549, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 791 with loss 1.802167534828186, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 792 with loss 1.7887061834335327, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 793 with loss 1.8979116678237915, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 794 with loss 1.828462839126587, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 795 with loss 1.8257992267608643, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 796 with loss 1.7950719594955444, accuracy 0.15740740299224854.\n",
      "Training epoch 1 batch 797 with loss 1.7266473770141602, accuracy 0.33888888359069824.\n",
      "Training epoch 1 batch 798 with loss 1.8176517486572266, accuracy 0.28333336114883423.\n",
      "Training epoch 1 batch 799 with loss 1.854382872581482, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 800 with loss 1.8664891719818115, accuracy 0.16428571939468384.\n",
      "Training epoch 1 batch 801 with loss 1.8333327770233154, accuracy 0.23333333432674408.\n",
      "Training epoch 1 batch 802 with loss 1.8048900365829468, accuracy 0.2916666865348816.\n",
      "Training epoch 1 batch 803 with loss 1.7905786037445068, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 804 with loss 1.7205817699432373, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 805 with loss 1.757699966430664, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 806 with loss 1.7784345149993896, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 807 with loss 1.7870490550994873, accuracy 0.2291666716337204.\n",
      "Training epoch 1 batch 808 with loss 1.816362977027893, accuracy 0.18611112236976624.\n",
      "Training epoch 1 batch 809 with loss 1.6349166631698608, accuracy 0.3611111044883728.\n",
      "Training epoch 1 batch 810 with loss 1.7277019023895264, accuracy 0.22777777910232544.\n",
      "Training epoch 1 batch 811 with loss 1.7766956090927124, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 812 with loss 1.842718482017517, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 813 with loss 1.7965259552001953, accuracy 0.1210317462682724.\n",
      "Training epoch 1 batch 814 with loss 1.7679774761199951, accuracy 0.2777777910232544.\n",
      "Training epoch 1 batch 815 with loss 1.9045253992080688, accuracy 0.2916666567325592.\n",
      "Training epoch 1 batch 816 with loss 1.9453532695770264, accuracy 0.15555556118488312.\n",
      "Training epoch 1 batch 817 with loss 1.7849401235580444, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 818 with loss 1.7597076892852783, accuracy 0.3194444477558136.\n",
      "Training epoch 1 batch 819 with loss 1.7690986394882202, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 820 with loss 1.8511369228363037, accuracy 0.065476194024086.\n",
      "Training epoch 1 batch 821 with loss 1.891349196434021, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 822 with loss 1.8897616863250732, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 823 with loss 1.8634735345840454, accuracy 0.15833333134651184.\n",
      "Training epoch 1 batch 824 with loss 1.8789279460906982, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 825 with loss 1.6807756423950195, accuracy 0.23333333432674408.\n",
      "Training epoch 1 batch 826 with loss 1.6415783166885376, accuracy 0.2797619104385376.\n",
      "Training epoch 1 batch 827 with loss 1.8465473651885986, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 828 with loss 1.7431552410125732, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 829 with loss 1.798173189163208, accuracy 0.23333334922790527.\n",
      "Training epoch 1 batch 830 with loss 1.7697607278823853, accuracy 0.21944445371627808.\n",
      "Training epoch 1 batch 831 with loss 1.7213037014007568, accuracy 0.15555556118488312.\n",
      "Training epoch 1 batch 832 with loss 1.6696512699127197, accuracy 0.19166666269302368.\n",
      "Training epoch 1 batch 833 with loss 1.7576748132705688, accuracy 0.14166666567325592.\n",
      "Training epoch 1 batch 834 with loss 1.8067312240600586, accuracy 0.20555555820465088.\n",
      "Training epoch 1 batch 835 with loss 1.7932943105697632, accuracy 0.13055555522441864.\n",
      "Training epoch 1 batch 836 with loss 1.7828261852264404, accuracy 0.222222238779068.\n",
      "Training epoch 1 batch 837 with loss 1.6931335926055908, accuracy 0.3392857313156128.\n",
      "Training epoch 1 batch 838 with loss 1.8142788410186768, accuracy 0.15555556118488312.\n",
      "Training epoch 1 batch 839 with loss 1.7537956237792969, accuracy 0.06111111491918564.\n",
      "Training epoch 1 batch 840 with loss 1.6582214832305908, accuracy 0.30000001192092896.\n",
      "Training epoch 1 batch 841 with loss 1.903935194015503, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 842 with loss 1.7370418310165405, accuracy 0.33888891339302063.\n",
      "Training epoch 1 batch 843 with loss 1.8408225774765015, accuracy 0.24166667461395264.\n",
      "Training epoch 1 batch 844 with loss 1.7540899515151978, accuracy 0.4138889014720917.\n",
      "Training epoch 1 batch 845 with loss 1.7729854583740234, accuracy 0.28333336114883423.\n",
      "Training epoch 1 batch 846 with loss 1.7601397037506104, accuracy 0.13333334028720856.\n",
      "Training epoch 1 batch 847 with loss 1.7135813236236572, accuracy 0.24000000953674316.\n",
      "Training epoch 1 batch 848 with loss 1.8131608963012695, accuracy 0.13333334028720856.\n",
      "Training epoch 1 batch 849 with loss 1.8443177938461304, accuracy 0.20555555820465088.\n",
      "Training epoch 1 batch 850 with loss 1.82315194606781, accuracy 0.16388890147209167.\n",
      "Training epoch 1 batch 851 with loss 1.741646409034729, accuracy 0.31111112236976624.\n",
      "Training epoch 1 batch 852 with loss 1.7558519840240479, accuracy 0.3333333432674408.\n",
      "Training epoch 1 batch 853 with loss 1.7728294134140015, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 854 with loss 1.7742655277252197, accuracy 0.3194444477558136.\n",
      "Training epoch 1 batch 855 with loss 1.773242712020874, accuracy 0.28333336114883423.\n",
      "Training epoch 1 batch 856 with loss 1.8870716094970703, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 857 with loss 1.8023544549942017, accuracy 0.18888890743255615.\n",
      "Training epoch 1 batch 858 with loss 1.7221364974975586, accuracy 0.40000003576278687.\n",
      "Training epoch 1 batch 859 with loss 1.8443024158477783, accuracy 0.2900000214576721.\n",
      "Training epoch 1 batch 860 with loss 1.8500807285308838, accuracy 0.25925925374031067.\n",
      "Training epoch 1 batch 861 with loss 1.8735198974609375, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 862 with loss 1.858646035194397, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 863 with loss 1.8939294815063477, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 864 with loss 1.779998779296875, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 865 with loss 1.8046891689300537, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 866 with loss 1.8128995895385742, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 867 with loss 1.8163635730743408, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 868 with loss 1.7809162139892578, accuracy 0.2083333283662796.\n",
      "Training epoch 1 batch 869 with loss 1.8073021173477173, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 870 with loss 1.820448875427246, accuracy 0.1488095223903656.\n",
      "Training epoch 1 batch 871 with loss 1.7644590139389038, accuracy 0.3055555522441864.\n",
      "Training epoch 1 batch 872 with loss 1.7169806957244873, accuracy 0.25555557012557983.\n",
      "Training epoch 1 batch 873 with loss 1.7870938777923584, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 874 with loss 1.8824020624160767, accuracy 0.06111111491918564.\n",
      "Training epoch 1 batch 875 with loss 1.8218660354614258, accuracy 0.02777777798473835.\n",
      "Training epoch 1 batch 876 with loss 1.8322811126708984, accuracy 0.10277777910232544.\n",
      "Training epoch 1 batch 877 with loss 1.6217248439788818, accuracy 0.5388888716697693.\n",
      "Training epoch 1 batch 878 with loss 1.7266318798065186, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 879 with loss 1.8551956415176392, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 880 with loss 1.7953824996948242, accuracy 0.1626984179019928.\n",
      "Training epoch 1 batch 881 with loss 1.8198280334472656, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 882 with loss 1.8317954540252686, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 883 with loss 1.7533483505249023, accuracy 0.130952388048172.\n",
      "Training epoch 1 batch 884 with loss 1.7336050271987915, accuracy 0.10833333432674408.\n",
      "Training epoch 1 batch 885 with loss 1.7925641536712646, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 886 with loss 1.7175028324127197, accuracy 0.13611111044883728.\n",
      "Training epoch 1 batch 887 with loss 1.7889541387557983, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 888 with loss 1.8447030782699585, accuracy 0.05681818351149559.\n",
      "Training epoch 1 batch 889 with loss 1.8722593784332275, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 890 with loss 1.8578031063079834, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 891 with loss 1.6885452270507812, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 892 with loss 1.7514978647232056, accuracy 0.2916666865348816.\n",
      "Training epoch 1 batch 893 with loss 1.7661259174346924, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 894 with loss 1.775841474533081, accuracy 0.22777777910232544.\n",
      "Training epoch 1 batch 895 with loss 1.8019094467163086, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 896 with loss 1.7599670886993408, accuracy 0.3055555522441864.\n",
      "Training epoch 1 batch 897 with loss 1.8485643863677979, accuracy 0.25.\n",
      "Training epoch 1 batch 898 with loss 1.8387092351913452, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 899 with loss 1.8425003290176392, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 900 with loss 1.779799222946167, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 901 with loss 1.7836602926254272, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 902 with loss 1.7478506565093994, accuracy 0.1714285910129547.\n",
      "Training epoch 1 batch 903 with loss 1.8044277429580688, accuracy 0.1765872985124588.\n",
      "Training epoch 1 batch 904 with loss 1.7270952463150024, accuracy 0.18611112236976624.\n",
      "Training epoch 1 batch 905 with loss 1.7111046314239502, accuracy 0.3194444477558136.\n",
      "Training epoch 1 batch 906 with loss 1.8116734027862549, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 907 with loss 1.788743257522583, accuracy 0.2571428716182709.\n",
      "Training epoch 1 batch 908 with loss 1.7920057773590088, accuracy 0.13055555522441864.\n",
      "Training epoch 1 batch 909 with loss 1.7523819208145142, accuracy 0.18333333730697632.\n",
      "Training epoch 1 batch 910 with loss 1.850790023803711, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 911 with loss 1.7552735805511475, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 912 with loss 1.7625224590301514, accuracy 0.21388889849185944.\n",
      "Training epoch 1 batch 913 with loss 1.7804491519927979, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 914 with loss 1.7858740091323853, accuracy 0.3849206566810608.\n",
      "Training epoch 1 batch 915 with loss 1.8272186517715454, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 916 with loss 1.8791606426239014, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 917 with loss 1.7698585987091064, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 918 with loss 1.9110822677612305, accuracy 0.02083333395421505.\n",
      "Training epoch 1 batch 919 with loss 1.6897436380386353, accuracy 0.3722222149372101.\n",
      "Training epoch 1 batch 920 with loss 1.7751821279525757, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 921 with loss 1.8028749227523804, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 922 with loss 1.6673723459243774, accuracy 0.3055555522441864.\n",
      "Training epoch 1 batch 923 with loss 1.7979533672332764, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 924 with loss 1.7775819301605225, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 925 with loss 1.7658010721206665, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 926 with loss 1.7037737369537354, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 927 with loss 1.8853505849838257, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 928 with loss 1.8297698497772217, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 929 with loss 1.8153234720230103, accuracy 0.0793650820851326.\n",
      "Training epoch 1 batch 930 with loss 1.721970558166504, accuracy 0.375.\n",
      "Training epoch 1 batch 931 with loss 1.7669531106948853, accuracy 0.29722222685813904.\n",
      "Training epoch 1 batch 932 with loss 1.7741817235946655, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 933 with loss 1.7215735912322998, accuracy 0.4087301790714264.\n",
      "Training epoch 1 batch 934 with loss 1.7248951196670532, accuracy 0.24166667461395264.\n",
      "Training epoch 1 batch 935 with loss 1.9238865375518799, accuracy 0.13055555522441864.\n",
      "Training epoch 1 batch 936 with loss 1.7402576208114624, accuracy 0.28333333134651184.\n",
      "Training epoch 1 batch 937 with loss 1.714021921157837, accuracy 0.3055555522441864.\n",
      "Training epoch 1 batch 938 with loss 1.751673936843872, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 939 with loss 1.9094135761260986, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 940 with loss 1.7763664722442627, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 941 with loss 1.7116200923919678, accuracy 0.0694444477558136.\n",
      "Training epoch 1 batch 942 with loss 1.7805709838867188, accuracy 0.2013888955116272.\n",
      "Training epoch 1 batch 943 with loss 1.8899284601211548, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 944 with loss 1.7979843616485596, accuracy 0.12777778506278992.\n",
      "Training epoch 1 batch 945 with loss 1.753103256225586, accuracy 0.29722222685813904.\n",
      "Training epoch 1 batch 946 with loss 1.7275164127349854, accuracy 0.35317462682724.\n",
      "Training epoch 1 batch 947 with loss 1.7775638103485107, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 948 with loss 1.752328872680664, accuracy 0.222222238779068.\n",
      "Training epoch 1 batch 949 with loss 1.8916711807250977, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 950 with loss 1.779526948928833, accuracy 0.3432539701461792.\n",
      "Training epoch 1 batch 951 with loss 1.7815784215927124, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 952 with loss 1.752913475036621, accuracy 0.3222222328186035.\n",
      "Training epoch 1 batch 953 with loss 1.817840337753296, accuracy 0.23333334922790527.\n",
      "Training epoch 1 batch 954 with loss 1.8318164348602295, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 955 with loss 1.7830337285995483, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 956 with loss 1.8329813480377197, accuracy 0.0.\n",
      "Training epoch 1 batch 957 with loss 1.8512423038482666, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 958 with loss 1.637542724609375, accuracy 0.3888889253139496.\n",
      "Training epoch 1 batch 959 with loss 1.7381235361099243, accuracy 0.22777777910232544.\n",
      "Training epoch 1 batch 960 with loss 1.776837944984436, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 961 with loss 1.8038139343261719, accuracy 0.22777777910232544.\n",
      "Training epoch 1 batch 962 with loss 1.7902767658233643, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 963 with loss 1.806199073791504, accuracy 0.125.\n",
      "Training epoch 1 batch 964 with loss 1.7808916568756104, accuracy 0.2777777910232544.\n",
      "Training epoch 1 batch 965 with loss 1.8001009225845337, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 966 with loss 1.7228477001190186, accuracy 0.15833334624767303.\n",
      "Training epoch 1 batch 967 with loss 1.828617811203003, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 968 with loss 1.7690155506134033, accuracy 0.1865079402923584.\n",
      "Training epoch 1 batch 969 with loss 1.8200111389160156, accuracy 0.29722222685813904.\n",
      "Training epoch 1 batch 970 with loss 1.7039978504180908, accuracy 0.3305555582046509.\n",
      "Training epoch 1 batch 971 with loss 1.7212607860565186, accuracy 0.3055555522441864.\n",
      "Training epoch 1 batch 972 with loss 1.773974061012268, accuracy 0.236111119389534.\n",
      "Training epoch 1 batch 973 with loss 1.7791872024536133, accuracy 0.22777777910232544.\n",
      "Training epoch 1 batch 974 with loss 1.8290462493896484, accuracy 0.02380952425301075.\n",
      "Training epoch 1 batch 975 with loss 1.7787601947784424, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 976 with loss 1.7469298839569092, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 977 with loss 1.8536193370819092, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 978 with loss 1.7755546569824219, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 979 with loss 1.7820783853530884, accuracy 0.17500001192092896.\n",
      "Training epoch 1 batch 980 with loss 1.7143787145614624, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 981 with loss 1.7332627773284912, accuracy 0.18888889253139496.\n",
      "Training epoch 1 batch 982 with loss 1.850123405456543, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 983 with loss 1.8179748058319092, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 984 with loss 1.8792186975479126, accuracy 0.0.\n",
      "Training epoch 1 batch 985 with loss 1.8102697134017944, accuracy 0.0793650820851326.\n",
      "Training epoch 1 batch 986 with loss 1.7653636932373047, accuracy 0.2666666805744171.\n",
      "Training epoch 1 batch 987 with loss 1.7684284448623657, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 988 with loss 1.7300875186920166, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 989 with loss 1.8369064331054688, accuracy 0.15833334624767303.\n",
      "Training epoch 1 batch 990 with loss 1.8071691989898682, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 991 with loss 1.8490674495697021, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 992 with loss 1.8560497760772705, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 993 with loss 1.8309322595596313, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 994 with loss 1.7944790124893188, accuracy 0.25.\n",
      "Training epoch 1 batch 995 with loss 1.8514530658721924, accuracy 0.144841268658638.\n",
      "Training epoch 1 batch 996 with loss 1.8051360845565796, accuracy 0.06111111491918564.\n",
      "Training epoch 1 batch 997 with loss 1.824407935142517, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 998 with loss 1.7807331085205078, accuracy 0.20000001788139343.\n",
      "Training epoch 1 batch 999 with loss 1.771905541419983, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 1000 with loss 1.7617619037628174, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 1001 with loss 1.7204768657684326, accuracy 0.32380953431129456.\n",
      "Training epoch 1 batch 1002 with loss 1.7750333547592163, accuracy 0.0476190485060215.\n",
      "Training epoch 1 batch 1003 with loss 1.7903106212615967, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 1004 with loss 1.8528074026107788, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 1005 with loss 1.7609901428222656, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 1006 with loss 1.852286696434021, accuracy 0.25.\n",
      "Training epoch 1 batch 1007 with loss 1.865545630455017, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 1008 with loss 1.805159568786621, accuracy 0.27222225069999695.\n",
      "Training epoch 1 batch 1009 with loss 1.6829993724822998, accuracy 0.4186508059501648.\n",
      "Training epoch 1 batch 1010 with loss 1.8009662628173828, accuracy 0.22638890147209167.\n",
      "Training epoch 1 batch 1011 with loss 1.7953141927719116, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 1012 with loss 1.7013301849365234, accuracy 0.19166666269302368.\n",
      "Training epoch 1 batch 1013 with loss 1.7440754175186157, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 1014 with loss 1.8958584070205688, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 1015 with loss 1.8325860500335693, accuracy 0.1626984179019928.\n",
      "Training epoch 1 batch 1016 with loss 1.7648941278457642, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 1017 with loss 1.8160455226898193, accuracy 0.190476194024086.\n",
      "Training epoch 1 batch 1018 with loss 1.8715482950210571, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 1019 with loss 1.8472121953964233, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 1020 with loss 1.6702264547348022, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 1021 with loss 1.7517461776733398, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 1022 with loss 1.7090389728546143, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 1023 with loss 1.695725440979004, accuracy 0.3194444477558136.\n",
      "Training epoch 1 batch 1024 with loss 1.7118360996246338, accuracy 0.3222222328186035.\n",
      "Training epoch 1 batch 1025 with loss 1.8729168176651, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 1026 with loss 1.8068029880523682, accuracy 0.2083333283662796.\n",
      "Training epoch 1 batch 1027 with loss 1.7338531017303467, accuracy 0.28333333134651184.\n",
      "Training epoch 1 batch 1028 with loss 1.9256772994995117, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 1029 with loss 1.7719342708587646, accuracy 0.1319444477558136.\n",
      "Training epoch 1 batch 1030 with loss 1.8163913488388062, accuracy 0.0.\n",
      "Training epoch 1 batch 1031 with loss 1.7119890451431274, accuracy 0.3888889253139496.\n",
      "Training epoch 1 batch 1032 with loss 1.8510310649871826, accuracy 0.2083333283662796.\n",
      "Training epoch 1 batch 1033 with loss 1.79726243019104, accuracy 0.15833334624767303.\n",
      "Training epoch 1 batch 1034 with loss 1.8563635349273682, accuracy 0.125.\n",
      "Training epoch 1 batch 1035 with loss 1.7234653234481812, accuracy 0.22380954027175903.\n",
      "Training epoch 1 batch 1036 with loss 1.7888189554214478, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 1037 with loss 1.806894302368164, accuracy 0.3166666626930237.\n",
      "Training epoch 1 batch 1038 with loss 1.7942445278167725, accuracy 0.25.\n",
      "Training epoch 1 batch 1039 with loss 1.806078314781189, accuracy 0.22777777910232544.\n",
      "Training epoch 1 batch 1040 with loss 1.818864107131958, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 1041 with loss 1.7656978368759155, accuracy 0.18611110746860504.\n",
      "Training epoch 1 batch 1042 with loss 1.8026634454727173, accuracy 0.1805555671453476.\n",
      "Training epoch 1 batch 1043 with loss 1.7886018753051758, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 1044 with loss 1.8019431829452515, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1045 with loss 1.8105722665786743, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 1046 with loss 1.8186098337173462, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 1047 with loss 1.7864189147949219, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 1048 with loss 1.8541858196258545, accuracy 0.24166667461395264.\n",
      "Training epoch 1 batch 1049 with loss 1.8432395458221436, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 1050 with loss 1.745483160018921, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 1051 with loss 1.8416433334350586, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 1052 with loss 1.7875945568084717, accuracy 0.12916666269302368.\n",
      "Training epoch 1 batch 1053 with loss 1.7382469177246094, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 1054 with loss 1.850058913230896, accuracy 0.1041666716337204.\n",
      "Training epoch 1 batch 1055 with loss 1.6703113317489624, accuracy 0.3611111342906952.\n",
      "Training epoch 1 batch 1056 with loss 1.8297231197357178, accuracy 0.23333334922790527.\n",
      "Training epoch 1 batch 1057 with loss 1.763999581336975, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 1058 with loss 1.8053436279296875, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 1059 with loss 1.8492076396942139, accuracy 0.204365074634552.\n",
      "Training epoch 1 batch 1060 with loss 1.7838585376739502, accuracy 0.25158730149269104.\n",
      "Training epoch 1 batch 1061 with loss 1.7363828420639038, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 1062 with loss 1.7825901508331299, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 1063 with loss 1.6760650873184204, accuracy 0.4722222089767456.\n",
      "Training epoch 1 batch 1064 with loss 1.6736514568328857, accuracy 0.16388890147209167.\n",
      "Training epoch 1 batch 1065 with loss 1.9063888788223267, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1066 with loss 1.7447841167449951, accuracy 0.3055555522441864.\n",
      "Training epoch 1 batch 1067 with loss 1.8012431859970093, accuracy 0.38055557012557983.\n",
      "Training epoch 1 batch 1068 with loss 1.8233795166015625, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 1069 with loss 1.7705516815185547, accuracy 0.0793650820851326.\n",
      "Training epoch 1 batch 1070 with loss 1.8728748559951782, accuracy 0.0.\n",
      "Training epoch 1 batch 1071 with loss 1.8083469867706299, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 1072 with loss 1.7762095928192139, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 1073 with loss 1.7595008611679077, accuracy 0.12333333492279053.\n",
      "Training epoch 1 batch 1074 with loss 1.7340885400772095, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 1075 with loss 1.7530180215835571, accuracy 0.23333334922790527.\n",
      "Training epoch 1 batch 1076 with loss 1.7821571826934814, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 1077 with loss 1.8067594766616821, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 1078 with loss 1.7180688381195068, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 1079 with loss 1.8490049839019775, accuracy 0.18571430444717407.\n",
      "Training epoch 1 batch 1080 with loss 1.779471755027771, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 1081 with loss 1.8835546970367432, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 1082 with loss 1.72561776638031, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 1083 with loss 1.735323190689087, accuracy 0.28333333134651184.\n",
      "Training epoch 1 batch 1084 with loss 1.7118918895721436, accuracy 0.18333333730697632.\n",
      "Training epoch 1 batch 1085 with loss 1.6841474771499634, accuracy 0.347222238779068.\n",
      "Training epoch 1 batch 1086 with loss 1.8720937967300415, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 1087 with loss 1.7599000930786133, accuracy 0.1805555671453476.\n",
      "Training epoch 1 batch 1088 with loss 1.792115569114685, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 1089 with loss 1.855860948562622, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1090 with loss 1.8486230373382568, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 1091 with loss 1.7997499704360962, accuracy 0.25.\n",
      "Training epoch 1 batch 1092 with loss 1.833749771118164, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 1093 with loss 1.8223659992218018, accuracy 0.1130952388048172.\n",
      "Training epoch 1 batch 1094 with loss 1.7455825805664062, accuracy 0.2666666805744171.\n",
      "Training epoch 1 batch 1095 with loss 1.8524792194366455, accuracy 0.0694444477558136.\n",
      "Training epoch 1 batch 1096 with loss 1.8488553762435913, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 1097 with loss 1.7979137897491455, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1098 with loss 1.818992257118225, accuracy 0.14166666567325592.\n",
      "Training epoch 1 batch 1099 with loss 1.837658166885376, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1100 with loss 1.836856484413147, accuracy 0.20000001788139343.\n",
      "Training epoch 1 batch 1101 with loss 1.7784401178359985, accuracy 0.2888889014720917.\n",
      "Training epoch 1 batch 1102 with loss 1.8508174419403076, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1103 with loss 1.8359606266021729, accuracy 0.25.\n",
      "Training epoch 1 batch 1104 with loss 1.8432512283325195, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 1105 with loss 1.8544349670410156, accuracy 0.18611112236976624.\n",
      "Training epoch 1 batch 1106 with loss 1.801560640335083, accuracy 0.3611111044883728.\n",
      "Training epoch 1 batch 1107 with loss 1.8143068552017212, accuracy 0.1031746044754982.\n",
      "Training epoch 1 batch 1108 with loss 1.7980051040649414, accuracy 0.1041666716337204.\n",
      "Training epoch 1 batch 1109 with loss 1.8325506448745728, accuracy 0.2182539701461792.\n",
      "Training epoch 1 batch 1110 with loss 1.7955882549285889, accuracy 0.125.\n",
      "Training epoch 1 batch 1111 with loss 1.8449655771255493, accuracy 0.16388888657093048.\n",
      "Training epoch 1 batch 1112 with loss 1.9044755697250366, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 1113 with loss 1.9134553670883179, accuracy 0.144841268658638.\n",
      "Training epoch 1 batch 1114 with loss 1.8449426889419556, accuracy 0.125.\n",
      "Training epoch 1 batch 1115 with loss 1.8015064001083374, accuracy 0.3222222328186035.\n",
      "Training epoch 1 batch 1116 with loss 1.851078987121582, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 1117 with loss 1.822629690170288, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1118 with loss 1.7728245258331299, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 1119 with loss 1.7939088344573975, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 1120 with loss 1.7327854633331299, accuracy 0.3750000298023224.\n",
      "Training epoch 1 batch 1121 with loss 1.7714818716049194, accuracy 0.22777777910232544.\n",
      "Training epoch 1 batch 1122 with loss 1.7863330841064453, accuracy 0.2750000059604645.\n",
      "Training epoch 1 batch 1123 with loss 1.855485200881958, accuracy 0.1071428582072258.\n",
      "Training epoch 1 batch 1124 with loss 1.8233966827392578, accuracy 0.17777778208255768.\n",
      "Training epoch 1 batch 1125 with loss 1.8314802646636963, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 1126 with loss 1.7942577600479126, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 1127 with loss 1.8613951206207275, accuracy 0.15833333134651184.\n",
      "Training epoch 1 batch 1128 with loss 1.7597837448120117, accuracy 0.18611112236976624.\n",
      "Training epoch 1 batch 1129 with loss 1.9161237478256226, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 1130 with loss 1.8068767786026, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 1131 with loss 1.7914516925811768, accuracy 0.190476194024086.\n",
      "Training epoch 1 batch 1132 with loss 1.8243086338043213, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 1133 with loss 1.810462236404419, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 1134 with loss 1.8219144344329834, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 1135 with loss 1.872934341430664, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 1136 with loss 1.852098822593689, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 1137 with loss 1.8729381561279297, accuracy 0.0793650820851326.\n",
      "Training epoch 1 batch 1138 with loss 1.8873695135116577, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1139 with loss 1.8210208415985107, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 1140 with loss 1.730698823928833, accuracy 0.25.\n",
      "Training epoch 1 batch 1141 with loss 1.8293888568878174, accuracy 0.0892857164144516.\n",
      "Training epoch 1 batch 1142 with loss 1.7994062900543213, accuracy 0.22499999403953552.\n",
      "Training epoch 1 batch 1143 with loss 1.766160249710083, accuracy 0.1180555522441864.\n",
      "Training epoch 1 batch 1144 with loss 1.7499603033065796, accuracy 0.12962962687015533.\n",
      "Training epoch 1 batch 1145 with loss 1.8725764751434326, accuracy 0.02083333395421505.\n",
      "Training epoch 1 batch 1146 with loss 1.8108770847320557, accuracy 0.15555556118488312.\n",
      "Training epoch 1 batch 1147 with loss 1.7859666347503662, accuracy 0.1726190447807312.\n",
      "Training epoch 1 batch 1148 with loss 1.712472677230835, accuracy 0.222222238779068.\n",
      "Training epoch 1 batch 1149 with loss 1.7826026678085327, accuracy 0.2361111044883728.\n",
      "Training epoch 1 batch 1150 with loss 1.7049649953842163, accuracy 0.38333335518836975.\n",
      "Training epoch 1 batch 1151 with loss 1.8200355768203735, accuracy 0.17777778208255768.\n",
      "Training epoch 1 batch 1152 with loss 1.7980865240097046, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 1153 with loss 1.8258987665176392, accuracy 0.0.\n",
      "Training epoch 1 batch 1154 with loss 1.7707325220108032, accuracy 0.236111119389534.\n",
      "Training epoch 1 batch 1155 with loss 1.750341773033142, accuracy 0.20555555820465088.\n",
      "Training epoch 1 batch 1156 with loss 1.786269187927246, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 1157 with loss 1.767900824546814, accuracy 0.3333333432674408.\n",
      "Training epoch 1 batch 1158 with loss 1.7284409999847412, accuracy 0.31111112236976624.\n",
      "Training epoch 1 batch 1159 with loss 1.7376359701156616, accuracy 0.2460317611694336.\n",
      "Training epoch 1 batch 1160 with loss 1.7872114181518555, accuracy 0.3472222089767456.\n",
      "Training epoch 1 batch 1161 with loss 1.7863022089004517, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 1162 with loss 1.7525659799575806, accuracy 0.3194444477558136.\n",
      "Training epoch 1 batch 1163 with loss 1.7737070322036743, accuracy 0.23333333432674408.\n",
      "Training epoch 1 batch 1164 with loss 1.8395099639892578, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 1165 with loss 1.831335425376892, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 1166 with loss 1.802419900894165, accuracy 0.18333333730697632.\n",
      "Training epoch 1 batch 1167 with loss 1.7431942224502563, accuracy 0.19166667759418488.\n",
      "Training epoch 1 batch 1168 with loss 1.7881262302398682, accuracy 0.0694444477558136.\n",
      "Training epoch 1 batch 1169 with loss 1.7910550832748413, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 1170 with loss 1.7936443090438843, accuracy 0.15833333134651184.\n",
      "Training epoch 1 batch 1171 with loss 1.8267700672149658, accuracy 0.22777777910232544.\n",
      "Training epoch 1 batch 1172 with loss 1.7877092361450195, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 1173 with loss 1.8275703191757202, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 1174 with loss 1.7257522344589233, accuracy 0.25555557012557983.\n",
      "Training epoch 1 batch 1175 with loss 1.839360237121582, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 1176 with loss 1.7488343715667725, accuracy 0.2420634925365448.\n",
      "Training epoch 1 batch 1177 with loss 1.8621736764907837, accuracy 0.13055555522441864.\n",
      "Training epoch 1 batch 1178 with loss 1.7660472393035889, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 1179 with loss 1.887590765953064, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 1180 with loss 1.8724441528320312, accuracy 0.0793650820851326.\n",
      "Training epoch 1 batch 1181 with loss 1.8349568843841553, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 1182 with loss 1.9202384948730469, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 1183 with loss 1.8299672603607178, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 1184 with loss 1.8688876628875732, accuracy 0.25.\n",
      "Training epoch 1 batch 1185 with loss 1.8361332416534424, accuracy 0.2291666865348816.\n",
      "Training epoch 1 batch 1186 with loss 1.8341996669769287, accuracy 0.0.\n",
      "Training epoch 1 batch 1187 with loss 1.8387037515640259, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 1188 with loss 1.791200876235962, accuracy 0.28928571939468384.\n",
      "Training epoch 1 batch 1189 with loss 1.7731869220733643, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 1190 with loss 1.786516547203064, accuracy 0.1488095223903656.\n",
      "Training epoch 1 batch 1191 with loss 1.8753875494003296, accuracy 0.32499998807907104.\n",
      "Training epoch 1 batch 1192 with loss 1.6536219120025635, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 1193 with loss 1.7251062393188477, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 1194 with loss 1.7539966106414795, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 1195 with loss 1.8681901693344116, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 1196 with loss 1.7408552169799805, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 1197 with loss 1.8008153438568115, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 1198 with loss 1.7380273342132568, accuracy 0.2888889014720917.\n",
      "Training epoch 1 batch 1199 with loss 1.675672173500061, accuracy 0.38055554032325745.\n",
      "Training epoch 1 batch 1200 with loss 1.7953643798828125, accuracy 0.1488095223903656.\n",
      "Training epoch 1 batch 1201 with loss 1.83852219581604, accuracy 0.13055555522441864.\n",
      "Training epoch 1 batch 1202 with loss 1.872852087020874, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 1203 with loss 1.773590087890625, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 1204 with loss 1.7512258291244507, accuracy 0.5277777910232544.\n",
      "Training epoch 1 batch 1205 with loss 1.8660837411880493, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 1206 with loss 1.8626559972763062, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 1207 with loss 1.8187862634658813, accuracy 0.25.\n",
      "Training epoch 1 batch 1208 with loss 1.7835239171981812, accuracy 0.284722238779068.\n",
      "Training epoch 1 batch 1209 with loss 1.8328139781951904, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1210 with loss 1.8217319250106812, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 1211 with loss 1.843383550643921, accuracy 0.3194444477558136.\n",
      "Training epoch 1 batch 1212 with loss 1.7485967874526978, accuracy 0.33095237612724304.\n",
      "Training epoch 1 batch 1213 with loss 1.7824245691299438, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 1214 with loss 1.7490001916885376, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 1215 with loss 1.8643763065338135, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1216 with loss 1.9505317211151123, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 1217 with loss 1.8637222051620483, accuracy 0.25.\n",
      "Training epoch 1 batch 1218 with loss 1.7367366552352905, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 1219 with loss 1.7836917638778687, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 1220 with loss 1.7896578311920166, accuracy 0.25.\n",
      "Training epoch 1 batch 1221 with loss 1.7806228399276733, accuracy 0.0694444477558136.\n",
      "Training epoch 1 batch 1222 with loss 1.830979347229004, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 1223 with loss 1.8255736827850342, accuracy 0.02083333395421505.\n",
      "Training epoch 1 batch 1224 with loss 1.822360634803772, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 1225 with loss 1.839808702468872, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1226 with loss 1.8261159658432007, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 1227 with loss 1.9294509887695312, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1228 with loss 1.8014857769012451, accuracy 0.24722222983837128.\n",
      "Training epoch 1 batch 1229 with loss 1.763985276222229, accuracy 0.25.\n",
      "Training epoch 1 batch 1230 with loss 1.786299467086792, accuracy 0.25.\n",
      "Training epoch 1 batch 1231 with loss 1.8965829610824585, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 1232 with loss 1.7970855236053467, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1233 with loss 1.785613775253296, accuracy 0.3333333730697632.\n",
      "Training epoch 1 batch 1234 with loss 1.8931095600128174, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 1235 with loss 1.872243881225586, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 1236 with loss 1.8286702632904053, accuracy 0.1875.\n",
      "Training epoch 1 batch 1237 with loss 1.7290929555892944, accuracy 0.38055557012557983.\n",
      "Training epoch 1 batch 1238 with loss 1.826517105102539, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 1239 with loss 1.8507219552993774, accuracy 0.190476194024086.\n",
      "Training epoch 1 batch 1240 with loss 1.8005599975585938, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 1241 with loss 1.8129119873046875, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 1242 with loss 1.7897920608520508, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 1243 with loss 1.8466171026229858, accuracy 0.0625.\n",
      "Training epoch 1 batch 1244 with loss 1.7702913284301758, accuracy 0.2430555522441864.\n",
      "Training epoch 1 batch 1245 with loss 1.8816697597503662, accuracy 0.125.\n",
      "Training epoch 1 batch 1246 with loss 1.7556778192520142, accuracy 0.19722221791744232.\n",
      "Training epoch 1 batch 1247 with loss 1.8322054147720337, accuracy 0.3333333432674408.\n",
      "Training epoch 1 batch 1248 with loss 1.8153877258300781, accuracy 0.1805555671453476.\n",
      "Training epoch 1 batch 1249 with loss 1.8173134326934814, accuracy 0.02380952425301075.\n",
      "Training epoch 1 batch 1250 with loss 1.7492250204086304, accuracy 0.21944445371627808.\n",
      "Training epoch 1 batch 1251 with loss 1.781333327293396, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 1252 with loss 1.8611150979995728, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 1253 with loss 1.7913017272949219, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 1254 with loss 1.8508754968643188, accuracy 0.13055555522441864.\n",
      "Training epoch 1 batch 1255 with loss 1.7540836334228516, accuracy 0.2750000059604645.\n",
      "Training epoch 1 batch 1256 with loss 1.7097091674804688, accuracy 0.3333333432674408.\n",
      "Training epoch 1 batch 1257 with loss 1.8870031833648682, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 1258 with loss 1.7896463871002197, accuracy 0.21944443881511688.\n",
      "Training epoch 1 batch 1259 with loss 1.7703332901000977, accuracy 0.24484127759933472.\n",
      "Training epoch 1 batch 1260 with loss 1.6708457469940186, accuracy 0.4305555820465088.\n",
      "Training epoch 1 batch 1261 with loss 1.919103980064392, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1262 with loss 1.7646020650863647, accuracy 0.1865079402923584.\n",
      "Training epoch 1 batch 1263 with loss 1.8022587299346924, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1264 with loss 1.8342235088348389, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 1265 with loss 1.8224207162857056, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 1266 with loss 1.8114198446273804, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 1267 with loss 1.775179147720337, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 1268 with loss 1.766940712928772, accuracy 0.1458333432674408.\n",
      "Training epoch 1 batch 1269 with loss 1.8969173431396484, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 1270 with loss 1.7984695434570312, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 1271 with loss 1.7746118307113647, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 1272 with loss 1.728001594543457, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 1273 with loss 1.854186773300171, accuracy 0.25.\n",
      "Training epoch 1 batch 1274 with loss 1.649437665939331, accuracy 0.5138888955116272.\n",
      "Training epoch 1 batch 1275 with loss 1.749634027481079, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 1276 with loss 1.807015061378479, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 1277 with loss 1.8190644979476929, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 1278 with loss 1.72012460231781, accuracy 0.3333333432674408.\n",
      "Training epoch 1 batch 1279 with loss 1.833685278892517, accuracy 0.36666667461395264.\n",
      "Training epoch 1 batch 1280 with loss 1.8068478107452393, accuracy 0.2611111104488373.\n",
      "Training epoch 1 batch 1281 with loss 1.8792450428009033, accuracy 0.03999999910593033.\n",
      "Training epoch 1 batch 1282 with loss 1.6900173425674438, accuracy 0.25.\n",
      "Training epoch 1 batch 1283 with loss 1.8879867792129517, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 1284 with loss 1.7699482440948486, accuracy 0.402777761220932.\n",
      "Training epoch 1 batch 1285 with loss 1.7117221355438232, accuracy 0.4444444477558136.\n",
      "Training epoch 1 batch 1286 with loss 1.7839189767837524, accuracy 0.22777777910232544.\n",
      "Training epoch 1 batch 1287 with loss 1.6787745952606201, accuracy 0.4027777910232544.\n",
      "Training epoch 1 batch 1288 with loss 1.8119080066680908, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 1289 with loss 1.9080159664154053, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 1290 with loss 1.8466819524765015, accuracy 0.15833334624767303.\n",
      "Training epoch 1 batch 1291 with loss 1.7643029689788818, accuracy 0.22142857313156128.\n",
      "Training epoch 1 batch 1292 with loss 1.8258802890777588, accuracy 0.39444446563720703.\n",
      "Training epoch 1 batch 1293 with loss 1.8333942890167236, accuracy 0.1071428582072258.\n",
      "Training epoch 1 batch 1294 with loss 1.8534520864486694, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 1295 with loss 1.8107963800430298, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 1296 with loss 1.7325537204742432, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 1297 with loss 1.8499246835708618, accuracy 0.3472222089767456.\n",
      "Training epoch 1 batch 1298 with loss 1.7823654413223267, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 1299 with loss 1.86576247215271, accuracy 0.0.\n",
      "Training epoch 1 batch 1300 with loss 1.8420078754425049, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 1301 with loss 1.8825676441192627, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 1302 with loss 1.7910820245742798, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 1303 with loss 1.8527616262435913, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1304 with loss 1.773619294166565, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1305 with loss 1.9088456630706787, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 1306 with loss 1.6672414541244507, accuracy 0.409722238779068.\n",
      "Training epoch 1 batch 1307 with loss 1.8334767818450928, accuracy 0.20555555820465088.\n",
      "Training epoch 1 batch 1308 with loss 1.694350242614746, accuracy 0.29920634627342224.\n",
      "Training epoch 1 batch 1309 with loss 1.828439474105835, accuracy 0.24761906266212463.\n",
      "Training epoch 1 batch 1310 with loss 1.7592852115631104, accuracy 0.20555555820465088.\n",
      "Training epoch 1 batch 1311 with loss 1.7873857021331787, accuracy 0.1626984179019928.\n",
      "Training epoch 1 batch 1312 with loss 1.7893174886703491, accuracy 0.07857143133878708.\n",
      "Training epoch 1 batch 1313 with loss 1.8375704288482666, accuracy 0.3305555582046509.\n",
      "Training epoch 1 batch 1314 with loss 1.8260078430175781, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1315 with loss 1.7931429147720337, accuracy 0.0.\n",
      "Training epoch 1 batch 1316 with loss 1.8260242938995361, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 1317 with loss 1.8723055124282837, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 1318 with loss 1.7398502826690674, accuracy 0.19166667759418488.\n",
      "Training epoch 1 batch 1319 with loss 1.7893365621566772, accuracy 0.17380952835083008.\n",
      "Training epoch 1 batch 1320 with loss 1.7522642612457275, accuracy 0.22777777910232544.\n",
      "Training epoch 1 batch 1321 with loss 1.900198221206665, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 1322 with loss 1.7638664245605469, accuracy 0.25.\n",
      "Training epoch 1 batch 1323 with loss 1.8360145092010498, accuracy 0.3055555522441864.\n",
      "Training epoch 1 batch 1324 with loss 1.7335636615753174, accuracy 0.4722222089767456.\n",
      "Training epoch 1 batch 1325 with loss 1.8362210988998413, accuracy 0.13055555522441864.\n",
      "Training epoch 1 batch 1326 with loss 1.8804534673690796, accuracy 0.13055555522441864.\n",
      "Training epoch 1 batch 1327 with loss 1.7978070974349976, accuracy 0.18333333730697632.\n",
      "Training epoch 1 batch 1328 with loss 1.8924200534820557, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 1329 with loss 1.8255590200424194, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 1330 with loss 1.7801660299301147, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 1331 with loss 1.8017507791519165, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1332 with loss 1.7967039346694946, accuracy 0.37261906266212463.\n",
      "Training epoch 1 batch 1333 with loss 1.8228044509887695, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 1334 with loss 1.8097623586654663, accuracy 0.11269842088222504.\n",
      "Training epoch 1 batch 1335 with loss 1.9529508352279663, accuracy 0.125.\n",
      "Training epoch 1 batch 1336 with loss 1.732969045639038, accuracy 0.3916666805744171.\n",
      "Training epoch 1 batch 1337 with loss 1.751800298690796, accuracy 0.3630952537059784.\n",
      "Training epoch 1 batch 1338 with loss 1.8340622186660767, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 1339 with loss 1.869812250137329, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 1340 with loss 1.8005746603012085, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 1341 with loss 1.7850511074066162, accuracy 0.4722222089767456.\n",
      "Training epoch 1 batch 1342 with loss 1.8059580326080322, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 1343 with loss 1.748146653175354, accuracy 0.25555557012557983.\n",
      "Training epoch 1 batch 1344 with loss 1.7355201244354248, accuracy 0.24537037312984467.\n",
      "Training epoch 1 batch 1345 with loss 1.8027652502059937, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 1346 with loss 1.7610909938812256, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 1347 with loss 1.7989200353622437, accuracy 0.10833333432674408.\n",
      "Training epoch 1 batch 1348 with loss 1.8182309865951538, accuracy 0.130952388048172.\n",
      "Training epoch 1 batch 1349 with loss 1.7181310653686523, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 1350 with loss 1.798588752746582, accuracy 0.33888888359069824.\n",
      "Training epoch 1 batch 1351 with loss 1.7937456369400024, accuracy 0.15595239400863647.\n",
      "Training epoch 1 batch 1352 with loss 1.7846782207489014, accuracy 0.2083333432674408.\n",
      "Training epoch 1 batch 1353 with loss 1.6251709461212158, accuracy 0.4055555760860443.\n",
      "Training epoch 1 batch 1354 with loss 1.841965675354004, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 1355 with loss 1.7864967584609985, accuracy 0.222222238779068.\n",
      "Training epoch 1 batch 1356 with loss 1.762486219406128, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 1357 with loss 1.7928622961044312, accuracy 0.3194444477558136.\n",
      "Training epoch 1 batch 1358 with loss 1.8786598443984985, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 1359 with loss 1.8401765823364258, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 1360 with loss 1.8252365589141846, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 1361 with loss 1.8370383977890015, accuracy 0.1180555522441864.\n",
      "Training epoch 1 batch 1362 with loss 1.7952067852020264, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 1363 with loss 1.8146556615829468, accuracy 0.2750000059604645.\n",
      "Training epoch 1 batch 1364 with loss 1.8112943172454834, accuracy 0.222222238779068.\n",
      "Training epoch 1 batch 1365 with loss 1.8734009265899658, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 1366 with loss 1.8199656009674072, accuracy 0.05714286118745804.\n",
      "Training epoch 1 batch 1367 with loss 1.730987548828125, accuracy 0.25.\n",
      "Training epoch 1 batch 1368 with loss 1.8794416189193726, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 1369 with loss 1.8738949298858643, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 1370 with loss 1.7350654602050781, accuracy 0.2888889014720917.\n",
      "Training epoch 1 batch 1371 with loss 1.7737680673599243, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 1372 with loss 1.8379859924316406, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 1373 with loss 1.8405948877334595, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 1374 with loss 1.799735426902771, accuracy 0.190476194024086.\n",
      "Training epoch 1 batch 1375 with loss 1.7967933416366577, accuracy 0.19166667759418488.\n",
      "Training epoch 1 batch 1376 with loss 1.832716703414917, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 1377 with loss 1.8271093368530273, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 1378 with loss 1.7936127185821533, accuracy 0.3472222089767456.\n",
      "Training epoch 1 batch 1379 with loss 1.791598916053772, accuracy 0.2321428507566452.\n",
      "Training epoch 1 batch 1380 with loss 1.74563729763031, accuracy 0.24722224473953247.\n",
      "Training epoch 1 batch 1381 with loss 1.7007322311401367, accuracy 0.4166666567325592.\n",
      "Training epoch 1 batch 1382 with loss 1.8883864879608154, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 1383 with loss 1.727957010269165, accuracy 0.21587303280830383.\n",
      "Training epoch 1 batch 1384 with loss 1.8046023845672607, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 1385 with loss 1.8533293008804321, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 1386 with loss 1.823439598083496, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 1387 with loss 1.8232078552246094, accuracy 0.28333333134651184.\n",
      "Training epoch 1 batch 1388 with loss 1.802692174911499, accuracy 0.1488095223903656.\n",
      "Training epoch 1 batch 1389 with loss 1.8093398809432983, accuracy 0.2043650895357132.\n",
      "Training epoch 1 batch 1390 with loss 1.6753524541854858, accuracy 0.3583333492279053.\n",
      "Training epoch 1 batch 1391 with loss 1.8192903995513916, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 1392 with loss 1.7394979000091553, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 1393 with loss 1.7502555847167969, accuracy 0.1746031790971756.\n",
      "Training epoch 1 batch 1394 with loss 1.829258680343628, accuracy 0.18611112236976624.\n",
      "Training epoch 1 batch 1395 with loss 1.9421669244766235, accuracy 0.125.\n",
      "Training epoch 1 batch 1396 with loss 1.7686424255371094, accuracy 0.3166666626930237.\n",
      "Training epoch 1 batch 1397 with loss 1.7411556243896484, accuracy 0.13055555522441864.\n",
      "Training epoch 1 batch 1398 with loss 1.8244367837905884, accuracy 0.15833334624767303.\n",
      "Training epoch 1 batch 1399 with loss 1.7343333959579468, accuracy 0.25.\n",
      "Training epoch 1 batch 1400 with loss 1.7620652914047241, accuracy 0.26944443583488464.\n",
      "Training epoch 1 batch 1401 with loss 1.7707589864730835, accuracy 0.2888889014720917.\n",
      "Training epoch 1 batch 1402 with loss 1.731309175491333, accuracy 0.1041666641831398.\n",
      "Training epoch 1 batch 1403 with loss 1.88116455078125, accuracy 0.23333333432674408.\n",
      "Training epoch 1 batch 1404 with loss 1.8682806491851807, accuracy 0.0.\n",
      "Training epoch 1 batch 1405 with loss 1.7568237781524658, accuracy 0.14444445073604584.\n",
      "Training epoch 1 batch 1406 with loss 1.8743854761123657, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 1407 with loss 1.8076099157333374, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 1408 with loss 1.7468448877334595, accuracy 0.2003968358039856.\n",
      "Training epoch 1 batch 1409 with loss 1.7559549808502197, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 1410 with loss 1.8623234033584595, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 1411 with loss 1.76120126247406, accuracy 0.25555557012557983.\n",
      "Training epoch 1 batch 1412 with loss 1.8223724365234375, accuracy 0.1071428656578064.\n",
      "Training epoch 1 batch 1413 with loss 1.773253083229065, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 1414 with loss 1.7857773303985596, accuracy 0.06666667014360428.\n",
      "Training epoch 1 batch 1415 with loss 1.7438490390777588, accuracy 0.22777777910232544.\n",
      "Training epoch 1 batch 1416 with loss 1.7236671447753906, accuracy 0.125.\n",
      "Training epoch 1 batch 1417 with loss 1.7587909698486328, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 1418 with loss 1.7114717960357666, accuracy 0.3333333432674408.\n",
      "Training epoch 1 batch 1419 with loss 1.7909454107284546, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 1420 with loss 1.7783234119415283, accuracy 0.14603175222873688.\n",
      "Training epoch 1 batch 1421 with loss 1.832932472229004, accuracy 0.13611111044883728.\n",
      "Training epoch 1 batch 1422 with loss 1.8056366443634033, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 1423 with loss 1.7446024417877197, accuracy 0.06111111491918564.\n",
      "Training epoch 1 batch 1424 with loss 1.7050540447235107, accuracy 0.375.\n",
      "Training epoch 1 batch 1425 with loss 1.741050362586975, accuracy 0.29722222685813904.\n",
      "Training epoch 1 batch 1426 with loss 1.8466930389404297, accuracy 0.3333333432674408.\n",
      "Training epoch 1 batch 1427 with loss 1.7748868465423584, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 1428 with loss 1.7715305089950562, accuracy 0.2250000238418579.\n",
      "Training epoch 1 batch 1429 with loss 1.8143078088760376, accuracy 0.19166666269302368.\n",
      "Training epoch 1 batch 1430 with loss 1.8806651830673218, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 1431 with loss 1.8210989236831665, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 1432 with loss 1.70121169090271, accuracy 0.4138889014720917.\n",
      "Training epoch 1 batch 1433 with loss 1.7455285787582397, accuracy 0.26851850748062134.\n",
      "Training epoch 1 batch 1434 with loss 1.8247947692871094, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 1435 with loss 1.7727832794189453, accuracy 0.125.\n",
      "Training epoch 1 batch 1436 with loss 1.7947187423706055, accuracy 0.1666666716337204.\n",
      "Training epoch 1 batch 1437 with loss 1.7913732528686523, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 1438 with loss 1.7979164123535156, accuracy 0.2222222238779068.\n",
      "Training epoch 1 batch 1439 with loss 1.7555631399154663, accuracy 0.3333333432674408.\n",
      "Training epoch 1 batch 1440 with loss 1.8881323337554932, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 1441 with loss 1.8705476522445679, accuracy 0.22380952537059784.\n",
      "Training epoch 1 batch 1442 with loss 1.7125476598739624, accuracy 0.2750000059604645.\n",
      "Training epoch 1 batch 1443 with loss 1.7548131942749023, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 1444 with loss 1.7795274257659912, accuracy 0.15000000596046448.\n",
      "Training epoch 1 batch 1445 with loss 1.8961502313613892, accuracy 0.25555557012557983.\n",
      "Training epoch 1 batch 1446 with loss 1.8050947189331055, accuracy 0.22777779400348663.\n",
      "Training epoch 1 batch 1447 with loss 1.8195483684539795, accuracy 0.1210317462682724.\n",
      "Training epoch 1 batch 1448 with loss 1.793695092201233, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 1449 with loss 1.8432416915893555, accuracy 0.0793650820851326.\n",
      "Training epoch 1 batch 1450 with loss 1.8280296325683594, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 1451 with loss 1.8784990310668945, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 1452 with loss 1.7190126180648804, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 1453 with loss 1.8580753803253174, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 1454 with loss 1.7311080694198608, accuracy 0.17222222685813904.\n",
      "Training epoch 1 batch 1455 with loss 1.7366710901260376, accuracy 0.4305555820465088.\n",
      "Training epoch 1 batch 1456 with loss 1.7607126235961914, accuracy 0.22777777910232544.\n",
      "Training epoch 1 batch 1457 with loss 1.853441834449768, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1458 with loss 1.8178894519805908, accuracy 0.08888889104127884.\n",
      "Training epoch 1 batch 1459 with loss 1.858062744140625, accuracy 0.03333333507180214.\n",
      "Training epoch 1 batch 1460 with loss 1.7286717891693115, accuracy 0.24166667461395264.\n",
      "Training epoch 1 batch 1461 with loss 1.8308789730072021, accuracy 0.1944444477558136.\n",
      "Training epoch 1 batch 1462 with loss 1.7955482006072998, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 1463 with loss 1.7644226551055908, accuracy 0.3499999940395355.\n",
      "Training epoch 1 batch 1464 with loss 1.785871148109436, accuracy 0.2888889014720917.\n",
      "Training epoch 1 batch 1465 with loss 1.890785574913025, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 1466 with loss 1.6868703365325928, accuracy 0.2611111104488373.\n",
      "Training epoch 1 batch 1467 with loss 1.8175405263900757, accuracy 0.13055555522441864.\n",
      "Training epoch 1 batch 1468 with loss 1.8783267736434937, accuracy 0.0555555559694767.\n",
      "Training epoch 1 batch 1469 with loss 1.8180582523345947, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 1470 with loss 1.8438661098480225, accuracy 0.3333333432674408.\n",
      "Training epoch 1 batch 1471 with loss 1.8637096881866455, accuracy 0.11666667461395264.\n",
      "Training epoch 1 batch 1472 with loss 1.8794149160385132, accuracy 0.1805555522441864.\n",
      "Training epoch 1 batch 1473 with loss 1.772324800491333, accuracy 0.20000000298023224.\n",
      "Training epoch 1 batch 1474 with loss 1.8008005619049072, accuracy 0.130952388048172.\n",
      "Training epoch 1 batch 1475 with loss 1.8252347707748413, accuracy 0.2888889014720917.\n",
      "Training epoch 1 batch 1476 with loss 1.811758041381836, accuracy 0.16984127461910248.\n",
      "Training epoch 1 batch 1477 with loss 1.8941701650619507, accuracy 0.0833333358168602.\n",
      "Training epoch 1 batch 1478 with loss 1.8727251291275024, accuracy 0.2638888955116272.\n",
      "Training epoch 1 batch 1479 with loss 1.8428685665130615, accuracy 0.1388888955116272.\n",
      "Training epoch 1 batch 1480 with loss 1.7097177505493164, accuracy 0.3166666626930237.\n",
      "Training epoch 1 batch 1481 with loss 1.8212573528289795, accuracy 0.0416666679084301.\n",
      "Training epoch 1 batch 1482 with loss 1.799817442893982, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 1483 with loss 1.792639970779419, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 1484 with loss 1.749932885169983, accuracy 0.30714285373687744.\n",
      "Training epoch 1 batch 1485 with loss 1.8632732629776, accuracy 0.1111111119389534.\n",
      "Training epoch 1 batch 1486 with loss 1.8942196369171143, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 1487 with loss 1.7468315362930298, accuracy 0.23333334922790527.\n",
      "Training epoch 1 batch 1488 with loss 1.6870098114013672, accuracy 0.2847222089767456.\n",
      "Training epoch 1 batch 1489 with loss 1.8114116191864014, accuracy 0.23333333432674408.\n",
      "Training epoch 1 batch 1490 with loss 1.9898574352264404, accuracy 0.06111111491918564.\n",
      "Training epoch 1 batch 1491 with loss 1.828578233718872, accuracy 0.125.\n",
      "Training epoch 1 batch 1492 with loss 1.8106180429458618, accuracy 0.07500000298023224.\n",
      "Training epoch 1 batch 1493 with loss 1.7828991413116455, accuracy 0.3611111342906952.\n",
      "Training epoch 1 batch 1494 with loss 1.8205420970916748, accuracy 0.0972222238779068.\n",
      "Training epoch 1 batch 1495 with loss 1.811806321144104, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 1496 with loss 1.8732020854949951, accuracy 0.0763888880610466.\n",
      "Training epoch 1 batch 1497 with loss 1.8613981008529663, accuracy 0.12222222983837128.\n",
      "Training epoch 1 batch 1498 with loss 1.7313086986541748, accuracy 0.1527777761220932.\n",
      "Training epoch 1 batch 1499 with loss 1.920556664466858, accuracy 0.14444445073604584.\n",
      "Test batch 0 with loss 1.749823808670044 and accuracy 0.21944445371627808.\n",
      "Test batch 1 with loss 1.7362334728240967 and accuracy 0.2916666865348816.\n",
      "Test batch 2 with loss 1.7952600717544556 and accuracy 0.1319444477558136.\n",
      "Test batch 3 with loss 1.8411624431610107 and accuracy 0.2708333432674408.\n",
      "Test batch 4 with loss 1.8589153289794922 and accuracy 0.2083333432674408.\n",
      "Test batch 5 with loss 1.864010214805603 and accuracy 0.08888889104127884.\n",
      "Test batch 6 with loss 1.8347934484481812 and accuracy 0.125.\n",
      "Test batch 7 with loss 1.8487170934677124 and accuracy 0.0833333358168602.\n",
      "Test batch 8 with loss 1.6568994522094727 and accuracy 0.3361110985279083.\n",
      "Test batch 9 with loss 1.8825037479400635 and accuracy 0.0694444477558136.\n",
      "Test batch 10 with loss 1.7401959896087646 and accuracy 0.25.\n",
      "Test batch 11 with loss 1.8064943552017212 and accuracy 0.03333333507180214.\n",
      "Test batch 12 with loss 1.7983976602554321 and accuracy 0.11666667461395264.\n",
      "Test batch 13 with loss 1.7902151346206665 and accuracy 0.0833333358168602.\n",
      "Test batch 14 with loss 1.8614118099212646 and accuracy 0.0833333358168602.\n",
      "Test batch 15 with loss 1.7628370523452759 and accuracy 0.3888888955116272.\n",
      "Test batch 16 with loss 1.8975255489349365 and accuracy 0.1666666716337204.\n",
      "Test batch 17 with loss 1.8656911849975586 and accuracy 0.0416666679084301.\n",
      "Test batch 18 with loss 1.7986778020858765 and accuracy 0.125.\n",
      "Test batch 19 with loss 1.7907559871673584 and accuracy 0.28333333134651184.\n",
      "Test batch 20 with loss 1.7813936471939087 and accuracy 0.24166667461395264.\n",
      "Test batch 21 with loss 1.8287980556488037 and accuracy 0.3055555522441864.\n",
      "Test batch 22 with loss 1.7363042831420898 and accuracy 0.3333333432674408.\n",
      "Test batch 23 with loss 1.869523048400879 and accuracy 0.1388888955116272.\n",
      "Test batch 24 with loss 1.8141148090362549 and accuracy 0.08888889104127884.\n",
      "Test batch 25 with loss 1.8521595001220703 and accuracy 0.12222222983837128.\n",
      "Test batch 26 with loss 1.811292052268982 and accuracy 0.1111111119389534.\n",
      "Test batch 27 with loss 1.9086240530014038 and accuracy 0.1111111119389534.\n",
      "Test batch 28 with loss 1.7920271158218384 and accuracy 0.1111111119389534.\n",
      "Test batch 29 with loss 1.808825135231018 and accuracy 0.06666667014360428.\n",
      "Test batch 30 with loss 1.9068982601165771 and accuracy 0.0833333358168602.\n",
      "Test batch 31 with loss 1.8038465976715088 and accuracy 0.1587301641702652.\n",
      "Test batch 32 with loss 1.853534460067749 and accuracy 0.1111111119389534.\n",
      "Test batch 33 with loss 1.7958208322525024 and accuracy 0.204365074634552.\n",
      "Test batch 34 with loss 1.880064606666565 and accuracy 0.1825396865606308.\n",
      "Test batch 35 with loss 1.858605146408081 and accuracy 0.0.\n",
      "Test batch 36 with loss 1.8327178955078125 and accuracy 0.2083333432674408.\n",
      "Test batch 37 with loss 1.859231948852539 and accuracy 0.07500000298023224.\n",
      "Test batch 38 with loss 1.8590662479400635 and accuracy 0.08888889104127884.\n",
      "Test batch 39 with loss 1.8521713018417358 and accuracy 0.1111111119389534.\n",
      "Test batch 40 with loss 1.7628008127212524 and accuracy 0.1944444477558136.\n",
      "Test batch 41 with loss 1.8505550622940063 and accuracy 0.1944444477558136.\n",
      "Test batch 42 with loss 1.8535114526748657 and accuracy 0.2666666805744171.\n",
      "Test batch 43 with loss 1.7540353536605835 and accuracy 0.23095238208770752.\n",
      "Test batch 44 with loss 1.7832790613174438 and accuracy 0.25555557012557983.\n",
      "Test batch 45 with loss 1.752015471458435 and accuracy 0.1587301641702652.\n",
      "Test batch 46 with loss 1.8319776058197021 and accuracy 0.0416666679084301.\n",
      "Test batch 47 with loss 1.9080190658569336 and accuracy 0.03333333507180214.\n",
      "Test batch 48 with loss 1.778649926185608 and accuracy 0.20666667819023132.\n",
      "Test batch 49 with loss 1.8018099069595337 and accuracy 0.1388888955116272.\n",
      "Test batch 50 with loss 1.9115450382232666 and accuracy 0.0.\n",
      "Test batch 51 with loss 1.8567711114883423 and accuracy 0.0.\n",
      "Test batch 52 with loss 1.7699782848358154 and accuracy 0.201388880610466.\n",
      "Test batch 53 with loss 1.8860390186309814 and accuracy 0.0625.\n",
      "Test batch 54 with loss 1.8438535928726196 and accuracy 0.065476194024086.\n",
      "Test batch 55 with loss 1.8695005178451538 and accuracy 0.08888889104127884.\n",
      "Test batch 56 with loss 1.8799316883087158 and accuracy 0.03333333507180214.\n",
      "Test batch 57 with loss 1.9060323238372803 and accuracy 0.1527777761220932.\n",
      "Test batch 58 with loss 1.8201545476913452 and accuracy 0.28333333134651184.\n",
      "Test batch 59 with loss 1.8878657817840576 and accuracy 0.125.\n",
      "Test batch 60 with loss 1.8433361053466797 and accuracy 0.2361111044883728.\n",
      "Test batch 61 with loss 1.7870328426361084 and accuracy 0.3571428656578064.\n",
      "Test batch 62 with loss 1.8194324970245361 and accuracy 0.1041666716337204.\n",
      "Test batch 63 with loss 1.8133310079574585 and accuracy 0.17222222685813904.\n",
      "Test batch 64 with loss 1.820730209350586 and accuracy 0.14444445073604584.\n",
      "Test batch 65 with loss 1.803471326828003 and accuracy 0.3027777671813965.\n",
      "Test batch 66 with loss 1.8518779277801514 and accuracy 0.125.\n",
      "Test batch 67 with loss 1.7497965097427368 and accuracy 0.21666666865348816.\n",
      "Test batch 68 with loss 1.9120597839355469 and accuracy 0.065476194024086.\n",
      "Test batch 69 with loss 1.820948600769043 and accuracy 0.130952388048172.\n",
      "Test batch 70 with loss 1.8393456935882568 and accuracy 0.07500000298023224.\n",
      "Test batch 71 with loss 1.8402988910675049 and accuracy 0.222222238779068.\n",
      "Test batch 72 with loss 1.8298746347427368 and accuracy 0.15833334624767303.\n",
      "Test batch 73 with loss 1.8664594888687134 and accuracy 0.0694444477558136.\n",
      "Test batch 74 with loss 1.7699906826019287 and accuracy 0.12222222983837128.\n",
      "Test batch 75 with loss 1.8034608364105225 and accuracy 0.21666666865348816.\n",
      "Test batch 76 with loss 1.8724844455718994 and accuracy 0.1666666716337204.\n",
      "Test batch 77 with loss 1.895650863647461 and accuracy 0.1388888955116272.\n",
      "Test batch 78 with loss 1.8853031396865845 and accuracy 0.02777777798473835.\n",
      "Test batch 79 with loss 1.7559287548065186 and accuracy 0.33888891339302063.\n",
      "Test batch 80 with loss 1.7583234310150146 and accuracy 0.3194444477558136.\n",
      "Test batch 81 with loss 1.909360647201538 and accuracy 0.11269842088222504.\n",
      "Test batch 82 with loss 1.820752739906311 and accuracy 0.18888889253139496.\n",
      "Test batch 83 with loss 1.7931842803955078 and accuracy 0.2361111044883728.\n",
      "Test batch 84 with loss 1.786208152770996 and accuracy 0.26944446563720703.\n",
      "Test batch 85 with loss 1.8166930675506592 and accuracy 0.29722222685813904.\n",
      "Test batch 86 with loss 1.7567036151885986 and accuracy 0.12222222983837128.\n",
      "Test batch 87 with loss 1.9437406063079834 and accuracy 0.0833333358168602.\n",
      "Test batch 88 with loss 1.7658990621566772 and accuracy 0.2043650895357132.\n",
      "Test batch 89 with loss 1.862093210220337 and accuracy 0.1666666716337204.\n",
      "Test batch 90 with loss 1.8323217630386353 and accuracy 0.1666666716337204.\n",
      "Test batch 91 with loss 1.8129867315292358 and accuracy 0.1111111119389534.\n",
      "Test batch 92 with loss 1.8485714197158813 and accuracy 0.1388888955116272.\n",
      "Test batch 93 with loss 1.8415296077728271 and accuracy 0.15833333134651184.\n",
      "Test batch 94 with loss 1.8338305950164795 and accuracy 0.0555555559694767.\n",
      "Test batch 95 with loss 1.8177719116210938 and accuracy 0.1875.\n",
      "Test batch 96 with loss 1.8718278408050537 and accuracy 0.222222238779068.\n",
      "Test batch 97 with loss 1.9476356506347656 and accuracy 0.0833333358168602.\n",
      "Test batch 98 with loss 1.754464864730835 and accuracy 0.125.\n",
      "Test batch 99 with loss 1.8081750869750977 and accuracy 0.125.\n",
      "Test batch 100 with loss 1.830592155456543 and accuracy 0.1111111119389534.\n",
      "Test batch 101 with loss 1.8576892614364624 and accuracy 0.25555557012557983.\n",
      "Test batch 102 with loss 1.785058617591858 and accuracy 0.1111111119389534.\n",
      "Test batch 103 with loss 1.8764121532440186 and accuracy 0.0833333358168602.\n",
      "Test batch 104 with loss 1.8521316051483154 and accuracy 0.0555555559694767.\n",
      "Test batch 105 with loss 1.8843555450439453 and accuracy 0.33888891339302063.\n",
      "Test batch 106 with loss 1.727614164352417 and accuracy 0.33095237612724304.\n",
      "Test batch 107 with loss 1.7962796688079834 and accuracy 0.1527777761220932.\n",
      "Test batch 108 with loss 1.8081495761871338 and accuracy 0.2460317462682724.\n",
      "Test batch 109 with loss 1.8900359869003296 and accuracy 0.2361111342906952.\n",
      "Test batch 110 with loss 1.825696587562561 and accuracy 0.13055555522441864.\n",
      "Test batch 111 with loss 1.7678501605987549 and accuracy 0.10833333432674408.\n",
      "Test batch 112 with loss 1.7614288330078125 and accuracy 0.13055555522441864.\n",
      "Test batch 113 with loss 1.8123760223388672 and accuracy 0.15555556118488312.\n",
      "Test batch 114 with loss 1.9396216869354248 and accuracy 0.0.\n",
      "Test batch 115 with loss 1.8052552938461304 and accuracy 0.41428571939468384.\n",
      "Test batch 116 with loss 1.862548828125 and accuracy 0.1726190596818924.\n",
      "Test batch 117 with loss 1.7798540592193604 and accuracy 0.2083333432674408.\n",
      "Test batch 118 with loss 1.7838436365127563 and accuracy 0.2777777910232544.\n",
      "Test batch 119 with loss 1.8536920547485352 and accuracy 0.0833333358168602.\n",
      "Test batch 120 with loss 1.8510042428970337 and accuracy 0.0694444477558136.\n",
      "Test batch 121 with loss 1.844617486000061 and accuracy 0.17777778208255768.\n",
      "Test batch 122 with loss 1.804857611656189 and accuracy 0.1805555671453476.\n",
      "Test batch 123 with loss 1.9064252376556396 and accuracy 0.0416666679084301.\n",
      "Test batch 124 with loss 1.775237798690796 and accuracy 0.2361111044883728.\n",
      "Test batch 125 with loss 1.8464314937591553 and accuracy 0.0416666679084301.\n",
      "Test batch 126 with loss 1.8341009616851807 and accuracy 0.0476190485060215.\n",
      "Test batch 127 with loss 1.7259464263916016 and accuracy 0.3166666626930237.\n",
      "Test batch 128 with loss 1.883941650390625 and accuracy 0.10277777910232544.\n",
      "Test batch 129 with loss 1.7486867904663086 and accuracy 0.2222222238779068.\n",
      "Test batch 130 with loss 1.8576114177703857 and accuracy 0.2083333432674408.\n",
      "Test batch 131 with loss 1.838538408279419 and accuracy 0.0833333358168602.\n",
      "Test batch 132 with loss 1.8691257238388062 and accuracy 0.15000000596046448.\n",
      "Test batch 133 with loss 1.7858994007110596 and accuracy 0.1527777761220932.\n",
      "Test batch 134 with loss 1.8996937274932861 and accuracy 0.08888889104127884.\n",
      "Test batch 135 with loss 1.8812415599822998 and accuracy 0.2750000059604645.\n",
      "Test batch 136 with loss 1.7556043863296509 and accuracy 0.0972222238779068.\n",
      "Test batch 137 with loss 1.8573198318481445 and accuracy 0.14166666567325592.\n",
      "Test batch 138 with loss 1.8691152334213257 and accuracy 0.10000000894069672.\n",
      "Test batch 139 with loss 1.783172369003296 and accuracy 0.2599206268787384.\n",
      "Test batch 140 with loss 1.7089545726776123 and accuracy 0.3333333134651184.\n",
      "Test batch 141 with loss 1.8701603412628174 and accuracy 0.1626984179019928.\n",
      "Test batch 142 with loss 1.7840665578842163 and accuracy 0.3777777850627899.\n",
      "Test batch 143 with loss 1.7633705139160156 and accuracy 0.20000001788139343.\n",
      "Test batch 144 with loss 1.8655297756195068 and accuracy 0.1111111119389534.\n",
      "Test batch 145 with loss 1.7181774377822876 and accuracy 0.19603174924850464.\n",
      "Test batch 146 with loss 1.8414360284805298 and accuracy 0.2083333432674408.\n",
      "Test batch 147 with loss 1.8701292276382446 and accuracy 0.14444445073604584.\n",
      "Test batch 148 with loss 1.9366929531097412 and accuracy 0.0.\n",
      "Test batch 149 with loss 1.8736129999160767 and accuracy 0.125.\n",
      "Test batch 150 with loss 1.8451850414276123 and accuracy 0.2777777910232544.\n",
      "Test batch 151 with loss 1.8028221130371094 and accuracy 0.1071428582072258.\n",
      "Test batch 152 with loss 1.7891569137573242 and accuracy 0.4138889014720917.\n",
      "Test batch 153 with loss 1.8372066020965576 and accuracy 0.2142857164144516.\n",
      "Test batch 154 with loss 1.7919219732284546 and accuracy 0.222222238779068.\n",
      "Test batch 155 with loss 1.8479595184326172 and accuracy 0.1666666716337204.\n",
      "Test batch 156 with loss 1.7850673198699951 and accuracy 0.125.\n",
      "Test batch 157 with loss 1.7584764957427979 and accuracy 0.5249999761581421.\n",
      "Test batch 158 with loss 1.842764139175415 and accuracy 0.14444445073604584.\n",
      "Test batch 159 with loss 1.806481122970581 and accuracy 0.1111111119389534.\n",
      "Test batch 160 with loss 1.8204209804534912 and accuracy 0.1944444477558136.\n",
      "Test batch 161 with loss 1.8123794794082642 and accuracy 0.0555555559694767.\n",
      "Test batch 162 with loss 1.780217170715332 and accuracy 0.10833333432674408.\n",
      "Test batch 163 with loss 1.698930025100708 and accuracy 0.17777778208255768.\n",
      "Test batch 164 with loss 1.7647536993026733 and accuracy 0.14444445073604584.\n",
      "Test batch 165 with loss 1.7798378467559814 and accuracy 0.2083333432674408.\n",
      "Test batch 166 with loss 1.744277000427246 and accuracy 0.3499999940395355.\n",
      "Test batch 167 with loss 1.7917835712432861 and accuracy 0.4611111283302307.\n",
      "Test batch 168 with loss 1.801364541053772 and accuracy 0.16851851344108582.\n",
      "Test batch 169 with loss 1.7697131633758545 and accuracy 0.22499999403953552.\n",
      "Test batch 170 with loss 1.7648566961288452 and accuracy 0.11666667461395264.\n",
      "Test batch 171 with loss 1.8884048461914062 and accuracy 0.0555555559694767.\n",
      "Test batch 172 with loss 1.8105857372283936 and accuracy 0.1111111119389534.\n",
      "Test batch 173 with loss 1.7861039638519287 and accuracy 0.347222238779068.\n",
      "Test batch 174 with loss 1.8251441717147827 and accuracy 0.1388888955116272.\n",
      "Test batch 175 with loss 1.9011735916137695 and accuracy 0.0833333358168602.\n",
      "Test batch 176 with loss 1.8372348546981812 and accuracy 0.125.\n",
      "Test batch 177 with loss 1.6817255020141602 and accuracy 0.30000001192092896.\n",
      "Test batch 178 with loss 1.8260209560394287 and accuracy 0.1944444477558136.\n",
      "Test batch 179 with loss 1.8927751779556274 and accuracy 0.1111111119389534.\n",
      "Test batch 180 with loss 1.848401427268982 and accuracy 0.1944444477558136.\n",
      "Test batch 181 with loss 1.7800127267837524 and accuracy 0.2876984179019928.\n",
      "Test batch 182 with loss 1.811253309249878 and accuracy 0.15000000596046448.\n",
      "Test batch 183 with loss 1.731141448020935 and accuracy 0.25.\n",
      "Test batch 184 with loss 1.891463279724121 and accuracy 0.125.\n",
      "Test batch 185 with loss 1.86672842502594 and accuracy 0.0694444477558136.\n",
      "Test batch 186 with loss 1.835566520690918 and accuracy 0.0833333358168602.\n",
      "Test batch 187 with loss 1.8238166570663452 and accuracy 0.03333333507180214.\n",
      "Test batch 188 with loss 1.7606960535049438 and accuracy 0.2638888955116272.\n",
      "Test batch 189 with loss 1.8118871450424194 and accuracy 0.0972222238779068.\n",
      "Test batch 190 with loss 1.7726562023162842 and accuracy 0.28333333134651184.\n",
      "Test batch 191 with loss 1.8172690868377686 and accuracy 0.190476194024086.\n",
      "Test batch 192 with loss 1.7893644571304321 and accuracy 0.24722222983837128.\n",
      "Test batch 193 with loss 1.7912956476211548 and accuracy 0.1805555671453476.\n",
      "Test batch 194 with loss 1.8219776153564453 and accuracy 0.2083333432674408.\n",
      "Test batch 195 with loss 1.8385875225067139 and accuracy 0.0833333358168602.\n",
      "Test batch 196 with loss 1.8648828268051147 and accuracy 0.15833333134651184.\n",
      "Test batch 197 with loss 1.7349332571029663 and accuracy 0.3055555522441864.\n",
      "Test batch 198 with loss 1.8608310222625732 and accuracy 0.1319444477558136.\n",
      "Test batch 199 with loss 1.8468544483184814 and accuracy 0.125.\n",
      "Test batch 200 with loss 1.8582258224487305 and accuracy 0.0833333358168602.\n",
      "Test batch 201 with loss 1.828478455543518 and accuracy 0.0972222238779068.\n",
      "Test batch 202 with loss 1.839996099472046 and accuracy 0.0833333358168602.\n",
      "Test batch 203 with loss 1.7688173055648804 and accuracy 0.1944444477558136.\n",
      "Test batch 204 with loss 1.7919120788574219 and accuracy 0.0555555559694767.\n",
      "Test batch 205 with loss 1.8145360946655273 and accuracy 0.11666667461395264.\n",
      "Test batch 206 with loss 1.8340730667114258 and accuracy 0.19722223281860352.\n",
      "Test batch 207 with loss 1.8336910009384155 and accuracy 0.1388888955116272.\n",
      "Test batch 208 with loss 1.759368896484375 and accuracy 0.2916666865348816.\n",
      "Test batch 209 with loss 1.7701084613800049 and accuracy 0.33888888359069824.\n",
      "Test batch 210 with loss 1.8336982727050781 and accuracy 0.1349206417798996.\n",
      "Test batch 211 with loss 1.8899924755096436 and accuracy 0.07500000298023224.\n",
      "Test batch 212 with loss 1.8705580234527588 and accuracy 0.2750000059604645.\n",
      "Test batch 213 with loss 1.806760549545288 and accuracy 0.1527777761220932.\n",
      "Test batch 214 with loss 1.855847716331482 and accuracy 0.125.\n",
      "Test batch 215 with loss 1.7991762161254883 and accuracy 0.3500000238418579.\n",
      "Test batch 216 with loss 1.8207134008407593 and accuracy 0.10000000894069672.\n",
      "Test batch 217 with loss 1.8121073246002197 and accuracy 0.20555555820465088.\n",
      "Test batch 218 with loss 1.8202221393585205 and accuracy 0.20000000298023224.\n",
      "Test batch 219 with loss 1.8807960748672485 and accuracy 0.1111111119389534.\n",
      "Test batch 220 with loss 1.8130954504013062 and accuracy 0.08888889104127884.\n",
      "Test batch 221 with loss 1.891431450843811 and accuracy 0.0972222238779068.\n",
      "Test batch 222 with loss 1.795897126197815 and accuracy 0.17222222685813904.\n",
      "Test batch 223 with loss 1.8639562129974365 and accuracy 0.0833333358168602.\n",
      "Test batch 224 with loss 1.7893699407577515 and accuracy 0.10277777910232544.\n",
      "Test batch 225 with loss 1.7706331014633179 and accuracy 0.2083333432674408.\n",
      "Test batch 226 with loss 1.9259212017059326 and accuracy 0.0.\n",
      "Test batch 227 with loss 1.8275213241577148 and accuracy 0.0972222238779068.\n",
      "Test batch 228 with loss 1.9156863689422607 and accuracy 0.130952388048172.\n",
      "Test batch 229 with loss 1.813423752784729 and accuracy 0.16428571939468384.\n",
      "Test batch 230 with loss 1.8452327251434326 and accuracy 0.17777778208255768.\n",
      "Test batch 231 with loss 1.8055626153945923 and accuracy 0.11666667461395264.\n",
      "Test batch 232 with loss 1.8690288066864014 and accuracy 0.02777777798473835.\n",
      "Test batch 233 with loss 1.8730661869049072 and accuracy 0.0416666679084301.\n",
      "Test batch 234 with loss 1.8031898736953735 and accuracy 0.11666667461395264.\n",
      "Test batch 235 with loss 1.8405565023422241 and accuracy 0.125.\n",
      "Test batch 236 with loss 1.817659616470337 and accuracy 0.29722222685813904.\n",
      "Test batch 237 with loss 1.8784286975860596 and accuracy 0.0972222238779068.\n",
      "Test batch 238 with loss 1.7291762828826904 and accuracy 0.21388888359069824.\n",
      "Test batch 239 with loss 1.7668735980987549 and accuracy 0.1666666716337204.\n",
      "Test batch 240 with loss 1.7634626626968384 and accuracy 0.31666669249534607.\n",
      "Test batch 241 with loss 1.8550317287445068 and accuracy 0.03333333507180214.\n",
      "Test batch 242 with loss 1.8664095401763916 and accuracy 0.2142857313156128.\n",
      "Test batch 243 with loss 1.799445390701294 and accuracy 0.16388888657093048.\n",
      "Test batch 244 with loss 1.7598488330841064 and accuracy 0.33888888359069824.\n",
      "Test batch 245 with loss 1.8862552642822266 and accuracy 0.075396828353405.\n",
      "Test batch 246 with loss 1.79824697971344 and accuracy 0.2876984179019928.\n",
      "Test batch 247 with loss 1.8359358310699463 and accuracy 0.1071428582072258.\n",
      "Test batch 248 with loss 1.7892532348632812 and accuracy 0.2916666865348816.\n",
      "Test batch 249 with loss 1.825368881225586 and accuracy 0.3333333134651184.\n",
      "Test batch 250 with loss 1.779433250427246 and accuracy 0.2361111044883728.\n",
      "Test batch 251 with loss 1.791430115699768 and accuracy 0.0416666679084301.\n",
      "Test batch 252 with loss 1.7367362976074219 and accuracy 0.1666666716337204.\n",
      "Test batch 253 with loss 1.764367699623108 and accuracy 0.1944444477558136.\n",
      "Test batch 254 with loss 1.805426836013794 and accuracy 0.21388889849185944.\n",
      "Test batch 255 with loss 1.8295406103134155 and accuracy 0.0.\n",
      "Test batch 256 with loss 1.7881557941436768 and accuracy 0.3333333432674408.\n",
      "Test batch 257 with loss 1.8478447198867798 and accuracy 0.0793650820851326.\n",
      "Test batch 258 with loss 1.7749788761138916 and accuracy 0.2222222238779068.\n",
      "Test batch 259 with loss 1.8384370803833008 and accuracy 0.03333333507180214.\n",
      "Test batch 260 with loss 1.7997872829437256 and accuracy 0.0793650820851326.\n",
      "Test batch 261 with loss 1.8670337200164795 and accuracy 0.0833333358168602.\n",
      "Test batch 262 with loss 1.860522985458374 and accuracy 0.0694444477558136.\n",
      "Test batch 263 with loss 1.878077507019043 and accuracy 0.07500000298023224.\n",
      "Test batch 264 with loss 1.8177554607391357 and accuracy 0.17222222685813904.\n",
      "Test batch 265 with loss 1.8850339651107788 and accuracy 0.07500000298023224.\n",
      "Test batch 266 with loss 1.875003457069397 and accuracy 0.236111119389534.\n",
      "Test batch 267 with loss 1.680818796157837 and accuracy 0.2986111044883728.\n",
      "Test batch 268 with loss 1.8281402587890625 and accuracy 0.0972222238779068.\n",
      "Test batch 269 with loss 1.8674862384796143 and accuracy 0.0515873022377491.\n",
      "Test batch 270 with loss 1.8294929265975952 and accuracy 0.2291666716337204.\n",
      "Test batch 271 with loss 1.8842418193817139 and accuracy 0.0833333358168602.\n",
      "Test batch 272 with loss 1.8447942733764648 and accuracy 0.0833333358168602.\n",
      "Test batch 273 with loss 1.798750877380371 and accuracy 0.1666666716337204.\n",
      "Test batch 274 with loss 1.8649097681045532 and accuracy 0.1388888955116272.\n",
      "Test batch 275 with loss 1.8018821477890015 and accuracy 0.11666667461395264.\n",
      "Test batch 276 with loss 1.8168275356292725 and accuracy 0.07500000298023224.\n",
      "Test batch 277 with loss 1.7503591775894165 and accuracy 0.24166668951511383.\n",
      "Test batch 278 with loss 1.7597042322158813 and accuracy 0.15000000596046448.\n",
      "Test batch 279 with loss 1.6912683248519897 and accuracy 0.2777777910232544.\n",
      "Test batch 280 with loss 1.9287879467010498 and accuracy 0.0416666679084301.\n",
      "Test batch 281 with loss 1.776320457458496 and accuracy 0.06111111491918564.\n",
      "Test batch 282 with loss 1.7683513164520264 and accuracy 0.1388888955116272.\n",
      "Test batch 283 with loss 1.8855388164520264 and accuracy 0.2420634925365448.\n",
      "Test batch 284 with loss 1.7934612035751343 and accuracy 0.22777777910232544.\n",
      "Test batch 285 with loss 1.8276211023330688 and accuracy 0.111111119389534.\n",
      "Test batch 286 with loss 1.8150312900543213 and accuracy 0.125.\n",
      "Test batch 287 with loss 1.758195161819458 and accuracy 0.1865079402923584.\n",
      "Test batch 288 with loss 1.7870228290557861 and accuracy 0.10000000894069672.\n",
      "Test batch 289 with loss 1.761152982711792 and accuracy 0.1944444477558136.\n",
      "Test batch 290 with loss 1.8023498058319092 and accuracy 0.1319444477558136.\n",
      "Test batch 291 with loss 1.8612089157104492 and accuracy 0.07500000298023224.\n",
      "Test batch 292 with loss 1.7868738174438477 and accuracy 0.222222238779068.\n",
      "Test batch 293 with loss 1.8971598148345947 and accuracy 0.0833333358168602.\n",
      "Test batch 294 with loss 1.8467270135879517 and accuracy 0.29722222685813904.\n",
      "Test batch 295 with loss 1.9046550989151 and accuracy 0.0833333358168602.\n",
      "Test batch 296 with loss 1.7506463527679443 and accuracy 0.1944444477558136.\n",
      "Test batch 297 with loss 1.8687307834625244 and accuracy 0.0555555559694767.\n",
      "Test batch 298 with loss 1.823650598526001 and accuracy 0.1944444477558136.\n",
      "Test batch 299 with loss 1.861371636390686 and accuracy 0.0476190485060215.\n",
      "Test batch 300 with loss 1.9616972208023071 and accuracy 0.0416666679084301.\n",
      "Test batch 301 with loss 1.7812020778656006 and accuracy 0.10833333432674408.\n",
      "Test batch 302 with loss 1.8777488470077515 and accuracy 0.1944444477558136.\n",
      "Test batch 303 with loss 1.789050817489624 and accuracy 0.1805555522441864.\n",
      "Test batch 304 with loss 1.8641382455825806 and accuracy 0.1527777761220932.\n",
      "Test batch 305 with loss 1.8952062129974365 and accuracy 0.0416666679084301.\n",
      "Test batch 306 with loss 1.838012933731079 and accuracy 0.25.\n",
      "Test batch 307 with loss 1.7533975839614868 and accuracy 0.21388889849185944.\n",
      "Test batch 308 with loss 1.6899442672729492 and accuracy 0.3194444477558136.\n",
      "Test batch 309 with loss 1.7779881954193115 and accuracy 0.2916666567325592.\n",
      "Test batch 310 with loss 1.7394908666610718 and accuracy 0.3444444537162781.\n",
      "Test batch 311 with loss 1.9126813411712646 and accuracy 0.0694444477558136.\n",
      "Test batch 312 with loss 1.8966524600982666 and accuracy 0.17222222685813904.\n",
      "Test batch 313 with loss 1.8364372253417969 and accuracy 0.17592592537403107.\n",
      "Test batch 314 with loss 1.8647836446762085 and accuracy 0.1666666716337204.\n",
      "Test batch 315 with loss 1.8659803867340088 and accuracy 0.0555555559694767.\n",
      "Test batch 316 with loss 1.735848069190979 and accuracy 0.15833333134651184.\n",
      "Test batch 317 with loss 1.78225839138031 and accuracy 0.2888889014720917.\n",
      "Test batch 318 with loss 1.8913748264312744 and accuracy 0.2777777910232544.\n",
      "Test batch 319 with loss 1.7295023202896118 and accuracy 0.17777778208255768.\n",
      "Test batch 320 with loss 1.8275798559188843 and accuracy 0.2916666567325592.\n",
      "Test batch 321 with loss 1.7066484689712524 and accuracy 0.3611111044883728.\n",
      "Test batch 322 with loss 1.938176155090332 and accuracy 0.0.\n",
      "Test batch 323 with loss 1.9006502628326416 and accuracy 0.236111119389534.\n",
      "Test batch 324 with loss 1.7824615240097046 and accuracy 0.1388888955116272.\n",
      "Test batch 325 with loss 1.8108749389648438 and accuracy 0.0555555559694767.\n",
      "Test batch 326 with loss 1.8752597570419312 and accuracy 0.0972222238779068.\n",
      "Test batch 327 with loss 1.8408880233764648 and accuracy 0.08095238357782364.\n",
      "Test batch 328 with loss 1.7125266790390015 and accuracy 0.44166669249534607.\n",
      "Test batch 329 with loss 1.8295021057128906 and accuracy 0.13055555522441864.\n",
      "Test batch 330 with loss 1.7455122470855713 and accuracy 0.222222238779068.\n",
      "Test batch 331 with loss 1.8238636255264282 and accuracy 0.15000000596046448.\n",
      "Test batch 332 with loss 1.7419925928115845 and accuracy 0.1805555522441864.\n",
      "Test batch 333 with loss 1.7954587936401367 and accuracy 0.1944444477558136.\n",
      "Test batch 334 with loss 1.8135515451431274 and accuracy 0.1527777761220932.\n",
      "Test batch 335 with loss 1.8532403707504272 and accuracy 0.03333333507180214.\n",
      "Test batch 336 with loss 1.865140676498413 and accuracy 0.11269842088222504.\n",
      "Test batch 337 with loss 1.7749931812286377 and accuracy 0.222222238779068.\n",
      "Test batch 338 with loss 1.8666632175445557 and accuracy 0.25.\n",
      "Test batch 339 with loss 1.841670036315918 and accuracy 0.22777777910232544.\n",
      "Test batch 340 with loss 1.8760391473770142 and accuracy 0.06111111491918564.\n",
      "Test batch 341 with loss 1.7871023416519165 and accuracy 0.1666666716337204.\n",
      "Test batch 342 with loss 1.766396164894104 and accuracy 0.2083333432674408.\n",
      "Test batch 343 with loss 1.7695192098617554 and accuracy 0.10000000149011612.\n",
      "Test batch 344 with loss 1.770848274230957 and accuracy 0.13055555522441864.\n",
      "Test batch 345 with loss 1.8249719142913818 and accuracy 0.1666666716337204.\n",
      "Test batch 346 with loss 1.7254377603530884 and accuracy 0.21388889849185944.\n",
      "Test batch 347 with loss 1.8337684869766235 and accuracy 0.0972222238779068.\n",
      "Test batch 348 with loss 1.7755693197250366 and accuracy 0.20555555820465088.\n",
      "Test batch 349 with loss 1.8377444744110107 and accuracy 0.03333333507180214.\n",
      "Test batch 350 with loss 1.8120768070220947 and accuracy 0.0833333358168602.\n",
      "Test batch 351 with loss 1.804255485534668 and accuracy 0.2083333432674408.\n",
      "Test batch 352 with loss 1.8710107803344727 and accuracy 0.10000000894069672.\n",
      "Test batch 353 with loss 1.7556922435760498 and accuracy 0.3638888895511627.\n",
      "Test batch 354 with loss 1.863486647605896 and accuracy 0.15000000596046448.\n",
      "Test batch 355 with loss 1.7669742107391357 and accuracy 0.2777777910232544.\n",
      "Test batch 356 with loss 1.8542829751968384 and accuracy 0.0694444477558136.\n",
      "Test batch 357 with loss 1.8034045696258545 and accuracy 0.2361111044883728.\n",
      "Test batch 358 with loss 1.985655426979065 and accuracy 0.02380952425301075.\n",
      "Test batch 359 with loss 1.8164926767349243 and accuracy 0.190476194024086.\n",
      "Test batch 360 with loss 1.8844833374023438 and accuracy 0.111111119389534.\n",
      "Test batch 361 with loss 1.8547048568725586 and accuracy 0.03333333507180214.\n",
      "Test batch 362 with loss 1.79757821559906 and accuracy 0.3500000238418579.\n",
      "Test batch 363 with loss 1.7597100734710693 and accuracy 0.1349206417798996.\n",
      "Test batch 364 with loss 1.7423999309539795 and accuracy 0.3392857313156128.\n",
      "Test batch 365 with loss 1.8042348623275757 and accuracy 0.03333333507180214.\n",
      "Test batch 366 with loss 1.8345178365707397 and accuracy 0.17222222685813904.\n",
      "Test batch 367 with loss 1.6872613430023193 and accuracy 0.2916666567325592.\n",
      "Test batch 368 with loss 1.6916999816894531 and accuracy 0.3194444477558136.\n",
      "Test batch 369 with loss 1.773646354675293 and accuracy 0.19722223281860352.\n",
      "Test batch 370 with loss 1.7657550573349 and accuracy 0.0833333358168602.\n",
      "Test batch 371 with loss 1.8851745128631592 and accuracy 0.125.\n",
      "Test batch 372 with loss 1.8452907800674438 and accuracy 0.0833333358168602.\n",
      "Test batch 373 with loss 1.860682487487793 and accuracy 0.190476194024086.\n",
      "Test batch 374 with loss 1.844321608543396 and accuracy 0.2638888955116272.\n",
      "Test batch 375 with loss 1.8261677026748657 and accuracy 0.1527777761220932.\n",
      "Test batch 376 with loss 1.7812141180038452 and accuracy 0.14166666567325592.\n",
      "Test batch 377 with loss 1.8433805704116821 and accuracy 0.1626984179019928.\n",
      "Test batch 378 with loss 1.8322452306747437 and accuracy 0.06666667014360428.\n",
      "Test batch 379 with loss 1.8327033519744873 and accuracy 0.18333333730697632.\n",
      "Test batch 380 with loss 1.7926967144012451 and accuracy 0.20000000298023224.\n",
      "Test batch 381 with loss 1.750745415687561 and accuracy 0.1388888955116272.\n",
      "Test batch 382 with loss 1.7678800821304321 and accuracy 0.10000000894069672.\n",
      "Test batch 383 with loss 1.796055555343628 and accuracy 0.07500000298023224.\n",
      "Test batch 384 with loss 1.7817714214324951 and accuracy 0.22777777910232544.\n",
      "Test batch 385 with loss 1.8157402276992798 and accuracy 0.3611111044883728.\n",
      "Test batch 386 with loss 1.8449313640594482 and accuracy 0.07500000298023224.\n",
      "Test batch 387 with loss 1.7887064218521118 and accuracy 0.15000000596046448.\n",
      "Test batch 388 with loss 1.7518329620361328 and accuracy 0.3305555582046509.\n",
      "Test batch 389 with loss 1.7990028858184814 and accuracy 0.36666667461395264.\n",
      "Test batch 390 with loss 1.8063812255859375 and accuracy 0.1527777761220932.\n",
      "Test batch 391 with loss 1.726601243019104 and accuracy 0.21388889849185944.\n",
      "Test batch 392 with loss 1.7796576023101807 and accuracy 0.14444445073604584.\n",
      "Test batch 393 with loss 1.9453243017196655 and accuracy 0.125.\n",
      "Test batch 394 with loss 1.7991113662719727 and accuracy 0.0555555559694767.\n",
      "Test batch 395 with loss 1.8548921346664429 and accuracy 0.1388888955116272.\n",
      "Test batch 396 with loss 1.7748075723648071 and accuracy 0.0694444477558136.\n",
      "Test batch 397 with loss 1.8556703329086304 and accuracy 0.0555555559694767.\n",
      "Test batch 398 with loss 1.9060026407241821 and accuracy 0.15000000596046448.\n",
      "Test batch 399 with loss 1.8619804382324219 and accuracy 0.1527777761220932.\n",
      "Test batch 400 with loss 1.8395999670028687 and accuracy 0.125.\n",
      "Test batch 401 with loss 1.7321014404296875 and accuracy 0.3333333432674408.\n",
      "Test batch 402 with loss 1.8149080276489258 and accuracy 0.3035714328289032.\n",
      "Test batch 403 with loss 1.8165607452392578 and accuracy 0.22777777910232544.\n",
      "Test batch 404 with loss 1.8141425848007202 and accuracy 0.07500000298023224.\n",
      "Test batch 405 with loss 1.8352333307266235 and accuracy 0.1388888955116272.\n",
      "Test batch 406 with loss 1.8700010776519775 and accuracy 0.1388888955116272.\n",
      "Test batch 407 with loss 1.8512792587280273 and accuracy 0.02777777798473835.\n",
      "Test batch 408 with loss 1.8047006130218506 and accuracy 0.1388888955116272.\n",
      "Test batch 409 with loss 1.7475935220718384 and accuracy 0.22777777910232544.\n",
      "Test batch 410 with loss 1.790831208229065 and accuracy 0.07500000298023224.\n",
      "Test batch 411 with loss 1.9224086999893188 and accuracy 0.0972222238779068.\n",
      "Test batch 412 with loss 1.7868388891220093 and accuracy 0.2750000059604645.\n",
      "Test batch 413 with loss 1.8438518047332764 and accuracy 0.09444444626569748.\n",
      "Test batch 414 with loss 1.867546796798706 and accuracy 0.0416666679084301.\n",
      "Test batch 415 with loss 1.8236653804779053 and accuracy 0.125.\n",
      "Test batch 416 with loss 1.8737328052520752 and accuracy 0.17222222685813904.\n",
      "Test batch 417 with loss 1.8691858053207397 and accuracy 0.0972222238779068.\n",
      "Test batch 418 with loss 1.728994607925415 and accuracy 0.3888888955116272.\n",
      "Test batch 419 with loss 1.8541843891143799 and accuracy 0.1666666716337204.\n",
      "Test batch 420 with loss 1.8464769124984741 and accuracy 0.18518519401550293.\n",
      "Test batch 421 with loss 1.8111398220062256 and accuracy 0.14444445073604584.\n",
      "Test batch 422 with loss 1.8170702457427979 and accuracy 0.2142857164144516.\n",
      "Test batch 423 with loss 1.7746703624725342 and accuracy 0.06666667014360428.\n",
      "Test batch 424 with loss 1.7706925868988037 and accuracy 0.16388890147209167.\n",
      "Test batch 425 with loss 1.8799512386322021 and accuracy 0.14166666567325592.\n",
      "Test batch 426 with loss 1.818813681602478 and accuracy 0.1111111119389534.\n",
      "Test batch 427 with loss 1.8340250253677368 and accuracy 0.0972222238779068.\n",
      "Test batch 428 with loss 1.8277184963226318 and accuracy 0.08888889104127884.\n",
      "Test batch 429 with loss 1.8289979696273804 and accuracy 0.21944445371627808.\n",
      "Test batch 430 with loss 1.7606837749481201 and accuracy 0.347222238779068.\n",
      "Test batch 431 with loss 1.841827154159546 and accuracy 0.0476190485060215.\n",
      "Test batch 432 with loss 1.7961899042129517 and accuracy 0.07500000298023224.\n",
      "Test batch 433 with loss 1.801566481590271 and accuracy 0.03333333507180214.\n",
      "Test batch 434 with loss 1.8396408557891846 and accuracy 0.2083333432674408.\n",
      "Test batch 435 with loss 1.8752191066741943 and accuracy 0.125.\n",
      "Test batch 436 with loss 1.7903692722320557 and accuracy 0.0694444477558136.\n",
      "Test batch 437 with loss 1.7913624048233032 and accuracy 0.0833333358168602.\n",
      "Test batch 438 with loss 1.7273404598236084 and accuracy 0.3222222328186035.\n",
      "Test batch 439 with loss 1.8119890689849854 and accuracy 0.1944444477558136.\n",
      "Test batch 440 with loss 1.824702501296997 and accuracy 0.2571428716182709.\n",
      "Test batch 441 with loss 1.8554731607437134 and accuracy 0.10000000149011612.\n",
      "Test batch 442 with loss 1.8072459697723389 and accuracy 0.125.\n",
      "Test batch 443 with loss 1.7486884593963623 and accuracy 0.13750000298023224.\n",
      "Test batch 444 with loss 1.8633842468261719 and accuracy 0.11666666716337204.\n",
      "Test batch 445 with loss 1.8508336544036865 and accuracy 0.1666666865348816.\n",
      "Test batch 446 with loss 1.7741672992706299 and accuracy 0.2222222238779068.\n",
      "Test batch 447 with loss 1.8663848638534546 and accuracy 0.1111111119389534.\n",
      "Test batch 448 with loss 1.8604955673217773 and accuracy 0.2500000298023224.\n",
      "Test batch 449 with loss 1.9161250591278076 and accuracy 0.1666666716337204.\n",
      "Test batch 450 with loss 1.84256112575531 and accuracy 0.0833333358168602.\n",
      "Test batch 451 with loss 1.9093563556671143 and accuracy 0.0833333358168602.\n",
      "Test batch 452 with loss 1.7449966669082642 and accuracy 0.22777777910232544.\n",
      "Test batch 453 with loss 1.8148231506347656 and accuracy 0.1666666716337204.\n",
      "Test batch 454 with loss 1.7073614597320557 and accuracy 0.1071428582072258.\n",
      "Test batch 455 with loss 1.8839085102081299 and accuracy 0.0694444477558136.\n",
      "Test batch 456 with loss 1.769330382347107 and accuracy 0.1041666716337204.\n",
      "Test batch 457 with loss 1.8980509042739868 and accuracy 0.06111111491918564.\n",
      "Test batch 458 with loss 1.7792339324951172 and accuracy 0.1527777761220932.\n",
      "Test batch 459 with loss 1.8300718069076538 and accuracy 0.1875.\n",
      "Test batch 460 with loss 1.7342170476913452 and accuracy 0.14166666567325592.\n",
      "Test batch 461 with loss 1.895395278930664 and accuracy 0.1388888955116272.\n",
      "Test batch 462 with loss 1.9379281997680664 and accuracy 0.02777777798473835.\n",
      "Test batch 463 with loss 1.8707151412963867 and accuracy 0.1388888955116272.\n",
      "Test batch 464 with loss 1.867266058921814 and accuracy 0.0416666679084301.\n",
      "Test batch 465 with loss 1.7496535778045654 and accuracy 0.17083334922790527.\n",
      "Test batch 466 with loss 1.80350661277771 and accuracy 0.1944444477558136.\n",
      "Test batch 467 with loss 1.789471983909607 and accuracy 0.472222238779068.\n",
      "Test batch 468 with loss 1.7675466537475586 and accuracy 0.1825396865606308.\n",
      "Test batch 469 with loss 1.8038969039916992 and accuracy 0.18888889253139496.\n",
      "Test batch 470 with loss 1.7599256038665771 and accuracy 0.1527777761220932.\n",
      "Test batch 471 with loss 1.8093363046646118 and accuracy 0.2750000059604645.\n",
      "Test batch 472 with loss 1.8341137170791626 and accuracy 0.05185185372829437.\n",
      "Test batch 473 with loss 1.7831417322158813 and accuracy 0.0833333358168602.\n",
      "Test batch 474 with loss 1.8480192422866821 and accuracy 0.0476190485060215.\n",
      "Test batch 475 with loss 1.8330570459365845 and accuracy 0.125.\n",
      "Test batch 476 with loss 1.883927345275879 and accuracy 0.06666667014360428.\n",
      "Test batch 477 with loss 1.8043336868286133 and accuracy 0.11666667461395264.\n",
      "Test batch 478 with loss 1.8305699825286865 and accuracy 0.1111111119389534.\n",
      "Test batch 479 with loss 1.8067117929458618 and accuracy 0.2341269850730896.\n",
      "Test batch 480 with loss 1.8057401180267334 and accuracy 0.06666667014360428.\n",
      "Test batch 481 with loss 1.8321088552474976 and accuracy 0.222222238779068.\n",
      "Test batch 482 with loss 1.9029080867767334 and accuracy 0.1111111119389534.\n",
      "Test batch 483 with loss 1.8752233982086182 and accuracy 0.0.\n",
      "Test batch 484 with loss 1.7850841283798218 and accuracy 0.1865079402923584.\n",
      "Test batch 485 with loss 1.876932144165039 and accuracy 0.1388888955116272.\n",
      "Test batch 486 with loss 1.8914235830307007 and accuracy 0.12037037312984467.\n",
      "Test batch 487 with loss 1.7952619791030884 and accuracy 0.236111119389534.\n",
      "Test batch 488 with loss 1.777012586593628 and accuracy 0.0625.\n",
      "Test batch 489 with loss 1.8540903329849243 and accuracy 0.0416666679084301.\n",
      "Test batch 490 with loss 1.7889807224273682 and accuracy 0.375.\n",
      "Test batch 491 with loss 1.7607228755950928 and accuracy 0.3333333432674408.\n",
      "Test batch 492 with loss 1.7612262964248657 and accuracy 0.28333336114883423.\n",
      "Test batch 493 with loss 1.7883756160736084 and accuracy 0.222222238779068.\n",
      "Test batch 494 with loss 1.7289364337921143 and accuracy 0.23333333432674408.\n",
      "Test batch 495 with loss 1.8177688121795654 and accuracy 0.125.\n",
      "Test batch 496 with loss 1.7640037536621094 and accuracy 0.25.\n",
      "Test batch 497 with loss 1.892120122909546 and accuracy 0.03333333507180214.\n",
      "Test batch 498 with loss 1.8205302953720093 and accuracy 0.2222222238779068.\n",
      "Test batch 499 with loss 1.7935569286346436 and accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 0 with loss 1.7322584390640259, accuracy 0.2888889014720917.\n",
      "Training epoch 2 batch 1 with loss 1.741093635559082, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 2 with loss 1.6957107782363892, accuracy 0.2666666805744171.\n",
      "Training epoch 2 batch 3 with loss 1.82785165309906, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 4 with loss 1.7997077703475952, accuracy 0.25555557012557983.\n",
      "Training epoch 2 batch 5 with loss 1.8424923419952393, accuracy 0.17777778208255768.\n",
      "Training epoch 2 batch 6 with loss 1.7813870906829834, accuracy 0.23333333432674408.\n",
      "Training epoch 2 batch 7 with loss 1.6643235683441162, accuracy 0.3888888955116272.\n",
      "Training epoch 2 batch 8 with loss 1.681430459022522, accuracy 0.26944443583488464.\n",
      "Training epoch 2 batch 9 with loss 1.7273378372192383, accuracy 0.2805555760860443.\n",
      "Training epoch 2 batch 10 with loss 1.7763373851776123, accuracy 0.10277777910232544.\n",
      "Training epoch 2 batch 11 with loss 1.7551586627960205, accuracy 0.2936508059501648.\n",
      "Training epoch 2 batch 12 with loss 1.7980495691299438, accuracy 0.15833334624767303.\n",
      "Training epoch 2 batch 13 with loss 1.7469604015350342, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 14 with loss 1.723901391029358, accuracy 0.23333334922790527.\n",
      "Training epoch 2 batch 15 with loss 1.7037105560302734, accuracy 0.2888889014720917.\n",
      "Training epoch 2 batch 16 with loss 1.7838213443756104, accuracy 0.347222238779068.\n",
      "Training epoch 2 batch 17 with loss 1.7560688257217407, accuracy 0.3055555820465088.\n",
      "Training epoch 2 batch 18 with loss 1.6689876317977905, accuracy 0.204365074634552.\n",
      "Training epoch 2 batch 19 with loss 1.657400369644165, accuracy 0.22500000894069672.\n",
      "Training epoch 2 batch 20 with loss 1.8397352695465088, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 21 with loss 1.7163479328155518, accuracy 0.2805555462837219.\n",
      "Training epoch 2 batch 22 with loss 1.666494369506836, accuracy 0.27142858505249023.\n",
      "Training epoch 2 batch 23 with loss 1.7111599445343018, accuracy 0.23095238208770752.\n",
      "Training epoch 2 batch 24 with loss 1.7641738653182983, accuracy 0.3888888955116272.\n",
      "Training epoch 2 batch 25 with loss 1.7593542337417603, accuracy 0.20000000298023224.\n",
      "Training epoch 2 batch 26 with loss 1.738105058670044, accuracy 0.2611111104488373.\n",
      "Training epoch 2 batch 27 with loss 1.7723805904388428, accuracy 0.3432539701461792.\n",
      "Training epoch 2 batch 28 with loss 1.7532085180282593, accuracy 0.3166666626930237.\n",
      "Training epoch 2 batch 29 with loss 1.6825838088989258, accuracy 0.18095238506793976.\n",
      "Training epoch 2 batch 30 with loss 1.756710410118103, accuracy 0.2361111044883728.\n",
      "Training epoch 2 batch 31 with loss 1.787205696105957, accuracy 0.20555555820465088.\n",
      "Training epoch 2 batch 32 with loss 1.6628011465072632, accuracy 0.23333334922790527.\n",
      "Training epoch 2 batch 33 with loss 1.7802479267120361, accuracy 0.32499998807907104.\n",
      "Training epoch 2 batch 34 with loss 1.8236627578735352, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 35 with loss 1.7102794647216797, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 36 with loss 1.718387246131897, accuracy 0.31111112236976624.\n",
      "Training epoch 2 batch 37 with loss 1.6876859664916992, accuracy 0.23333334922790527.\n",
      "Training epoch 2 batch 38 with loss 1.7424951791763306, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 39 with loss 1.8055460453033447, accuracy 0.06084655970335007.\n",
      "Training epoch 2 batch 40 with loss 1.7634445428848267, accuracy 0.11428572237491608.\n",
      "Training epoch 2 batch 41 with loss 1.6990375518798828, accuracy 0.24166667461395264.\n",
      "Training epoch 2 batch 42 with loss 1.6622517108917236, accuracy 0.3916666805744171.\n",
      "Training epoch 2 batch 43 with loss 1.7266960144042969, accuracy 0.0694444477558136.\n",
      "Training epoch 2 batch 44 with loss 1.7204748392105103, accuracy 0.23333334922790527.\n",
      "Training epoch 2 batch 45 with loss 1.7978522777557373, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 46 with loss 1.863878846168518, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 47 with loss 1.7389304637908936, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 48 with loss 1.7271339893341064, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 49 with loss 1.7244045734405518, accuracy 0.38055557012557983.\n",
      "Training epoch 2 batch 50 with loss 1.7659517526626587, accuracy 0.3500000238418579.\n",
      "Training epoch 2 batch 51 with loss 1.7734134197235107, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 52 with loss 1.720502257347107, accuracy 0.3444444537162781.\n",
      "Training epoch 2 batch 53 with loss 1.6753885746002197, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 54 with loss 1.6967194080352783, accuracy 0.30277779698371887.\n",
      "Training epoch 2 batch 55 with loss 1.7852189540863037, accuracy 0.2111111283302307.\n",
      "Training epoch 2 batch 56 with loss 1.7693805694580078, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 57 with loss 1.7036516666412354, accuracy 0.22638888657093048.\n",
      "Training epoch 2 batch 58 with loss 1.7042497396469116, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 59 with loss 1.7796465158462524, accuracy 0.2142857313156128.\n",
      "Training epoch 2 batch 60 with loss 1.784064531326294, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 61 with loss 1.7826370000839233, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 62 with loss 1.8191688060760498, accuracy 0.125.\n",
      "Training epoch 2 batch 63 with loss 1.8147830963134766, accuracy 0.32499998807907104.\n",
      "Training epoch 2 batch 64 with loss 1.757824182510376, accuracy 0.25.\n",
      "Training epoch 2 batch 65 with loss 1.7970911264419556, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 66 with loss 1.7061069011688232, accuracy 0.2888889014720917.\n",
      "Training epoch 2 batch 67 with loss 1.7238123416900635, accuracy 0.2420634925365448.\n",
      "Training epoch 2 batch 68 with loss 1.8017946481704712, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 69 with loss 1.7813074588775635, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 70 with loss 1.8321685791015625, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 71 with loss 1.7440372705459595, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 72 with loss 1.7386852502822876, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 73 with loss 1.7789726257324219, accuracy 0.31666669249534607.\n",
      "Training epoch 2 batch 74 with loss 1.7916057109832764, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 75 with loss 1.7354393005371094, accuracy 0.3888888955116272.\n",
      "Training epoch 2 batch 76 with loss 1.789516806602478, accuracy 0.0694444477558136.\n",
      "Training epoch 2 batch 77 with loss 1.7496387958526611, accuracy 0.21666666865348816.\n",
      "Training epoch 2 batch 78 with loss 1.9331181049346924, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 79 with loss 1.7745510339736938, accuracy 0.24722224473953247.\n",
      "Training epoch 2 batch 80 with loss 1.6973278522491455, accuracy 0.4166666567325592.\n",
      "Training epoch 2 batch 81 with loss 1.8785667419433594, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 82 with loss 1.737217664718628, accuracy 0.13333334028720856.\n",
      "Training epoch 2 batch 83 with loss 1.703031301498413, accuracy 0.5041666626930237.\n",
      "Training epoch 2 batch 84 with loss 1.8137117624282837, accuracy 0.20000001788139343.\n",
      "Training epoch 2 batch 85 with loss 1.7612497806549072, accuracy 0.12222222983837128.\n",
      "Training epoch 2 batch 86 with loss 1.8074827194213867, accuracy 0.22499999403953552.\n",
      "Training epoch 2 batch 87 with loss 1.6974948644638062, accuracy 0.25.\n",
      "Training epoch 2 batch 88 with loss 1.6961524486541748, accuracy 0.2083333283662796.\n",
      "Training epoch 2 batch 89 with loss 1.7498550415039062, accuracy 0.19166667759418488.\n",
      "Training epoch 2 batch 90 with loss 1.786023497581482, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 91 with loss 1.785337209701538, accuracy 0.1875.\n",
      "Training epoch 2 batch 92 with loss 1.8439918756484985, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 93 with loss 1.8121875524520874, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 94 with loss 1.7570865154266357, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 95 with loss 1.8040597438812256, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 96 with loss 1.7422434091567993, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 97 with loss 1.8081992864608765, accuracy 0.22499999403953552.\n",
      "Training epoch 2 batch 98 with loss 1.8058459758758545, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 99 with loss 1.8264343738555908, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 100 with loss 1.7412841320037842, accuracy 0.1865079402923584.\n",
      "Training epoch 2 batch 101 with loss 1.6495692729949951, accuracy 0.4027777910232544.\n",
      "Training epoch 2 batch 102 with loss 1.788903832435608, accuracy 0.2611111104488373.\n",
      "Training epoch 2 batch 103 with loss 1.5697468519210815, accuracy 0.3194444477558136.\n",
      "Training epoch 2 batch 104 with loss 1.8942352533340454, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 105 with loss 1.7341035604476929, accuracy 0.347222238779068.\n",
      "Training epoch 2 batch 106 with loss 1.8240690231323242, accuracy 0.12222222983837128.\n",
      "Training epoch 2 batch 107 with loss 1.6912591457366943, accuracy 0.3305555582046509.\n",
      "Training epoch 2 batch 108 with loss 1.705505609512329, accuracy 0.2888889014720917.\n",
      "Training epoch 2 batch 109 with loss 1.6810499429702759, accuracy 0.3333333432674408.\n",
      "Training epoch 2 batch 110 with loss 1.886541724205017, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 111 with loss 1.7297677993774414, accuracy 0.29722222685813904.\n",
      "Training epoch 2 batch 112 with loss 1.7934900522232056, accuracy 0.1210317462682724.\n",
      "Training epoch 2 batch 113 with loss 1.7766406536102295, accuracy 0.1488095223903656.\n",
      "Training epoch 2 batch 114 with loss 1.7070276737213135, accuracy 0.20555555820465088.\n",
      "Training epoch 2 batch 115 with loss 1.7896223068237305, accuracy 0.25.\n",
      "Training epoch 2 batch 116 with loss 1.6850290298461914, accuracy 0.3472222089767456.\n",
      "Training epoch 2 batch 117 with loss 1.7821861505508423, accuracy 0.17777778208255768.\n",
      "Training epoch 2 batch 118 with loss 1.7885522842407227, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 119 with loss 1.7339613437652588, accuracy 0.21388889849185944.\n",
      "Training epoch 2 batch 120 with loss 1.7688090801239014, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 121 with loss 1.838861107826233, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 122 with loss 1.805793046951294, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 123 with loss 1.6737444400787354, accuracy 0.3682539761066437.\n",
      "Training epoch 2 batch 124 with loss 1.8225984573364258, accuracy 0.125.\n",
      "Training epoch 2 batch 125 with loss 1.8115513324737549, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 126 with loss 1.6671228408813477, accuracy 0.5277777910232544.\n",
      "Training epoch 2 batch 127 with loss 1.728090524673462, accuracy 0.32499998807907104.\n",
      "Training epoch 2 batch 128 with loss 1.782467246055603, accuracy 0.23888888955116272.\n",
      "Training epoch 2 batch 129 with loss 1.7447090148925781, accuracy 0.21111111342906952.\n",
      "Training epoch 2 batch 130 with loss 1.7375333309173584, accuracy 0.21388888359069824.\n",
      "Training epoch 2 batch 131 with loss 1.798667311668396, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 132 with loss 1.8207809925079346, accuracy 0.18611112236976624.\n",
      "Training epoch 2 batch 133 with loss 1.6970924139022827, accuracy 0.1537037193775177.\n",
      "Training epoch 2 batch 134 with loss 1.7221990823745728, accuracy 0.3055555820465088.\n",
      "Training epoch 2 batch 135 with loss 1.705855131149292, accuracy 0.46666666865348816.\n",
      "Training epoch 2 batch 136 with loss 1.6241035461425781, accuracy 0.3027777671813965.\n",
      "Training epoch 2 batch 137 with loss 1.791055679321289, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 138 with loss 1.7650684118270874, accuracy 0.0714285746216774.\n",
      "Training epoch 2 batch 139 with loss 1.7556527853012085, accuracy 0.1805555671453476.\n",
      "Training epoch 2 batch 140 with loss 1.7661464214324951, accuracy 0.3115079402923584.\n",
      "Training epoch 2 batch 141 with loss 1.7083632946014404, accuracy 0.347222238779068.\n",
      "Training epoch 2 batch 142 with loss 1.7446216344833374, accuracy 0.2043650895357132.\n",
      "Training epoch 2 batch 143 with loss 1.7045730352401733, accuracy 0.2944444417953491.\n",
      "Training epoch 2 batch 144 with loss 1.7632882595062256, accuracy 0.28333336114883423.\n",
      "Training epoch 2 batch 145 with loss 1.8651933670043945, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 146 with loss 1.7791321277618408, accuracy 0.12222222983837128.\n",
      "Training epoch 2 batch 147 with loss 1.7072789669036865, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 148 with loss 1.7609800100326538, accuracy 0.09444444626569748.\n",
      "Training epoch 2 batch 149 with loss 1.753390908241272, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 150 with loss 1.7206398248672485, accuracy 0.35555556416511536.\n",
      "Training epoch 2 batch 151 with loss 1.6894395351409912, accuracy 0.5166667103767395.\n",
      "Training epoch 2 batch 152 with loss 1.783865213394165, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 153 with loss 1.7943174839019775, accuracy 0.22777777910232544.\n",
      "Training epoch 2 batch 154 with loss 1.7518832683563232, accuracy 0.2361111044883728.\n",
      "Training epoch 2 batch 155 with loss 1.843258261680603, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 156 with loss 1.8872381448745728, accuracy 0.25.\n",
      "Training epoch 2 batch 157 with loss 1.7763303518295288, accuracy 0.14166668057441711.\n",
      "Training epoch 2 batch 158 with loss 1.7467048168182373, accuracy 0.13055555522441864.\n",
      "Training epoch 2 batch 159 with loss 1.8032119274139404, accuracy 0.1964285671710968.\n",
      "Training epoch 2 batch 160 with loss 1.7984778881072998, accuracy 0.3166666626930237.\n",
      "Training epoch 2 batch 161 with loss 1.759844183921814, accuracy 0.2750000059604645.\n",
      "Training epoch 2 batch 162 with loss 1.764561414718628, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 163 with loss 1.7060550451278687, accuracy 0.2460317611694336.\n",
      "Training epoch 2 batch 164 with loss 1.7093489170074463, accuracy 0.3541666865348816.\n",
      "Training epoch 2 batch 165 with loss 1.692776083946228, accuracy 0.25.\n",
      "Training epoch 2 batch 166 with loss 1.7807714939117432, accuracy 0.3166666626930237.\n",
      "Training epoch 2 batch 167 with loss 1.7190732955932617, accuracy 0.19722223281860352.\n",
      "Training epoch 2 batch 168 with loss 1.7961218357086182, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 169 with loss 1.7350276708602905, accuracy 0.2666666805744171.\n",
      "Training epoch 2 batch 170 with loss 1.726136565208435, accuracy 0.25.\n",
      "Training epoch 2 batch 171 with loss 1.7262916564941406, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 172 with loss 1.777756929397583, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 173 with loss 1.7630621194839478, accuracy 0.31111112236976624.\n",
      "Training epoch 2 batch 174 with loss 1.6416345834732056, accuracy 0.4305555522441864.\n",
      "Training epoch 2 batch 175 with loss 1.729138970375061, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 176 with loss 1.752505898475647, accuracy 0.3444444537162781.\n",
      "Training epoch 2 batch 177 with loss 1.7475535869598389, accuracy 0.125.\n",
      "Training epoch 2 batch 178 with loss 1.741949439048767, accuracy 0.24166667461395264.\n",
      "Training epoch 2 batch 179 with loss 1.82037353515625, accuracy 0.25.\n",
      "Training epoch 2 batch 180 with loss 1.7713407278060913, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 181 with loss 1.7049195766448975, accuracy 0.17777778208255768.\n",
      "Training epoch 2 batch 182 with loss 1.715420126914978, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 183 with loss 1.7347307205200195, accuracy 0.3194444477558136.\n",
      "Training epoch 2 batch 184 with loss 1.8554017543792725, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 185 with loss 1.6768338680267334, accuracy 0.22777779400348663.\n",
      "Training epoch 2 batch 186 with loss 1.7190380096435547, accuracy 0.15555556118488312.\n",
      "Training epoch 2 batch 187 with loss 1.7582166194915771, accuracy 0.1726190447807312.\n",
      "Training epoch 2 batch 188 with loss 1.7430349588394165, accuracy 0.18611112236976624.\n",
      "Training epoch 2 batch 189 with loss 1.8323901891708374, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 190 with loss 1.7345908880233765, accuracy 0.18333333730697632.\n",
      "Training epoch 2 batch 191 with loss 1.8257701396942139, accuracy 0.125.\n",
      "Training epoch 2 batch 192 with loss 1.7588618993759155, accuracy 0.1726190447807312.\n",
      "Training epoch 2 batch 193 with loss 1.7846133708953857, accuracy 0.14166666567325592.\n",
      "Training epoch 2 batch 194 with loss 1.8116929531097412, accuracy 0.3500000238418579.\n",
      "Training epoch 2 batch 195 with loss 1.7920668125152588, accuracy 0.20555555820465088.\n",
      "Training epoch 2 batch 196 with loss 1.8290908336639404, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 197 with loss 1.7618459463119507, accuracy 0.25.\n",
      "Training epoch 2 batch 198 with loss 1.7902030944824219, accuracy 0.3888888955116272.\n",
      "Training epoch 2 batch 199 with loss 1.7875158786773682, accuracy 0.1587301641702652.\n",
      "Training epoch 2 batch 200 with loss 1.8046009540557861, accuracy 0.125.\n",
      "Training epoch 2 batch 201 with loss 1.7543971538543701, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 202 with loss 1.7373815774917603, accuracy 0.45000001788139343.\n",
      "Training epoch 2 batch 203 with loss 1.7535167932510376, accuracy 0.2611111104488373.\n",
      "Training epoch 2 batch 204 with loss 1.6885299682617188, accuracy 0.222222238779068.\n",
      "Training epoch 2 batch 205 with loss 1.6761500835418701, accuracy 0.25555554032325745.\n",
      "Training epoch 2 batch 206 with loss 1.734297513961792, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 207 with loss 1.731255292892456, accuracy 0.23888888955116272.\n",
      "Training epoch 2 batch 208 with loss 1.7447764873504639, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 209 with loss 1.6761823892593384, accuracy 0.2750000059604645.\n",
      "Training epoch 2 batch 210 with loss 1.8556101322174072, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 211 with loss 1.6755237579345703, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 212 with loss 1.7593599557876587, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 213 with loss 1.8139568567276, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 214 with loss 1.8553292751312256, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 215 with loss 1.9027467966079712, accuracy 0.20000001788139343.\n",
      "Training epoch 2 batch 216 with loss 1.655657410621643, accuracy 0.3222222328186035.\n",
      "Training epoch 2 batch 217 with loss 1.7795072793960571, accuracy 0.26851850748062134.\n",
      "Training epoch 2 batch 218 with loss 1.8531138896942139, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 219 with loss 1.780509352684021, accuracy 0.20000001788139343.\n",
      "Training epoch 2 batch 220 with loss 1.8142001628875732, accuracy 0.21388888359069824.\n",
      "Training epoch 2 batch 221 with loss 1.782573938369751, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 222 with loss 1.7883789539337158, accuracy 0.21388888359069824.\n",
      "Training epoch 2 batch 223 with loss 1.813589334487915, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 224 with loss 1.7424131631851196, accuracy 0.2569444477558136.\n",
      "Training epoch 2 batch 225 with loss 1.7702915668487549, accuracy 0.1587301641702652.\n",
      "Training epoch 2 batch 226 with loss 1.876685380935669, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 227 with loss 1.7420637607574463, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 228 with loss 1.826425552368164, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 229 with loss 1.8315798044204712, accuracy 0.16388890147209167.\n",
      "Training epoch 2 batch 230 with loss 1.7809892892837524, accuracy 0.5333333611488342.\n",
      "Training epoch 2 batch 231 with loss 1.8148998022079468, accuracy 0.210317462682724.\n",
      "Training epoch 2 batch 232 with loss 1.8176183700561523, accuracy 0.3055555820465088.\n",
      "Training epoch 2 batch 233 with loss 1.8926798105239868, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 234 with loss 1.7959426641464233, accuracy 0.130952388048172.\n",
      "Training epoch 2 batch 235 with loss 1.8221629858016968, accuracy 0.23333334922790527.\n",
      "Training epoch 2 batch 236 with loss 1.7065950632095337, accuracy 0.3194444477558136.\n",
      "Training epoch 2 batch 237 with loss 1.8058111667633057, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 238 with loss 1.7541282176971436, accuracy 0.39444446563720703.\n",
      "Training epoch 2 batch 239 with loss 1.747819185256958, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 240 with loss 1.691127061843872, accuracy 0.3194444477558136.\n",
      "Training epoch 2 batch 241 with loss 1.7175209522247314, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 242 with loss 1.7787784337997437, accuracy 0.22083333134651184.\n",
      "Training epoch 2 batch 243 with loss 1.7753385305404663, accuracy 0.3888888955116272.\n",
      "Training epoch 2 batch 244 with loss 1.6303303241729736, accuracy 0.38333335518836975.\n",
      "Training epoch 2 batch 245 with loss 1.7779144048690796, accuracy 0.27936509251594543.\n",
      "Training epoch 2 batch 246 with loss 1.7504431009292603, accuracy 0.21388888359069824.\n",
      "Training epoch 2 batch 247 with loss 1.6957290172576904, accuracy 0.2916666567325592.\n",
      "Training epoch 2 batch 248 with loss 1.8496230840682983, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 249 with loss 1.7378699779510498, accuracy 0.20000001788139343.\n",
      "Training epoch 2 batch 250 with loss 1.6582088470458984, accuracy 0.40833333134651184.\n",
      "Training epoch 2 batch 251 with loss 1.7363773584365845, accuracy 0.24166667461395264.\n",
      "Training epoch 2 batch 252 with loss 1.6829332113265991, accuracy 0.3333333432674408.\n",
      "Training epoch 2 batch 253 with loss 1.7047431468963623, accuracy 0.472222238779068.\n",
      "Training epoch 2 batch 254 with loss 1.9519275426864624, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 255 with loss 1.7832815647125244, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 256 with loss 1.8800508975982666, accuracy 0.0.\n",
      "Training epoch 2 batch 257 with loss 1.744985580444336, accuracy 0.18333333730697632.\n",
      "Training epoch 2 batch 258 with loss 1.8699756860733032, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 259 with loss 1.6447193622589111, accuracy 0.25.\n",
      "Training epoch 2 batch 260 with loss 1.7658840417861938, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 261 with loss 1.8442825078964233, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 262 with loss 1.7206560373306274, accuracy 0.3027777671813965.\n",
      "Training epoch 2 batch 263 with loss 1.6450783014297485, accuracy 0.4861111044883728.\n",
      "Training epoch 2 batch 264 with loss 1.8566824197769165, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 265 with loss 1.7865766286849976, accuracy 0.125.\n",
      "Training epoch 2 batch 266 with loss 1.7640304565429688, accuracy 0.2083333283662796.\n",
      "Training epoch 2 batch 267 with loss 1.7667396068572998, accuracy 0.28333333134651184.\n",
      "Training epoch 2 batch 268 with loss 1.7659028768539429, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 269 with loss 1.7579606771469116, accuracy 0.30277779698371887.\n",
      "Training epoch 2 batch 270 with loss 1.793370246887207, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 271 with loss 1.8132470846176147, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 272 with loss 1.648263692855835, accuracy 0.5666666626930237.\n",
      "Training epoch 2 batch 273 with loss 1.740155577659607, accuracy 0.3361111283302307.\n",
      "Training epoch 2 batch 274 with loss 1.6909878253936768, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 275 with loss 1.7859227657318115, accuracy 0.3583333194255829.\n",
      "Training epoch 2 batch 276 with loss 1.9270782470703125, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 277 with loss 1.737466812133789, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 278 with loss 1.7825186252593994, accuracy 0.22777777910232544.\n",
      "Training epoch 2 batch 279 with loss 1.8477405309677124, accuracy 0.125.\n",
      "Training epoch 2 batch 280 with loss 1.6870756149291992, accuracy 0.2539682686328888.\n",
      "Training epoch 2 batch 281 with loss 1.720656394958496, accuracy 0.2750000059604645.\n",
      "Training epoch 2 batch 282 with loss 1.8455966711044312, accuracy 0.15555556118488312.\n",
      "Training epoch 2 batch 283 with loss 1.7924683094024658, accuracy 0.2420634925365448.\n",
      "Training epoch 2 batch 284 with loss 1.7897226810455322, accuracy 0.25.\n",
      "Training epoch 2 batch 285 with loss 1.8170102834701538, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 286 with loss 1.8691141605377197, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 287 with loss 1.798532485961914, accuracy 0.125.\n",
      "Training epoch 2 batch 288 with loss 1.8225376605987549, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 289 with loss 1.7975229024887085, accuracy 0.13055555522441864.\n",
      "Training epoch 2 batch 290 with loss 1.7455021142959595, accuracy 0.222222238779068.\n",
      "Training epoch 2 batch 291 with loss 1.7970314025878906, accuracy 0.329365074634552.\n",
      "Training epoch 2 batch 292 with loss 1.8557484149932861, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 293 with loss 1.7806450128555298, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 294 with loss 1.8633372783660889, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 295 with loss 1.7436059713363647, accuracy 0.21666666865348816.\n",
      "Training epoch 2 batch 296 with loss 1.7846533060073853, accuracy 0.20000000298023224.\n",
      "Training epoch 2 batch 297 with loss 1.7695245742797852, accuracy 0.1170634925365448.\n",
      "Training epoch 2 batch 298 with loss 1.8184629678726196, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 299 with loss 1.7841463088989258, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 300 with loss 1.7945048809051514, accuracy 0.32500001788139343.\n",
      "Training epoch 2 batch 301 with loss 1.8014869689941406, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 302 with loss 1.7729532718658447, accuracy 0.31666669249534607.\n",
      "Training epoch 2 batch 303 with loss 1.7349389791488647, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 304 with loss 1.7760145664215088, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 305 with loss 1.7751096487045288, accuracy 0.24166667461395264.\n",
      "Training epoch 2 batch 306 with loss 1.7910587787628174, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 307 with loss 1.8408502340316772, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 308 with loss 1.7082107067108154, accuracy 0.25.\n",
      "Training epoch 2 batch 309 with loss 1.7964547872543335, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 310 with loss 1.6650230884552002, accuracy 0.2750000059604645.\n",
      "Training epoch 2 batch 311 with loss 1.8747475147247314, accuracy 0.14166666567325592.\n",
      "Training epoch 2 batch 312 with loss 1.7618589401245117, accuracy 0.33888891339302063.\n",
      "Training epoch 2 batch 313 with loss 1.8050063848495483, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 314 with loss 1.6951013803482056, accuracy 0.36666667461395264.\n",
      "Training epoch 2 batch 315 with loss 1.7739086151123047, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 316 with loss 1.7473738193511963, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 317 with loss 1.7177356481552124, accuracy 0.2916666567325592.\n",
      "Training epoch 2 batch 318 with loss 1.8142707347869873, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 319 with loss 1.8013668060302734, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 320 with loss 1.7813119888305664, accuracy 0.2666666805744171.\n",
      "Training epoch 2 batch 321 with loss 1.6651579141616821, accuracy 0.13809524476528168.\n",
      "Training epoch 2 batch 322 with loss 1.8257595300674438, accuracy 0.03333333507180214.\n",
      "Training epoch 2 batch 323 with loss 1.8494532108306885, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 324 with loss 1.7999261617660522, accuracy 0.2111111283302307.\n",
      "Training epoch 2 batch 325 with loss 1.8115453720092773, accuracy 0.12037037312984467.\n",
      "Training epoch 2 batch 326 with loss 1.749516248703003, accuracy 0.28333333134651184.\n",
      "Training epoch 2 batch 327 with loss 1.6401790380477905, accuracy 0.34857141971588135.\n",
      "Training epoch 2 batch 328 with loss 1.776585340499878, accuracy 0.3115079402923584.\n",
      "Training epoch 2 batch 329 with loss 1.787299394607544, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 330 with loss 1.8256762027740479, accuracy 0.18888889253139496.\n",
      "Training epoch 2 batch 331 with loss 1.8127620220184326, accuracy 0.1736111044883728.\n",
      "Training epoch 2 batch 332 with loss 1.8162027597427368, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 333 with loss 1.7777321338653564, accuracy 0.1736111044883728.\n",
      "Training epoch 2 batch 334 with loss 1.77317214012146, accuracy 0.15555556118488312.\n",
      "Training epoch 2 batch 335 with loss 1.758144736289978, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 336 with loss 1.7279335260391235, accuracy 0.28333333134651184.\n",
      "Training epoch 2 batch 337 with loss 1.7512391805648804, accuracy 0.23888888955116272.\n",
      "Training epoch 2 batch 338 with loss 1.7519538402557373, accuracy 0.2569444477558136.\n",
      "Training epoch 2 batch 339 with loss 1.8361202478408813, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 340 with loss 1.8032610416412354, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 341 with loss 1.728043794631958, accuracy 0.2003968209028244.\n",
      "Training epoch 2 batch 342 with loss 1.7581040859222412, accuracy 0.29722222685813904.\n",
      "Training epoch 2 batch 343 with loss 1.8160648345947266, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 344 with loss 1.867189645767212, accuracy 0.16388888657093048.\n",
      "Training epoch 2 batch 345 with loss 1.7257436513900757, accuracy 0.25.\n",
      "Training epoch 2 batch 346 with loss 1.9204025268554688, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 347 with loss 1.7618122100830078, accuracy 0.35277777910232544.\n",
      "Training epoch 2 batch 348 with loss 1.6954352855682373, accuracy 0.18333333730697632.\n",
      "Training epoch 2 batch 349 with loss 1.7614476680755615, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 350 with loss 1.721948266029358, accuracy 0.0892857164144516.\n",
      "Training epoch 2 batch 351 with loss 1.8160648345947266, accuracy 0.1031746044754982.\n",
      "Training epoch 2 batch 352 with loss 1.7605116367340088, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 353 with loss 1.6665033102035522, accuracy 0.3305555582046509.\n",
      "Training epoch 2 batch 354 with loss 1.8120256662368774, accuracy 0.0694444477558136.\n",
      "Training epoch 2 batch 355 with loss 1.7784016132354736, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 356 with loss 1.8296926021575928, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 357 with loss 1.7849397659301758, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 358 with loss 1.7275621891021729, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 359 with loss 1.7237716913223267, accuracy 0.20000001788139343.\n",
      "Training epoch 2 batch 360 with loss 1.8036569356918335, accuracy 0.10000000894069672.\n",
      "Training epoch 2 batch 361 with loss 1.8041269779205322, accuracy 0.125.\n",
      "Training epoch 2 batch 362 with loss 1.794904112815857, accuracy 0.0625.\n",
      "Training epoch 2 batch 363 with loss 1.7966665029525757, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 364 with loss 1.8865642547607422, accuracy 0.2142857313156128.\n",
      "Training epoch 2 batch 365 with loss 1.6880366802215576, accuracy 0.31111112236976624.\n",
      "Training epoch 2 batch 366 with loss 1.817805290222168, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 367 with loss 1.7844102382659912, accuracy 0.1349206417798996.\n",
      "Training epoch 2 batch 368 with loss 1.777703046798706, accuracy 0.3333333432674408.\n",
      "Training epoch 2 batch 369 with loss 1.8198922872543335, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 370 with loss 1.8248094320297241, accuracy 0.21666666865348816.\n",
      "Training epoch 2 batch 371 with loss 1.8015146255493164, accuracy 0.1626984179019928.\n",
      "Training epoch 2 batch 372 with loss 1.8415101766586304, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 373 with loss 1.780167818069458, accuracy 0.210317462682724.\n",
      "Training epoch 2 batch 374 with loss 1.7765748500823975, accuracy 0.125.\n",
      "Training epoch 2 batch 375 with loss 1.7491686344146729, accuracy 0.32500001788139343.\n",
      "Training epoch 2 batch 376 with loss 1.733215093612671, accuracy 0.3333333432674408.\n",
      "Training epoch 2 batch 377 with loss 1.7518812417984009, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 378 with loss 1.854975700378418, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 379 with loss 1.7281663417816162, accuracy 0.341269850730896.\n",
      "Training epoch 2 batch 380 with loss 1.8106666803359985, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 381 with loss 1.8241560459136963, accuracy 0.1547619104385376.\n",
      "Training epoch 2 batch 382 with loss 1.8505465984344482, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 383 with loss 1.8245818614959717, accuracy 0.14166668057441711.\n",
      "Training epoch 2 batch 384 with loss 1.805800437927246, accuracy 0.0793650820851326.\n",
      "Training epoch 2 batch 385 with loss 1.7299467325210571, accuracy 0.2944444417953491.\n",
      "Training epoch 2 batch 386 with loss 1.6934871673583984, accuracy 0.3704761862754822.\n",
      "Training epoch 2 batch 387 with loss 1.8600986003875732, accuracy 0.0.\n",
      "Training epoch 2 batch 388 with loss 1.772701621055603, accuracy 0.2769841253757477.\n",
      "Training epoch 2 batch 389 with loss 1.7282159328460693, accuracy 0.25555557012557983.\n",
      "Training epoch 2 batch 390 with loss 1.7642978429794312, accuracy 0.20000000298023224.\n",
      "Training epoch 2 batch 391 with loss 1.6942352056503296, accuracy 0.3492063581943512.\n",
      "Training epoch 2 batch 392 with loss 1.802703619003296, accuracy 0.13650794327259064.\n",
      "Training epoch 2 batch 393 with loss 1.7562891244888306, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 394 with loss 1.7481632232666016, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 395 with loss 1.7829859256744385, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 396 with loss 1.8057388067245483, accuracy 0.2571428716182709.\n",
      "Training epoch 2 batch 397 with loss 1.7699826955795288, accuracy 0.1726190596818924.\n",
      "Training epoch 2 batch 398 with loss 1.8490550518035889, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 399 with loss 1.8513810634613037, accuracy 0.10833333432674408.\n",
      "Training epoch 2 batch 400 with loss 1.8491837978363037, accuracy 0.23333334922790527.\n",
      "Training epoch 2 batch 401 with loss 1.8573745489120483, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 402 with loss 1.7870668172836304, accuracy 0.1458333283662796.\n",
      "Training epoch 2 batch 403 with loss 1.7936809062957764, accuracy 0.0793650820851326.\n",
      "Training epoch 2 batch 404 with loss 1.728088617324829, accuracy 0.1031746044754982.\n",
      "Training epoch 2 batch 405 with loss 1.709267258644104, accuracy 0.3333333432674408.\n",
      "Training epoch 2 batch 406 with loss 1.6802858114242554, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 407 with loss 1.737897276878357, accuracy 0.25555557012557983.\n",
      "Training epoch 2 batch 408 with loss 1.8244699239730835, accuracy 0.25.\n",
      "Training epoch 2 batch 409 with loss 1.8058836460113525, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 410 with loss 1.7470626831054688, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 411 with loss 1.7672288417816162, accuracy 0.23333334922790527.\n",
      "Training epoch 2 batch 412 with loss 1.7089036703109741, accuracy 0.21388888359069824.\n",
      "Training epoch 2 batch 413 with loss 1.8311727046966553, accuracy 0.10833333432674408.\n",
      "Training epoch 2 batch 414 with loss 1.7206108570098877, accuracy 0.3222222328186035.\n",
      "Training epoch 2 batch 415 with loss 1.8005406856536865, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 416 with loss 1.763989806175232, accuracy 0.14166666567325592.\n",
      "Training epoch 2 batch 417 with loss 1.7875349521636963, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 418 with loss 1.7938026189804077, accuracy 0.13333334028720856.\n",
      "Training epoch 2 batch 419 with loss 1.8230793476104736, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 420 with loss 1.8173933029174805, accuracy 0.19166666269302368.\n",
      "Training epoch 2 batch 421 with loss 1.7072370052337646, accuracy 0.3125.\n",
      "Training epoch 2 batch 422 with loss 1.7892487049102783, accuracy 0.22777777910232544.\n",
      "Training epoch 2 batch 423 with loss 1.6776071786880493, accuracy 0.2341269850730896.\n",
      "Training epoch 2 batch 424 with loss 1.7815282344818115, accuracy 0.1726190447807312.\n",
      "Training epoch 2 batch 425 with loss 1.768193006515503, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 426 with loss 1.8357645273208618, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 427 with loss 1.8381950855255127, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 428 with loss 1.7609894275665283, accuracy 0.3333333432674408.\n",
      "Training epoch 2 batch 429 with loss 1.6731908321380615, accuracy 0.130952388048172.\n",
      "Training epoch 2 batch 430 with loss 1.725132703781128, accuracy 0.236111119389534.\n",
      "Training epoch 2 batch 431 with loss 1.7271955013275146, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 432 with loss 1.7243366241455078, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 433 with loss 1.8928343057632446, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 434 with loss 1.6635557413101196, accuracy 0.4166666865348816.\n",
      "Training epoch 2 batch 435 with loss 1.80767822265625, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 436 with loss 1.7913849353790283, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 437 with loss 1.735023856163025, accuracy 0.4365079402923584.\n",
      "Training epoch 2 batch 438 with loss 1.8375308513641357, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 439 with loss 1.742008924484253, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 440 with loss 1.712658166885376, accuracy 0.18333333730697632.\n",
      "Training epoch 2 batch 441 with loss 1.793733835220337, accuracy 0.1626984179019928.\n",
      "Training epoch 2 batch 442 with loss 1.7715654373168945, accuracy 0.3027777671813965.\n",
      "Training epoch 2 batch 443 with loss 1.7721046209335327, accuracy 0.3472222089767456.\n",
      "Training epoch 2 batch 444 with loss 1.7310419082641602, accuracy 0.19722223281860352.\n",
      "Training epoch 2 batch 445 with loss 1.6753833293914795, accuracy 0.4277777671813965.\n",
      "Training epoch 2 batch 446 with loss 1.8147188425064087, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 447 with loss 1.721116304397583, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 448 with loss 1.7424167394638062, accuracy 0.21666666865348816.\n",
      "Training epoch 2 batch 449 with loss 1.693063497543335, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 450 with loss 1.8628015518188477, accuracy 0.125.\n",
      "Training epoch 2 batch 451 with loss 1.8032373189926147, accuracy 0.16388890147209167.\n",
      "Training epoch 2 batch 452 with loss 1.8813543319702148, accuracy 0.03333333507180214.\n",
      "Training epoch 2 batch 453 with loss 1.8852674961090088, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 454 with loss 1.777491807937622, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 455 with loss 1.7707312107086182, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 456 with loss 1.7909599542617798, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 457 with loss 1.8414199352264404, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 458 with loss 1.8633266687393188, accuracy 0.125.\n",
      "Training epoch 2 batch 459 with loss 1.817021131515503, accuracy 0.2202381044626236.\n",
      "Training epoch 2 batch 460 with loss 1.7209336757659912, accuracy 0.1865079402923584.\n",
      "Training epoch 2 batch 461 with loss 1.7790353298187256, accuracy 0.22499999403953552.\n",
      "Training epoch 2 batch 462 with loss 1.7314636707305908, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 463 with loss 1.7429840564727783, accuracy 0.16388890147209167.\n",
      "Training epoch 2 batch 464 with loss 1.6487823724746704, accuracy 0.2916666865348816.\n",
      "Training epoch 2 batch 465 with loss 1.7926585674285889, accuracy 0.1319444477558136.\n",
      "Training epoch 2 batch 466 with loss 1.7028526067733765, accuracy 0.4027777910232544.\n",
      "Training epoch 2 batch 467 with loss 1.7598298788070679, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 468 with loss 1.6959079504013062, accuracy 0.40000003576278687.\n",
      "Training epoch 2 batch 469 with loss 1.8185917139053345, accuracy 0.02777777798473835.\n",
      "Training epoch 2 batch 470 with loss 1.757293462753296, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 471 with loss 1.7833751440048218, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 472 with loss 1.8252760171890259, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 473 with loss 1.7949254512786865, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 474 with loss 1.7524235248565674, accuracy 0.13333334028720856.\n",
      "Training epoch 2 batch 475 with loss 1.8592612743377686, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 476 with loss 1.7999629974365234, accuracy 0.23333333432674408.\n",
      "Training epoch 2 batch 477 with loss 1.8024632930755615, accuracy 0.24166667461395264.\n",
      "Training epoch 2 batch 478 with loss 1.8117364645004272, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 479 with loss 1.8226850032806396, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 480 with loss 1.7509905099868774, accuracy 0.25.\n",
      "Training epoch 2 batch 481 with loss 1.8021717071533203, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 482 with loss 1.7914600372314453, accuracy 0.18611112236976624.\n",
      "Training epoch 2 batch 483 with loss 1.7788591384887695, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 484 with loss 1.716353416442871, accuracy 0.3968254029750824.\n",
      "Training epoch 2 batch 485 with loss 1.8194280862808228, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 486 with loss 1.710378646850586, accuracy 0.18518517911434174.\n",
      "Training epoch 2 batch 487 with loss 1.6618540287017822, accuracy 0.2976190447807312.\n",
      "Training epoch 2 batch 488 with loss 1.8209915161132812, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 489 with loss 1.666486382484436, accuracy 0.20555555820465088.\n",
      "Training epoch 2 batch 490 with loss 1.9411952495574951, accuracy 0.0763888880610466.\n",
      "Training epoch 2 batch 491 with loss 1.77372145652771, accuracy 0.20000000298023224.\n",
      "Training epoch 2 batch 492 with loss 1.7935640811920166, accuracy 0.10277777910232544.\n",
      "Training epoch 2 batch 493 with loss 1.7501108646392822, accuracy 0.26944443583488464.\n",
      "Training epoch 2 batch 494 with loss 1.8497447967529297, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 495 with loss 1.750872015953064, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 496 with loss 1.7884855270385742, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 497 with loss 1.7270772457122803, accuracy 0.25.\n",
      "Training epoch 2 batch 498 with loss 1.8672956228256226, accuracy 0.0793650820851326.\n",
      "Training epoch 2 batch 499 with loss 1.8431854248046875, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 500 with loss 1.7850488424301147, accuracy 0.28333333134651184.\n",
      "Training epoch 2 batch 501 with loss 1.7618980407714844, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 502 with loss 1.9243195056915283, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 503 with loss 1.748705506324768, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 504 with loss 1.835436463356018, accuracy 0.21111111342906952.\n",
      "Training epoch 2 batch 505 with loss 1.8310190439224243, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 506 with loss 1.7103513479232788, accuracy 0.21388889849185944.\n",
      "Training epoch 2 batch 507 with loss 1.8313974142074585, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 508 with loss 1.7700382471084595, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 509 with loss 1.7623399496078491, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 510 with loss 1.8073253631591797, accuracy 0.1944444626569748.\n",
      "Training epoch 2 batch 511 with loss 1.8016688823699951, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 512 with loss 1.700059175491333, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 513 with loss 1.831006407737732, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 514 with loss 1.6969406604766846, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 515 with loss 1.8078930377960205, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 516 with loss 1.9080383777618408, accuracy 0.0.\n",
      "Training epoch 2 batch 517 with loss 1.9241857528686523, accuracy 0.1071428582072258.\n",
      "Training epoch 2 batch 518 with loss 1.7928466796875, accuracy 0.21388888359069824.\n",
      "Training epoch 2 batch 519 with loss 1.7747529745101929, accuracy 0.2361111044883728.\n",
      "Training epoch 2 batch 520 with loss 1.7780574560165405, accuracy 0.19166667759418488.\n",
      "Training epoch 2 batch 521 with loss 1.9153690338134766, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 522 with loss 1.7809900045394897, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 523 with loss 1.7772729396820068, accuracy 0.2916666567325592.\n",
      "Training epoch 2 batch 524 with loss 1.8663303852081299, accuracy 0.10833333432674408.\n",
      "Training epoch 2 batch 525 with loss 1.8374627828598022, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 526 with loss 1.755854845046997, accuracy 0.3222222328186035.\n",
      "Training epoch 2 batch 527 with loss 1.7252023220062256, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 528 with loss 1.8105945587158203, accuracy 0.20555555820465088.\n",
      "Training epoch 2 batch 529 with loss 1.7626224756240845, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 530 with loss 1.6982805728912354, accuracy 0.1736111044883728.\n",
      "Training epoch 2 batch 531 with loss 1.8262075185775757, accuracy 0.236111119389534.\n",
      "Training epoch 2 batch 532 with loss 1.841271162033081, accuracy 0.065476194024086.\n",
      "Training epoch 2 batch 533 with loss 1.7722418308258057, accuracy 0.2916666567325592.\n",
      "Training epoch 2 batch 534 with loss 1.641974687576294, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 535 with loss 1.8641475439071655, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 536 with loss 1.7437717914581299, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 537 with loss 1.8442165851593018, accuracy 0.18333333730697632.\n",
      "Training epoch 2 batch 538 with loss 1.7900384664535522, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 539 with loss 1.9048311710357666, accuracy 0.1031746044754982.\n",
      "Training epoch 2 batch 540 with loss 1.8201192617416382, accuracy 0.20000001788139343.\n",
      "Training epoch 2 batch 541 with loss 1.7602564096450806, accuracy 0.23888888955116272.\n",
      "Training epoch 2 batch 542 with loss 1.7329508066177368, accuracy 0.2876984179019928.\n",
      "Training epoch 2 batch 543 with loss 1.849660873413086, accuracy 0.2182539701461792.\n",
      "Training epoch 2 batch 544 with loss 1.7646219730377197, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 545 with loss 1.8156392574310303, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 546 with loss 1.8331235647201538, accuracy 0.0476190485060215.\n",
      "Training epoch 2 batch 547 with loss 1.7262442111968994, accuracy 0.125.\n",
      "Training epoch 2 batch 548 with loss 1.8032300472259521, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 549 with loss 1.9181201457977295, accuracy 0.0694444477558136.\n",
      "Training epoch 2 batch 550 with loss 1.8392975330352783, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 551 with loss 1.7643663883209229, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 552 with loss 1.8040988445281982, accuracy 0.20555557310581207.\n",
      "Training epoch 2 batch 553 with loss 1.811518907546997, accuracy 0.1527777910232544.\n",
      "Training epoch 2 batch 554 with loss 1.7471898794174194, accuracy 0.3916666805744171.\n",
      "Training epoch 2 batch 555 with loss 1.7613540887832642, accuracy 0.2083333283662796.\n",
      "Training epoch 2 batch 556 with loss 1.7018028497695923, accuracy 0.2916666865348816.\n",
      "Training epoch 2 batch 557 with loss 1.776932716369629, accuracy 0.125.\n",
      "Training epoch 2 batch 558 with loss 1.777777075767517, accuracy 0.33888891339302063.\n",
      "Training epoch 2 batch 559 with loss 1.7331711053848267, accuracy 0.2888889014720917.\n",
      "Training epoch 2 batch 560 with loss 1.7898305654525757, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 561 with loss 1.8164474964141846, accuracy 0.125.\n",
      "Training epoch 2 batch 562 with loss 1.7320911884307861, accuracy 0.22777777910232544.\n",
      "Training epoch 2 batch 563 with loss 1.6484966278076172, accuracy 0.454365074634552.\n",
      "Training epoch 2 batch 564 with loss 1.8594920635223389, accuracy 0.15833333134651184.\n",
      "Training epoch 2 batch 565 with loss 1.8145558834075928, accuracy 0.24166667461395264.\n",
      "Training epoch 2 batch 566 with loss 1.7372808456420898, accuracy 0.28333333134651184.\n",
      "Training epoch 2 batch 567 with loss 1.755953073501587, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 568 with loss 1.7519562244415283, accuracy 0.347222238779068.\n",
      "Training epoch 2 batch 569 with loss 1.7257391214370728, accuracy 0.4277777671813965.\n",
      "Training epoch 2 batch 570 with loss 1.7299420833587646, accuracy 0.15925925970077515.\n",
      "Training epoch 2 batch 571 with loss 1.7806813716888428, accuracy 0.4466666579246521.\n",
      "Training epoch 2 batch 572 with loss 1.7220083475112915, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 573 with loss 1.9107831716537476, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 574 with loss 1.8305063247680664, accuracy 0.23333334922790527.\n",
      "Training epoch 2 batch 575 with loss 1.741098165512085, accuracy 0.12222222983837128.\n",
      "Training epoch 2 batch 576 with loss 1.8224128484725952, accuracy 0.222222238779068.\n",
      "Training epoch 2 batch 577 with loss 1.7599499225616455, accuracy 0.3750000298023224.\n",
      "Training epoch 2 batch 578 with loss 1.7199798822402954, accuracy 0.30158731341362.\n",
      "Training epoch 2 batch 579 with loss 1.834770917892456, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 580 with loss 1.7887852191925049, accuracy 0.10000000149011612.\n",
      "Training epoch 2 batch 581 with loss 1.7873115539550781, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 582 with loss 1.783399224281311, accuracy 0.25.\n",
      "Training epoch 2 batch 583 with loss 1.753400206565857, accuracy 0.31111112236976624.\n",
      "Training epoch 2 batch 584 with loss 1.8001960515975952, accuracy 0.19166666269302368.\n",
      "Training epoch 2 batch 585 with loss 1.6813480854034424, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 586 with loss 1.8203704357147217, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 587 with loss 1.8561102151870728, accuracy 0.12222222983837128.\n",
      "Training epoch 2 batch 588 with loss 1.7345529794692993, accuracy 0.26851850748062134.\n",
      "Training epoch 2 batch 589 with loss 1.8121309280395508, accuracy 0.10277777910232544.\n",
      "Training epoch 2 batch 590 with loss 1.6732677221298218, accuracy 0.12857143580913544.\n",
      "Training epoch 2 batch 591 with loss 1.7244256734848022, accuracy 0.24722224473953247.\n",
      "Training epoch 2 batch 592 with loss 1.7876685857772827, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 593 with loss 1.7726080417633057, accuracy 0.2888889014720917.\n",
      "Training epoch 2 batch 594 with loss 1.8606674671173096, accuracy 0.05714286118745804.\n",
      "Training epoch 2 batch 595 with loss 1.7887630462646484, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 596 with loss 1.7805919647216797, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 597 with loss 1.8645105361938477, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 598 with loss 1.8495193719863892, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 599 with loss 1.816316843032837, accuracy 0.21388889849185944.\n",
      "Training epoch 2 batch 600 with loss 1.741987943649292, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 601 with loss 1.8555361032485962, accuracy 0.17658731341362.\n",
      "Training epoch 2 batch 602 with loss 1.668217420578003, accuracy 0.2916666865348816.\n",
      "Training epoch 2 batch 603 with loss 1.7966880798339844, accuracy 0.144841268658638.\n",
      "Training epoch 2 batch 604 with loss 1.7398221492767334, accuracy 0.1071428582072258.\n",
      "Training epoch 2 batch 605 with loss 1.775822639465332, accuracy 0.25.\n",
      "Training epoch 2 batch 606 with loss 1.8415043354034424, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 607 with loss 1.7349841594696045, accuracy 0.24722222983837128.\n",
      "Training epoch 2 batch 608 with loss 1.857095718383789, accuracy 0.125.\n",
      "Training epoch 2 batch 609 with loss 1.7927261590957642, accuracy 0.17777778208255768.\n",
      "Training epoch 2 batch 610 with loss 1.7599213123321533, accuracy 0.2916666567325592.\n",
      "Training epoch 2 batch 611 with loss 1.7800655364990234, accuracy 0.2916666567325592.\n",
      "Training epoch 2 batch 612 with loss 1.7053182125091553, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 613 with loss 1.766669511795044, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 614 with loss 1.812780737876892, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 615 with loss 1.7332851886749268, accuracy 0.375.\n",
      "Training epoch 2 batch 616 with loss 1.6617934703826904, accuracy 0.5347222089767456.\n",
      "Training epoch 2 batch 617 with loss 1.7161632776260376, accuracy 0.23888888955116272.\n",
      "Training epoch 2 batch 618 with loss 1.6973787546157837, accuracy 0.210317462682724.\n",
      "Training epoch 2 batch 619 with loss 1.8802440166473389, accuracy 0.0694444477558136.\n",
      "Training epoch 2 batch 620 with loss 1.854184865951538, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 621 with loss 1.714186668395996, accuracy 0.22777777910232544.\n",
      "Training epoch 2 batch 622 with loss 1.7174867391586304, accuracy 0.4920634925365448.\n",
      "Training epoch 2 batch 623 with loss 1.7899115085601807, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 624 with loss 1.7082502841949463, accuracy 0.14166668057441711.\n",
      "Training epoch 2 batch 625 with loss 1.8049201965332031, accuracy 0.15555556118488312.\n",
      "Training epoch 2 batch 626 with loss 1.7565386295318604, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 627 with loss 1.848575234413147, accuracy 0.2460317462682724.\n",
      "Training epoch 2 batch 628 with loss 1.720592737197876, accuracy 0.347222238779068.\n",
      "Training epoch 2 batch 629 with loss 1.9164478778839111, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 630 with loss 1.790897011756897, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 631 with loss 1.771356225013733, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 632 with loss 1.704983115196228, accuracy 0.4750000238418579.\n",
      "Training epoch 2 batch 633 with loss 1.7801949977874756, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 634 with loss 1.80214524269104, accuracy 0.19166667759418488.\n",
      "Training epoch 2 batch 635 with loss 1.7982995510101318, accuracy 0.2611111104488373.\n",
      "Training epoch 2 batch 636 with loss 1.8416054248809814, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 637 with loss 1.813236951828003, accuracy 0.25555557012557983.\n",
      "Training epoch 2 batch 638 with loss 1.748835802078247, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 639 with loss 1.8379738330841064, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 640 with loss 1.770594835281372, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 641 with loss 1.8242079019546509, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 642 with loss 1.83231520652771, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 643 with loss 1.7425849437713623, accuracy 0.29722222685813904.\n",
      "Training epoch 2 batch 644 with loss 1.7887613773345947, accuracy 0.25.\n",
      "Training epoch 2 batch 645 with loss 1.7044029235839844, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 646 with loss 1.8230139017105103, accuracy 0.15555556118488312.\n",
      "Training epoch 2 batch 647 with loss 1.7469011545181274, accuracy 0.04444444552063942.\n",
      "Training epoch 2 batch 648 with loss 1.798501968383789, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 649 with loss 1.7160284519195557, accuracy 0.2738095223903656.\n",
      "Training epoch 2 batch 650 with loss 1.7047078609466553, accuracy 0.2750000059604645.\n",
      "Training epoch 2 batch 651 with loss 1.7073971033096313, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 652 with loss 1.9381376504898071, accuracy 0.22380952537059784.\n",
      "Training epoch 2 batch 653 with loss 1.8144710063934326, accuracy 0.32499998807907104.\n",
      "Training epoch 2 batch 654 with loss 1.7488250732421875, accuracy 0.25.\n",
      "Training epoch 2 batch 655 with loss 1.9108606576919556, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 656 with loss 1.8177297115325928, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 657 with loss 1.789113998413086, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 658 with loss 1.774053931236267, accuracy 0.222222238779068.\n",
      "Training epoch 2 batch 659 with loss 1.813868522644043, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 660 with loss 1.798545479774475, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 661 with loss 1.7558075189590454, accuracy 0.3611111044883728.\n",
      "Training epoch 2 batch 662 with loss 1.7949901819229126, accuracy 0.2916666865348816.\n",
      "Training epoch 2 batch 663 with loss 1.8457937240600586, accuracy 0.02380952425301075.\n",
      "Training epoch 2 batch 664 with loss 1.7167537212371826, accuracy 0.2527777850627899.\n",
      "Training epoch 2 batch 665 with loss 1.724914312362671, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 666 with loss 1.8705250024795532, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 667 with loss 1.6821305751800537, accuracy 0.42499998211860657.\n",
      "Training epoch 2 batch 668 with loss 1.840667486190796, accuracy 0.11428572237491608.\n",
      "Training epoch 2 batch 669 with loss 1.737842321395874, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 670 with loss 1.829326868057251, accuracy 0.3222222328186035.\n",
      "Training epoch 2 batch 671 with loss 1.6871974468231201, accuracy 0.3500000238418579.\n",
      "Training epoch 2 batch 672 with loss 1.7562429904937744, accuracy 0.190476194024086.\n",
      "Training epoch 2 batch 673 with loss 1.8430103063583374, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 674 with loss 1.7474870681762695, accuracy 0.25555554032325745.\n",
      "Training epoch 2 batch 675 with loss 1.7136904001235962, accuracy 0.13055555522441864.\n",
      "Training epoch 2 batch 676 with loss 1.8268941640853882, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 677 with loss 1.7373167276382446, accuracy 0.3638888895511627.\n",
      "Training epoch 2 batch 678 with loss 1.7464392185211182, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 679 with loss 1.7003066539764404, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 680 with loss 1.7737334966659546, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 681 with loss 1.7889912128448486, accuracy 0.2361111044883728.\n",
      "Training epoch 2 batch 682 with loss 1.7549364566802979, accuracy 0.22777777910232544.\n",
      "Training epoch 2 batch 683 with loss 1.7593014240264893, accuracy 0.3166666626930237.\n",
      "Training epoch 2 batch 684 with loss 1.657130479812622, accuracy 0.4305555522441864.\n",
      "Training epoch 2 batch 685 with loss 1.8026971817016602, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 686 with loss 1.7988433837890625, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 687 with loss 1.831120491027832, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 688 with loss 1.649118423461914, accuracy 0.3305555582046509.\n",
      "Training epoch 2 batch 689 with loss 1.8284761905670166, accuracy 0.125.\n",
      "Training epoch 2 batch 690 with loss 1.8109362125396729, accuracy 0.1071428582072258.\n",
      "Training epoch 2 batch 691 with loss 1.75790536403656, accuracy 0.3194444477558136.\n",
      "Training epoch 2 batch 692 with loss 1.763721227645874, accuracy 0.2361111044883728.\n",
      "Training epoch 2 batch 693 with loss 1.7845414876937866, accuracy 0.31666669249534607.\n",
      "Training epoch 2 batch 694 with loss 1.8144657611846924, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 695 with loss 1.9182008504867554, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 696 with loss 1.7483257055282593, accuracy 0.2785714566707611.\n",
      "Training epoch 2 batch 697 with loss 1.882164716720581, accuracy 0.0.\n",
      "Training epoch 2 batch 698 with loss 1.868639588356018, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 699 with loss 1.7301381826400757, accuracy 0.21388888359069824.\n",
      "Training epoch 2 batch 700 with loss 1.7793563604354858, accuracy 0.1805555671453476.\n",
      "Training epoch 2 batch 701 with loss 1.814419150352478, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 702 with loss 1.8895221948623657, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 703 with loss 1.8001638650894165, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 704 with loss 1.7636455297470093, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 705 with loss 1.87956964969635, accuracy 0.20000001788139343.\n",
      "Training epoch 2 batch 706 with loss 1.7578697204589844, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 707 with loss 1.708469033241272, accuracy 0.2698412835597992.\n",
      "Training epoch 2 batch 708 with loss 1.789142370223999, accuracy 0.3214285671710968.\n",
      "Training epoch 2 batch 709 with loss 1.7439191341400146, accuracy 0.3333333432674408.\n",
      "Training epoch 2 batch 710 with loss 1.7111485004425049, accuracy 0.2916666567325592.\n",
      "Training epoch 2 batch 711 with loss 1.8454179763793945, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 712 with loss 1.8461599349975586, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 713 with loss 1.8096294403076172, accuracy 0.21666666865348816.\n",
      "Training epoch 2 batch 714 with loss 1.8922271728515625, accuracy 0.02380952425301075.\n",
      "Training epoch 2 batch 715 with loss 1.7627979516983032, accuracy 0.2291666716337204.\n",
      "Training epoch 2 batch 716 with loss 1.7644026279449463, accuracy 0.13214287161827087.\n",
      "Training epoch 2 batch 717 with loss 1.7553895711898804, accuracy 0.3333333432674408.\n",
      "Training epoch 2 batch 718 with loss 1.8145246505737305, accuracy 0.15833333134651184.\n",
      "Training epoch 2 batch 719 with loss 1.7077974081039429, accuracy 0.17817460000514984.\n",
      "Training epoch 2 batch 720 with loss 1.8541152477264404, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 721 with loss 1.7857410907745361, accuracy 0.2083333283662796.\n",
      "Training epoch 2 batch 722 with loss 1.8377103805541992, accuracy 0.17777778208255768.\n",
      "Training epoch 2 batch 723 with loss 1.8276774883270264, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 724 with loss 1.7407643795013428, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 725 with loss 1.7505537271499634, accuracy 0.125.\n",
      "Training epoch 2 batch 726 with loss 1.738149642944336, accuracy 0.18518519401550293.\n",
      "Training epoch 2 batch 727 with loss 1.824822187423706, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 728 with loss 1.7773332595825195, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 729 with loss 1.6911351680755615, accuracy 0.29722222685813904.\n",
      "Training epoch 2 batch 730 with loss 1.773622751235962, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 731 with loss 1.8239103555679321, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 732 with loss 1.9767916202545166, accuracy 0.0.\n",
      "Training epoch 2 batch 733 with loss 1.7920401096343994, accuracy 0.18611110746860504.\n",
      "Training epoch 2 batch 734 with loss 1.7419307231903076, accuracy 0.25555557012557983.\n",
      "Training epoch 2 batch 735 with loss 1.6916720867156982, accuracy 0.23888888955116272.\n",
      "Training epoch 2 batch 736 with loss 1.821494698524475, accuracy 0.15555556118488312.\n",
      "Training epoch 2 batch 737 with loss 1.8537346124649048, accuracy 0.13055555522441864.\n",
      "Training epoch 2 batch 738 with loss 1.8013784885406494, accuracy 0.16190476715564728.\n",
      "Training epoch 2 batch 739 with loss 1.7059276103973389, accuracy 0.13611111044883728.\n",
      "Training epoch 2 batch 740 with loss 1.7206424474716187, accuracy 0.3083333373069763.\n",
      "Training epoch 2 batch 741 with loss 1.8011724948883057, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 742 with loss 1.6872085332870483, accuracy 0.329365074634552.\n",
      "Training epoch 2 batch 743 with loss 1.7836898565292358, accuracy 0.3194444477558136.\n",
      "Training epoch 2 batch 744 with loss 1.7575972080230713, accuracy 0.3333333432674408.\n",
      "Training epoch 2 batch 745 with loss 1.9474166631698608, accuracy 0.03333333507180214.\n",
      "Training epoch 2 batch 746 with loss 1.7295650243759155, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 747 with loss 1.785650610923767, accuracy 0.2142857164144516.\n",
      "Training epoch 2 batch 748 with loss 1.8518762588500977, accuracy 0.13055555522441864.\n",
      "Training epoch 2 batch 749 with loss 1.8281946182250977, accuracy 0.130952388048172.\n",
      "Training epoch 2 batch 750 with loss 1.7933318614959717, accuracy 0.236111119389534.\n",
      "Training epoch 2 batch 751 with loss 1.8478018045425415, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 752 with loss 1.7643039226531982, accuracy 0.375.\n",
      "Training epoch 2 batch 753 with loss 1.8114017248153687, accuracy 0.25.\n",
      "Training epoch 2 batch 754 with loss 1.7818405628204346, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 755 with loss 1.8290971517562866, accuracy 0.21388888359069824.\n",
      "Training epoch 2 batch 756 with loss 1.8105792999267578, accuracy 0.0763888880610466.\n",
      "Training epoch 2 batch 757 with loss 1.62224543094635, accuracy 0.5222222805023193.\n",
      "Training epoch 2 batch 758 with loss 1.7879199981689453, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 759 with loss 1.764183759689331, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 760 with loss 1.7640140056610107, accuracy 0.18333333730697632.\n",
      "Training epoch 2 batch 761 with loss 1.8198474645614624, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 762 with loss 1.7346746921539307, accuracy 0.15555556118488312.\n",
      "Training epoch 2 batch 763 with loss 1.7690064907073975, accuracy 0.236111119389534.\n",
      "Training epoch 2 batch 764 with loss 1.7523248195648193, accuracy 0.5138888955116272.\n",
      "Training epoch 2 batch 765 with loss 1.8169234991073608, accuracy 0.15833333134651184.\n",
      "Training epoch 2 batch 766 with loss 1.7412410974502563, accuracy 0.3055555820465088.\n",
      "Training epoch 2 batch 767 with loss 1.7837650775909424, accuracy 0.18333333730697632.\n",
      "Training epoch 2 batch 768 with loss 1.888106346130371, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 769 with loss 1.7258630990982056, accuracy 0.17500001192092896.\n",
      "Training epoch 2 batch 770 with loss 1.8259150981903076, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 771 with loss 1.831669569015503, accuracy 0.125.\n",
      "Training epoch 2 batch 772 with loss 1.7897708415985107, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 773 with loss 1.7589197158813477, accuracy 0.10185185074806213.\n",
      "Training epoch 2 batch 774 with loss 1.7943384647369385, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 775 with loss 1.7926450967788696, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 776 with loss 1.7386623620986938, accuracy 0.3444444537162781.\n",
      "Training epoch 2 batch 777 with loss 1.7960643768310547, accuracy 0.2738095223903656.\n",
      "Training epoch 2 batch 778 with loss 1.8205915689468384, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 779 with loss 1.701637625694275, accuracy 0.2083333283662796.\n",
      "Training epoch 2 batch 780 with loss 1.7474416494369507, accuracy 0.1726190447807312.\n",
      "Training epoch 2 batch 781 with loss 1.7360092401504517, accuracy 0.3583333492279053.\n",
      "Training epoch 2 batch 782 with loss 1.8387683629989624, accuracy 0.19166666269302368.\n",
      "Training epoch 2 batch 783 with loss 1.7764785289764404, accuracy 0.2420634925365448.\n",
      "Training epoch 2 batch 784 with loss 1.7876596450805664, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 785 with loss 1.849692702293396, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 786 with loss 1.7823517322540283, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 787 with loss 1.8328602313995361, accuracy 0.15833333134651184.\n",
      "Training epoch 2 batch 788 with loss 1.8294912576675415, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 789 with loss 1.7339750528335571, accuracy 0.37222224473953247.\n",
      "Training epoch 2 batch 790 with loss 1.7930415868759155, accuracy 0.2361111044883728.\n",
      "Training epoch 2 batch 791 with loss 1.7503421306610107, accuracy 0.111111119389534.\n",
      "Training epoch 2 batch 792 with loss 1.7763843536376953, accuracy 0.4166666567325592.\n",
      "Training epoch 2 batch 793 with loss 1.646937370300293, accuracy 0.5250000357627869.\n",
      "Training epoch 2 batch 794 with loss 1.7511314153671265, accuracy 0.25555557012557983.\n",
      "Training epoch 2 batch 795 with loss 1.8377048969268799, accuracy 0.06111111491918564.\n",
      "Training epoch 2 batch 796 with loss 1.87796151638031, accuracy 0.12222222983837128.\n",
      "Training epoch 2 batch 797 with loss 1.7325093746185303, accuracy 0.3888888955116272.\n",
      "Training epoch 2 batch 798 with loss 1.9051768779754639, accuracy 0.02777777798473835.\n",
      "Training epoch 2 batch 799 with loss 1.7710987329483032, accuracy 0.444444477558136.\n",
      "Training epoch 2 batch 800 with loss 1.7798579931259155, accuracy 0.24722221493721008.\n",
      "Training epoch 2 batch 801 with loss 1.7713139057159424, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 802 with loss 1.769735336303711, accuracy 0.2666666805744171.\n",
      "Training epoch 2 batch 803 with loss 1.6515905857086182, accuracy 0.5055555701255798.\n",
      "Training epoch 2 batch 804 with loss 1.807051420211792, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 805 with loss 1.8878370523452759, accuracy 0.1805555671453476.\n",
      "Training epoch 2 batch 806 with loss 1.8347091674804688, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 807 with loss 1.7811241149902344, accuracy 0.20555555820465088.\n",
      "Training epoch 2 batch 808 with loss 1.7658731937408447, accuracy 0.125.\n",
      "Training epoch 2 batch 809 with loss 1.7252967357635498, accuracy 0.18333333730697632.\n",
      "Training epoch 2 batch 810 with loss 1.8445346355438232, accuracy 0.1488095223903656.\n",
      "Training epoch 2 batch 811 with loss 1.7140569686889648, accuracy 0.2916666865348816.\n",
      "Training epoch 2 batch 812 with loss 1.7439044713974, accuracy 0.21944445371627808.\n",
      "Training epoch 2 batch 813 with loss 1.732027292251587, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 814 with loss 1.7932780981063843, accuracy 0.125.\n",
      "Training epoch 2 batch 815 with loss 1.904773473739624, accuracy 0.17777778208255768.\n",
      "Training epoch 2 batch 816 with loss 1.7351627349853516, accuracy 0.3500000238418579.\n",
      "Training epoch 2 batch 817 with loss 1.753659963607788, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 818 with loss 1.8199021816253662, accuracy 0.12222222983837128.\n",
      "Training epoch 2 batch 819 with loss 1.698089361190796, accuracy 0.21388889849185944.\n",
      "Training epoch 2 batch 820 with loss 1.862540602684021, accuracy 0.06111111491918564.\n",
      "Training epoch 2 batch 821 with loss 1.7796303033828735, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 822 with loss 1.9737094640731812, accuracy 0.03333333507180214.\n",
      "Training epoch 2 batch 823 with loss 2.0157506465911865, accuracy 0.03333333507180214.\n",
      "Training epoch 2 batch 824 with loss 1.733582854270935, accuracy 0.3571428656578064.\n",
      "Training epoch 2 batch 825 with loss 1.874249815940857, accuracy 0.190476194024086.\n",
      "Training epoch 2 batch 826 with loss 1.8882789611816406, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 827 with loss 1.7642691135406494, accuracy 0.18333333730697632.\n",
      "Training epoch 2 batch 828 with loss 1.8389708995819092, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 829 with loss 1.8232536315917969, accuracy 0.15833333134651184.\n",
      "Training epoch 2 batch 830 with loss 1.7876678705215454, accuracy 0.0694444477558136.\n",
      "Training epoch 2 batch 831 with loss 1.8413417339324951, accuracy 0.03333333507180214.\n",
      "Training epoch 2 batch 832 with loss 1.8658134937286377, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 833 with loss 1.6242876052856445, accuracy 0.4444444477558136.\n",
      "Training epoch 2 batch 834 with loss 1.8555021286010742, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 835 with loss 1.7676299810409546, accuracy 0.18333333730697632.\n",
      "Training epoch 2 batch 836 with loss 1.8494476079940796, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 837 with loss 1.7673473358154297, accuracy 0.17777778208255768.\n",
      "Training epoch 2 batch 838 with loss 1.795728325843811, accuracy 0.3083333373069763.\n",
      "Training epoch 2 batch 839 with loss 1.8900458812713623, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 840 with loss 1.9032366275787354, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 841 with loss 1.922128438949585, accuracy 0.18333333730697632.\n",
      "Training epoch 2 batch 842 with loss 1.7676279544830322, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 843 with loss 1.8826109170913696, accuracy 0.125.\n",
      "Training epoch 2 batch 844 with loss 1.885636568069458, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 845 with loss 1.7469526529312134, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 846 with loss 1.7846143245697021, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 847 with loss 1.7985961437225342, accuracy 0.23333333432674408.\n",
      "Training epoch 2 batch 848 with loss 1.848731279373169, accuracy 0.1805555671453476.\n",
      "Training epoch 2 batch 849 with loss 1.7518361806869507, accuracy 0.375.\n",
      "Training epoch 2 batch 850 with loss 1.7100601196289062, accuracy 0.16190476715564728.\n",
      "Training epoch 2 batch 851 with loss 1.7776613235473633, accuracy 0.23333334922790527.\n",
      "Training epoch 2 batch 852 with loss 1.6838982105255127, accuracy 0.4333333373069763.\n",
      "Training epoch 2 batch 853 with loss 1.710601806640625, accuracy 0.20555555820465088.\n",
      "Training epoch 2 batch 854 with loss 1.7787322998046875, accuracy 0.22777777910232544.\n",
      "Training epoch 2 batch 855 with loss 1.8543096780776978, accuracy 0.0793650820851326.\n",
      "Training epoch 2 batch 856 with loss 1.6849281787872314, accuracy 0.40833333134651184.\n",
      "Training epoch 2 batch 857 with loss 1.8197170495986938, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 858 with loss 1.8795725107192993, accuracy 0.14166666567325592.\n",
      "Training epoch 2 batch 859 with loss 1.7513625621795654, accuracy 0.23333333432674408.\n",
      "Training epoch 2 batch 860 with loss 1.8571913242340088, accuracy 0.329365074634552.\n",
      "Training epoch 2 batch 861 with loss 1.7236454486846924, accuracy 0.4126984179019928.\n",
      "Training epoch 2 batch 862 with loss 1.7896091938018799, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 863 with loss 1.7583812475204468, accuracy 0.3611111342906952.\n",
      "Training epoch 2 batch 864 with loss 1.7147548198699951, accuracy 0.29027777910232544.\n",
      "Training epoch 2 batch 865 with loss 1.7471287250518799, accuracy 0.1319444477558136.\n",
      "Training epoch 2 batch 866 with loss 1.8352705240249634, accuracy 0.06111111491918564.\n",
      "Training epoch 2 batch 867 with loss 1.7777248620986938, accuracy 0.4138888716697693.\n",
      "Training epoch 2 batch 868 with loss 1.7822221517562866, accuracy 0.2666666805744171.\n",
      "Training epoch 2 batch 869 with loss 1.692779541015625, accuracy 0.26111114025115967.\n",
      "Training epoch 2 batch 870 with loss 1.910020112991333, accuracy 0.0694444477558136.\n",
      "Training epoch 2 batch 871 with loss 1.8326356410980225, accuracy 0.2916666567325592.\n",
      "Training epoch 2 batch 872 with loss 1.7562103271484375, accuracy 0.2916666567325592.\n",
      "Training epoch 2 batch 873 with loss 1.6437371969223022, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 874 with loss 1.7755495309829712, accuracy 0.28333336114883423.\n",
      "Training epoch 2 batch 875 with loss 1.7703787088394165, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 876 with loss 1.7806005477905273, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 877 with loss 1.7903244495391846, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 878 with loss 1.7023996114730835, accuracy 0.236111119389534.\n",
      "Training epoch 2 batch 879 with loss 1.7717939615249634, accuracy 0.20555555820465088.\n",
      "Training epoch 2 batch 880 with loss 1.7718318700790405, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 881 with loss 1.7545201778411865, accuracy 0.3253968358039856.\n",
      "Training epoch 2 batch 882 with loss 1.785201072692871, accuracy 0.2876984179019928.\n",
      "Training epoch 2 batch 883 with loss 1.8130171298980713, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 884 with loss 1.7353923320770264, accuracy 0.24166667461395264.\n",
      "Training epoch 2 batch 885 with loss 1.7872825860977173, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 886 with loss 1.7584874629974365, accuracy 0.2242063581943512.\n",
      "Training epoch 2 batch 887 with loss 1.910276174545288, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 888 with loss 1.894932508468628, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 889 with loss 1.7921829223632812, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 890 with loss 1.7704002857208252, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 891 with loss 1.8447959423065186, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 892 with loss 1.8881309032440186, accuracy 0.22777777910232544.\n",
      "Training epoch 2 batch 893 with loss 1.793460488319397, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 894 with loss 1.7519630193710327, accuracy 0.2460317611694336.\n",
      "Training epoch 2 batch 895 with loss 1.8030540943145752, accuracy 0.22380952537059784.\n",
      "Training epoch 2 batch 896 with loss 1.755173683166504, accuracy 0.18888889253139496.\n",
      "Training epoch 2 batch 897 with loss 1.7164808511734009, accuracy 0.25.\n",
      "Training epoch 2 batch 898 with loss 1.7560691833496094, accuracy 0.1865079402923584.\n",
      "Training epoch 2 batch 899 with loss 1.721270203590393, accuracy 0.3194444477558136.\n",
      "Training epoch 2 batch 900 with loss 1.6649091243743896, accuracy 0.21388888359069824.\n",
      "Training epoch 2 batch 901 with loss 1.8531854152679443, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 902 with loss 1.768969178199768, accuracy 0.21111111342906952.\n",
      "Training epoch 2 batch 903 with loss 1.794020414352417, accuracy 0.12222222983837128.\n",
      "Training epoch 2 batch 904 with loss 1.7420017719268799, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 905 with loss 1.7361301183700562, accuracy 0.2083333283662796.\n",
      "Training epoch 2 batch 906 with loss 1.7714515924453735, accuracy 0.31111112236976624.\n",
      "Training epoch 2 batch 907 with loss 1.7208423614501953, accuracy 0.2361111044883728.\n",
      "Training epoch 2 batch 908 with loss 1.8050711154937744, accuracy 0.25.\n",
      "Training epoch 2 batch 909 with loss 1.7685028314590454, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 910 with loss 1.7862539291381836, accuracy 0.33888888359069824.\n",
      "Training epoch 2 batch 911 with loss 1.815609335899353, accuracy 0.1130952388048172.\n",
      "Training epoch 2 batch 912 with loss 1.7912744283676147, accuracy 0.144841268658638.\n",
      "Training epoch 2 batch 913 with loss 1.811123251914978, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 914 with loss 1.7222394943237305, accuracy 0.22777779400348663.\n",
      "Training epoch 2 batch 915 with loss 1.7707910537719727, accuracy 0.18333333730697632.\n",
      "Training epoch 2 batch 916 with loss 1.758591890335083, accuracy 0.3166666626930237.\n",
      "Training epoch 2 batch 917 with loss 1.6964435577392578, accuracy 0.3541666865348816.\n",
      "Training epoch 2 batch 918 with loss 1.7115707397460938, accuracy 0.3472222089767456.\n",
      "Training epoch 2 batch 919 with loss 1.856273889541626, accuracy 0.125.\n",
      "Training epoch 2 batch 920 with loss 1.8477274179458618, accuracy 0.1765872985124588.\n",
      "Training epoch 2 batch 921 with loss 1.8728326559066772, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 922 with loss 1.8663707971572876, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 923 with loss 1.7200078964233398, accuracy 0.375.\n",
      "Training epoch 2 batch 924 with loss 1.7453968524932861, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 925 with loss 1.863358497619629, accuracy 0.12222222983837128.\n",
      "Training epoch 2 batch 926 with loss 1.7475999593734741, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 927 with loss 1.7873271703720093, accuracy 0.1805555671453476.\n",
      "Training epoch 2 batch 928 with loss 1.790980577468872, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 929 with loss 1.8154290914535522, accuracy 0.15833333134651184.\n",
      "Training epoch 2 batch 930 with loss 1.771965742111206, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 931 with loss 1.8841454982757568, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 932 with loss 1.7787710428237915, accuracy 0.25555557012557983.\n",
      "Training epoch 2 batch 933 with loss 1.7553431987762451, accuracy 0.19166666269302368.\n",
      "Training epoch 2 batch 934 with loss 1.7041467428207397, accuracy 0.25.\n",
      "Training epoch 2 batch 935 with loss 1.8502944707870483, accuracy 0.13055555522441864.\n",
      "Training epoch 2 batch 936 with loss 1.792891263961792, accuracy 0.125.\n",
      "Training epoch 2 batch 937 with loss 1.7877213954925537, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 938 with loss 1.7163441181182861, accuracy 0.4027777910232544.\n",
      "Training epoch 2 batch 939 with loss 1.7387481927871704, accuracy 0.29722222685813904.\n",
      "Training epoch 2 batch 940 with loss 1.8352693319320679, accuracy 0.125.\n",
      "Training epoch 2 batch 941 with loss 1.7720496654510498, accuracy 0.3392857313156128.\n",
      "Training epoch 2 batch 942 with loss 1.7748186588287354, accuracy 0.21388888359069824.\n",
      "Training epoch 2 batch 943 with loss 1.8112541437149048, accuracy 0.18611112236976624.\n",
      "Training epoch 2 batch 944 with loss 1.7418161630630493, accuracy 0.1805555671453476.\n",
      "Training epoch 2 batch 945 with loss 1.82611083984375, accuracy 0.02777777798473835.\n",
      "Training epoch 2 batch 946 with loss 1.7852249145507812, accuracy 0.375.\n",
      "Training epoch 2 batch 947 with loss 1.7501699924468994, accuracy 0.40833336114883423.\n",
      "Training epoch 2 batch 948 with loss 1.7369295358657837, accuracy 0.1349206417798996.\n",
      "Training epoch 2 batch 949 with loss 1.8404388427734375, accuracy 0.06111111491918564.\n",
      "Training epoch 2 batch 950 with loss 1.919671654701233, accuracy 0.125.\n",
      "Training epoch 2 batch 951 with loss 1.7674436569213867, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 952 with loss 1.7903521060943604, accuracy 0.3777777850627899.\n",
      "Training epoch 2 batch 953 with loss 1.8019750118255615, accuracy 0.2569444477558136.\n",
      "Training epoch 2 batch 954 with loss 1.842487096786499, accuracy 0.0.\n",
      "Training epoch 2 batch 955 with loss 1.7897707223892212, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 956 with loss 1.837552785873413, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 957 with loss 1.7885723114013672, accuracy 0.13055555522441864.\n",
      "Training epoch 2 batch 958 with loss 1.9074583053588867, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 959 with loss 1.8243789672851562, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 960 with loss 1.8003714084625244, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 961 with loss 1.8157819509506226, accuracy 0.3888888955116272.\n",
      "Training epoch 2 batch 962 with loss 1.8428760766983032, accuracy 0.02777777798473835.\n",
      "Training epoch 2 batch 963 with loss 1.859908103942871, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 964 with loss 1.8196563720703125, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 965 with loss 1.6775684356689453, accuracy 0.2611111104488373.\n",
      "Training epoch 2 batch 966 with loss 1.6895666122436523, accuracy 0.2361111044883728.\n",
      "Training epoch 2 batch 967 with loss 1.7585235834121704, accuracy 0.125.\n",
      "Training epoch 2 batch 968 with loss 1.9634344577789307, accuracy 0.0.\n",
      "Training epoch 2 batch 969 with loss 1.7758973836898804, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 970 with loss 1.8150421380996704, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 971 with loss 1.8661330938339233, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 972 with loss 1.7879066467285156, accuracy 0.16388890147209167.\n",
      "Training epoch 2 batch 973 with loss 1.8388988971710205, accuracy 0.25555557012557983.\n",
      "Training epoch 2 batch 974 with loss 1.7353957891464233, accuracy 0.347222238779068.\n",
      "Training epoch 2 batch 975 with loss 1.7846546173095703, accuracy 0.13611111044883728.\n",
      "Training epoch 2 batch 976 with loss 1.7825736999511719, accuracy 0.1805555671453476.\n",
      "Training epoch 2 batch 977 with loss 1.7702620029449463, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 978 with loss 1.8236114978790283, accuracy 0.13333334028720856.\n",
      "Training epoch 2 batch 979 with loss 1.8628145456314087, accuracy 0.22499999403953552.\n",
      "Training epoch 2 batch 980 with loss 1.7297919988632202, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 981 with loss 1.7255268096923828, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 982 with loss 1.809833288192749, accuracy 0.20000000298023224.\n",
      "Training epoch 2 batch 983 with loss 1.848575234413147, accuracy 0.06111111491918564.\n",
      "Training epoch 2 batch 984 with loss 1.7729301452636719, accuracy 0.21944445371627808.\n",
      "Training epoch 2 batch 985 with loss 1.839162826538086, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 986 with loss 1.7105926275253296, accuracy 0.2916666567325592.\n",
      "Training epoch 2 batch 987 with loss 1.7193381786346436, accuracy 0.125.\n",
      "Training epoch 2 batch 988 with loss 1.759670615196228, accuracy 0.24722221493721008.\n",
      "Training epoch 2 batch 989 with loss 1.855315923690796, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 990 with loss 1.8823072910308838, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 991 with loss 1.8244154453277588, accuracy 0.10000000894069672.\n",
      "Training epoch 2 batch 992 with loss 1.7900354862213135, accuracy 0.03333333507180214.\n",
      "Training epoch 2 batch 993 with loss 1.785186767578125, accuracy 0.13809524476528168.\n",
      "Training epoch 2 batch 994 with loss 1.856781005859375, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 995 with loss 1.8135894536972046, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 996 with loss 1.7507984638214111, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 997 with loss 1.7067821025848389, accuracy 0.375.\n",
      "Training epoch 2 batch 998 with loss 1.7808269262313843, accuracy 0.3333333432674408.\n",
      "Training epoch 2 batch 999 with loss 1.7634652853012085, accuracy 0.2666666805744171.\n",
      "Training epoch 2 batch 1000 with loss 1.7558590173721313, accuracy 0.4226190447807312.\n",
      "Training epoch 2 batch 1001 with loss 1.7616599798202515, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 1002 with loss 1.7503570318222046, accuracy 0.277777761220932.\n",
      "Training epoch 2 batch 1003 with loss 1.8065147399902344, accuracy 0.13333334028720856.\n",
      "Training epoch 2 batch 1004 with loss 1.8430439233779907, accuracy 0.3055555820465088.\n",
      "Training epoch 2 batch 1005 with loss 1.7413883209228516, accuracy 0.34880951046943665.\n",
      "Training epoch 2 batch 1006 with loss 1.8250226974487305, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 1007 with loss 1.811751127243042, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 1008 with loss 1.9122200012207031, accuracy 0.065476194024086.\n",
      "Training epoch 2 batch 1009 with loss 1.7740751504898071, accuracy 0.375.\n",
      "Training epoch 2 batch 1010 with loss 1.7813268899917603, accuracy 0.31111112236976624.\n",
      "Training epoch 2 batch 1011 with loss 1.786036729812622, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1012 with loss 1.7655442953109741, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 1013 with loss 1.739890456199646, accuracy 0.24722222983837128.\n",
      "Training epoch 2 batch 1014 with loss 1.7732775211334229, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 1015 with loss 1.9388792514801025, accuracy 0.06111111491918564.\n",
      "Training epoch 2 batch 1016 with loss 1.771145224571228, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 1017 with loss 1.8088515996932983, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 1018 with loss 1.8301433324813843, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1019 with loss 1.8288516998291016, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 1020 with loss 1.8003177642822266, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1021 with loss 1.822536826133728, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 1022 with loss 1.8080089092254639, accuracy 0.09444444626569748.\n",
      "Training epoch 2 batch 1023 with loss 1.793321967124939, accuracy 0.13750000298023224.\n",
      "Training epoch 2 batch 1024 with loss 1.8557603359222412, accuracy 0.21388889849185944.\n",
      "Training epoch 2 batch 1025 with loss 1.7844305038452148, accuracy 0.1269841343164444.\n",
      "Training epoch 2 batch 1026 with loss 1.7790361642837524, accuracy 0.1269841343164444.\n",
      "Training epoch 2 batch 1027 with loss 1.8156635761260986, accuracy 0.3055555820465088.\n",
      "Training epoch 2 batch 1028 with loss 1.8441540002822876, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 1029 with loss 1.8649463653564453, accuracy 0.1210317462682724.\n",
      "Training epoch 2 batch 1030 with loss 1.8600690364837646, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 1031 with loss 1.7780368328094482, accuracy 0.2666666805744171.\n",
      "Training epoch 2 batch 1032 with loss 1.8522169589996338, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 1033 with loss 1.7785362005233765, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 1034 with loss 1.9507324695587158, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 1035 with loss 1.8058116436004639, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 1036 with loss 1.8255679607391357, accuracy 0.3472222089767456.\n",
      "Training epoch 2 batch 1037 with loss 1.723369836807251, accuracy 0.38333335518836975.\n",
      "Training epoch 2 batch 1038 with loss 1.8250370025634766, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 1039 with loss 1.8184211254119873, accuracy 0.35185185074806213.\n",
      "Training epoch 2 batch 1040 with loss 1.8203325271606445, accuracy 0.2321428656578064.\n",
      "Training epoch 2 batch 1041 with loss 1.7962863445281982, accuracy 0.29722222685813904.\n",
      "Training epoch 2 batch 1042 with loss 1.7200806140899658, accuracy 0.24722222983837128.\n",
      "Training epoch 2 batch 1043 with loss 1.8865550756454468, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 1044 with loss 1.798547387123108, accuracy 0.1488095223903656.\n",
      "Training epoch 2 batch 1045 with loss 1.7823024988174438, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 1046 with loss 1.7847086191177368, accuracy 0.222222238779068.\n",
      "Training epoch 2 batch 1047 with loss 1.8157790899276733, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 1048 with loss 1.7592105865478516, accuracy 0.23333333432674408.\n",
      "Training epoch 2 batch 1049 with loss 1.7635984420776367, accuracy 0.2916666567325592.\n",
      "Training epoch 2 batch 1050 with loss 1.846744179725647, accuracy 0.33095240592956543.\n",
      "Training epoch 2 batch 1051 with loss 1.7799499034881592, accuracy 0.31111112236976624.\n",
      "Training epoch 2 batch 1052 with loss 1.8927781581878662, accuracy 0.1349206417798996.\n",
      "Training epoch 2 batch 1053 with loss 1.8724836111068726, accuracy 0.0.\n",
      "Training epoch 2 batch 1054 with loss 1.7189009189605713, accuracy 0.2500000298023224.\n",
      "Training epoch 2 batch 1055 with loss 1.831337571144104, accuracy 0.25.\n",
      "Training epoch 2 batch 1056 with loss 1.7742048501968384, accuracy 0.12037037312984467.\n",
      "Training epoch 2 batch 1057 with loss 1.7070516347885132, accuracy 0.2430555522441864.\n",
      "Training epoch 2 batch 1058 with loss 1.6611753702163696, accuracy 0.45000001788139343.\n",
      "Training epoch 2 batch 1059 with loss 1.8239555358886719, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 1060 with loss 1.7918580770492554, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 1061 with loss 1.744314193725586, accuracy 0.32500001788139343.\n",
      "Training epoch 2 batch 1062 with loss 1.8535875082015991, accuracy 0.02777777798473835.\n",
      "Training epoch 2 batch 1063 with loss 1.7864681482315063, accuracy 0.3333333432674408.\n",
      "Training epoch 2 batch 1064 with loss 1.823508620262146, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1065 with loss 1.8204047679901123, accuracy 0.29722222685813904.\n",
      "Training epoch 2 batch 1066 with loss 1.764025092124939, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 1067 with loss 1.8030483722686768, accuracy 0.1031746044754982.\n",
      "Training epoch 2 batch 1068 with loss 1.7082229852676392, accuracy 0.43611112236976624.\n",
      "Training epoch 2 batch 1069 with loss 1.850311040878296, accuracy 0.25.\n",
      "Training epoch 2 batch 1070 with loss 1.7977285385131836, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 1071 with loss 1.7582662105560303, accuracy 0.3444444537162781.\n",
      "Training epoch 2 batch 1072 with loss 1.802893042564392, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 1073 with loss 1.868152379989624, accuracy 0.3115079402923584.\n",
      "Training epoch 2 batch 1074 with loss 1.6703369617462158, accuracy 0.375.\n",
      "Training epoch 2 batch 1075 with loss 1.9118525981903076, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 1076 with loss 1.7923438549041748, accuracy 0.03333333507180214.\n",
      "Training epoch 2 batch 1077 with loss 1.820326566696167, accuracy 0.13055555522441864.\n",
      "Training epoch 2 batch 1078 with loss 1.8163225650787354, accuracy 0.25.\n",
      "Training epoch 2 batch 1079 with loss 1.9317305088043213, accuracy 0.02083333395421505.\n",
      "Training epoch 2 batch 1080 with loss 1.6830203533172607, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 1081 with loss 1.7456696033477783, accuracy 0.30158731341362.\n",
      "Training epoch 2 batch 1082 with loss 1.841497778892517, accuracy 0.10833333432674408.\n",
      "Training epoch 2 batch 1083 with loss 1.8183215856552124, accuracy 0.0763888880610466.\n",
      "Training epoch 2 batch 1084 with loss 1.8210712671279907, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 1085 with loss 1.7767314910888672, accuracy 0.24166667461395264.\n",
      "Training epoch 2 batch 1086 with loss 1.760973572731018, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 1087 with loss 1.8176307678222656, accuracy 0.15833333134651184.\n",
      "Training epoch 2 batch 1088 with loss 1.7845312356948853, accuracy 0.2321428656578064.\n",
      "Training epoch 2 batch 1089 with loss 1.827850341796875, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 1090 with loss 1.7304518222808838, accuracy 0.25.\n",
      "Training epoch 2 batch 1091 with loss 1.7672449350357056, accuracy 0.13055555522441864.\n",
      "Training epoch 2 batch 1092 with loss 1.814997911453247, accuracy 0.24166667461395264.\n",
      "Training epoch 2 batch 1093 with loss 1.8463866710662842, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 1094 with loss 1.889835000038147, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 1095 with loss 1.721335768699646, accuracy 0.10833333432674408.\n",
      "Training epoch 2 batch 1096 with loss 1.8385604619979858, accuracy 0.125.\n",
      "Training epoch 2 batch 1097 with loss 1.848687767982483, accuracy 0.2083333283662796.\n",
      "Training epoch 2 batch 1098 with loss 1.8383042812347412, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 1099 with loss 1.7269397974014282, accuracy 0.3333333432674408.\n",
      "Training epoch 2 batch 1100 with loss 1.884075403213501, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1101 with loss 1.857967734336853, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 1102 with loss 1.8554437160491943, accuracy 0.09047619253396988.\n",
      "Training epoch 2 batch 1103 with loss 1.719670295715332, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 1104 with loss 1.7581100463867188, accuracy 0.11428572237491608.\n",
      "Training epoch 2 batch 1105 with loss 1.7276045083999634, accuracy 0.125.\n",
      "Training epoch 2 batch 1106 with loss 1.7994499206542969, accuracy 0.1587301641702652.\n",
      "Training epoch 2 batch 1107 with loss 1.8138608932495117, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 1108 with loss 1.7223596572875977, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 1109 with loss 1.7823251485824585, accuracy 0.13055555522441864.\n",
      "Training epoch 2 batch 1110 with loss 1.8696396350860596, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1111 with loss 1.7822914123535156, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 1112 with loss 1.7346868515014648, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 1113 with loss 1.7994651794433594, accuracy 0.22777777910232544.\n",
      "Training epoch 2 batch 1114 with loss 1.7533628940582275, accuracy 0.27222222089767456.\n",
      "Training epoch 2 batch 1115 with loss 1.8260548114776611, accuracy 0.18611110746860504.\n",
      "Training epoch 2 batch 1116 with loss 1.8611772060394287, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 1117 with loss 1.751947045326233, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 1118 with loss 1.8677476644515991, accuracy 0.25555557012557983.\n",
      "Training epoch 2 batch 1119 with loss 1.797305703163147, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 1120 with loss 1.7100903987884521, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 1121 with loss 1.8579479455947876, accuracy 0.18611112236976624.\n",
      "Training epoch 2 batch 1122 with loss 1.6932798624038696, accuracy 0.5166666507720947.\n",
      "Training epoch 2 batch 1123 with loss 1.7557052373886108, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 1124 with loss 1.874639868736267, accuracy 0.0694444477558136.\n",
      "Training epoch 2 batch 1125 with loss 1.910315752029419, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 1126 with loss 1.8801065683364868, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 1127 with loss 1.635108232498169, accuracy 0.4472222328186035.\n",
      "Training epoch 2 batch 1128 with loss 1.8050779104232788, accuracy 0.2611111104488373.\n",
      "Training epoch 2 batch 1129 with loss 1.821390151977539, accuracy 0.16388888657093048.\n",
      "Training epoch 2 batch 1130 with loss 1.8288848400115967, accuracy 0.22777777910232544.\n",
      "Training epoch 2 batch 1131 with loss 1.8490947484970093, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 1132 with loss 1.714451789855957, accuracy 0.3472222089767456.\n",
      "Training epoch 2 batch 1133 with loss 1.829685926437378, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 1134 with loss 1.7680898904800415, accuracy 0.14166668057441711.\n",
      "Training epoch 2 batch 1135 with loss 1.8072364330291748, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1136 with loss 1.759352445602417, accuracy 0.35476189851760864.\n",
      "Training epoch 2 batch 1137 with loss 1.8263137340545654, accuracy 0.22380952537059784.\n",
      "Training epoch 2 batch 1138 with loss 1.6761643886566162, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 1139 with loss 1.716509222984314, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 1140 with loss 1.8797181844711304, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1141 with loss 1.8279893398284912, accuracy 0.018518518656492233.\n",
      "Training epoch 2 batch 1142 with loss 1.7347596883773804, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 1143 with loss 1.8530635833740234, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 1144 with loss 1.8446333408355713, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1145 with loss 1.8094152212142944, accuracy 0.125.\n",
      "Training epoch 2 batch 1146 with loss 1.8662904500961304, accuracy 0.0.\n",
      "Training epoch 2 batch 1147 with loss 1.7063922882080078, accuracy 0.22500000894069672.\n",
      "Training epoch 2 batch 1148 with loss 1.8077596426010132, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 1149 with loss 1.8027918338775635, accuracy 0.10833333432674408.\n",
      "Training epoch 2 batch 1150 with loss 1.8217014074325562, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1151 with loss 1.7713956832885742, accuracy 0.22500000894069672.\n",
      "Training epoch 2 batch 1152 with loss 1.7959699630737305, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 1153 with loss 1.662937879562378, accuracy 0.32777777314186096.\n",
      "Training epoch 2 batch 1154 with loss 1.8101403713226318, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 1155 with loss 1.8273324966430664, accuracy 0.13055555522441864.\n",
      "Training epoch 2 batch 1156 with loss 1.9049291610717773, accuracy 0.28240740299224854.\n",
      "Training epoch 2 batch 1157 with loss 1.8119786977767944, accuracy 0.25555557012557983.\n",
      "Training epoch 2 batch 1158 with loss 1.8434925079345703, accuracy 0.02777777798473835.\n",
      "Training epoch 2 batch 1159 with loss 1.8049259185791016, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 1160 with loss 1.8933175802230835, accuracy 0.39444443583488464.\n",
      "Training epoch 2 batch 1161 with loss 1.8302685022354126, accuracy 0.2666666805744171.\n",
      "Training epoch 2 batch 1162 with loss 1.9450969696044922, accuracy 0.10833333432674408.\n",
      "Training epoch 2 batch 1163 with loss 1.8531639575958252, accuracy 0.1458333432674408.\n",
      "Training epoch 2 batch 1164 with loss 1.7724428176879883, accuracy 0.23333334922790527.\n",
      "Training epoch 2 batch 1165 with loss 1.805238127708435, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 1166 with loss 1.9030221700668335, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 1167 with loss 1.8580782413482666, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 1168 with loss 1.8629592657089233, accuracy 0.03333333507180214.\n",
      "Training epoch 2 batch 1169 with loss 1.82350754737854, accuracy 0.02380952425301075.\n",
      "Training epoch 2 batch 1170 with loss 1.7915828227996826, accuracy 0.24166667461395264.\n",
      "Training epoch 2 batch 1171 with loss 1.7663638591766357, accuracy 0.2611111104488373.\n",
      "Training epoch 2 batch 1172 with loss 1.8451858758926392, accuracy 0.17777778208255768.\n",
      "Training epoch 2 batch 1173 with loss 1.7967058420181274, accuracy 0.0694444477558136.\n",
      "Training epoch 2 batch 1174 with loss 1.904114007949829, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 1175 with loss 1.8731062412261963, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 1176 with loss 1.8856614828109741, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 1177 with loss 1.770141839981079, accuracy 0.3492063581943512.\n",
      "Training epoch 2 batch 1178 with loss 1.8313289880752563, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1179 with loss 1.8663021326065063, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 1180 with loss 1.836758017539978, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1181 with loss 1.8560678958892822, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 1182 with loss 1.8481744527816772, accuracy 0.25.\n",
      "Training epoch 2 batch 1183 with loss 1.8906433582305908, accuracy 0.03333333507180214.\n",
      "Training epoch 2 batch 1184 with loss 1.7906898260116577, accuracy 0.18611112236976624.\n",
      "Training epoch 2 batch 1185 with loss 1.7724981307983398, accuracy 0.18611110746860504.\n",
      "Training epoch 2 batch 1186 with loss 1.7321068048477173, accuracy 0.125.\n",
      "Training epoch 2 batch 1187 with loss 1.7167084217071533, accuracy 0.17777778208255768.\n",
      "Training epoch 2 batch 1188 with loss 1.844323754310608, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 1189 with loss 1.7749433517456055, accuracy 0.15833333134651184.\n",
      "Training epoch 2 batch 1190 with loss 1.7119152545928955, accuracy 0.3638888895511627.\n",
      "Training epoch 2 batch 1191 with loss 1.770672082901001, accuracy 0.3472222089767456.\n",
      "Training epoch 2 batch 1192 with loss 1.7201588153839111, accuracy 0.3861111104488373.\n",
      "Training epoch 2 batch 1193 with loss 1.8338006734848022, accuracy 0.14047619700431824.\n",
      "Training epoch 2 batch 1194 with loss 1.8897912502288818, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 1195 with loss 1.7556180953979492, accuracy 0.14166668057441711.\n",
      "Training epoch 2 batch 1196 with loss 1.851971983909607, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 1197 with loss 1.7230573892593384, accuracy 0.20714285969734192.\n",
      "Training epoch 2 batch 1198 with loss 1.8086246252059937, accuracy 0.3055555820465088.\n",
      "Training epoch 2 batch 1199 with loss 1.9099394083023071, accuracy 0.125.\n",
      "Training epoch 2 batch 1200 with loss 1.8520997762680054, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 1201 with loss 1.7605526447296143, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1202 with loss 1.7285703420639038, accuracy 0.375.\n",
      "Training epoch 2 batch 1203 with loss 1.8624012470245361, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 1204 with loss 1.806992530822754, accuracy 0.23333334922790527.\n",
      "Training epoch 2 batch 1205 with loss 1.9112813472747803, accuracy 0.0.\n",
      "Training epoch 2 batch 1206 with loss 1.7589120864868164, accuracy 0.22777777910232544.\n",
      "Training epoch 2 batch 1207 with loss 1.8329664468765259, accuracy 0.31111112236976624.\n",
      "Training epoch 2 batch 1208 with loss 1.7801134586334229, accuracy 0.21388888359069824.\n",
      "Training epoch 2 batch 1209 with loss 1.8686708211898804, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 1210 with loss 1.8867778778076172, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 1211 with loss 1.8745851516723633, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 1212 with loss 1.8369243144989014, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 1213 with loss 1.817469835281372, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 1214 with loss 1.7585722208023071, accuracy 0.21944445371627808.\n",
      "Training epoch 2 batch 1215 with loss 1.7479524612426758, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 1216 with loss 1.7265771627426147, accuracy 0.22777777910232544.\n",
      "Training epoch 2 batch 1217 with loss 1.8434383869171143, accuracy 0.2361111044883728.\n",
      "Training epoch 2 batch 1218 with loss 1.6958904266357422, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 1219 with loss 1.8641713857650757, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 1220 with loss 1.7774362564086914, accuracy 0.125.\n",
      "Training epoch 2 batch 1221 with loss 1.7570648193359375, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 1222 with loss 2.0053298473358154, accuracy 0.03333333507180214.\n",
      "Training epoch 2 batch 1223 with loss 1.8984076976776123, accuracy 0.222222238779068.\n",
      "Training epoch 2 batch 1224 with loss 1.8286497592926025, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 1225 with loss 1.867387056350708, accuracy 0.03333333507180214.\n",
      "Training epoch 2 batch 1226 with loss 1.7564523220062256, accuracy 0.27037036418914795.\n",
      "Training epoch 2 batch 1227 with loss 1.7788379192352295, accuracy 0.3500000238418579.\n",
      "Training epoch 2 batch 1228 with loss 1.7439186573028564, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 1229 with loss 1.8272390365600586, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 1230 with loss 1.824463129043579, accuracy 0.26944443583488464.\n",
      "Training epoch 2 batch 1231 with loss 1.8081213235855103, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1232 with loss 1.734985113143921, accuracy 0.43518519401550293.\n",
      "Training epoch 2 batch 1233 with loss 1.743568778038025, accuracy 0.2708333432674408.\n",
      "Training epoch 2 batch 1234 with loss 1.8844181299209595, accuracy 0.111111119389534.\n",
      "Training epoch 2 batch 1235 with loss 1.7212320566177368, accuracy 0.21944445371627808.\n",
      "Training epoch 2 batch 1236 with loss 1.8366420269012451, accuracy 0.10833333432674408.\n",
      "Training epoch 2 batch 1237 with loss 1.66636061668396, accuracy 0.3888888955116272.\n",
      "Training epoch 2 batch 1238 with loss 1.713393211364746, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 1239 with loss 1.8273708820343018, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 1240 with loss 1.726053237915039, accuracy 0.34166666865348816.\n",
      "Training epoch 2 batch 1241 with loss 1.7693641185760498, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 1242 with loss 1.8271701335906982, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 1243 with loss 1.7020676136016846, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 1244 with loss 1.779381513595581, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 1245 with loss 1.7871177196502686, accuracy 0.20555555820465088.\n",
      "Training epoch 2 batch 1246 with loss 1.7511488199234009, accuracy 0.21388888359069824.\n",
      "Training epoch 2 batch 1247 with loss 1.8314374685287476, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 1248 with loss 1.7807337045669556, accuracy 0.3333333432674408.\n",
      "Training epoch 2 batch 1249 with loss 1.8569759130477905, accuracy 0.2916666865348816.\n",
      "Training epoch 2 batch 1250 with loss 1.8481639623641968, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 1251 with loss 1.9019778966903687, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 1252 with loss 1.810567855834961, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 1253 with loss 1.7001044750213623, accuracy 0.25.\n",
      "Training epoch 2 batch 1254 with loss 1.8080368041992188, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 1255 with loss 1.7774536609649658, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 1256 with loss 1.776074767112732, accuracy 0.33888888359069824.\n",
      "Training epoch 2 batch 1257 with loss 1.8564231395721436, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 1258 with loss 1.7117936611175537, accuracy 0.18333333730697632.\n",
      "Training epoch 2 batch 1259 with loss 1.7990334033966064, accuracy 0.3253968358039856.\n",
      "Training epoch 2 batch 1260 with loss 1.7602390050888062, accuracy 0.2611111104488373.\n",
      "Training epoch 2 batch 1261 with loss 1.7517244815826416, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 1262 with loss 1.7384974956512451, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 1263 with loss 1.8640544414520264, accuracy 0.25555557012557983.\n",
      "Training epoch 2 batch 1264 with loss 1.8005406856536865, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 1265 with loss 1.8190199136734009, accuracy 0.1527777910232544.\n",
      "Training epoch 2 batch 1266 with loss 1.8132368326187134, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 1267 with loss 1.8466068506240845, accuracy 0.125.\n",
      "Training epoch 2 batch 1268 with loss 1.8420867919921875, accuracy 0.16388890147209167.\n",
      "Training epoch 2 batch 1269 with loss 1.8142563104629517, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 1270 with loss 1.8192005157470703, accuracy 0.111111119389534.\n",
      "Training epoch 2 batch 1271 with loss 1.7667722702026367, accuracy 0.20555555820465088.\n",
      "Training epoch 2 batch 1272 with loss 1.77290940284729, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1273 with loss 1.7823381423950195, accuracy 0.05714286118745804.\n",
      "Training epoch 2 batch 1274 with loss 1.8116381168365479, accuracy 0.2666666805744171.\n",
      "Training epoch 2 batch 1275 with loss 1.8914178609848022, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 1276 with loss 1.7203590869903564, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 1277 with loss 1.8315746784210205, accuracy 0.2182539701461792.\n",
      "Training epoch 2 batch 1278 with loss 1.8119510412216187, accuracy 0.3263888955116272.\n",
      "Training epoch 2 batch 1279 with loss 1.8009859323501587, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 1280 with loss 1.7873351573944092, accuracy 0.1031746044754982.\n",
      "Training epoch 2 batch 1281 with loss 1.7562452554702759, accuracy 0.375.\n",
      "Training epoch 2 batch 1282 with loss 1.7581455707550049, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1283 with loss 1.8382253646850586, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 1284 with loss 1.7113037109375, accuracy 0.23055556416511536.\n",
      "Training epoch 2 batch 1285 with loss 1.765138864517212, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 1286 with loss 1.8135782480239868, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 1287 with loss 1.7439193725585938, accuracy 0.28333333134651184.\n",
      "Training epoch 2 batch 1288 with loss 1.815529227256775, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 1289 with loss 1.843796730041504, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 1290 with loss 1.8480002880096436, accuracy 0.26250001788139343.\n",
      "Training epoch 2 batch 1291 with loss 1.8287467956542969, accuracy 0.17777778208255768.\n",
      "Training epoch 2 batch 1292 with loss 1.7883055210113525, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 1293 with loss 1.8052326440811157, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1294 with loss 1.726639986038208, accuracy 0.20000000298023224.\n",
      "Training epoch 2 batch 1295 with loss 1.9669768810272217, accuracy 0.02380952425301075.\n",
      "Training epoch 2 batch 1296 with loss 1.7717616558074951, accuracy 0.25555557012557983.\n",
      "Training epoch 2 batch 1297 with loss 1.7498098611831665, accuracy 0.08095238357782364.\n",
      "Training epoch 2 batch 1298 with loss 1.9303395748138428, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 1299 with loss 1.7601044178009033, accuracy 0.08095238357782364.\n",
      "Training epoch 2 batch 1300 with loss 1.8369810581207275, accuracy 0.1805555671453476.\n",
      "Training epoch 2 batch 1301 with loss 1.832877516746521, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 1302 with loss 1.7913001775741577, accuracy 0.125.\n",
      "Training epoch 2 batch 1303 with loss 1.6749989986419678, accuracy 0.24761906266212463.\n",
      "Training epoch 2 batch 1304 with loss 1.8027414083480835, accuracy 0.15000000596046448.\n",
      "Training epoch 2 batch 1305 with loss 1.7617883682250977, accuracy 0.2944444417953491.\n",
      "Training epoch 2 batch 1306 with loss 1.8770954608917236, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1307 with loss 1.8939268589019775, accuracy 0.0.\n",
      "Training epoch 2 batch 1308 with loss 1.7924792766571045, accuracy 0.125.\n",
      "Training epoch 2 batch 1309 with loss 1.8206698894500732, accuracy 0.222222238779068.\n",
      "Training epoch 2 batch 1310 with loss 1.7431122064590454, accuracy 0.236111119389534.\n",
      "Training epoch 2 batch 1311 with loss 1.7395092248916626, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 1312 with loss 1.9101022481918335, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 1313 with loss 1.85024893283844, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 1314 with loss 1.7678407430648804, accuracy 0.2111111283302307.\n",
      "Training epoch 2 batch 1315 with loss 1.7739559412002563, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 1316 with loss 1.7926222085952759, accuracy 0.222222238779068.\n",
      "Training epoch 2 batch 1317 with loss 1.781818151473999, accuracy 0.19722223281860352.\n",
      "Training epoch 2 batch 1318 with loss 1.805436372756958, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 1319 with loss 1.8325920104980469, accuracy 0.16388890147209167.\n",
      "Training epoch 2 batch 1320 with loss 1.7986087799072266, accuracy 0.3472222089767456.\n",
      "Training epoch 2 batch 1321 with loss 1.7468379735946655, accuracy 0.25.\n",
      "Training epoch 2 batch 1322 with loss 1.9885480403900146, accuracy 0.06111111491918564.\n",
      "Training epoch 2 batch 1323 with loss 1.6861379146575928, accuracy 0.23472222685813904.\n",
      "Training epoch 2 batch 1324 with loss 1.773503065109253, accuracy 0.20000000298023224.\n",
      "Training epoch 2 batch 1325 with loss 1.740722894668579, accuracy 0.22777777910232544.\n",
      "Training epoch 2 batch 1326 with loss 1.8205512762069702, accuracy 0.125.\n",
      "Training epoch 2 batch 1327 with loss 1.7923551797866821, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 1328 with loss 1.8336257934570312, accuracy 0.1587301641702652.\n",
      "Training epoch 2 batch 1329 with loss 1.7482664585113525, accuracy 0.20000000298023224.\n",
      "Training epoch 2 batch 1330 with loss 1.7643449306488037, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 1331 with loss 1.8074285984039307, accuracy 0.25.\n",
      "Training epoch 2 batch 1332 with loss 1.7814171314239502, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 1333 with loss 1.8209816217422485, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 1334 with loss 1.8220313787460327, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 1335 with loss 1.7708556652069092, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 1336 with loss 1.8555454015731812, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 1337 with loss 1.8475841283798218, accuracy 0.22777777910232544.\n",
      "Training epoch 2 batch 1338 with loss 1.7667919397354126, accuracy 0.17222222685813904.\n",
      "Training epoch 2 batch 1339 with loss 1.7972590923309326, accuracy 0.2420634925365448.\n",
      "Training epoch 2 batch 1340 with loss 1.8623806238174438, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 1341 with loss 1.8293319940567017, accuracy 0.09880952537059784.\n",
      "Training epoch 2 batch 1342 with loss 1.837720274925232, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 1343 with loss 1.8629274368286133, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 1344 with loss 1.8361060619354248, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 1345 with loss 1.7233836650848389, accuracy 0.25.\n",
      "Training epoch 2 batch 1346 with loss 1.856711983680725, accuracy 0.23333334922790527.\n",
      "Training epoch 2 batch 1347 with loss 1.9129400253295898, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 1348 with loss 1.8438383340835571, accuracy 0.09583333134651184.\n",
      "Training epoch 2 batch 1349 with loss 1.7922642230987549, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 1350 with loss 1.765941858291626, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 1351 with loss 1.8346983194351196, accuracy 0.03333333507180214.\n",
      "Training epoch 2 batch 1352 with loss 1.8372433185577393, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 1353 with loss 1.762616515159607, accuracy 0.222222238779068.\n",
      "Training epoch 2 batch 1354 with loss 1.8255529403686523, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 1355 with loss 1.7918670177459717, accuracy 0.0.\n",
      "Training epoch 2 batch 1356 with loss 1.7786270380020142, accuracy 0.24166667461395264.\n",
      "Training epoch 2 batch 1357 with loss 1.8029670715332031, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 1358 with loss 1.737177848815918, accuracy 0.23333333432674408.\n",
      "Training epoch 2 batch 1359 with loss 1.8070640563964844, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 1360 with loss 1.94619619846344, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 1361 with loss 1.7776018381118774, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 1362 with loss 1.8233661651611328, accuracy 0.125.\n",
      "Training epoch 2 batch 1363 with loss 1.8286603689193726, accuracy 0.22777777910232544.\n",
      "Training epoch 2 batch 1364 with loss 1.7926899194717407, accuracy 0.2916666567325592.\n",
      "Training epoch 2 batch 1365 with loss 1.8215434551239014, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 1366 with loss 1.7190687656402588, accuracy 0.2142857313156128.\n",
      "Training epoch 2 batch 1367 with loss 1.7745920419692993, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 1368 with loss 1.9456069469451904, accuracy 0.0.\n",
      "Training epoch 2 batch 1369 with loss 1.7706832885742188, accuracy 0.24722222983837128.\n",
      "Training epoch 2 batch 1370 with loss 1.7502272129058838, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 1371 with loss 1.7725595235824585, accuracy 0.111111119389534.\n",
      "Training epoch 2 batch 1372 with loss 1.8027355670928955, accuracy 0.2666666805744171.\n",
      "Training epoch 2 batch 1373 with loss 1.761373519897461, accuracy 0.35277777910232544.\n",
      "Training epoch 2 batch 1374 with loss 1.8694822788238525, accuracy 0.125.\n",
      "Training epoch 2 batch 1375 with loss 1.7105436325073242, accuracy 0.3333333432674408.\n",
      "Training epoch 2 batch 1376 with loss 1.7831709384918213, accuracy 0.09583333879709244.\n",
      "Training epoch 2 batch 1377 with loss 1.8291562795639038, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 1378 with loss 1.89811110496521, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 1379 with loss 1.800916075706482, accuracy 0.19722223281860352.\n",
      "Training epoch 2 batch 1380 with loss 1.7008476257324219, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 1381 with loss 1.7313390970230103, accuracy 0.3888888955116272.\n",
      "Training epoch 2 batch 1382 with loss 1.806471824645996, accuracy 0.13055555522441864.\n",
      "Training epoch 2 batch 1383 with loss 1.7515071630477905, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 1384 with loss 1.8272062540054321, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 1385 with loss 1.7948732376098633, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1386 with loss 1.7817844152450562, accuracy 0.10833333432674408.\n",
      "Training epoch 2 batch 1387 with loss 1.8490943908691406, accuracy 0.1111111119389534.\n",
      "Training epoch 2 batch 1388 with loss 1.8024399280548096, accuracy 0.0892857164144516.\n",
      "Training epoch 2 batch 1389 with loss 1.705746054649353, accuracy 0.16388890147209167.\n",
      "Training epoch 2 batch 1390 with loss 1.6962316036224365, accuracy 0.3611111342906952.\n",
      "Training epoch 2 batch 1391 with loss 1.9104483127593994, accuracy 0.0793650820851326.\n",
      "Training epoch 2 batch 1392 with loss 1.7906835079193115, accuracy 0.25.\n",
      "Training epoch 2 batch 1393 with loss 1.7528746128082275, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 1394 with loss 1.7915904521942139, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 1395 with loss 1.8595082759857178, accuracy 0.32500001788139343.\n",
      "Training epoch 2 batch 1396 with loss 1.9146244525909424, accuracy 0.0.\n",
      "Training epoch 2 batch 1397 with loss 1.8312257528305054, accuracy 0.283730149269104.\n",
      "Training epoch 2 batch 1398 with loss 1.7925783395767212, accuracy 0.28333333134651184.\n",
      "Training epoch 2 batch 1399 with loss 1.811033844947815, accuracy 0.3333333432674408.\n",
      "Training epoch 2 batch 1400 with loss 1.7728807926177979, accuracy 0.22777777910232544.\n",
      "Training epoch 2 batch 1401 with loss 1.7121474742889404, accuracy 0.3222222328186035.\n",
      "Training epoch 2 batch 1402 with loss 1.789759635925293, accuracy 0.125.\n",
      "Training epoch 2 batch 1403 with loss 1.8007694482803345, accuracy 0.02777777798473835.\n",
      "Training epoch 2 batch 1404 with loss 1.7424774169921875, accuracy 0.3222222328186035.\n",
      "Training epoch 2 batch 1405 with loss 1.8242477178573608, accuracy 0.03333333507180214.\n",
      "Training epoch 2 batch 1406 with loss 1.8034534454345703, accuracy 0.0972222238779068.\n",
      "Training epoch 2 batch 1407 with loss 1.7443946599960327, accuracy 0.2777777910232544.\n",
      "Training epoch 2 batch 1408 with loss 1.813180685043335, accuracy 0.2638888955116272.\n",
      "Training epoch 2 batch 1409 with loss 1.843735694885254, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 1410 with loss 1.807084321975708, accuracy 0.1527777761220932.\n",
      "Training epoch 2 batch 1411 with loss 1.722957968711853, accuracy 0.4097222089767456.\n",
      "Training epoch 2 batch 1412 with loss 1.694716453552246, accuracy 0.3769841194152832.\n",
      "Training epoch 2 batch 1413 with loss 1.7964767217636108, accuracy 0.15555556118488312.\n",
      "Training epoch 2 batch 1414 with loss 1.7222645282745361, accuracy 0.19166667759418488.\n",
      "Training epoch 2 batch 1415 with loss 1.754838228225708, accuracy 0.10833333432674408.\n",
      "Training epoch 2 batch 1416 with loss 1.8155410289764404, accuracy 0.1805555522441864.\n",
      "Training epoch 2 batch 1417 with loss 1.8303289413452148, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 1418 with loss 1.724776268005371, accuracy 0.25.\n",
      "Training epoch 2 batch 1419 with loss 1.7605584859848022, accuracy 0.10000000894069672.\n",
      "Training epoch 2 batch 1420 with loss 1.861659049987793, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 1421 with loss 1.842484712600708, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 1422 with loss 1.836093544960022, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 1423 with loss 1.8887519836425781, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 1424 with loss 1.8783680200576782, accuracy 0.1547619104385376.\n",
      "Training epoch 2 batch 1425 with loss 1.738660216331482, accuracy 0.2222222238779068.\n",
      "Training epoch 2 batch 1426 with loss 1.7443031072616577, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 1427 with loss 1.8105195760726929, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1428 with loss 1.7761805057525635, accuracy 0.18888890743255615.\n",
      "Training epoch 2 batch 1429 with loss 1.7983039617538452, accuracy 0.20000000298023224.\n",
      "Training epoch 2 batch 1430 with loss 1.7596489191055298, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 1431 with loss 1.7966969013214111, accuracy 0.42222222685813904.\n",
      "Training epoch 2 batch 1432 with loss 1.8723430633544922, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 1433 with loss 1.9057867527008057, accuracy 0.21944445371627808.\n",
      "Training epoch 2 batch 1434 with loss 1.8499910831451416, accuracy 0.15833333134651184.\n",
      "Training epoch 2 batch 1435 with loss 1.835451364517212, accuracy 0.18611112236976624.\n",
      "Training epoch 2 batch 1436 with loss 1.7308332920074463, accuracy 0.20000000298023224.\n",
      "Training epoch 2 batch 1437 with loss 1.925274133682251, accuracy 0.0.\n",
      "Training epoch 2 batch 1438 with loss 1.8636806011199951, accuracy 0.2847222089767456.\n",
      "Training epoch 2 batch 1439 with loss 1.8332149982452393, accuracy 0.03333333507180214.\n",
      "Training epoch 2 batch 1440 with loss 1.7712217569351196, accuracy 0.25555557012557983.\n",
      "Training epoch 2 batch 1441 with loss 1.7876195907592773, accuracy 0.28928571939468384.\n",
      "Training epoch 2 batch 1442 with loss 1.8778762817382812, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 1443 with loss 1.8844791650772095, accuracy 0.21388888359069824.\n",
      "Training epoch 2 batch 1444 with loss 1.8278392553329468, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1445 with loss 1.7375643253326416, accuracy 0.31111112236976624.\n",
      "Training epoch 2 batch 1446 with loss 1.7196069955825806, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 1447 with loss 1.7705618143081665, accuracy 0.1031746044754982.\n",
      "Training epoch 2 batch 1448 with loss 1.8471081256866455, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 1449 with loss 1.8251657485961914, accuracy 0.11666666716337204.\n",
      "Training epoch 2 batch 1450 with loss 1.7882505655288696, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 1451 with loss 1.900019884109497, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 1452 with loss 1.755027413368225, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 1453 with loss 1.7414438724517822, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1454 with loss 1.8742914199829102, accuracy 0.1626984179019928.\n",
      "Training epoch 2 batch 1455 with loss 1.8038686513900757, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 1456 with loss 1.8032350540161133, accuracy 0.3194444477558136.\n",
      "Training epoch 2 batch 1457 with loss 1.8404672145843506, accuracy 0.06666667014360428.\n",
      "Training epoch 2 batch 1458 with loss 1.7983293533325195, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 1459 with loss 1.6722084283828735, accuracy 0.3055555522441864.\n",
      "Training epoch 2 batch 1460 with loss 1.7651681900024414, accuracy 0.236111119389534.\n",
      "Training epoch 2 batch 1461 with loss 1.818773627281189, accuracy 0.19166666269302368.\n",
      "Training epoch 2 batch 1462 with loss 1.8303499221801758, accuracy 0.25555557012557983.\n",
      "Training epoch 2 batch 1463 with loss 1.7651231288909912, accuracy 0.3500000238418579.\n",
      "Training epoch 2 batch 1464 with loss 1.8857215642929077, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1465 with loss 1.8193947076797485, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 1466 with loss 1.7643836736679077, accuracy 0.3194444477558136.\n",
      "Training epoch 2 batch 1467 with loss 1.740687370300293, accuracy 0.2599206268787384.\n",
      "Training epoch 2 batch 1468 with loss 1.6680104732513428, accuracy 0.3142857253551483.\n",
      "Training epoch 2 batch 1469 with loss 1.725754737854004, accuracy 0.1666666716337204.\n",
      "Training epoch 2 batch 1470 with loss 1.7841517925262451, accuracy 0.236111119389534.\n",
      "Training epoch 2 batch 1471 with loss 1.8154369592666626, accuracy 0.1944444477558136.\n",
      "Training epoch 2 batch 1472 with loss 1.7755401134490967, accuracy 0.4166666567325592.\n",
      "Training epoch 2 batch 1473 with loss 1.8456485271453857, accuracy 0.03333333507180214.\n",
      "Training epoch 2 batch 1474 with loss 1.7898826599121094, accuracy 0.0515873022377491.\n",
      "Training epoch 2 batch 1475 with loss 1.72837233543396, accuracy 0.375.\n",
      "Training epoch 2 batch 1476 with loss 1.679181456565857, accuracy 0.3333333432674408.\n",
      "Training epoch 2 batch 1477 with loss 1.7488210201263428, accuracy 0.3055555820465088.\n",
      "Training epoch 2 batch 1478 with loss 1.8040157556533813, accuracy 0.11666667461395264.\n",
      "Training epoch 2 batch 1479 with loss 1.8471858501434326, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 1480 with loss 1.8262535333633423, accuracy 0.13055555522441864.\n",
      "Training epoch 2 batch 1481 with loss 1.842869520187378, accuracy 0.07500000298023224.\n",
      "Training epoch 2 batch 1482 with loss 1.8222978115081787, accuracy 0.190476194024086.\n",
      "Training epoch 2 batch 1483 with loss 1.7898778915405273, accuracy 0.0416666679084301.\n",
      "Training epoch 2 batch 1484 with loss 1.8815650939941406, accuracy 0.0833333358168602.\n",
      "Training epoch 2 batch 1485 with loss 1.7511060237884521, accuracy 0.1388888955116272.\n",
      "Training epoch 2 batch 1486 with loss 1.9060285091400146, accuracy 0.0555555559694767.\n",
      "Training epoch 2 batch 1487 with loss 1.7570664882659912, accuracy 0.2708333432674408.\n",
      "Training epoch 2 batch 1488 with loss 1.7393710613250732, accuracy 0.3611111044883728.\n",
      "Training epoch 2 batch 1489 with loss 1.9145596027374268, accuracy 0.130952388048172.\n",
      "Training epoch 2 batch 1490 with loss 1.7899329662322998, accuracy 0.2083333432674408.\n",
      "Training epoch 2 batch 1491 with loss 1.7552543878555298, accuracy 0.2361111044883728.\n",
      "Training epoch 2 batch 1492 with loss 1.7003862857818604, accuracy 0.4027777910232544.\n",
      "Training epoch 2 batch 1493 with loss 1.7765041589736938, accuracy 0.17777778208255768.\n",
      "Training epoch 2 batch 1494 with loss 1.84981369972229, accuracy 0.14444445073604584.\n",
      "Training epoch 2 batch 1495 with loss 1.8428300619125366, accuracy 0.1626984179019928.\n",
      "Training epoch 2 batch 1496 with loss 1.8590497970581055, accuracy 0.08888889104127884.\n",
      "Training epoch 2 batch 1497 with loss 1.7440156936645508, accuracy 0.1814814805984497.\n",
      "Training epoch 2 batch 1498 with loss 1.8067505359649658, accuracy 0.25.\n",
      "Training epoch 2 batch 1499 with loss 1.8255016803741455, accuracy 0.2222222238779068.\n",
      "Test batch 0 with loss 1.878761649131775 and accuracy 0.06111111491918564.\n",
      "Test batch 1 with loss 1.8003027439117432 and accuracy 0.25555557012557983.\n",
      "Test batch 2 with loss 1.7002322673797607 and accuracy 0.16944444179534912.\n",
      "Test batch 3 with loss 1.9060487747192383 and accuracy 0.23888888955116272.\n",
      "Test batch 4 with loss 1.7201592922210693 and accuracy 0.3849206566810608.\n",
      "Test batch 5 with loss 1.8085365295410156 and accuracy 0.2222222238779068.\n",
      "Test batch 6 with loss 1.7746931314468384 and accuracy 0.32500001788139343.\n",
      "Test batch 7 with loss 1.8157117366790771 and accuracy 0.1111111119389534.\n",
      "Test batch 8 with loss 1.885727882385254 and accuracy 0.10833333432674408.\n",
      "Test batch 9 with loss 1.7787673473358154 and accuracy 0.0694444477558136.\n",
      "Test batch 10 with loss 1.835097074508667 and accuracy 0.21388888359069824.\n",
      "Test batch 11 with loss 1.794997215270996 and accuracy 0.22380952537059784.\n",
      "Test batch 12 with loss 1.8016002178192139 and accuracy 0.1388888955116272.\n",
      "Test batch 13 with loss 1.8582165241241455 and accuracy 0.02380952425301075.\n",
      "Test batch 14 with loss 1.7321544885635376 and accuracy 0.10185185074806213.\n",
      "Test batch 15 with loss 1.9394420385360718 and accuracy 0.1626984179019928.\n",
      "Test batch 16 with loss 1.8163690567016602 and accuracy 0.1666666716337204.\n",
      "Test batch 17 with loss 1.802947998046875 and accuracy 0.15833333134651184.\n",
      "Test batch 18 with loss 1.903794527053833 and accuracy 0.07500000298023224.\n",
      "Test batch 19 with loss 1.809247612953186 and accuracy 0.3194444477558136.\n",
      "Test batch 20 with loss 1.9008533954620361 and accuracy 0.08888889104127884.\n",
      "Test batch 21 with loss 1.7980858087539673 and accuracy 0.222222238779068.\n",
      "Test batch 22 with loss 1.780991792678833 and accuracy 0.3125.\n",
      "Test batch 23 with loss 1.7831531763076782 and accuracy 0.3333333432674408.\n",
      "Test batch 24 with loss 1.7598615884780884 and accuracy 0.236111119389534.\n",
      "Test batch 25 with loss 1.8671795129776 and accuracy 0.0833333358168602.\n",
      "Test batch 26 with loss 1.803397536277771 and accuracy 0.47817462682724.\n",
      "Test batch 27 with loss 1.8534424304962158 and accuracy 0.10833333432674408.\n",
      "Test batch 28 with loss 1.8394300937652588 and accuracy 0.07500000298023224.\n",
      "Test batch 29 with loss 1.809394121170044 and accuracy 0.11666667461395264.\n",
      "Test batch 30 with loss 1.9166332483291626 and accuracy 0.03333333507180214.\n",
      "Test batch 31 with loss 1.8382110595703125 and accuracy 0.125.\n",
      "Test batch 32 with loss 1.750436544418335 and accuracy 0.2321428656578064.\n",
      "Test batch 33 with loss 1.777692198753357 and accuracy 0.24722224473953247.\n",
      "Test batch 34 with loss 1.7849912643432617 and accuracy 0.4000000059604645.\n",
      "Test batch 35 with loss 1.8777198791503906 and accuracy 0.20555555820465088.\n",
      "Test batch 36 with loss 1.849737524986267 and accuracy 0.14444445073604584.\n",
      "Test batch 37 with loss 1.7764543294906616 and accuracy 0.2611111104488373.\n",
      "Test batch 38 with loss 1.8960838317871094 and accuracy 0.15833333134651184.\n",
      "Test batch 39 with loss 1.8344980478286743 and accuracy 0.0416666679084301.\n",
      "Test batch 40 with loss 1.786973237991333 and accuracy 0.2777777910232544.\n",
      "Test batch 41 with loss 1.8389203548431396 and accuracy 0.0694444477558136.\n",
      "Test batch 42 with loss 1.939465880393982 and accuracy 0.0.\n",
      "Test batch 43 with loss 1.8247953653335571 and accuracy 0.31018516421318054.\n",
      "Test batch 44 with loss 1.780808448791504 and accuracy 0.2638888955116272.\n",
      "Test batch 45 with loss 1.9182090759277344 and accuracy 0.1527777761220932.\n",
      "Test batch 46 with loss 1.7536461353302002 and accuracy 0.25.\n",
      "Test batch 47 with loss 1.7608118057250977 and accuracy 0.25.\n",
      "Test batch 48 with loss 1.857675552368164 and accuracy 0.222222238779068.\n",
      "Test batch 49 with loss 1.8235061168670654 and accuracy 0.1527777761220932.\n",
      "Test batch 50 with loss 1.8280338048934937 and accuracy 0.18611110746860504.\n",
      "Test batch 51 with loss 1.8646084070205688 and accuracy 0.2638888955116272.\n",
      "Test batch 52 with loss 1.7909536361694336 and accuracy 0.125.\n",
      "Test batch 53 with loss 1.758854866027832 and accuracy 0.2936508059501648.\n",
      "Test batch 54 with loss 1.8889687061309814 and accuracy 0.02777777798473835.\n",
      "Test batch 55 with loss 1.7832180261611938 and accuracy 0.12222222983837128.\n",
      "Test batch 56 with loss 1.7285833358764648 and accuracy 0.3888888955116272.\n",
      "Test batch 57 with loss 1.7748368978500366 and accuracy 0.557539701461792.\n",
      "Test batch 58 with loss 1.778681755065918 and accuracy 0.1388888955116272.\n",
      "Test batch 59 with loss 1.808272361755371 and accuracy 0.2611111104488373.\n",
      "Test batch 60 with loss 1.9066135883331299 and accuracy 0.0555555559694767.\n",
      "Test batch 61 with loss 1.8378292322158813 and accuracy 0.32499998807907104.\n",
      "Test batch 62 with loss 1.8172870874404907 and accuracy 0.2936508059501648.\n",
      "Test batch 63 with loss 1.8137340545654297 and accuracy 0.1666666716337204.\n",
      "Test batch 64 with loss 1.7902028560638428 and accuracy 0.17333334684371948.\n",
      "Test batch 65 with loss 1.765589714050293 and accuracy 0.2916666865348816.\n",
      "Test batch 66 with loss 1.858141303062439 and accuracy 0.1805555671453476.\n",
      "Test batch 67 with loss 1.7646353244781494 and accuracy 0.21666666865348816.\n",
      "Test batch 68 with loss 1.9223378896713257 and accuracy 0.0555555559694767.\n",
      "Test batch 69 with loss 1.7900081872940063 and accuracy 0.25.\n",
      "Test batch 70 with loss 1.850481629371643 and accuracy 0.11666667461395264.\n",
      "Test batch 71 with loss 1.7454710006713867 and accuracy 0.43611112236976624.\n",
      "Test batch 72 with loss 1.8855130672454834 and accuracy 0.03333333507180214.\n",
      "Test batch 73 with loss 1.801358938217163 and accuracy 0.1111111119389534.\n",
      "Test batch 74 with loss 1.9291083812713623 and accuracy 0.0833333358168602.\n",
      "Test batch 75 with loss 1.829831838607788 and accuracy 0.0833333358168602.\n",
      "Test batch 76 with loss 1.8411824703216553 and accuracy 0.0833333358168602.\n",
      "Test batch 77 with loss 1.8259351253509521 and accuracy 0.24166667461395264.\n",
      "Test batch 78 with loss 1.766964316368103 and accuracy 0.190476194024086.\n",
      "Test batch 79 with loss 1.822901964187622 and accuracy 0.17222222685813904.\n",
      "Test batch 80 with loss 1.9238088130950928 and accuracy 0.15555556118488312.\n",
      "Test batch 81 with loss 1.8090498447418213 and accuracy 0.15555556118488312.\n",
      "Test batch 82 with loss 1.7374684810638428 and accuracy 0.22777777910232544.\n",
      "Test batch 83 with loss 1.82634699344635 and accuracy 0.125.\n",
      "Test batch 84 with loss 1.8561947345733643 and accuracy 0.2083333432674408.\n",
      "Test batch 85 with loss 1.797873854637146 and accuracy 0.10972222685813904.\n",
      "Test batch 86 with loss 1.854966402053833 and accuracy 0.02777777798473835.\n",
      "Test batch 87 with loss 1.781966209411621 and accuracy 0.1111111119389534.\n",
      "Test batch 88 with loss 1.7239696979522705 and accuracy 0.2182539701461792.\n",
      "Test batch 89 with loss 1.7360645532608032 and accuracy 0.2888889014720917.\n",
      "Test batch 90 with loss 1.8362696170806885 and accuracy 0.2666666805744171.\n",
      "Test batch 91 with loss 1.8441007137298584 and accuracy 0.10833333432674408.\n",
      "Test batch 92 with loss 1.7804973125457764 and accuracy 0.125.\n",
      "Test batch 93 with loss 1.8928234577178955 and accuracy 0.0555555559694767.\n",
      "Test batch 94 with loss 1.812562346458435 and accuracy 0.0416666679084301.\n",
      "Test batch 95 with loss 1.873202919960022 and accuracy 0.02777777798473835.\n",
      "Test batch 96 with loss 1.809360146522522 and accuracy 0.2777777910232544.\n",
      "Test batch 97 with loss 1.8773800134658813 and accuracy 0.22777779400348663.\n",
      "Test batch 98 with loss 1.836400032043457 and accuracy 0.2777777910232544.\n",
      "Test batch 99 with loss 1.824009656906128 and accuracy 0.236111119389534.\n",
      "Test batch 100 with loss 1.9110009670257568 and accuracy 0.13055555522441864.\n",
      "Test batch 101 with loss 1.7409136295318604 and accuracy 0.30714285373687744.\n",
      "Test batch 102 with loss 1.883247971534729 and accuracy 0.17222222685813904.\n",
      "Test batch 103 with loss 1.8216679096221924 and accuracy 0.2750000059604645.\n",
      "Test batch 104 with loss 1.9172614812850952 and accuracy 0.1527777761220932.\n",
      "Test batch 105 with loss 1.8708711862564087 and accuracy 0.25555557012557983.\n",
      "Test batch 106 with loss 1.848587989807129 and accuracy 0.18611110746860504.\n",
      "Test batch 107 with loss 1.78338623046875 and accuracy 0.3194444477558136.\n",
      "Test batch 108 with loss 1.7810375690460205 and accuracy 0.0476190485060215.\n",
      "Test batch 109 with loss 1.7791589498519897 and accuracy 0.3115079402923584.\n",
      "Test batch 110 with loss 1.803603172302246 and accuracy 0.0833333358168602.\n",
      "Test batch 111 with loss 1.7963815927505493 and accuracy 0.2750000059604645.\n",
      "Test batch 112 with loss 1.7603614330291748 and accuracy 0.1666666716337204.\n",
      "Test batch 113 with loss 1.7011396884918213 and accuracy 0.2666666805744171.\n",
      "Test batch 114 with loss 1.8471721410751343 and accuracy 0.0833333358168602.\n",
      "Test batch 115 with loss 1.7780838012695312 and accuracy 0.0863095223903656.\n",
      "Test batch 116 with loss 1.8935258388519287 and accuracy 0.1527777761220932.\n",
      "Test batch 117 with loss 1.844482421875 and accuracy 0.1388888955116272.\n",
      "Test batch 118 with loss 1.8106954097747803 and accuracy 0.31111112236976624.\n",
      "Test batch 119 with loss 1.783045768737793 and accuracy 0.222222238779068.\n",
      "Test batch 120 with loss 1.830822229385376 and accuracy 0.3166666626930237.\n",
      "Test batch 121 with loss 1.853049874305725 and accuracy 0.0972222238779068.\n",
      "Test batch 122 with loss 1.8181174993515015 and accuracy 0.15000000596046448.\n",
      "Test batch 123 with loss 1.8586927652359009 and accuracy 0.15000000596046448.\n",
      "Test batch 124 with loss 1.875078558921814 and accuracy 0.0416666679084301.\n",
      "Test batch 125 with loss 1.8986459970474243 and accuracy 0.11666667461395264.\n",
      "Test batch 126 with loss 1.8444026708602905 and accuracy 0.347222238779068.\n",
      "Test batch 127 with loss 1.746694564819336 and accuracy 0.1388888955116272.\n",
      "Test batch 128 with loss 1.7799098491668701 and accuracy 0.1527777910232544.\n",
      "Test batch 129 with loss 1.9233434200286865 and accuracy 0.0416666679084301.\n",
      "Test batch 130 with loss 1.8200241327285767 and accuracy 0.10833333432674408.\n",
      "Test batch 131 with loss 1.8477022647857666 and accuracy 0.144841268658638.\n",
      "Test batch 132 with loss 1.8638849258422852 and accuracy 0.09444444626569748.\n",
      "Test batch 133 with loss 1.738342523574829 and accuracy 0.1666666716337204.\n",
      "Test batch 134 with loss 1.759148359298706 and accuracy 0.3027777671813965.\n",
      "Test batch 135 with loss 1.871000051498413 and accuracy 0.14444445073604584.\n",
      "Test batch 136 with loss 1.878152847290039 and accuracy 0.1111111119389534.\n",
      "Test batch 137 with loss 1.7735099792480469 and accuracy 0.25.\n",
      "Test batch 138 with loss 1.8022825717926025 and accuracy 0.13333334028720856.\n",
      "Test batch 139 with loss 1.8804371356964111 and accuracy 0.1666666716337204.\n",
      "Test batch 140 with loss 1.8646981716156006 and accuracy 0.0793650820851326.\n",
      "Test batch 141 with loss 1.8720121383666992 and accuracy 0.11666667461395264.\n",
      "Test batch 142 with loss 1.7811553478240967 and accuracy 0.10833333432674408.\n",
      "Test batch 143 with loss 1.8499720096588135 and accuracy 0.0833333358168602.\n",
      "Test batch 144 with loss 1.8114335536956787 and accuracy 0.17777778208255768.\n",
      "Test batch 145 with loss 1.8792937994003296 and accuracy 0.2738095223903656.\n",
      "Test batch 146 with loss 1.7654298543930054 and accuracy 0.1666666716337204.\n",
      "Test batch 147 with loss 1.9078859090805054 and accuracy 0.03333333507180214.\n",
      "Test batch 148 with loss 1.8023252487182617 and accuracy 0.14444445073604584.\n",
      "Test batch 149 with loss 1.9295707941055298 and accuracy 0.0.\n",
      "Test batch 150 with loss 1.7689874172210693 and accuracy 0.1805555671453476.\n",
      "Test batch 151 with loss 1.828289270401001 and accuracy 0.1111111119389534.\n",
      "Test batch 152 with loss 1.7832683324813843 and accuracy 0.11666667461395264.\n",
      "Test batch 153 with loss 1.7587525844573975 and accuracy 0.2777777910232544.\n",
      "Test batch 154 with loss 1.9466432332992554 and accuracy 0.0416666679084301.\n",
      "Test batch 155 with loss 1.8675382137298584 and accuracy 0.0833333358168602.\n",
      "Test batch 156 with loss 1.8093960285186768 and accuracy 0.1666666716337204.\n",
      "Test batch 157 with loss 1.8446626663208008 and accuracy 0.1666666716337204.\n",
      "Test batch 158 with loss 1.7731748819351196 and accuracy 0.222222238779068.\n",
      "Test batch 159 with loss 1.8233537673950195 and accuracy 0.125.\n",
      "Test batch 160 with loss 1.799471139907837 and accuracy 0.10833333432674408.\n",
      "Test batch 161 with loss 1.8792378902435303 and accuracy 0.0555555559694767.\n",
      "Test batch 162 with loss 1.7209126949310303 and accuracy 0.2777777910232544.\n",
      "Test batch 163 with loss 1.9321630001068115 and accuracy 0.0416666679084301.\n",
      "Test batch 164 with loss 1.736514687538147 and accuracy 0.1805555522441864.\n",
      "Test batch 165 with loss 1.9161885976791382 and accuracy 0.1111111119389534.\n",
      "Test batch 166 with loss 1.8014070987701416 and accuracy 0.1527777761220932.\n",
      "Test batch 167 with loss 1.835669755935669 and accuracy 0.35277777910232544.\n",
      "Test batch 168 with loss 1.7949802875518799 and accuracy 0.1875.\n",
      "Test batch 169 with loss 1.841436743736267 and accuracy 0.13055555522441864.\n",
      "Test batch 170 with loss 1.7922601699829102 and accuracy 0.12037037312984467.\n",
      "Test batch 171 with loss 1.7472320795059204 and accuracy 0.25.\n",
      "Test batch 172 with loss 1.844727873802185 and accuracy 0.1388888955116272.\n",
      "Test batch 173 with loss 1.8756773471832275 and accuracy 0.1111111119389534.\n",
      "Test batch 174 with loss 1.7254880666732788 and accuracy 0.21944445371627808.\n",
      "Test batch 175 with loss 1.8562612533569336 and accuracy 0.2013888955116272.\n",
      "Test batch 176 with loss 1.803532361984253 and accuracy 0.2083333432674408.\n",
      "Test batch 177 with loss 1.770412802696228 and accuracy 0.1666666716337204.\n",
      "Test batch 178 with loss 1.779226541519165 and accuracy 0.11269842088222504.\n",
      "Test batch 179 with loss 1.6673915386199951 and accuracy 0.222222238779068.\n",
      "Test batch 180 with loss 1.7878901958465576 and accuracy 0.1666666716337204.\n",
      "Test batch 181 with loss 1.883933663368225 and accuracy 0.13055555522441864.\n",
      "Test batch 182 with loss 1.8037080764770508 and accuracy 0.1805555671453476.\n",
      "Test batch 183 with loss 1.846899390220642 and accuracy 0.22500000894069672.\n",
      "Test batch 184 with loss 1.8319282531738281 and accuracy 0.25.\n",
      "Test batch 185 with loss 1.8378576040267944 and accuracy 0.14047619700431824.\n",
      "Test batch 186 with loss 1.7043311595916748 and accuracy 0.23333334922790527.\n",
      "Test batch 187 with loss 1.7804542779922485 and accuracy 0.12777778506278992.\n",
      "Test batch 188 with loss 1.8096338510513306 and accuracy 0.11428572237491608.\n",
      "Test batch 189 with loss 1.7545621395111084 and accuracy 0.2857142984867096.\n",
      "Test batch 190 with loss 1.9554872512817383 and accuracy 0.02380952425301075.\n",
      "Test batch 191 with loss 1.8669512271881104 and accuracy 0.1388888955116272.\n",
      "Test batch 192 with loss 1.752760887145996 and accuracy 0.1527777761220932.\n",
      "Test batch 193 with loss 1.9807201623916626 and accuracy 0.0.\n",
      "Test batch 194 with loss 1.7963588237762451 and accuracy 0.11269842088222504.\n",
      "Test batch 195 with loss 1.8859221935272217 and accuracy 0.1527777761220932.\n",
      "Test batch 196 with loss 1.8310855627059937 and accuracy 0.3055555820465088.\n",
      "Test batch 197 with loss 1.8105767965316772 and accuracy 0.0833333358168602.\n",
      "Test batch 198 with loss 1.8501253128051758 and accuracy 0.21547618508338928.\n",
      "Test batch 199 with loss 1.8293874263763428 and accuracy 0.23148149251937866.\n",
      "Test batch 200 with loss 1.863000512123108 and accuracy 0.11666667461395264.\n",
      "Test batch 201 with loss 1.854530692100525 and accuracy 0.11666667461395264.\n",
      "Test batch 202 with loss 1.863660454750061 and accuracy 0.03333333507180214.\n",
      "Test batch 203 with loss 1.8873142004013062 and accuracy 0.0833333358168602.\n",
      "Test batch 204 with loss 1.8632618188858032 and accuracy 0.1111111119389534.\n",
      "Test batch 205 with loss 1.8724431991577148 and accuracy 0.17685185372829437.\n",
      "Test batch 206 with loss 1.9654443264007568 and accuracy 0.0.\n",
      "Test batch 207 with loss 1.854459524154663 and accuracy 0.2430555522441864.\n",
      "Test batch 208 with loss 1.803322434425354 and accuracy 0.2986111342906952.\n",
      "Test batch 209 with loss 1.7364475727081299 and accuracy 0.15555556118488312.\n",
      "Test batch 210 with loss 1.8051702976226807 and accuracy 0.2666666805744171.\n",
      "Test batch 211 with loss 1.7770659923553467 and accuracy 0.1388888955116272.\n",
      "Test batch 212 with loss 1.7771759033203125 and accuracy 0.09583333134651184.\n",
      "Test batch 213 with loss 1.8072559833526611 and accuracy 0.1388888955116272.\n",
      "Test batch 214 with loss 1.7721846103668213 and accuracy 0.25.\n",
      "Test batch 215 with loss 1.790006399154663 and accuracy 0.1111111119389534.\n",
      "Test batch 216 with loss 1.8105701208114624 and accuracy 0.125.\n",
      "Test batch 217 with loss 1.7485519647598267 and accuracy 0.190476194024086.\n",
      "Test batch 218 with loss 1.8551361560821533 and accuracy 0.14444445073604584.\n",
      "Test batch 219 with loss 1.9284722805023193 and accuracy 0.0416666679084301.\n",
      "Test batch 220 with loss 1.8260619640350342 and accuracy 0.2976190447807312.\n",
      "Test batch 221 with loss 1.8775455951690674 and accuracy 0.07500000298023224.\n",
      "Test batch 222 with loss 1.867957353591919 and accuracy 0.2638888955116272.\n",
      "Test batch 223 with loss 1.7684123516082764 and accuracy 0.111111119389534.\n",
      "Test batch 224 with loss 1.8076426982879639 and accuracy 0.1388888955116272.\n",
      "Test batch 225 with loss 1.752341866493225 and accuracy 0.25.\n",
      "Test batch 226 with loss 1.9178531169891357 and accuracy 0.2222222238779068.\n",
      "Test batch 227 with loss 1.8419740200042725 and accuracy 0.10000000149011612.\n",
      "Test batch 228 with loss 1.949044942855835 and accuracy 0.1805555522441864.\n",
      "Test batch 229 with loss 1.9009339809417725 and accuracy 0.0416666679084301.\n",
      "Test batch 230 with loss 1.8180259466171265 and accuracy 0.2638888955116272.\n",
      "Test batch 231 with loss 1.856097936630249 and accuracy 0.11666667461395264.\n",
      "Test batch 232 with loss 1.939846396446228 and accuracy 0.12222222983837128.\n",
      "Test batch 233 with loss 1.8440256118774414 and accuracy 0.1805555522441864.\n",
      "Test batch 234 with loss 1.857253074645996 and accuracy 0.13055555522441864.\n",
      "Test batch 235 with loss 1.80653977394104 and accuracy 0.12222222983837128.\n",
      "Test batch 236 with loss 1.8172191381454468 and accuracy 0.1388888955116272.\n",
      "Test batch 237 with loss 1.7619205713272095 and accuracy 0.2361111044883728.\n",
      "Test batch 238 with loss 1.7889652252197266 and accuracy 0.1944444477558136.\n",
      "Test batch 239 with loss 1.6585273742675781 and accuracy 0.4333333373069763.\n",
      "Test batch 240 with loss 1.8450380563735962 and accuracy 0.0476190485060215.\n",
      "Test batch 241 with loss 1.929221749305725 and accuracy 0.0972222238779068.\n",
      "Test batch 242 with loss 1.7344214916229248 and accuracy 0.2142857313156128.\n",
      "Test batch 243 with loss 1.8749881982803345 and accuracy 0.17777778208255768.\n",
      "Test batch 244 with loss 1.706260323524475 and accuracy 0.21388889849185944.\n",
      "Test batch 245 with loss 1.6880985498428345 and accuracy 0.3333333432674408.\n",
      "Test batch 246 with loss 1.700864553451538 and accuracy 0.23055556416511536.\n",
      "Test batch 247 with loss 1.873703956604004 and accuracy 0.1944444477558136.\n",
      "Test batch 248 with loss 1.802209496498108 and accuracy 0.2777777910232544.\n",
      "Test batch 249 with loss 1.8779376745224 and accuracy 0.08888889104127884.\n",
      "Test batch 250 with loss 1.7318769693374634 and accuracy 0.2222222238779068.\n",
      "Test batch 251 with loss 1.8252098560333252 and accuracy 0.125.\n",
      "Test batch 252 with loss 1.918932318687439 and accuracy 0.06666667014360428.\n",
      "Test batch 253 with loss 1.8261207342147827 and accuracy 0.12222222983837128.\n",
      "Test batch 254 with loss 1.796757698059082 and accuracy 0.25.\n",
      "Test batch 255 with loss 1.7536112070083618 and accuracy 0.23333333432674408.\n",
      "Test batch 256 with loss 1.7899329662322998 and accuracy 0.39444443583488464.\n",
      "Test batch 257 with loss 1.738502860069275 and accuracy 0.1805555522441864.\n",
      "Test batch 258 with loss 1.9089256525039673 and accuracy 0.0694444477558136.\n",
      "Test batch 259 with loss 1.8879159688949585 and accuracy 0.11666667461395264.\n",
      "Test batch 260 with loss 1.8323787450790405 and accuracy 0.05714286118745804.\n",
      "Test batch 261 with loss 1.7664562463760376 and accuracy 0.3361111283302307.\n",
      "Test batch 262 with loss 1.7623569965362549 and accuracy 0.07500000298023224.\n",
      "Test batch 263 with loss 1.775207281112671 and accuracy 0.2083333432674408.\n",
      "Test batch 264 with loss 1.8191713094711304 and accuracy 0.15833333134651184.\n",
      "Test batch 265 with loss 1.8488311767578125 and accuracy 0.15000000596046448.\n",
      "Test batch 266 with loss 1.9038299322128296 and accuracy 0.3055555522441864.\n",
      "Test batch 267 with loss 1.8821007013320923 and accuracy 0.17222222685813904.\n",
      "Test batch 268 with loss 1.7681061029434204 and accuracy 0.17500001192092896.\n",
      "Test batch 269 with loss 1.789870023727417 and accuracy 0.0833333358168602.\n",
      "Test batch 270 with loss 1.7356252670288086 and accuracy 0.1388888955116272.\n",
      "Test batch 271 with loss 1.8572511672973633 and accuracy 0.1666666716337204.\n",
      "Test batch 272 with loss 1.856264352798462 and accuracy 0.0416666679084301.\n",
      "Test batch 273 with loss 1.7979618310928345 and accuracy 0.1031746044754982.\n",
      "Test batch 274 with loss 1.876442551612854 and accuracy 0.19722223281860352.\n",
      "Test batch 275 with loss 1.7767696380615234 and accuracy 0.07500000298023224.\n",
      "Test batch 276 with loss 1.8367435932159424 and accuracy 0.0833333358168602.\n",
      "Test batch 277 with loss 1.8309669494628906 and accuracy 0.1736111044883728.\n",
      "Test batch 278 with loss 1.8534395694732666 and accuracy 0.1388888955116272.\n",
      "Test batch 279 with loss 1.7900140285491943 and accuracy 0.24166667461395264.\n",
      "Test batch 280 with loss 1.8540222644805908 and accuracy 0.1388888955116272.\n",
      "Test batch 281 with loss 1.7842988967895508 and accuracy 0.12222222983837128.\n",
      "Test batch 282 with loss 1.7467041015625 and accuracy 0.25833332538604736.\n",
      "Test batch 283 with loss 1.8927783966064453 and accuracy 0.0.\n",
      "Test batch 284 with loss 1.7720390558242798 and accuracy 0.26944446563720703.\n",
      "Test batch 285 with loss 1.8571994304656982 and accuracy 0.2750000059604645.\n",
      "Test batch 286 with loss 1.8048326969146729 and accuracy 0.06666667014360428.\n",
      "Test batch 287 with loss 1.8696048259735107 and accuracy 0.15000000596046448.\n",
      "Test batch 288 with loss 1.8533735275268555 and accuracy 0.0694444477558136.\n",
      "Test batch 289 with loss 1.7924392223358154 and accuracy 0.10277777910232544.\n",
      "Test batch 290 with loss 1.8246803283691406 and accuracy 0.125.\n",
      "Test batch 291 with loss 1.7956459522247314 and accuracy 0.0694444477558136.\n",
      "Test batch 292 with loss 1.8418201208114624 and accuracy 0.13523809611797333.\n",
      "Test batch 293 with loss 1.8867788314819336 and accuracy 0.0.\n",
      "Test batch 294 with loss 1.8057674169540405 and accuracy 0.2698412835597992.\n",
      "Test batch 295 with loss 1.7742866277694702 and accuracy 0.0555555559694767.\n",
      "Test batch 296 with loss 1.7981491088867188 and accuracy 0.2988095283508301.\n",
      "Test batch 297 with loss 1.84677255153656 and accuracy 0.12962962687015533.\n",
      "Test batch 298 with loss 1.800919532775879 and accuracy 0.1666666716337204.\n",
      "Test batch 299 with loss 1.8262332677841187 and accuracy 0.0972222238779068.\n",
      "Test batch 300 with loss 1.8925946950912476 and accuracy 0.09444445371627808.\n",
      "Test batch 301 with loss 1.9369064569473267 and accuracy 0.03333333507180214.\n",
      "Test batch 302 with loss 1.8913640975952148 and accuracy 0.0.\n",
      "Test batch 303 with loss 1.804078459739685 and accuracy 0.15833334624767303.\n",
      "Test batch 304 with loss 1.7607330083847046 and accuracy 0.23888888955116272.\n",
      "Test batch 305 with loss 1.8342115879058838 and accuracy 0.03333333507180214.\n",
      "Test batch 306 with loss 1.7976804971694946 and accuracy 0.1805555522441864.\n",
      "Test batch 307 with loss 1.8730919361114502 and accuracy 0.1388888955116272.\n",
      "Test batch 308 with loss 1.8775428533554077 and accuracy 0.02777777798473835.\n",
      "Test batch 309 with loss 1.8006795644760132 and accuracy 0.1388888955116272.\n",
      "Test batch 310 with loss 1.865593671798706 and accuracy 0.1071428582072258.\n",
      "Test batch 311 with loss 1.8258838653564453 and accuracy 0.2222222238779068.\n",
      "Test batch 312 with loss 1.9276635646820068 and accuracy 0.11666667461395264.\n",
      "Test batch 313 with loss 1.8580694198608398 and accuracy 0.17222222685813904.\n",
      "Test batch 314 with loss 1.7953526973724365 and accuracy 0.2611111104488373.\n",
      "Test batch 315 with loss 1.740408182144165 and accuracy 0.1805555522441864.\n",
      "Test batch 316 with loss 1.8787171840667725 and accuracy 0.0972222238779068.\n",
      "Test batch 317 with loss 1.838468313217163 and accuracy 0.0694444477558136.\n",
      "Test batch 318 with loss 1.8578382730484009 and accuracy 0.190476194024086.\n",
      "Test batch 319 with loss 1.8389780521392822 and accuracy 0.1805555522441864.\n",
      "Test batch 320 with loss 1.7865015268325806 and accuracy 0.1388888955116272.\n",
      "Test batch 321 with loss 1.8156483173370361 and accuracy 0.11666667461395264.\n",
      "Test batch 322 with loss 1.7818067073822021 and accuracy 0.3365079462528229.\n",
      "Test batch 323 with loss 1.8467919826507568 and accuracy 0.03333333507180214.\n",
      "Test batch 324 with loss 1.8033186197280884 and accuracy 0.3432539701461792.\n",
      "Test batch 325 with loss 1.8892492055892944 and accuracy 0.03333333507180214.\n",
      "Test batch 326 with loss 1.8863967657089233 and accuracy 0.06666667014360428.\n",
      "Test batch 327 with loss 1.8761355876922607 and accuracy 0.03333333507180214.\n",
      "Test batch 328 with loss 1.854993224143982 and accuracy 0.06666667014360428.\n",
      "Test batch 329 with loss 1.8110231161117554 and accuracy 0.25.\n",
      "Test batch 330 with loss 1.8126319646835327 and accuracy 0.1805555522441864.\n",
      "Test batch 331 with loss 1.990828514099121 and accuracy 0.0.\n",
      "Test batch 332 with loss 1.8314435482025146 and accuracy 0.2777777910232544.\n",
      "Test batch 333 with loss 1.8858044147491455 and accuracy 0.0972222238779068.\n",
      "Test batch 334 with loss 1.8695094585418701 and accuracy 0.0833333358168602.\n",
      "Test batch 335 with loss 1.7945226430892944 and accuracy 0.1041666716337204.\n",
      "Test batch 336 with loss 1.8792781829833984 and accuracy 0.0.\n",
      "Test batch 337 with loss 1.8896839618682861 and accuracy 0.12222222983837128.\n",
      "Test batch 338 with loss 1.8842607736587524 and accuracy 0.125.\n",
      "Test batch 339 with loss 1.8960422277450562 and accuracy 0.02777777798473835.\n",
      "Test batch 340 with loss 1.9227209091186523 and accuracy 0.0555555559694767.\n",
      "Test batch 341 with loss 1.8784573078155518 and accuracy 0.1666666716337204.\n",
      "Test batch 342 with loss 1.7810564041137695 and accuracy 0.11666667461395264.\n",
      "Test batch 343 with loss 1.7966887950897217 and accuracy 0.0555555559694767.\n",
      "Test batch 344 with loss 1.8717819452285767 and accuracy 0.2638888955116272.\n",
      "Test batch 345 with loss 1.7916589975357056 and accuracy 0.25.\n",
      "Test batch 346 with loss 1.8300378322601318 and accuracy 0.18611110746860504.\n",
      "Test batch 347 with loss 1.904465675354004 and accuracy 0.125.\n",
      "Test batch 348 with loss 1.8432722091674805 and accuracy 0.1111111119389534.\n",
      "Test batch 349 with loss 1.7569034099578857 and accuracy 0.2638888955116272.\n",
      "Test batch 350 with loss 1.7892011404037476 and accuracy 0.1944444477558136.\n",
      "Test batch 351 with loss 1.7296794652938843 and accuracy 0.3777777850627899.\n",
      "Test batch 352 with loss 1.8253694772720337 and accuracy 0.1527777761220932.\n",
      "Test batch 353 with loss 1.7830345630645752 and accuracy 0.2083333432674408.\n",
      "Test batch 354 with loss 1.827492117881775 and accuracy 0.20000000298023224.\n",
      "Test batch 355 with loss 1.7600269317626953 and accuracy 0.17777778208255768.\n",
      "Test batch 356 with loss 1.7440494298934937 and accuracy 0.17222222685813904.\n",
      "Test batch 357 with loss 1.8487685918807983 and accuracy 0.1527777761220932.\n",
      "Test batch 358 with loss 1.7097063064575195 and accuracy 0.3333333432674408.\n",
      "Test batch 359 with loss 1.7583831548690796 and accuracy 0.2666666805744171.\n",
      "Test batch 360 with loss 1.8253633975982666 and accuracy 0.03333333507180214.\n",
      "Test batch 361 with loss 1.9090001583099365 and accuracy 0.1111111119389534.\n",
      "Test batch 362 with loss 1.8504765033721924 and accuracy 0.08888889104127884.\n",
      "Test batch 363 with loss 1.7880500555038452 and accuracy 0.25.\n",
      "Test batch 364 with loss 1.8128067255020142 and accuracy 0.23333334922790527.\n",
      "Test batch 365 with loss 1.867990493774414 and accuracy 0.0515873022377491.\n",
      "Test batch 366 with loss 1.8189527988433838 and accuracy 0.08888889104127884.\n",
      "Test batch 367 with loss 1.7099378108978271 and accuracy 0.17222222685813904.\n",
      "Test batch 368 with loss 1.9560855627059937 and accuracy 0.08888889104127884.\n",
      "Test batch 369 with loss 1.8530290126800537 and accuracy 0.20000001788139343.\n",
      "Test batch 370 with loss 1.9153258800506592 and accuracy 0.18333333730697632.\n",
      "Test batch 371 with loss 1.7522027492523193 and accuracy 0.2738095223903656.\n",
      "Test batch 372 with loss 1.7917006015777588 and accuracy 0.0972222238779068.\n",
      "Test batch 373 with loss 1.7343683242797852 and accuracy 0.1666666716337204.\n",
      "Test batch 374 with loss 1.7856298685073853 and accuracy 0.5.\n",
      "Test batch 375 with loss 1.715595006942749 and accuracy 0.2916666567325592.\n",
      "Test batch 376 with loss 1.8670990467071533 and accuracy 0.11666667461395264.\n",
      "Test batch 377 with loss 1.8378270864486694 and accuracy 0.1111111119389534.\n",
      "Test batch 378 with loss 1.9227126836776733 and accuracy 0.02777777798473835.\n",
      "Test batch 379 with loss 1.9051055908203125 and accuracy 0.0.\n",
      "Test batch 380 with loss 1.7365859746932983 and accuracy 0.2777777910232544.\n",
      "Test batch 381 with loss 1.771954894065857 and accuracy 0.25555557012557983.\n",
      "Test batch 382 with loss 1.8480236530303955 and accuracy 0.0555555559694767.\n",
      "Test batch 383 with loss 1.762215256690979 and accuracy 0.15000000596046448.\n",
      "Test batch 384 with loss 1.7903382778167725 and accuracy 0.0972222238779068.\n",
      "Test batch 385 with loss 1.9556200504302979 and accuracy 0.2083333432674408.\n",
      "Test batch 386 with loss 1.848114013671875 and accuracy 0.0555555559694767.\n",
      "Test batch 387 with loss 1.8230860233306885 and accuracy 0.1805555522441864.\n",
      "Test batch 388 with loss 1.7742948532104492 and accuracy 0.22777777910232544.\n",
      "Test batch 389 with loss 1.8403046131134033 and accuracy 0.1388888955116272.\n",
      "Test batch 390 with loss 1.816305160522461 and accuracy 0.02777777798473835.\n",
      "Test batch 391 with loss 1.788029670715332 and accuracy 0.34166666865348816.\n",
      "Test batch 392 with loss 1.847489356994629 and accuracy 0.14444445073604584.\n",
      "Test batch 393 with loss 1.828688621520996 and accuracy 0.0.\n",
      "Test batch 394 with loss 1.7705904245376587 and accuracy 0.1388888955116272.\n",
      "Test batch 395 with loss 1.738431692123413 and accuracy 0.17222222685813904.\n",
      "Test batch 396 with loss 1.7773765325546265 and accuracy 0.1388888955116272.\n",
      "Test batch 397 with loss 1.8894474506378174 and accuracy 0.0416666679084301.\n",
      "Test batch 398 with loss 1.8316211700439453 and accuracy 0.2500000298023224.\n",
      "Test batch 399 with loss 1.7852990627288818 and accuracy 0.1944444477558136.\n",
      "Test batch 400 with loss 1.8032381534576416 and accuracy 0.125.\n",
      "Test batch 401 with loss 1.7391189336776733 and accuracy 0.10000000894069672.\n",
      "Test batch 402 with loss 1.7593271732330322 and accuracy 0.3194444477558136.\n",
      "Test batch 403 with loss 1.9283769130706787 and accuracy 0.065476194024086.\n",
      "Test batch 404 with loss 1.8178246021270752 and accuracy 0.1388888955116272.\n",
      "Test batch 405 with loss 1.8531204462051392 and accuracy 0.11666667461395264.\n",
      "Test batch 406 with loss 1.8352960348129272 and accuracy 0.1805555522441864.\n",
      "Test batch 407 with loss 1.7654714584350586 and accuracy 0.21944445371627808.\n",
      "Test batch 408 with loss 1.7196722030639648 and accuracy 0.41428571939468384.\n",
      "Test batch 409 with loss 1.9215679168701172 and accuracy 0.0555555559694767.\n",
      "Test batch 410 with loss 1.9248958826065063 and accuracy 0.0416666679084301.\n",
      "Test batch 411 with loss 1.8935827016830444 and accuracy 0.1111111119389534.\n",
      "Test batch 412 with loss 1.753578543663025 and accuracy 0.12380952388048172.\n",
      "Test batch 413 with loss 1.7875478267669678 and accuracy 0.625.\n",
      "Test batch 414 with loss 1.8057969808578491 and accuracy 0.1805555522441864.\n",
      "Test batch 415 with loss 1.9084930419921875 and accuracy 0.0694444477558136.\n",
      "Test batch 416 with loss 1.8354610204696655 and accuracy 0.0416666679084301.\n",
      "Test batch 417 with loss 1.9039443731307983 and accuracy 0.1388888955116272.\n",
      "Test batch 418 with loss 1.857194185256958 and accuracy 0.2777777910232544.\n",
      "Test batch 419 with loss 1.8701622486114502 and accuracy 0.14444445073604584.\n",
      "Test batch 420 with loss 1.8261276483535767 and accuracy 0.15000000596046448.\n",
      "Test batch 421 with loss 1.788560152053833 and accuracy 0.3055555522441864.\n",
      "Test batch 422 with loss 1.7836834192276 and accuracy 0.2638888955116272.\n",
      "Test batch 423 with loss 1.7960851192474365 and accuracy 0.2083333432674408.\n",
      "Test batch 424 with loss 1.8031307458877563 and accuracy 0.02380952425301075.\n",
      "Test batch 425 with loss 1.843764305114746 and accuracy 0.03333333507180214.\n",
      "Test batch 426 with loss 1.8658339977264404 and accuracy 0.125.\n",
      "Test batch 427 with loss 1.7453216314315796 and accuracy 0.35277777910232544.\n",
      "Test batch 428 with loss 1.7450830936431885 and accuracy 0.17222222685813904.\n",
      "Test batch 429 with loss 1.8651301860809326 and accuracy 0.07500000298023224.\n",
      "Test batch 430 with loss 1.7423423528671265 and accuracy 0.2083333432674408.\n",
      "Test batch 431 with loss 1.8718372583389282 and accuracy 0.02380952425301075.\n",
      "Test batch 432 with loss 1.8438276052474976 and accuracy 0.15000000596046448.\n",
      "Test batch 433 with loss 1.8271987438201904 and accuracy 0.16984127461910248.\n",
      "Test batch 434 with loss 1.7583290338516235 and accuracy 0.12222222983837128.\n",
      "Test batch 435 with loss 1.8163995742797852 and accuracy 0.2777777910232544.\n",
      "Test batch 436 with loss 1.9015569686889648 and accuracy 0.03333333507180214.\n",
      "Test batch 437 with loss 1.8044378757476807 and accuracy 0.0694444477558136.\n",
      "Test batch 438 with loss 1.8625940084457397 and accuracy 0.0416666679084301.\n",
      "Test batch 439 with loss 1.7055222988128662 and accuracy 0.4861111044883728.\n",
      "Test batch 440 with loss 1.7760874032974243 and accuracy 0.2777777910232544.\n",
      "Test batch 441 with loss 1.8055999279022217 and accuracy 0.08888889104127884.\n",
      "Test batch 442 with loss 1.857863426208496 and accuracy 0.125.\n",
      "Test batch 443 with loss 1.8563839197158813 and accuracy 0.02777777798473835.\n",
      "Test batch 444 with loss 1.8859249353408813 and accuracy 0.0416666679084301.\n",
      "Test batch 445 with loss 1.8427095413208008 and accuracy 0.0694444477558136.\n",
      "Test batch 446 with loss 1.7898023128509521 and accuracy 0.12222222983837128.\n",
      "Test batch 447 with loss 1.7930418252944946 and accuracy 0.15000000596046448.\n",
      "Test batch 448 with loss 1.7615312337875366 and accuracy 0.2638888955116272.\n",
      "Test batch 449 with loss 1.729414701461792 and accuracy 0.31111112236976624.\n",
      "Test batch 450 with loss 1.8156731128692627 and accuracy 0.2777777910232544.\n",
      "Test batch 451 with loss 1.8225488662719727 and accuracy 0.06111111491918564.\n",
      "Test batch 452 with loss 1.9231929779052734 and accuracy 0.02380952425301075.\n",
      "Test batch 453 with loss 1.7755037546157837 and accuracy 0.20555555820465088.\n",
      "Test batch 454 with loss 1.8878854513168335 and accuracy 0.0416666679084301.\n",
      "Test batch 455 with loss 1.8531224727630615 and accuracy 0.0416666679084301.\n",
      "Test batch 456 with loss 1.8701728582382202 and accuracy 0.1805555522441864.\n",
      "Test batch 457 with loss 1.822461724281311 and accuracy 0.1666666716337204.\n",
      "Test batch 458 with loss 1.8639453649520874 and accuracy 0.3222222328186035.\n",
      "Test batch 459 with loss 1.745888352394104 and accuracy 0.222222238779068.\n",
      "Test batch 460 with loss 1.8170902729034424 and accuracy 0.11666667461395264.\n",
      "Test batch 461 with loss 1.919976830482483 and accuracy 0.0.\n",
      "Test batch 462 with loss 1.8576291799545288 and accuracy 0.0833333358168602.\n",
      "Test batch 463 with loss 1.8445665836334229 and accuracy 0.0833333358168602.\n",
      "Test batch 464 with loss 1.854020118713379 and accuracy 0.0694444477558136.\n",
      "Test batch 465 with loss 1.8352501392364502 and accuracy 0.14444445073604584.\n",
      "Test batch 466 with loss 1.7544193267822266 and accuracy 0.2916666865348816.\n",
      "Test batch 467 with loss 1.7244354486465454 and accuracy 0.2083333432674408.\n",
      "Test batch 468 with loss 1.8169724941253662 and accuracy 0.18333333730697632.\n",
      "Test batch 469 with loss 1.8046066761016846 and accuracy 0.14444445073604584.\n",
      "Test batch 470 with loss 1.7551214694976807 and accuracy 0.2777777910232544.\n",
      "Test batch 471 with loss 1.8897058963775635 and accuracy 0.2083333432674408.\n",
      "Test batch 472 with loss 1.8401069641113281 and accuracy 0.1527777761220932.\n",
      "Test batch 473 with loss 1.8590564727783203 and accuracy 0.2142857164144516.\n",
      "Test batch 474 with loss 1.8525844812393188 and accuracy 0.03333333507180214.\n",
      "Test batch 475 with loss 1.7898142337799072 and accuracy 0.1388888955116272.\n",
      "Test batch 476 with loss 1.7661058902740479 and accuracy 0.12777778506278992.\n",
      "Test batch 477 with loss 1.9112815856933594 and accuracy 0.1388888955116272.\n",
      "Test batch 478 with loss 1.8568521738052368 and accuracy 0.13055555522441864.\n",
      "Test batch 479 with loss 1.8192030191421509 and accuracy 0.25.\n",
      "Test batch 480 with loss 1.8452415466308594 and accuracy 0.0972222238779068.\n",
      "Test batch 481 with loss 1.833670973777771 and accuracy 0.0416666679084301.\n",
      "Test batch 482 with loss 1.7847833633422852 and accuracy 0.10277777910232544.\n",
      "Test batch 483 with loss 1.7185519933700562 and accuracy 0.17500001192092896.\n",
      "Test batch 484 with loss 1.8538157939910889 and accuracy 0.0.\n",
      "Test batch 485 with loss 1.8314424753189087 and accuracy 0.18333333730697632.\n",
      "Test batch 486 with loss 1.8754924535751343 and accuracy 0.1527777761220932.\n",
      "Test batch 487 with loss 1.7707622051239014 and accuracy 0.42222222685813904.\n",
      "Test batch 488 with loss 1.854374885559082 and accuracy 0.2638888955116272.\n",
      "Test batch 489 with loss 1.762195348739624 and accuracy 0.22976191341876984.\n",
      "Test batch 490 with loss 1.7932865619659424 and accuracy 0.10833333432674408.\n",
      "Test batch 491 with loss 1.8743619918823242 and accuracy 0.2916666865348816.\n",
      "Test batch 492 with loss 1.753454566001892 and accuracy 0.25.\n",
      "Test batch 493 with loss 1.8365894556045532 and accuracy 0.23333333432674408.\n",
      "Test batch 494 with loss 1.8382728099822998 and accuracy 0.07500000298023224.\n",
      "Test batch 495 with loss 1.8773250579833984 and accuracy 0.13055555522441864.\n",
      "Test batch 496 with loss 1.8109066486358643 and accuracy 0.12222222983837128.\n",
      "Test batch 497 with loss 1.8133989572525024 and accuracy 0.1805555522441864.\n",
      "Test batch 498 with loss 1.8267123699188232 and accuracy 0.06111111491918564.\n",
      "Test batch 499 with loss 1.849784255027771 and accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 0 with loss 1.82546865940094, accuracy 0.125.\n",
      "Training epoch 3 batch 1 with loss 1.6948144435882568, accuracy 0.31111112236976624.\n",
      "Training epoch 3 batch 2 with loss 1.850937843322754, accuracy 0.2460317611694336.\n",
      "Training epoch 3 batch 3 with loss 1.8639848232269287, accuracy 0.06111111491918564.\n",
      "Training epoch 3 batch 4 with loss 1.760838508605957, accuracy 0.36666667461395264.\n",
      "Training epoch 3 batch 5 with loss 1.7650705575942993, accuracy 0.2361111044883728.\n",
      "Training epoch 3 batch 6 with loss 1.8204342126846313, accuracy 0.28333333134651184.\n",
      "Training epoch 3 batch 7 with loss 1.8178443908691406, accuracy 0.125.\n",
      "Training epoch 3 batch 8 with loss 1.7824627161026, accuracy 0.190476194024086.\n",
      "Training epoch 3 batch 9 with loss 1.8074089288711548, accuracy 0.09259259700775146.\n",
      "Training epoch 3 batch 10 with loss 1.7126137018203735, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 11 with loss 1.7672388553619385, accuracy 0.1736111044883728.\n",
      "Training epoch 3 batch 12 with loss 1.7524306774139404, accuracy 0.190476194024086.\n",
      "Training epoch 3 batch 13 with loss 1.7360185384750366, accuracy 0.17777778208255768.\n",
      "Training epoch 3 batch 14 with loss 1.734122633934021, accuracy 0.3055555522441864.\n",
      "Training epoch 3 batch 15 with loss 1.817736268043518, accuracy 0.1805555671453476.\n",
      "Training epoch 3 batch 16 with loss 1.7325618267059326, accuracy 0.2361111044883728.\n",
      "Training epoch 3 batch 17 with loss 1.7088314294815063, accuracy 0.2805555462837219.\n",
      "Training epoch 3 batch 18 with loss 1.9227685928344727, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 19 with loss 1.8405859470367432, accuracy 0.10833333432674408.\n",
      "Training epoch 3 batch 20 with loss 1.7181450128555298, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 21 with loss 1.6880037784576416, accuracy 0.3583333194255829.\n",
      "Training epoch 3 batch 22 with loss 1.8046337366104126, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 23 with loss 1.7992284297943115, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 24 with loss 1.7584915161132812, accuracy 0.2083333283662796.\n",
      "Training epoch 3 batch 25 with loss 1.7298429012298584, accuracy 0.19166666269302368.\n",
      "Training epoch 3 batch 26 with loss 1.841270089149475, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 27 with loss 1.6668617725372314, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 28 with loss 1.7326953411102295, accuracy 0.2638888955116272.\n",
      "Training epoch 3 batch 29 with loss 1.7107206583023071, accuracy 0.23055556416511536.\n",
      "Training epoch 3 batch 30 with loss 1.7861722707748413, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 31 with loss 1.8350623846054077, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 32 with loss 1.7593698501586914, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 33 with loss 1.8003199100494385, accuracy 0.25.\n",
      "Training epoch 3 batch 34 with loss 1.8165134191513062, accuracy 0.16825397312641144.\n",
      "Training epoch 3 batch 35 with loss 1.7689701318740845, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 36 with loss 1.7453477382659912, accuracy 0.3603174686431885.\n",
      "Training epoch 3 batch 37 with loss 1.7935279607772827, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 38 with loss 1.7946182489395142, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 39 with loss 1.684278130531311, accuracy 0.4236111342906952.\n",
      "Training epoch 3 batch 40 with loss 1.863258719444275, accuracy 0.3154761791229248.\n",
      "Training epoch 3 batch 41 with loss 1.7947113513946533, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 42 with loss 1.7519299983978271, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 43 with loss 1.707068681716919, accuracy 0.2666666805744171.\n",
      "Training epoch 3 batch 44 with loss 1.7452316284179688, accuracy 0.1547619104385376.\n",
      "Training epoch 3 batch 45 with loss 1.7888267040252686, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 46 with loss 1.8287975788116455, accuracy 0.2638888955116272.\n",
      "Training epoch 3 batch 47 with loss 1.8330243825912476, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 48 with loss 1.825035810470581, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 49 with loss 1.7683480978012085, accuracy 0.19166666269302368.\n",
      "Training epoch 3 batch 50 with loss 1.8028026819229126, accuracy 0.2111111283302307.\n",
      "Training epoch 3 batch 51 with loss 1.7769664525985718, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 52 with loss 1.7628129720687866, accuracy 0.2876984179019928.\n",
      "Training epoch 3 batch 53 with loss 1.7367784976959229, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 54 with loss 1.7481307983398438, accuracy 0.21388888359069824.\n",
      "Training epoch 3 batch 55 with loss 1.812965750694275, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 56 with loss 1.8330949544906616, accuracy 0.21388889849185944.\n",
      "Training epoch 3 batch 57 with loss 1.6948940753936768, accuracy 0.3500000238418579.\n",
      "Training epoch 3 batch 58 with loss 1.826442003250122, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 59 with loss 1.7542321681976318, accuracy 0.3611111342906952.\n",
      "Training epoch 3 batch 60 with loss 1.7011287212371826, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 61 with loss 1.7272837162017822, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 62 with loss 1.777247667312622, accuracy 0.21388888359069824.\n",
      "Training epoch 3 batch 63 with loss 1.6834309101104736, accuracy 0.4000000059604645.\n",
      "Training epoch 3 batch 64 with loss 1.7699291706085205, accuracy 0.24166667461395264.\n",
      "Training epoch 3 batch 65 with loss 1.7251081466674805, accuracy 0.4055555760860443.\n",
      "Training epoch 3 batch 66 with loss 1.886849045753479, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 67 with loss 1.822880506515503, accuracy 0.21388888359069824.\n",
      "Training epoch 3 batch 68 with loss 1.7446658611297607, accuracy 0.0694444477558136.\n",
      "Training epoch 3 batch 69 with loss 1.859662413597107, accuracy 0.03333333507180214.\n",
      "Training epoch 3 batch 70 with loss 1.6993459463119507, accuracy 0.36666667461395264.\n",
      "Training epoch 3 batch 71 with loss 1.777462363243103, accuracy 0.25.\n",
      "Training epoch 3 batch 72 with loss 1.6877787113189697, accuracy 0.222222238779068.\n",
      "Training epoch 3 batch 73 with loss 1.7488855123519897, accuracy 0.08095238357782364.\n",
      "Training epoch 3 batch 74 with loss 1.7314167022705078, accuracy 0.3948412835597992.\n",
      "Training epoch 3 batch 75 with loss 1.7451820373535156, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 76 with loss 1.808624029159546, accuracy 0.09259259700775146.\n",
      "Training epoch 3 batch 77 with loss 1.7610124349594116, accuracy 0.02777777798473835.\n",
      "Training epoch 3 batch 78 with loss 1.76571524143219, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 79 with loss 1.7466684579849243, accuracy 0.2750000059604645.\n",
      "Training epoch 3 batch 80 with loss 1.7109791040420532, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 81 with loss 1.7754024267196655, accuracy 0.4888889193534851.\n",
      "Training epoch 3 batch 82 with loss 1.770276665687561, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 83 with loss 1.7846797704696655, accuracy 0.02380952425301075.\n",
      "Training epoch 3 batch 84 with loss 1.742143988609314, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 85 with loss 1.7618465423583984, accuracy 0.236111119389534.\n",
      "Training epoch 3 batch 86 with loss 1.6282777786254883, accuracy 0.4305555522441864.\n",
      "Training epoch 3 batch 87 with loss 1.7369487285614014, accuracy 0.2083333283662796.\n",
      "Training epoch 3 batch 88 with loss 1.7326347827911377, accuracy 0.34166666865348816.\n",
      "Training epoch 3 batch 89 with loss 1.6353037357330322, accuracy 0.4166666567325592.\n",
      "Training epoch 3 batch 90 with loss 1.815258264541626, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 91 with loss 1.7646490335464478, accuracy 0.2361111044883728.\n",
      "Training epoch 3 batch 92 with loss 1.7652873992919922, accuracy 0.13055555522441864.\n",
      "Training epoch 3 batch 93 with loss 1.7043250799179077, accuracy 0.25.\n",
      "Training epoch 3 batch 94 with loss 1.707867980003357, accuracy 0.1805555522441864.\n",
      "Training epoch 3 batch 95 with loss 1.7914893627166748, accuracy 0.222222238779068.\n",
      "Training epoch 3 batch 96 with loss 1.8522554636001587, accuracy 0.3500000238418579.\n",
      "Training epoch 3 batch 97 with loss 1.784184455871582, accuracy 0.09444444626569748.\n",
      "Training epoch 3 batch 98 with loss 1.8865690231323242, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 99 with loss 1.770916223526001, accuracy 0.2291666865348816.\n",
      "Training epoch 3 batch 100 with loss 1.7808328866958618, accuracy 0.4166666567325592.\n",
      "Training epoch 3 batch 101 with loss 1.74600088596344, accuracy 0.3263889253139496.\n",
      "Training epoch 3 batch 102 with loss 1.751287817955017, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 103 with loss 1.6920928955078125, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 104 with loss 1.638190507888794, accuracy 0.39444446563720703.\n",
      "Training epoch 3 batch 105 with loss 1.8115999698638916, accuracy 0.22777777910232544.\n",
      "Training epoch 3 batch 106 with loss 1.8021621704101562, accuracy 0.1587301641702652.\n",
      "Training epoch 3 batch 107 with loss 1.6584678888320923, accuracy 0.5277777910232544.\n",
      "Training epoch 3 batch 108 with loss 1.820291519165039, accuracy 0.11666666716337204.\n",
      "Training epoch 3 batch 109 with loss 1.7951196432113647, accuracy 0.3444444537162781.\n",
      "Training epoch 3 batch 110 with loss 1.7973964214324951, accuracy 0.3361111283302307.\n",
      "Training epoch 3 batch 111 with loss 1.7403154373168945, accuracy 0.2083333283662796.\n",
      "Training epoch 3 batch 112 with loss 1.7905113697052002, accuracy 0.19166666269302368.\n",
      "Training epoch 3 batch 113 with loss 1.8408386707305908, accuracy 0.06111111491918564.\n",
      "Training epoch 3 batch 114 with loss 1.7490262985229492, accuracy 0.2976190745830536.\n",
      "Training epoch 3 batch 115 with loss 1.7440248727798462, accuracy 0.21388888359069824.\n",
      "Training epoch 3 batch 116 with loss 1.684747338294983, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 117 with loss 1.7959411144256592, accuracy 0.05416666716337204.\n",
      "Training epoch 3 batch 118 with loss 1.8373053073883057, accuracy 0.1597222238779068.\n",
      "Training epoch 3 batch 119 with loss 1.7762515544891357, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 120 with loss 1.7458490133285522, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 121 with loss 1.73061203956604, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 122 with loss 1.7680683135986328, accuracy 0.16388890147209167.\n",
      "Training epoch 3 batch 123 with loss 1.6989383697509766, accuracy 0.3240740895271301.\n",
      "Training epoch 3 batch 124 with loss 1.8291620016098022, accuracy 0.08095238357782364.\n",
      "Training epoch 3 batch 125 with loss 1.7458527088165283, accuracy 0.3333333432674408.\n",
      "Training epoch 3 batch 126 with loss 1.7318836450576782, accuracy 0.2152777761220932.\n",
      "Training epoch 3 batch 127 with loss 1.8131535053253174, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 128 with loss 1.8118785619735718, accuracy 0.2750000059604645.\n",
      "Training epoch 3 batch 129 with loss 1.8135547637939453, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 130 with loss 1.8123986721038818, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 131 with loss 1.753628134727478, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 132 with loss 1.779200553894043, accuracy 0.2380952537059784.\n",
      "Training epoch 3 batch 133 with loss 1.8296865224838257, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 134 with loss 1.7055778503417969, accuracy 0.25.\n",
      "Training epoch 3 batch 135 with loss 1.7353591918945312, accuracy 0.2750000059604645.\n",
      "Training epoch 3 batch 136 with loss 1.784178376197815, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 137 with loss 1.7745481729507446, accuracy 0.1210317462682724.\n",
      "Training epoch 3 batch 138 with loss 1.7938435077667236, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 139 with loss 1.7576411962509155, accuracy 0.2003968358039856.\n",
      "Training epoch 3 batch 140 with loss 1.7365163564682007, accuracy 0.3611111044883728.\n",
      "Training epoch 3 batch 141 with loss 1.8134742975234985, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 142 with loss 1.701181411743164, accuracy 0.22499999403953552.\n",
      "Training epoch 3 batch 143 with loss 1.7973836660385132, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 144 with loss 1.760441541671753, accuracy 0.09047619253396988.\n",
      "Training epoch 3 batch 145 with loss 1.8074058294296265, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 146 with loss 1.8896205425262451, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 147 with loss 1.7210838794708252, accuracy 0.25158730149269104.\n",
      "Training epoch 3 batch 148 with loss 1.6593793630599976, accuracy 0.5138888955116272.\n",
      "Training epoch 3 batch 149 with loss 1.721585988998413, accuracy 0.18333333730697632.\n",
      "Training epoch 3 batch 150 with loss 1.8254750967025757, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 151 with loss 1.8978198766708374, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 152 with loss 1.818357229232788, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 153 with loss 1.7744516134262085, accuracy 0.24166667461395264.\n",
      "Training epoch 3 batch 154 with loss 1.7448841333389282, accuracy 0.2750000059604645.\n",
      "Training epoch 3 batch 155 with loss 1.8610048294067383, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 156 with loss 1.8240749835968018, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 157 with loss 1.8438975811004639, accuracy 0.20158730447292328.\n",
      "Training epoch 3 batch 158 with loss 1.7854890823364258, accuracy 0.21111111342906952.\n",
      "Training epoch 3 batch 159 with loss 1.7469818592071533, accuracy 0.2638888955116272.\n",
      "Training epoch 3 batch 160 with loss 1.7738056182861328, accuracy 0.28333333134651184.\n",
      "Training epoch 3 batch 161 with loss 1.6367876529693604, accuracy 0.3293651044368744.\n",
      "Training epoch 3 batch 162 with loss 1.80124831199646, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 163 with loss 1.755306601524353, accuracy 0.23888888955116272.\n",
      "Training epoch 3 batch 164 with loss 1.8060163259506226, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 165 with loss 1.7458961009979248, accuracy 0.3611111044883728.\n",
      "Training epoch 3 batch 166 with loss 1.6501182317733765, accuracy 0.25.\n",
      "Training epoch 3 batch 167 with loss 1.675596833229065, accuracy 0.2152777761220932.\n",
      "Training epoch 3 batch 168 with loss 1.7994098663330078, accuracy 0.19907408952713013.\n",
      "Training epoch 3 batch 169 with loss 1.8111693859100342, accuracy 0.3444444537162781.\n",
      "Training epoch 3 batch 170 with loss 1.710282564163208, accuracy 0.24166667461395264.\n",
      "Training epoch 3 batch 171 with loss 1.7724449634552002, accuracy 0.0625.\n",
      "Training epoch 3 batch 172 with loss 1.7403829097747803, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 173 with loss 1.7720142602920532, accuracy 0.20000001788139343.\n",
      "Training epoch 3 batch 174 with loss 1.70413339138031, accuracy 0.3333333432674408.\n",
      "Training epoch 3 batch 175 with loss 1.742860198020935, accuracy 0.26944446563720703.\n",
      "Training epoch 3 batch 176 with loss 1.8936245441436768, accuracy 0.02777777798473835.\n",
      "Training epoch 3 batch 177 with loss 1.8821992874145508, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 178 with loss 1.6588947772979736, accuracy 0.4166666865348816.\n",
      "Training epoch 3 batch 179 with loss 1.7412421703338623, accuracy 0.1458333432674408.\n",
      "Training epoch 3 batch 180 with loss 1.689946174621582, accuracy 0.25.\n",
      "Training epoch 3 batch 181 with loss 1.7731260061264038, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 182 with loss 1.718392014503479, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 183 with loss 1.837673544883728, accuracy 0.10000000149011612.\n",
      "Training epoch 3 batch 184 with loss 1.8052505254745483, accuracy 0.1805555671453476.\n",
      "Training epoch 3 batch 185 with loss 1.7650578022003174, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 186 with loss 1.8099416494369507, accuracy 0.12222222983837128.\n",
      "Training epoch 3 batch 187 with loss 1.7675483226776123, accuracy 0.20555555820465088.\n",
      "Training epoch 3 batch 188 with loss 1.7800018787384033, accuracy 0.03333333507180214.\n",
      "Training epoch 3 batch 189 with loss 1.8035879135131836, accuracy 0.0694444477558136.\n",
      "Training epoch 3 batch 190 with loss 1.7669188976287842, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 191 with loss 1.7491356134414673, accuracy 0.3611111044883728.\n",
      "Training epoch 3 batch 192 with loss 1.786205530166626, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 193 with loss 1.7703155279159546, accuracy 0.2611111104488373.\n",
      "Training epoch 3 batch 194 with loss 1.765977144241333, accuracy 0.3611111044883728.\n",
      "Training epoch 3 batch 195 with loss 1.7616220712661743, accuracy 0.2361111044883728.\n",
      "Training epoch 3 batch 196 with loss 1.804024338722229, accuracy 0.11428572237491608.\n",
      "Training epoch 3 batch 197 with loss 1.7997783422470093, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 198 with loss 1.84140145778656, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 199 with loss 1.742762565612793, accuracy 0.10277777910232544.\n",
      "Training epoch 3 batch 200 with loss 1.7691822052001953, accuracy 0.17777778208255768.\n",
      "Training epoch 3 batch 201 with loss 1.707720160484314, accuracy 0.6361111402511597.\n",
      "Training epoch 3 batch 202 with loss 1.860669732093811, accuracy 0.15833334624767303.\n",
      "Training epoch 3 batch 203 with loss 1.7812912464141846, accuracy 0.15555556118488312.\n",
      "Training epoch 3 batch 204 with loss 1.6389449834823608, accuracy 0.43666666746139526.\n",
      "Training epoch 3 batch 205 with loss 1.6831716299057007, accuracy 0.3115079402923584.\n",
      "Training epoch 3 batch 206 with loss 1.833547592163086, accuracy 0.18333333730697632.\n",
      "Training epoch 3 batch 207 with loss 1.7538433074951172, accuracy 0.3194444477558136.\n",
      "Training epoch 3 batch 208 with loss 1.8553282022476196, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 209 with loss 1.7608394622802734, accuracy 0.1210317462682724.\n",
      "Training epoch 3 batch 210 with loss 1.7667402029037476, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 211 with loss 1.5979951620101929, accuracy 0.4416666626930237.\n",
      "Training epoch 3 batch 212 with loss 1.8745883703231812, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 213 with loss 1.7397674322128296, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 214 with loss 1.7971338033676147, accuracy 0.1130952388048172.\n",
      "Training epoch 3 batch 215 with loss 1.7867958545684814, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 216 with loss 1.8332430124282837, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 217 with loss 1.7105152606964111, accuracy 0.3055555522441864.\n",
      "Training epoch 3 batch 218 with loss 1.7501084804534912, accuracy 0.17777778208255768.\n",
      "Training epoch 3 batch 219 with loss 1.7622816562652588, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 220 with loss 1.783156394958496, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 221 with loss 1.8379281759262085, accuracy 0.125.\n",
      "Training epoch 3 batch 222 with loss 1.8115955591201782, accuracy 0.2958333492279053.\n",
      "Training epoch 3 batch 223 with loss 1.7952547073364258, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 224 with loss 1.7384802103042603, accuracy 0.2750000059604645.\n",
      "Training epoch 3 batch 225 with loss 1.7760696411132812, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 226 with loss 1.7772905826568604, accuracy 0.12222222983837128.\n",
      "Training epoch 3 batch 227 with loss 1.6899226903915405, accuracy 0.3750000298023224.\n",
      "Training epoch 3 batch 228 with loss 1.7766462564468384, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 229 with loss 1.8492742776870728, accuracy 0.125.\n",
      "Training epoch 3 batch 230 with loss 1.8186595439910889, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 231 with loss 1.7508682012557983, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 232 with loss 1.6574418544769287, accuracy 0.4583333134651184.\n",
      "Training epoch 3 batch 233 with loss 1.8434690237045288, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 234 with loss 1.7195736169815063, accuracy 0.4166666567325592.\n",
      "Training epoch 3 batch 235 with loss 1.6749227046966553, accuracy 0.1964285671710968.\n",
      "Training epoch 3 batch 236 with loss 1.703860878944397, accuracy 0.36666667461395264.\n",
      "Training epoch 3 batch 237 with loss 1.7480096817016602, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 238 with loss 1.7931264638900757, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 239 with loss 1.778236985206604, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 240 with loss 1.7732841968536377, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 241 with loss 1.8497012853622437, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 242 with loss 1.8123964071273804, accuracy 0.4583333432674408.\n",
      "Training epoch 3 batch 243 with loss 1.7279863357543945, accuracy 0.23333334922790527.\n",
      "Training epoch 3 batch 244 with loss 1.7699311971664429, accuracy 0.2708333432674408.\n",
      "Training epoch 3 batch 245 with loss 1.7312803268432617, accuracy 0.2460317462682724.\n",
      "Training epoch 3 batch 246 with loss 1.7941648960113525, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 247 with loss 1.7957290410995483, accuracy 0.4305555820465088.\n",
      "Training epoch 3 batch 248 with loss 1.6947300434112549, accuracy 0.222222238779068.\n",
      "Training epoch 3 batch 249 with loss 1.8192355632781982, accuracy 0.1527777910232544.\n",
      "Training epoch 3 batch 250 with loss 1.8490629196166992, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 251 with loss 1.7123295068740845, accuracy 0.39444443583488464.\n",
      "Training epoch 3 batch 252 with loss 1.669456124305725, accuracy 0.33095237612724304.\n",
      "Training epoch 3 batch 253 with loss 1.7213213443756104, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 254 with loss 1.7953821420669556, accuracy 0.19166666269302368.\n",
      "Training epoch 3 batch 255 with loss 1.7655225992202759, accuracy 0.2460317462682724.\n",
      "Training epoch 3 batch 256 with loss 1.8001655340194702, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 257 with loss 1.7321271896362305, accuracy 0.4694444537162781.\n",
      "Training epoch 3 batch 258 with loss 1.7872791290283203, accuracy 0.28333333134651184.\n",
      "Training epoch 3 batch 259 with loss 1.8500779867172241, accuracy 0.10277777910232544.\n",
      "Training epoch 3 batch 260 with loss 1.9284861087799072, accuracy 0.08888889104127884.\n",
      "Training epoch 3 batch 261 with loss 1.8678480386734009, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 262 with loss 1.7762826681137085, accuracy 0.29722222685813904.\n",
      "Training epoch 3 batch 263 with loss 1.7003002166748047, accuracy 0.24722221493721008.\n",
      "Training epoch 3 batch 264 with loss 1.7982518672943115, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 265 with loss 1.8075182437896729, accuracy 0.3777777850627899.\n",
      "Training epoch 3 batch 266 with loss 1.700122594833374, accuracy 0.2638888955116272.\n",
      "Training epoch 3 batch 267 with loss 1.6576486825942993, accuracy 0.289682537317276.\n",
      "Training epoch 3 batch 268 with loss 1.7011770009994507, accuracy 0.2670634984970093.\n",
      "Training epoch 3 batch 269 with loss 1.727352499961853, accuracy 0.2361111044883728.\n",
      "Training epoch 3 batch 270 with loss 1.8288911581039429, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 271 with loss 1.7165510654449463, accuracy 0.24722222983837128.\n",
      "Training epoch 3 batch 272 with loss 1.7965360879898071, accuracy 0.125.\n",
      "Training epoch 3 batch 273 with loss 1.8646589517593384, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 274 with loss 1.6675326824188232, accuracy 0.3333333432674408.\n",
      "Training epoch 3 batch 275 with loss 1.7852838039398193, accuracy 0.347222238779068.\n",
      "Training epoch 3 batch 276 with loss 1.6896082162857056, accuracy 0.4611111283302307.\n",
      "Training epoch 3 batch 277 with loss 1.8478107452392578, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 278 with loss 1.8405191898345947, accuracy 0.1488095223903656.\n",
      "Training epoch 3 batch 279 with loss 1.721376657485962, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 280 with loss 1.7867202758789062, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 281 with loss 1.7782680988311768, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 282 with loss 1.7725368738174438, accuracy 0.22777777910232544.\n",
      "Training epoch 3 batch 283 with loss 1.7275232076644897, accuracy 0.21984127163887024.\n",
      "Training epoch 3 batch 284 with loss 1.8675369024276733, accuracy 0.2142857164144516.\n",
      "Training epoch 3 batch 285 with loss 1.8115648031234741, accuracy 0.32500001788139343.\n",
      "Training epoch 3 batch 286 with loss 1.753055214881897, accuracy 0.4361111521720886.\n",
      "Training epoch 3 batch 287 with loss 1.7111469507217407, accuracy 0.35277777910232544.\n",
      "Training epoch 3 batch 288 with loss 1.7506473064422607, accuracy 0.3194444477558136.\n",
      "Training epoch 3 batch 289 with loss 1.8618934154510498, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 290 with loss 1.734513282775879, accuracy 0.3777777850627899.\n",
      "Training epoch 3 batch 291 with loss 1.7249444723129272, accuracy 0.2916666865348816.\n",
      "Training epoch 3 batch 292 with loss 1.8089640140533447, accuracy 0.1805555522441864.\n",
      "Training epoch 3 batch 293 with loss 1.6918678283691406, accuracy 0.3583333194255829.\n",
      "Training epoch 3 batch 294 with loss 1.7317302227020264, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 295 with loss 1.7364788055419922, accuracy 0.1626984179019928.\n",
      "Training epoch 3 batch 296 with loss 1.768204927444458, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 297 with loss 1.7423816919326782, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 298 with loss 1.74477219581604, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 299 with loss 1.9407449960708618, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 300 with loss 1.7657649517059326, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 301 with loss 1.7562021017074585, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 302 with loss 1.7334089279174805, accuracy 0.21111111342906952.\n",
      "Training epoch 3 batch 303 with loss 1.8251298666000366, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 304 with loss 1.8526134490966797, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 305 with loss 1.8061599731445312, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 306 with loss 1.716639757156372, accuracy 0.375.\n",
      "Training epoch 3 batch 307 with loss 1.7828714847564697, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 308 with loss 1.8641107082366943, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 309 with loss 1.6810309886932373, accuracy 0.32870373129844666.\n",
      "Training epoch 3 batch 310 with loss 1.8181285858154297, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 311 with loss 1.7744954824447632, accuracy 0.24722221493721008.\n",
      "Training epoch 3 batch 312 with loss 1.7826507091522217, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 313 with loss 1.7445204257965088, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 314 with loss 1.8510668277740479, accuracy 0.20555555820465088.\n",
      "Training epoch 3 batch 315 with loss 1.7605516910552979, accuracy 0.20555555820465088.\n",
      "Training epoch 3 batch 316 with loss 1.8531723022460938, accuracy 0.1825396865606308.\n",
      "Training epoch 3 batch 317 with loss 1.816626787185669, accuracy 0.18333333730697632.\n",
      "Training epoch 3 batch 318 with loss 1.7814022302627563, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 319 with loss 1.8287378549575806, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 320 with loss 1.88642156124115, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 321 with loss 1.8202331066131592, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 322 with loss 1.837878942489624, accuracy 0.1210317462682724.\n",
      "Training epoch 3 batch 323 with loss 1.8081779479980469, accuracy 0.1071428656578064.\n",
      "Training epoch 3 batch 324 with loss 1.7013862133026123, accuracy 0.15555556118488312.\n",
      "Training epoch 3 batch 325 with loss 1.8036954402923584, accuracy 0.15833333134651184.\n",
      "Training epoch 3 batch 326 with loss 1.7701857089996338, accuracy 0.0694444477558136.\n",
      "Training epoch 3 batch 327 with loss 1.7093807458877563, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 328 with loss 1.7982603311538696, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 329 with loss 1.8062944412231445, accuracy 0.28333333134651184.\n",
      "Training epoch 3 batch 330 with loss 1.712959885597229, accuracy 0.30666667222976685.\n",
      "Training epoch 3 batch 331 with loss 1.731615662574768, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 332 with loss 1.8223806619644165, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 333 with loss 1.725699782371521, accuracy 0.3115079402923584.\n",
      "Training epoch 3 batch 334 with loss 1.6979566812515259, accuracy 0.3583333492279053.\n",
      "Training epoch 3 batch 335 with loss 1.7544152736663818, accuracy 0.2916666567325592.\n",
      "Training epoch 3 batch 336 with loss 1.757788062095642, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 337 with loss 1.7345561981201172, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 338 with loss 1.7802441120147705, accuracy 0.21111111342906952.\n",
      "Training epoch 3 batch 339 with loss 1.7415745258331299, accuracy 0.3253968358039856.\n",
      "Training epoch 3 batch 340 with loss 1.756447434425354, accuracy 0.1964285671710968.\n",
      "Training epoch 3 batch 341 with loss 1.6894937753677368, accuracy 0.23333334922790527.\n",
      "Training epoch 3 batch 342 with loss 1.690101981163025, accuracy 0.23888888955116272.\n",
      "Training epoch 3 batch 343 with loss 1.6743751764297485, accuracy 0.222222238779068.\n",
      "Training epoch 3 batch 344 with loss 1.714655876159668, accuracy 0.1130952388048172.\n",
      "Training epoch 3 batch 345 with loss 1.8447729349136353, accuracy 0.10000000894069672.\n",
      "Training epoch 3 batch 346 with loss 1.8049609661102295, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 347 with loss 1.6771128177642822, accuracy 0.19722223281860352.\n",
      "Training epoch 3 batch 348 with loss 1.8196544647216797, accuracy 0.28333333134651184.\n",
      "Training epoch 3 batch 349 with loss 1.7992340326309204, accuracy 0.0694444477558136.\n",
      "Training epoch 3 batch 350 with loss 1.7214552164077759, accuracy 0.4333333373069763.\n",
      "Training epoch 3 batch 351 with loss 1.745415449142456, accuracy 0.38055557012557983.\n",
      "Training epoch 3 batch 352 with loss 1.7945798635482788, accuracy 0.236111119389534.\n",
      "Training epoch 3 batch 353 with loss 1.789300560951233, accuracy 0.12261904776096344.\n",
      "Training epoch 3 batch 354 with loss 1.7722342014312744, accuracy 0.25.\n",
      "Training epoch 3 batch 355 with loss 1.8000431060791016, accuracy 0.2916666865348816.\n",
      "Training epoch 3 batch 356 with loss 1.6279571056365967, accuracy 0.4138889014720917.\n",
      "Training epoch 3 batch 357 with loss 1.824035882949829, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 358 with loss 1.847934365272522, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 359 with loss 1.7652298212051392, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 360 with loss 1.7856323719024658, accuracy 0.23333333432674408.\n",
      "Training epoch 3 batch 361 with loss 1.8005483150482178, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 362 with loss 1.772237777709961, accuracy 0.4226190447807312.\n",
      "Training epoch 3 batch 363 with loss 1.7522058486938477, accuracy 0.23333333432674408.\n",
      "Training epoch 3 batch 364 with loss 1.799777626991272, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 365 with loss 1.7172420024871826, accuracy 0.19166666269302368.\n",
      "Training epoch 3 batch 366 with loss 1.868522047996521, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 367 with loss 1.6680662631988525, accuracy 0.2638888955116272.\n",
      "Training epoch 3 batch 368 with loss 1.8647594451904297, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 369 with loss 1.75283682346344, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 370 with loss 1.7246086597442627, accuracy 0.1587301641702652.\n",
      "Training epoch 3 batch 371 with loss 1.8672888278961182, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 372 with loss 1.688826322555542, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 373 with loss 1.7321062088012695, accuracy 0.19166666269302368.\n",
      "Training epoch 3 batch 374 with loss 1.787437081336975, accuracy 0.1071428582072258.\n",
      "Training epoch 3 batch 375 with loss 1.664210557937622, accuracy 0.25.\n",
      "Training epoch 3 batch 376 with loss 1.5839065313339233, accuracy 0.4555555582046509.\n",
      "Training epoch 3 batch 377 with loss 1.817352294921875, accuracy 0.1458333432674408.\n",
      "Training epoch 3 batch 378 with loss 1.785077691078186, accuracy 0.0892857164144516.\n",
      "Training epoch 3 batch 379 with loss 1.784035086631775, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 380 with loss 1.8094018697738647, accuracy 0.14000001549720764.\n",
      "Training epoch 3 batch 381 with loss 1.6761289834976196, accuracy 0.3333333432674408.\n",
      "Training epoch 3 batch 382 with loss 1.8245445489883423, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 383 with loss 1.7262033224105835, accuracy 0.38611114025115967.\n",
      "Training epoch 3 batch 384 with loss 1.7675822973251343, accuracy 0.18611112236976624.\n",
      "Training epoch 3 batch 385 with loss 1.7483497858047485, accuracy 0.3920634984970093.\n",
      "Training epoch 3 batch 386 with loss 1.650101900100708, accuracy 0.3333333432674408.\n",
      "Training epoch 3 batch 387 with loss 1.8578754663467407, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 388 with loss 1.7838366031646729, accuracy 0.1805555671453476.\n",
      "Training epoch 3 batch 389 with loss 1.775667428970337, accuracy 0.32499998807907104.\n",
      "Training epoch 3 batch 390 with loss 1.771921157836914, accuracy 0.329365074634552.\n",
      "Training epoch 3 batch 391 with loss 1.7507234811782837, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 392 with loss 1.780460000038147, accuracy 0.236111119389534.\n",
      "Training epoch 3 batch 393 with loss 1.7443435192108154, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 394 with loss 1.7956196069717407, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 395 with loss 1.7296922206878662, accuracy 0.1736111044883728.\n",
      "Training epoch 3 batch 396 with loss 1.8188644647598267, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 397 with loss 1.7243106365203857, accuracy 0.17380952835083008.\n",
      "Training epoch 3 batch 398 with loss 1.7747199535369873, accuracy 0.20555555820465088.\n",
      "Training epoch 3 batch 399 with loss 1.7861295938491821, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 400 with loss 1.7525835037231445, accuracy 0.18888889253139496.\n",
      "Training epoch 3 batch 401 with loss 1.6916701793670654, accuracy 0.363095223903656.\n",
      "Training epoch 3 batch 402 with loss 1.7654263973236084, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 403 with loss 1.7475942373275757, accuracy 0.15833333134651184.\n",
      "Training epoch 3 batch 404 with loss 1.8392146825790405, accuracy 0.2152777761220932.\n",
      "Training epoch 3 batch 405 with loss 1.7738956212997437, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 406 with loss 1.8268636465072632, accuracy 0.09444444626569748.\n",
      "Training epoch 3 batch 407 with loss 1.7674572467803955, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 408 with loss 1.777916669845581, accuracy 0.3055555522441864.\n",
      "Training epoch 3 batch 409 with loss 1.776475191116333, accuracy 0.23333334922790527.\n",
      "Training epoch 3 batch 410 with loss 1.8170528411865234, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 411 with loss 1.8752015829086304, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 412 with loss 1.7327468395233154, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 413 with loss 1.657295823097229, accuracy 0.19166667759418488.\n",
      "Training epoch 3 batch 414 with loss 1.8273766040802002, accuracy 0.2976190447807312.\n",
      "Training epoch 3 batch 415 with loss 1.7241872549057007, accuracy 0.2888889014720917.\n",
      "Training epoch 3 batch 416 with loss 1.8683189153671265, accuracy 0.130952388048172.\n",
      "Training epoch 3 batch 417 with loss 1.757930040359497, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 418 with loss 1.689823865890503, accuracy 0.2611111104488373.\n",
      "Training epoch 3 batch 419 with loss 1.8125261068344116, accuracy 0.15555556118488312.\n",
      "Training epoch 3 batch 420 with loss 1.8149915933609009, accuracy 0.36666667461395264.\n",
      "Training epoch 3 batch 421 with loss 1.8402036428451538, accuracy 0.21666666865348816.\n",
      "Training epoch 3 batch 422 with loss 1.7274024486541748, accuracy 0.13333334028720856.\n",
      "Training epoch 3 batch 423 with loss 1.821636438369751, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 424 with loss 1.922655701637268, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 425 with loss 1.7211418151855469, accuracy 0.45555558800697327.\n",
      "Training epoch 3 batch 426 with loss 1.6630964279174805, accuracy 0.42777779698371887.\n",
      "Training epoch 3 batch 427 with loss 1.9214162826538086, accuracy 0.03333333507180214.\n",
      "Training epoch 3 batch 428 with loss 1.694719910621643, accuracy 0.347222238779068.\n",
      "Training epoch 3 batch 429 with loss 1.7211112976074219, accuracy 0.21666666865348816.\n",
      "Training epoch 3 batch 430 with loss 1.82638418674469, accuracy 0.3305555582046509.\n",
      "Training epoch 3 batch 431 with loss 1.7276175022125244, accuracy 0.2380952537059784.\n",
      "Training epoch 3 batch 432 with loss 1.7907609939575195, accuracy 0.4305555820465088.\n",
      "Training epoch 3 batch 433 with loss 1.7611198425292969, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 434 with loss 1.7499544620513916, accuracy 0.39444446563720703.\n",
      "Training epoch 3 batch 435 with loss 1.7530100345611572, accuracy 0.3611111044883728.\n",
      "Training epoch 3 batch 436 with loss 1.7298578023910522, accuracy 0.3777777850627899.\n",
      "Training epoch 3 batch 437 with loss 1.7527040243148804, accuracy 0.2519841492176056.\n",
      "Training epoch 3 batch 438 with loss 1.8456647396087646, accuracy 0.10833333432674408.\n",
      "Training epoch 3 batch 439 with loss 1.8088983297348022, accuracy 0.3027777671813965.\n",
      "Training epoch 3 batch 440 with loss 1.8352785110473633, accuracy 0.2182539701461792.\n",
      "Training epoch 3 batch 441 with loss 1.7448838949203491, accuracy 0.4166666865348816.\n",
      "Training epoch 3 batch 442 with loss 1.7124783992767334, accuracy 0.23333333432674408.\n",
      "Training epoch 3 batch 443 with loss 1.8438259363174438, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 444 with loss 1.7606441974639893, accuracy 0.20000001788139343.\n",
      "Training epoch 3 batch 445 with loss 1.8567317724227905, accuracy 0.08750000596046448.\n",
      "Training epoch 3 batch 446 with loss 1.6600189208984375, accuracy 0.5092592835426331.\n",
      "Training epoch 3 batch 447 with loss 1.789280652999878, accuracy 0.25.\n",
      "Training epoch 3 batch 448 with loss 1.6590440273284912, accuracy 0.2916666865348816.\n",
      "Training epoch 3 batch 449 with loss 1.6753685474395752, accuracy 0.24166667461395264.\n",
      "Training epoch 3 batch 450 with loss 1.697678565979004, accuracy 0.26944446563720703.\n",
      "Training epoch 3 batch 451 with loss 1.6815969944000244, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 452 with loss 1.9263675212860107, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 453 with loss 1.8099533319473267, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 454 with loss 1.702111840248108, accuracy 0.3916666507720947.\n",
      "Training epoch 3 batch 455 with loss 1.7104876041412354, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 456 with loss 1.6490825414657593, accuracy 0.4166666865348816.\n",
      "Training epoch 3 batch 457 with loss 1.7830873727798462, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 458 with loss 1.7999635934829712, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 459 with loss 1.6290626525878906, accuracy 0.3948412835597992.\n",
      "Training epoch 3 batch 460 with loss 1.806504487991333, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 461 with loss 1.7039525508880615, accuracy 0.26944443583488464.\n",
      "Training epoch 3 batch 462 with loss 1.8208954334259033, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 463 with loss 1.9266395568847656, accuracy 0.02777777798473835.\n",
      "Training epoch 3 batch 464 with loss 1.7725107669830322, accuracy 0.3333333432674408.\n",
      "Training epoch 3 batch 465 with loss 1.7590268850326538, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 466 with loss 1.7424042224884033, accuracy 0.29722222685813904.\n",
      "Training epoch 3 batch 467 with loss 1.814771294593811, accuracy 0.16547620296478271.\n",
      "Training epoch 3 batch 468 with loss 1.8532512187957764, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 469 with loss 1.8052219152450562, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 470 with loss 1.7868868112564087, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 471 with loss 1.8112255334854126, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 472 with loss 1.8663194179534912, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 473 with loss 1.758715271949768, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 474 with loss 1.744842529296875, accuracy 0.2847222089767456.\n",
      "Training epoch 3 batch 475 with loss 1.8482834100723267, accuracy 0.0476190485060215.\n",
      "Training epoch 3 batch 476 with loss 1.7100270986557007, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 477 with loss 1.7662912607192993, accuracy 0.08888889104127884.\n",
      "Training epoch 3 batch 478 with loss 1.753308892250061, accuracy 0.1041666716337204.\n",
      "Training epoch 3 batch 479 with loss 1.8159151077270508, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 480 with loss 1.7806494235992432, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 481 with loss 1.8695173263549805, accuracy 0.2460317611694336.\n",
      "Training epoch 3 batch 482 with loss 1.7108204364776611, accuracy 0.4861111342906952.\n",
      "Training epoch 3 batch 483 with loss 1.8111746311187744, accuracy 0.3305555582046509.\n",
      "Training epoch 3 batch 484 with loss 1.8217109441757202, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 485 with loss 1.8346099853515625, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 486 with loss 1.8088115453720093, accuracy 0.222222238779068.\n",
      "Training epoch 3 batch 487 with loss 1.7662303447723389, accuracy 0.3222222328186035.\n",
      "Training epoch 3 batch 488 with loss 1.8122745752334595, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 489 with loss 1.809319257736206, accuracy 0.17658731341362.\n",
      "Training epoch 3 batch 490 with loss 1.7513444423675537, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 491 with loss 1.760451078414917, accuracy 0.3333333134651184.\n",
      "Training epoch 3 batch 492 with loss 1.8739160299301147, accuracy 0.25.\n",
      "Training epoch 3 batch 493 with loss 1.7923085689544678, accuracy 0.3214285671710968.\n",
      "Training epoch 3 batch 494 with loss 1.838335633277893, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 495 with loss 1.8163713216781616, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 496 with loss 1.6982864141464233, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 497 with loss 1.6841142177581787, accuracy 0.2708333432674408.\n",
      "Training epoch 3 batch 498 with loss 1.6912540197372437, accuracy 0.347222238779068.\n",
      "Training epoch 3 batch 499 with loss 1.7499806880950928, accuracy 0.23333334922790527.\n",
      "Training epoch 3 batch 500 with loss 1.8214391469955444, accuracy 0.125.\n",
      "Training epoch 3 batch 501 with loss 1.8492752313613892, accuracy 0.29722222685813904.\n",
      "Training epoch 3 batch 502 with loss 1.7133773565292358, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 503 with loss 1.7794374227523804, accuracy 0.2958333492279053.\n",
      "Training epoch 3 batch 504 with loss 1.8226407766342163, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 505 with loss 1.7608044147491455, accuracy 0.21388889849185944.\n",
      "Training epoch 3 batch 506 with loss 1.8016901016235352, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 507 with loss 1.7839205265045166, accuracy 0.17777778208255768.\n",
      "Training epoch 3 batch 508 with loss 1.7864223718643188, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 509 with loss 1.8036954402923584, accuracy 0.2738095223903656.\n",
      "Training epoch 3 batch 510 with loss 1.7729847431182861, accuracy 0.10000000894069672.\n",
      "Training epoch 3 batch 511 with loss 1.737122893333435, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 512 with loss 1.7982797622680664, accuracy 0.065476194024086.\n",
      "Training epoch 3 batch 513 with loss 1.8225891590118408, accuracy 0.21666666865348816.\n",
      "Training epoch 3 batch 514 with loss 1.8005403280258179, accuracy 0.0.\n",
      "Training epoch 3 batch 515 with loss 1.7421133518218994, accuracy 0.23888888955116272.\n",
      "Training epoch 3 batch 516 with loss 1.7571938037872314, accuracy 0.375.\n",
      "Training epoch 3 batch 517 with loss 1.8561407327651978, accuracy 0.125.\n",
      "Training epoch 3 batch 518 with loss 1.6713873147964478, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 519 with loss 1.8038288354873657, accuracy 0.26944446563720703.\n",
      "Training epoch 3 batch 520 with loss 1.8093221187591553, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 521 with loss 1.8106753826141357, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 522 with loss 1.914412260055542, accuracy 0.0.\n",
      "Training epoch 3 batch 523 with loss 1.9041240215301514, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 524 with loss 1.8745895624160767, accuracy 0.125.\n",
      "Training epoch 3 batch 525 with loss 1.7865444421768188, accuracy 0.18333333730697632.\n",
      "Training epoch 3 batch 526 with loss 1.8763084411621094, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 527 with loss 1.8157562017440796, accuracy 0.2666666805744171.\n",
      "Training epoch 3 batch 528 with loss 1.7978839874267578, accuracy 0.366666704416275.\n",
      "Training epoch 3 batch 529 with loss 1.8834154605865479, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 530 with loss 1.8329910039901733, accuracy 0.125.\n",
      "Training epoch 3 batch 531 with loss 1.7924331426620483, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 532 with loss 1.8309900760650635, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 533 with loss 1.8438981771469116, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 534 with loss 1.777480125427246, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 535 with loss 1.8921759128570557, accuracy 0.125.\n",
      "Training epoch 3 batch 536 with loss 1.8177299499511719, accuracy 0.236111119389534.\n",
      "Training epoch 3 batch 537 with loss 1.8605058193206787, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 538 with loss 1.880474328994751, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 539 with loss 1.759993314743042, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 540 with loss 1.7411950826644897, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 541 with loss 1.7474887371063232, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 542 with loss 1.9164583683013916, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 543 with loss 1.6675275564193726, accuracy 0.2202380895614624.\n",
      "Training epoch 3 batch 544 with loss 1.8583978414535522, accuracy 0.02500000037252903.\n",
      "Training epoch 3 batch 545 with loss 1.7740042209625244, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 546 with loss 1.8559753894805908, accuracy 0.24166667461395264.\n",
      "Training epoch 3 batch 547 with loss 1.7460992336273193, accuracy 0.210317462682724.\n",
      "Training epoch 3 batch 548 with loss 1.8179576396942139, accuracy 0.2599206268787384.\n",
      "Training epoch 3 batch 549 with loss 1.7554254531860352, accuracy 0.3166666626930237.\n",
      "Training epoch 3 batch 550 with loss 1.8279883861541748, accuracy 0.065476194024086.\n",
      "Training epoch 3 batch 551 with loss 1.7712409496307373, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 552 with loss 1.8470042943954468, accuracy 0.1626984179019928.\n",
      "Training epoch 3 batch 553 with loss 1.7566572427749634, accuracy 0.3194444477558136.\n",
      "Training epoch 3 batch 554 with loss 1.8037230968475342, accuracy 0.236111119389534.\n",
      "Training epoch 3 batch 555 with loss 1.8124425411224365, accuracy 0.1230158731341362.\n",
      "Training epoch 3 batch 556 with loss 1.7723013162612915, accuracy 0.3861111104488373.\n",
      "Training epoch 3 batch 557 with loss 1.8128207921981812, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 558 with loss 1.7493772506713867, accuracy 0.28333336114883423.\n",
      "Training epoch 3 batch 559 with loss 1.7213796377182007, accuracy 0.17129629850387573.\n",
      "Training epoch 3 batch 560 with loss 1.7049869298934937, accuracy 0.2420634925365448.\n",
      "Training epoch 3 batch 561 with loss 1.8415582180023193, accuracy 0.0694444477558136.\n",
      "Training epoch 3 batch 562 with loss 1.795074224472046, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 563 with loss 1.8559573888778687, accuracy 0.06111111491918564.\n",
      "Training epoch 3 batch 564 with loss 1.8103803396224976, accuracy 0.2611111104488373.\n",
      "Training epoch 3 batch 565 with loss 1.829911470413208, accuracy 0.13333334028720856.\n",
      "Training epoch 3 batch 566 with loss 1.7876691818237305, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 567 with loss 1.6381927728652954, accuracy 0.29722222685813904.\n",
      "Training epoch 3 batch 568 with loss 1.7335278987884521, accuracy 0.16388888657093048.\n",
      "Training epoch 3 batch 569 with loss 1.8667984008789062, accuracy 0.2666666805744171.\n",
      "Training epoch 3 batch 570 with loss 1.729303002357483, accuracy 0.236111119389534.\n",
      "Training epoch 3 batch 571 with loss 1.7804712057113647, accuracy 0.19166666269302368.\n",
      "Training epoch 3 batch 572 with loss 1.829761266708374, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 573 with loss 1.8783395290374756, accuracy 0.08888889104127884.\n",
      "Training epoch 3 batch 574 with loss 1.7433931827545166, accuracy 0.15555556118488312.\n",
      "Training epoch 3 batch 575 with loss 1.7019904851913452, accuracy 0.25.\n",
      "Training epoch 3 batch 576 with loss 1.884839653968811, accuracy 0.02380952425301075.\n",
      "Training epoch 3 batch 577 with loss 1.803017258644104, accuracy 0.125.\n",
      "Training epoch 3 batch 578 with loss 1.796769380569458, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 579 with loss 1.8258568048477173, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 580 with loss 1.7272937297821045, accuracy 0.3611111044883728.\n",
      "Training epoch 3 batch 581 with loss 1.7035659551620483, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 582 with loss 1.8601319789886475, accuracy 0.20555555820465088.\n",
      "Training epoch 3 batch 583 with loss 1.771069884300232, accuracy 0.2361111044883728.\n",
      "Training epoch 3 batch 584 with loss 1.8347623348236084, accuracy 0.4305555820465088.\n",
      "Training epoch 3 batch 585 with loss 1.790217399597168, accuracy 0.24722222983837128.\n",
      "Training epoch 3 batch 586 with loss 1.8237940073013306, accuracy 0.21111111342906952.\n",
      "Training epoch 3 batch 587 with loss 1.8123430013656616, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 588 with loss 1.769293189048767, accuracy 0.130952388048172.\n",
      "Training epoch 3 batch 589 with loss 1.7052891254425049, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 590 with loss 1.8069188594818115, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 591 with loss 1.8451650142669678, accuracy 0.1626984179019928.\n",
      "Training epoch 3 batch 592 with loss 1.7799046039581299, accuracy 0.22499999403953552.\n",
      "Training epoch 3 batch 593 with loss 1.9437475204467773, accuracy 0.0694444477558136.\n",
      "Training epoch 3 batch 594 with loss 1.7788549661636353, accuracy 0.1071428656578064.\n",
      "Training epoch 3 batch 595 with loss 1.8723821640014648, accuracy 0.24166667461395264.\n",
      "Training epoch 3 batch 596 with loss 1.8181679248809814, accuracy 0.2916666567325592.\n",
      "Training epoch 3 batch 597 with loss 1.7156680822372437, accuracy 0.22976189851760864.\n",
      "Training epoch 3 batch 598 with loss 1.8175045251846313, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 599 with loss 1.64541757106781, accuracy 0.37222224473953247.\n",
      "Training epoch 3 batch 600 with loss 1.6895567178726196, accuracy 0.2281746119260788.\n",
      "Training epoch 3 batch 601 with loss 1.6913259029388428, accuracy 0.42500001192092896.\n",
      "Training epoch 3 batch 602 with loss 1.8142602443695068, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 603 with loss 1.7147448062896729, accuracy 0.36269840598106384.\n",
      "Training epoch 3 batch 604 with loss 1.7061874866485596, accuracy 0.23333333432674408.\n",
      "Training epoch 3 batch 605 with loss 1.7050100564956665, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 606 with loss 1.7830737829208374, accuracy 0.34259259700775146.\n",
      "Training epoch 3 batch 607 with loss 1.838439702987671, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 608 with loss 1.746809959411621, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 609 with loss 1.716002106666565, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 610 with loss 1.8057361841201782, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 611 with loss 1.8291881084442139, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 612 with loss 1.7709159851074219, accuracy 0.21388888359069824.\n",
      "Training epoch 3 batch 613 with loss 1.7719112634658813, accuracy 0.2638888955116272.\n",
      "Training epoch 3 batch 614 with loss 1.8956153392791748, accuracy 0.29722222685813904.\n",
      "Training epoch 3 batch 615 with loss 1.7345454692840576, accuracy 0.3611111044883728.\n",
      "Training epoch 3 batch 616 with loss 1.833377480506897, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 617 with loss 1.7488645315170288, accuracy 0.125.\n",
      "Training epoch 3 batch 618 with loss 1.8225419521331787, accuracy 0.1805555671453476.\n",
      "Training epoch 3 batch 619 with loss 1.8330647945404053, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 620 with loss 1.7007287740707397, accuracy 0.21944445371627808.\n",
      "Training epoch 3 batch 621 with loss 1.7716697454452515, accuracy 0.16388888657093048.\n",
      "Training epoch 3 batch 622 with loss 1.8267017602920532, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 623 with loss 1.755070447921753, accuracy 0.3154761791229248.\n",
      "Training epoch 3 batch 624 with loss 1.8211281299591064, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 625 with loss 1.8012917041778564, accuracy 0.23888888955116272.\n",
      "Training epoch 3 batch 626 with loss 1.8308337926864624, accuracy 0.1865079402923584.\n",
      "Training epoch 3 batch 627 with loss 1.739747405052185, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 628 with loss 1.840978980064392, accuracy 0.1865079402923584.\n",
      "Training epoch 3 batch 629 with loss 1.780692458152771, accuracy 0.25.\n",
      "Training epoch 3 batch 630 with loss 1.931028127670288, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 631 with loss 1.7894424200057983, accuracy 0.3444444537162781.\n",
      "Training epoch 3 batch 632 with loss 1.796194314956665, accuracy 0.2182539701461792.\n",
      "Training epoch 3 batch 633 with loss 1.8055633306503296, accuracy 0.1805555522441864.\n",
      "Training epoch 3 batch 634 with loss 1.7196435928344727, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 635 with loss 1.7699434757232666, accuracy 0.29666668176651.\n",
      "Training epoch 3 batch 636 with loss 1.7588542699813843, accuracy 0.24166667461395264.\n",
      "Training epoch 3 batch 637 with loss 1.7751271724700928, accuracy 0.03333333507180214.\n",
      "Training epoch 3 batch 638 with loss 1.8018624782562256, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 639 with loss 1.847232460975647, accuracy 0.18333333730697632.\n",
      "Training epoch 3 batch 640 with loss 1.8502109050750732, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 641 with loss 1.781067132949829, accuracy 0.30000001192092896.\n",
      "Training epoch 3 batch 642 with loss 1.940481424331665, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 643 with loss 1.7796472311019897, accuracy 0.16333332657814026.\n",
      "Training epoch 3 batch 644 with loss 1.840139627456665, accuracy 0.3611111044883728.\n",
      "Training epoch 3 batch 645 with loss 1.7804498672485352, accuracy 0.19166666269302368.\n",
      "Training epoch 3 batch 646 with loss 1.8275775909423828, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 647 with loss 1.8307487964630127, accuracy 0.11666666716337204.\n",
      "Training epoch 3 batch 648 with loss 1.8083698749542236, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 649 with loss 1.785872220993042, accuracy 0.22777777910232544.\n",
      "Training epoch 3 batch 650 with loss 1.8247750997543335, accuracy 0.1349206417798996.\n",
      "Training epoch 3 batch 651 with loss 1.730114221572876, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 652 with loss 1.7256829738616943, accuracy 0.03333333507180214.\n",
      "Training epoch 3 batch 653 with loss 1.8083324432373047, accuracy 0.25.\n",
      "Training epoch 3 batch 654 with loss 1.777364730834961, accuracy 0.24166667461395264.\n",
      "Training epoch 3 batch 655 with loss 1.8313894271850586, accuracy 0.2916666865348816.\n",
      "Training epoch 3 batch 656 with loss 1.7526108026504517, accuracy 0.35873016715049744.\n",
      "Training epoch 3 batch 657 with loss 1.8139461278915405, accuracy 0.22777777910232544.\n",
      "Training epoch 3 batch 658 with loss 1.7401082515716553, accuracy 0.29722222685813904.\n",
      "Training epoch 3 batch 659 with loss 1.7255818843841553, accuracy 0.10000000894069672.\n",
      "Training epoch 3 batch 660 with loss 1.7765817642211914, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 661 with loss 1.8266732692718506, accuracy 0.23333334922790527.\n",
      "Training epoch 3 batch 662 with loss 1.8779938220977783, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 663 with loss 1.741337776184082, accuracy 0.1805555671453476.\n",
      "Training epoch 3 batch 664 with loss 1.714162826538086, accuracy 0.16031746566295624.\n",
      "Training epoch 3 batch 665 with loss 1.7152551412582397, accuracy 0.3499999940395355.\n",
      "Training epoch 3 batch 666 with loss 1.7324554920196533, accuracy 0.23888888955116272.\n",
      "Training epoch 3 batch 667 with loss 1.8712173700332642, accuracy 0.13055555522441864.\n",
      "Training epoch 3 batch 668 with loss 1.7622066736221313, accuracy 0.3125.\n",
      "Training epoch 3 batch 669 with loss 1.7519400119781494, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 670 with loss 1.8088979721069336, accuracy 0.14166668057441711.\n",
      "Training epoch 3 batch 671 with loss 1.8862959146499634, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 672 with loss 1.7863727807998657, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 673 with loss 1.7174808979034424, accuracy 0.2430555522441864.\n",
      "Training epoch 3 batch 674 with loss 1.7634446620941162, accuracy 0.24444445967674255.\n",
      "Training epoch 3 batch 675 with loss 1.8182051181793213, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 676 with loss 1.8106787204742432, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 677 with loss 1.8384695053100586, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 678 with loss 1.7251392602920532, accuracy 0.3194444477558136.\n",
      "Training epoch 3 batch 679 with loss 1.767401099205017, accuracy 0.1805555522441864.\n",
      "Training epoch 3 batch 680 with loss 1.7361652851104736, accuracy 0.3333333432674408.\n",
      "Training epoch 3 batch 681 with loss 1.7816156148910522, accuracy 0.29722222685813904.\n",
      "Training epoch 3 batch 682 with loss 1.7822258472442627, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 683 with loss 1.8778717517852783, accuracy 0.08888889104127884.\n",
      "Training epoch 3 batch 684 with loss 1.884452223777771, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 685 with loss 1.7845462560653687, accuracy 0.3444444537162781.\n",
      "Training epoch 3 batch 686 with loss 1.718258261680603, accuracy 0.3888888955116272.\n",
      "Training epoch 3 batch 687 with loss 1.8075107336044312, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 688 with loss 1.7794517278671265, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 689 with loss 1.765549898147583, accuracy 0.2152777910232544.\n",
      "Training epoch 3 batch 690 with loss 1.7714416980743408, accuracy 0.15833333134651184.\n",
      "Training epoch 3 batch 691 with loss 1.8491491079330444, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 692 with loss 1.8693172931671143, accuracy 0.22499999403953552.\n",
      "Training epoch 3 batch 693 with loss 1.8797247409820557, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 694 with loss 1.9504873752593994, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 695 with loss 1.7314908504486084, accuracy 0.08750000596046448.\n",
      "Training epoch 3 batch 696 with loss 1.830953598022461, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 697 with loss 1.7816635370254517, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 698 with loss 1.7387220859527588, accuracy 0.20555555820465088.\n",
      "Training epoch 3 batch 699 with loss 1.7960096597671509, accuracy 0.16388890147209167.\n",
      "Training epoch 3 batch 700 with loss 1.8850347995758057, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 701 with loss 1.8130038976669312, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 702 with loss 1.7930831909179688, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 703 with loss 1.7000138759613037, accuracy 0.375.\n",
      "Training epoch 3 batch 704 with loss 1.8550398349761963, accuracy 0.1071428582072258.\n",
      "Training epoch 3 batch 705 with loss 1.8346023559570312, accuracy 0.125.\n",
      "Training epoch 3 batch 706 with loss 1.709829568862915, accuracy 0.24722224473953247.\n",
      "Training epoch 3 batch 707 with loss 1.7141097784042358, accuracy 0.3392857313156128.\n",
      "Training epoch 3 batch 708 with loss 1.873734474182129, accuracy 0.13055555522441864.\n",
      "Training epoch 3 batch 709 with loss 1.667535424232483, accuracy 0.2888889014720917.\n",
      "Training epoch 3 batch 710 with loss 1.724698781967163, accuracy 0.25.\n",
      "Training epoch 3 batch 711 with loss 1.8044732809066772, accuracy 0.1765872985124588.\n",
      "Training epoch 3 batch 712 with loss 1.6618547439575195, accuracy 0.5805555582046509.\n",
      "Training epoch 3 batch 713 with loss 1.8306910991668701, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 714 with loss 1.7555166482925415, accuracy 0.2500000298023224.\n",
      "Training epoch 3 batch 715 with loss 1.7562196254730225, accuracy 0.1626984179019928.\n",
      "Training epoch 3 batch 716 with loss 1.6729999780654907, accuracy 0.5277777910232544.\n",
      "Training epoch 3 batch 717 with loss 1.8331899642944336, accuracy 0.12222222983837128.\n",
      "Training epoch 3 batch 718 with loss 1.7511634826660156, accuracy 0.210317462682724.\n",
      "Training epoch 3 batch 719 with loss 1.7811992168426514, accuracy 0.3194444477558136.\n",
      "Training epoch 3 batch 720 with loss 1.7892566919326782, accuracy 0.19166666269302368.\n",
      "Training epoch 3 batch 721 with loss 1.8428623676300049, accuracy 0.16428571939468384.\n",
      "Training epoch 3 batch 722 with loss 1.6377859115600586, accuracy 0.36666667461395264.\n",
      "Training epoch 3 batch 723 with loss 1.8963439464569092, accuracy 0.236111119389534.\n",
      "Training epoch 3 batch 724 with loss 1.824210524559021, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 725 with loss 1.826144814491272, accuracy 0.1488095223903656.\n",
      "Training epoch 3 batch 726 with loss 1.805851697921753, accuracy 0.125.\n",
      "Training epoch 3 batch 727 with loss 1.8087409734725952, accuracy 0.1805555522441864.\n",
      "Training epoch 3 batch 728 with loss 1.8121356964111328, accuracy 0.16388890147209167.\n",
      "Training epoch 3 batch 729 with loss 1.7205559015274048, accuracy 0.33888888359069824.\n",
      "Training epoch 3 batch 730 with loss 1.7583434581756592, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 731 with loss 1.7685325145721436, accuracy 0.2527777850627899.\n",
      "Training epoch 3 batch 732 with loss 1.8669769763946533, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 733 with loss 1.7816400527954102, accuracy 0.20555555820465088.\n",
      "Training epoch 3 batch 734 with loss 1.7391923666000366, accuracy 0.3571428656578064.\n",
      "Training epoch 3 batch 735 with loss 1.8508119583129883, accuracy 0.03333333507180214.\n",
      "Training epoch 3 batch 736 with loss 1.788326621055603, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 737 with loss 1.6832664012908936, accuracy 0.23333334922790527.\n",
      "Training epoch 3 batch 738 with loss 1.6982969045639038, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 739 with loss 1.8397226333618164, accuracy 0.222222238779068.\n",
      "Training epoch 3 batch 740 with loss 1.839356780052185, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 741 with loss 1.7502527236938477, accuracy 0.1071428656578064.\n",
      "Training epoch 3 batch 742 with loss 1.8583860397338867, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 743 with loss 1.7843835353851318, accuracy 0.19166667759418488.\n",
      "Training epoch 3 batch 744 with loss 1.8055353164672852, accuracy 0.125.\n",
      "Training epoch 3 batch 745 with loss 1.7845113277435303, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 746 with loss 1.7967369556427002, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 747 with loss 1.8473316431045532, accuracy 0.125.\n",
      "Training epoch 3 batch 748 with loss 1.6822103261947632, accuracy 0.31111112236976624.\n",
      "Training epoch 3 batch 749 with loss 1.6325738430023193, accuracy 0.3222222328186035.\n",
      "Training epoch 3 batch 750 with loss 1.8314380645751953, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 751 with loss 1.743883490562439, accuracy 0.25.\n",
      "Training epoch 3 batch 752 with loss 1.7611548900604248, accuracy 0.23333334922790527.\n",
      "Training epoch 3 batch 753 with loss 1.593815565109253, accuracy 0.3083333373069763.\n",
      "Training epoch 3 batch 754 with loss 1.7460362911224365, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 755 with loss 1.8631227016448975, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 756 with loss 1.7914888858795166, accuracy 0.26944446563720703.\n",
      "Training epoch 3 batch 757 with loss 1.7409921884536743, accuracy 0.190476194024086.\n",
      "Training epoch 3 batch 758 with loss 1.8169794082641602, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 759 with loss 1.767704963684082, accuracy 0.25.\n",
      "Training epoch 3 batch 760 with loss 1.7334877252578735, accuracy 0.15555556118488312.\n",
      "Training epoch 3 batch 761 with loss 1.7134488821029663, accuracy 0.2916666865348816.\n",
      "Training epoch 3 batch 762 with loss 1.719559669494629, accuracy 0.18611112236976624.\n",
      "Training epoch 3 batch 763 with loss 1.8354898691177368, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 764 with loss 1.663130760192871, accuracy 0.3055555820465088.\n",
      "Training epoch 3 batch 765 with loss 1.6974786520004272, accuracy 0.2698412835597992.\n",
      "Training epoch 3 batch 766 with loss 1.8255348205566406, accuracy 0.33194443583488464.\n",
      "Training epoch 3 batch 767 with loss 1.7578306198120117, accuracy 0.2888889014720917.\n",
      "Training epoch 3 batch 768 with loss 1.849299669265747, accuracy 0.15833333134651184.\n",
      "Training epoch 3 batch 769 with loss 1.8072665929794312, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 770 with loss 1.7262184619903564, accuracy 0.375.\n",
      "Training epoch 3 batch 771 with loss 1.750491738319397, accuracy 0.46388888359069824.\n",
      "Training epoch 3 batch 772 with loss 1.720381498336792, accuracy 0.3888888955116272.\n",
      "Training epoch 3 batch 773 with loss 1.73870849609375, accuracy 0.22777777910232544.\n",
      "Training epoch 3 batch 774 with loss 1.755157470703125, accuracy 0.3571428656578064.\n",
      "Training epoch 3 batch 775 with loss 1.7860475778579712, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 776 with loss 1.718609094619751, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 777 with loss 1.8636573553085327, accuracy 0.20555555820465088.\n",
      "Training epoch 3 batch 778 with loss 1.7914307117462158, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 779 with loss 1.8235137462615967, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 780 with loss 1.7995831966400146, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 781 with loss 1.8486305475234985, accuracy 0.1875.\n",
      "Training epoch 3 batch 782 with loss 1.8977628946304321, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 783 with loss 1.7965614795684814, accuracy 0.21111111342906952.\n",
      "Training epoch 3 batch 784 with loss 1.8007177114486694, accuracy 0.10833333432674408.\n",
      "Training epoch 3 batch 785 with loss 1.8745298385620117, accuracy 0.1805555522441864.\n",
      "Training epoch 3 batch 786 with loss 1.761845588684082, accuracy 0.36269843578338623.\n",
      "Training epoch 3 batch 787 with loss 1.8183519840240479, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 788 with loss 1.8786048889160156, accuracy 0.2888889014720917.\n",
      "Training epoch 3 batch 789 with loss 1.7491661310195923, accuracy 0.3472222089767456.\n",
      "Training epoch 3 batch 790 with loss 1.8391395807266235, accuracy 0.23333334922790527.\n",
      "Training epoch 3 batch 791 with loss 1.781150221824646, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 792 with loss 1.7037756443023682, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 793 with loss 1.716507911682129, accuracy 0.29722222685813904.\n",
      "Training epoch 3 batch 794 with loss 1.8915555477142334, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 795 with loss 1.9113047122955322, accuracy 0.3194444477558136.\n",
      "Training epoch 3 batch 796 with loss 1.7803518772125244, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 797 with loss 1.7661597728729248, accuracy 0.3611111342906952.\n",
      "Training epoch 3 batch 798 with loss 1.728393793106079, accuracy 0.2182539701461792.\n",
      "Training epoch 3 batch 799 with loss 1.901513695716858, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 800 with loss 1.7395553588867188, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 801 with loss 1.8155807256698608, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 802 with loss 1.6493383646011353, accuracy 0.28333333134651184.\n",
      "Training epoch 3 batch 803 with loss 1.7772252559661865, accuracy 0.29722222685813904.\n",
      "Training epoch 3 batch 804 with loss 1.7417224645614624, accuracy 0.32500001788139343.\n",
      "Training epoch 3 batch 805 with loss 1.6907682418823242, accuracy 0.4503968358039856.\n",
      "Training epoch 3 batch 806 with loss 1.8188931941986084, accuracy 0.0694444477558136.\n",
      "Training epoch 3 batch 807 with loss 1.7087624073028564, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 808 with loss 1.7463035583496094, accuracy 0.36666667461395264.\n",
      "Training epoch 3 batch 809 with loss 1.7872772216796875, accuracy 0.28333333134651184.\n",
      "Training epoch 3 batch 810 with loss 1.7316612005233765, accuracy 0.3194444477558136.\n",
      "Training epoch 3 batch 811 with loss 1.7323083877563477, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 812 with loss 1.8778632879257202, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 813 with loss 1.832446813583374, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 814 with loss 1.8415378332138062, accuracy 0.3055555522441864.\n",
      "Training epoch 3 batch 815 with loss 1.7637689113616943, accuracy 0.28333336114883423.\n",
      "Training epoch 3 batch 816 with loss 1.879734754562378, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 817 with loss 1.80380380153656, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 818 with loss 1.6955047845840454, accuracy 0.23333334922790527.\n",
      "Training epoch 3 batch 819 with loss 1.7830575704574585, accuracy 0.3916666805744171.\n",
      "Training epoch 3 batch 820 with loss 1.803680419921875, accuracy 0.1071428582072258.\n",
      "Training epoch 3 batch 821 with loss 1.8017610311508179, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 822 with loss 1.7552711963653564, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 823 with loss 1.821009635925293, accuracy 0.14047619700431824.\n",
      "Training epoch 3 batch 824 with loss 1.6412473917007446, accuracy 0.222222238779068.\n",
      "Training epoch 3 batch 825 with loss 1.707991600036621, accuracy 0.3194444477558136.\n",
      "Training epoch 3 batch 826 with loss 1.821502923965454, accuracy 0.1031746044754982.\n",
      "Training epoch 3 batch 827 with loss 1.8647944927215576, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 828 with loss 1.7929614782333374, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 829 with loss 1.8613849878311157, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 830 with loss 1.8002675771713257, accuracy 0.125.\n",
      "Training epoch 3 batch 831 with loss 1.7169303894042969, accuracy 0.3194444477558136.\n",
      "Training epoch 3 batch 832 with loss 1.7823994159698486, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 833 with loss 1.8445243835449219, accuracy 0.03333333507180214.\n",
      "Training epoch 3 batch 834 with loss 1.7437368631362915, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 835 with loss 1.8122882843017578, accuracy 0.32499998807907104.\n",
      "Training epoch 3 batch 836 with loss 1.7322498559951782, accuracy 0.125.\n",
      "Training epoch 3 batch 837 with loss 1.8509349822998047, accuracy 0.13055555522441864.\n",
      "Training epoch 3 batch 838 with loss 1.7615457773208618, accuracy 0.19166667759418488.\n",
      "Training epoch 3 batch 839 with loss 1.8338053226470947, accuracy 0.15833334624767303.\n",
      "Training epoch 3 batch 840 with loss 1.7489572763442993, accuracy 0.1319444477558136.\n",
      "Training epoch 3 batch 841 with loss 1.8350207805633545, accuracy 0.08888889104127884.\n",
      "Training epoch 3 batch 842 with loss 1.7635904550552368, accuracy 0.15833333134651184.\n",
      "Training epoch 3 batch 843 with loss 1.7454944849014282, accuracy 0.3166666626930237.\n",
      "Training epoch 3 batch 844 with loss 1.6590455770492554, accuracy 0.2944444417953491.\n",
      "Training epoch 3 batch 845 with loss 1.7327606678009033, accuracy 0.2750000059604645.\n",
      "Training epoch 3 batch 846 with loss 1.7711522579193115, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 847 with loss 1.8268893957138062, accuracy 0.23333334922790527.\n",
      "Training epoch 3 batch 848 with loss 1.804774522781372, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 849 with loss 1.8021576404571533, accuracy 0.0694444477558136.\n",
      "Training epoch 3 batch 850 with loss 1.824771523475647, accuracy 0.24166667461395264.\n",
      "Training epoch 3 batch 851 with loss 1.7376861572265625, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 852 with loss 1.7924301624298096, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 853 with loss 1.7576854228973389, accuracy 0.1597222238779068.\n",
      "Training epoch 3 batch 854 with loss 1.7566992044448853, accuracy 0.1805555671453476.\n",
      "Training epoch 3 batch 855 with loss 1.794273018836975, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 856 with loss 1.8834750652313232, accuracy 0.0.\n",
      "Training epoch 3 batch 857 with loss 1.7595773935317993, accuracy 0.30714285373687744.\n",
      "Training epoch 3 batch 858 with loss 1.8382545709609985, accuracy 0.1805555671453476.\n",
      "Training epoch 3 batch 859 with loss 1.7948095798492432, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 860 with loss 1.799822449684143, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 861 with loss 1.8454389572143555, accuracy 0.15833334624767303.\n",
      "Training epoch 3 batch 862 with loss 1.7952344417572021, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 863 with loss 1.7616655826568604, accuracy 0.4027777910232544.\n",
      "Training epoch 3 batch 864 with loss 1.7100766897201538, accuracy 0.125.\n",
      "Training epoch 3 batch 865 with loss 1.6608167886734009, accuracy 0.32499998807907104.\n",
      "Training epoch 3 batch 866 with loss 1.610784888267517, accuracy 0.3263888955116272.\n",
      "Training epoch 3 batch 867 with loss 1.785988211631775, accuracy 0.125.\n",
      "Training epoch 3 batch 868 with loss 1.7966928482055664, accuracy 0.190476194024086.\n",
      "Training epoch 3 batch 869 with loss 1.785325050354004, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 870 with loss 1.8782856464385986, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 871 with loss 1.659695029258728, accuracy 0.3583333492279053.\n",
      "Training epoch 3 batch 872 with loss 1.9244248867034912, accuracy 0.065476194024086.\n",
      "Training epoch 3 batch 873 with loss 1.865894079208374, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 874 with loss 1.7018646001815796, accuracy 0.5277777910232544.\n",
      "Training epoch 3 batch 875 with loss 1.9238907098770142, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 876 with loss 1.8730266094207764, accuracy 0.236111119389534.\n",
      "Training epoch 3 batch 877 with loss 1.7123243808746338, accuracy 0.26944443583488464.\n",
      "Training epoch 3 batch 878 with loss 1.7870585918426514, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 879 with loss 1.763620376586914, accuracy 0.2986111044883728.\n",
      "Training epoch 3 batch 880 with loss 1.9451929330825806, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 881 with loss 1.7277263402938843, accuracy 0.3492063581943512.\n",
      "Training epoch 3 batch 882 with loss 1.7447645664215088, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 883 with loss 1.7710374593734741, accuracy 0.2944444715976715.\n",
      "Training epoch 3 batch 884 with loss 1.876861333847046, accuracy 0.02777777798473835.\n",
      "Training epoch 3 batch 885 with loss 1.8900264501571655, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 886 with loss 1.7732946872711182, accuracy 0.255952388048172.\n",
      "Training epoch 3 batch 887 with loss 1.7735252380371094, accuracy 0.21984127163887024.\n",
      "Training epoch 3 batch 888 with loss 1.8371347188949585, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 889 with loss 1.831109642982483, accuracy 0.0.\n",
      "Training epoch 3 batch 890 with loss 1.763959288597107, accuracy 0.2888889014720917.\n",
      "Training epoch 3 batch 891 with loss 1.6611446142196655, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 892 with loss 1.8514864444732666, accuracy 0.0694444477558136.\n",
      "Training epoch 3 batch 893 with loss 1.8472293615341187, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 894 with loss 1.8228480815887451, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 895 with loss 1.6916944980621338, accuracy 0.30000001192092896.\n",
      "Training epoch 3 batch 896 with loss 1.772684097290039, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 897 with loss 1.9197101593017578, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 898 with loss 1.9349066019058228, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 899 with loss 1.7342965602874756, accuracy 0.2142857164144516.\n",
      "Training epoch 3 batch 900 with loss 1.7042267322540283, accuracy 0.21111111342906952.\n",
      "Training epoch 3 batch 901 with loss 1.8055288791656494, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 902 with loss 1.6726001501083374, accuracy 0.3908730149269104.\n",
      "Training epoch 3 batch 903 with loss 1.7642332315444946, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 904 with loss 1.725250482559204, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 905 with loss 1.804835319519043, accuracy 0.2083333283662796.\n",
      "Training epoch 3 batch 906 with loss 1.7484769821166992, accuracy 0.24166667461395264.\n",
      "Training epoch 3 batch 907 with loss 1.8143514394760132, accuracy 0.3444444537162781.\n",
      "Training epoch 3 batch 908 with loss 1.765404462814331, accuracy 0.30416667461395264.\n",
      "Training epoch 3 batch 909 with loss 1.82608163356781, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 910 with loss 1.7582664489746094, accuracy 0.255952388048172.\n",
      "Training epoch 3 batch 911 with loss 1.7512218952178955, accuracy 0.24722222983837128.\n",
      "Training epoch 3 batch 912 with loss 1.8355249166488647, accuracy 0.06111111491918564.\n",
      "Training epoch 3 batch 913 with loss 1.8687818050384521, accuracy 0.02777777798473835.\n",
      "Training epoch 3 batch 914 with loss 1.8095524311065674, accuracy 0.05416666716337204.\n",
      "Training epoch 3 batch 915 with loss 1.9423551559448242, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 916 with loss 1.8334722518920898, accuracy 0.22500000894069672.\n",
      "Training epoch 3 batch 917 with loss 1.761686086654663, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 918 with loss 1.7610673904418945, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 919 with loss 1.8199455738067627, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 920 with loss 1.7696841955184937, accuracy 0.2638888955116272.\n",
      "Training epoch 3 batch 921 with loss 1.7780663967132568, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 922 with loss 1.930715560913086, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 923 with loss 1.8460782766342163, accuracy 0.05185185372829437.\n",
      "Training epoch 3 batch 924 with loss 1.7493337392807007, accuracy 0.3305555582046509.\n",
      "Training epoch 3 batch 925 with loss 1.7486088275909424, accuracy 0.18611112236976624.\n",
      "Training epoch 3 batch 926 with loss 1.795157790184021, accuracy 0.24722221493721008.\n",
      "Training epoch 3 batch 927 with loss 1.837133765220642, accuracy 0.18611110746860504.\n",
      "Training epoch 3 batch 928 with loss 1.8456528186798096, accuracy 0.25.\n",
      "Training epoch 3 batch 929 with loss 1.7788646221160889, accuracy 0.1626984179019928.\n",
      "Training epoch 3 batch 930 with loss 1.820736289024353, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 931 with loss 1.7286208868026733, accuracy 0.523809552192688.\n",
      "Training epoch 3 batch 932 with loss 1.7153351306915283, accuracy 0.22499999403953552.\n",
      "Training epoch 3 batch 933 with loss 1.7510690689086914, accuracy 0.37222224473953247.\n",
      "Training epoch 3 batch 934 with loss 1.8188928365707397, accuracy 0.125.\n",
      "Training epoch 3 batch 935 with loss 1.701727271080017, accuracy 0.33888891339302063.\n",
      "Training epoch 3 batch 936 with loss 1.6896584033966064, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 937 with loss 1.8345807790756226, accuracy 0.2083333283662796.\n",
      "Training epoch 3 batch 938 with loss 1.7417179346084595, accuracy 0.4166666865348816.\n",
      "Training epoch 3 batch 939 with loss 1.7807973623275757, accuracy 0.12222222983837128.\n",
      "Training epoch 3 batch 940 with loss 1.8082454204559326, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 941 with loss 1.7840341329574585, accuracy 0.125.\n",
      "Training epoch 3 batch 942 with loss 1.8823951482772827, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 943 with loss 1.809569001197815, accuracy 0.144841268658638.\n",
      "Training epoch 3 batch 944 with loss 1.8330827951431274, accuracy 0.2708333432674408.\n",
      "Training epoch 3 batch 945 with loss 1.7424256801605225, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 946 with loss 1.823804497718811, accuracy 0.18611112236976624.\n",
      "Training epoch 3 batch 947 with loss 1.7577991485595703, accuracy 0.36666667461395264.\n",
      "Training epoch 3 batch 948 with loss 1.8373979330062866, accuracy 0.125.\n",
      "Training epoch 3 batch 949 with loss 1.8329912424087524, accuracy 0.10833333432674408.\n",
      "Training epoch 3 batch 950 with loss 1.7925224304199219, accuracy 0.19166667759418488.\n",
      "Training epoch 3 batch 951 with loss 1.7850717306137085, accuracy 0.1597222238779068.\n",
      "Training epoch 3 batch 952 with loss 1.8218380212783813, accuracy 0.0892857164144516.\n",
      "Training epoch 3 batch 953 with loss 1.8247566223144531, accuracy 0.065476194024086.\n",
      "Training epoch 3 batch 954 with loss 1.941788673400879, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 955 with loss 1.7956911325454712, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 956 with loss 1.7692077159881592, accuracy 0.3333333432674408.\n",
      "Training epoch 3 batch 957 with loss 1.746575117111206, accuracy 0.22777777910232544.\n",
      "Training epoch 3 batch 958 with loss 1.8564643859863281, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 959 with loss 1.700731873512268, accuracy 0.29722222685813904.\n",
      "Training epoch 3 batch 960 with loss 1.7500883340835571, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 961 with loss 1.7058532238006592, accuracy 0.4305555820465088.\n",
      "Training epoch 3 batch 962 with loss 1.8351370096206665, accuracy 0.1488095223903656.\n",
      "Training epoch 3 batch 963 with loss 1.9161055088043213, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 964 with loss 1.7771987915039062, accuracy 0.18333333730697632.\n",
      "Training epoch 3 batch 965 with loss 1.7820848226547241, accuracy 0.125.\n",
      "Training epoch 3 batch 966 with loss 1.89816415309906, accuracy 0.2361111044883728.\n",
      "Training epoch 3 batch 967 with loss 1.7528762817382812, accuracy 0.065476194024086.\n",
      "Training epoch 3 batch 968 with loss 1.8872349262237549, accuracy 0.12222222983837128.\n",
      "Training epoch 3 batch 969 with loss 1.7469145059585571, accuracy 0.1805555671453476.\n",
      "Training epoch 3 batch 970 with loss 1.8139560222625732, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 971 with loss 1.9432060718536377, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 972 with loss 1.7804985046386719, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 973 with loss 1.7100881338119507, accuracy 0.15833333134651184.\n",
      "Training epoch 3 batch 974 with loss 1.7107439041137695, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 975 with loss 1.8004415035247803, accuracy 0.32500001788139343.\n",
      "Training epoch 3 batch 976 with loss 1.7550125122070312, accuracy 0.3611111044883728.\n",
      "Training epoch 3 batch 977 with loss 1.71974778175354, accuracy 0.236111119389534.\n",
      "Training epoch 3 batch 978 with loss 1.8264585733413696, accuracy 0.17777778208255768.\n",
      "Training epoch 3 batch 979 with loss 1.773820161819458, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 980 with loss 1.7943052053451538, accuracy 0.19166666269302368.\n",
      "Training epoch 3 batch 981 with loss 1.824148178100586, accuracy 0.16388890147209167.\n",
      "Training epoch 3 batch 982 with loss 1.7615121603012085, accuracy 0.4166666567325592.\n",
      "Training epoch 3 batch 983 with loss 1.7143325805664062, accuracy 0.3222222328186035.\n",
      "Training epoch 3 batch 984 with loss 1.6708751916885376, accuracy 0.1805555522441864.\n",
      "Training epoch 3 batch 985 with loss 1.7685785293579102, accuracy 0.26944446563720703.\n",
      "Training epoch 3 batch 986 with loss 1.8311502933502197, accuracy 0.277777761220932.\n",
      "Training epoch 3 batch 987 with loss 1.791646957397461, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 988 with loss 1.8659340143203735, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 989 with loss 1.872420310974121, accuracy 0.20555555820465088.\n",
      "Training epoch 3 batch 990 with loss 1.8420852422714233, accuracy 0.2083333283662796.\n",
      "Training epoch 3 batch 991 with loss 1.7933623790740967, accuracy 0.2638888955116272.\n",
      "Training epoch 3 batch 992 with loss 1.9424937963485718, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 993 with loss 1.748761773109436, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 994 with loss 1.8307346105575562, accuracy 0.1726190596818924.\n",
      "Training epoch 3 batch 995 with loss 1.782571792602539, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 996 with loss 1.8193060159683228, accuracy 0.21388889849185944.\n",
      "Training epoch 3 batch 997 with loss 1.757520079612732, accuracy 0.23333334922790527.\n",
      "Training epoch 3 batch 998 with loss 1.7088851928710938, accuracy 0.125.\n",
      "Training epoch 3 batch 999 with loss 1.8527352809906006, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 1000 with loss 1.82918381690979, accuracy 0.08888889104127884.\n",
      "Training epoch 3 batch 1001 with loss 1.8569828271865845, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 1002 with loss 1.7881619930267334, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 1003 with loss 1.8316996097564697, accuracy 0.4027777910232544.\n",
      "Training epoch 3 batch 1004 with loss 1.8142915964126587, accuracy 0.2638888955116272.\n",
      "Training epoch 3 batch 1005 with loss 1.7487701177597046, accuracy 0.24722224473953247.\n",
      "Training epoch 3 batch 1006 with loss 1.8979400396347046, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 1007 with loss 1.8241417407989502, accuracy 0.10833333432674408.\n",
      "Training epoch 3 batch 1008 with loss 1.7519779205322266, accuracy 0.2666666805744171.\n",
      "Training epoch 3 batch 1009 with loss 1.6969630718231201, accuracy 0.33888888359069824.\n",
      "Training epoch 3 batch 1010 with loss 1.8587799072265625, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 1011 with loss 1.7239437103271484, accuracy 0.3472222089767456.\n",
      "Training epoch 3 batch 1012 with loss 1.8041670322418213, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 1013 with loss 1.7032743692398071, accuracy 0.2944444417953491.\n",
      "Training epoch 3 batch 1014 with loss 1.845907211303711, accuracy 0.0694444477558136.\n",
      "Training epoch 3 batch 1015 with loss 1.739060640335083, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 1016 with loss 1.7315895557403564, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 1017 with loss 1.809394121170044, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 1018 with loss 1.842276930809021, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 1019 with loss 1.7821680307388306, accuracy 0.13055555522441864.\n",
      "Training epoch 3 batch 1020 with loss 1.8151016235351562, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 1021 with loss 1.800154685974121, accuracy 0.25.\n",
      "Training epoch 3 batch 1022 with loss 1.8290884494781494, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 1023 with loss 1.8506044149398804, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 1024 with loss 1.8096396923065186, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 1025 with loss 1.7303285598754883, accuracy 0.5277777910232544.\n",
      "Training epoch 3 batch 1026 with loss 1.7852470874786377, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 1027 with loss 1.907495141029358, accuracy 0.3263888955116272.\n",
      "Training epoch 3 batch 1028 with loss 1.7288572788238525, accuracy 0.19722223281860352.\n",
      "Training epoch 3 batch 1029 with loss 1.7722904682159424, accuracy 0.31111112236976624.\n",
      "Training epoch 3 batch 1030 with loss 1.6694374084472656, accuracy 0.3166666626930237.\n",
      "Training epoch 3 batch 1031 with loss 1.7377140522003174, accuracy 0.13809524476528168.\n",
      "Training epoch 3 batch 1032 with loss 1.757446050643921, accuracy 0.3611111044883728.\n",
      "Training epoch 3 batch 1033 with loss 1.7842607498168945, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 1034 with loss 1.76310133934021, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 1035 with loss 1.8541240692138672, accuracy 0.13333334028720856.\n",
      "Training epoch 3 batch 1036 with loss 1.8110624551773071, accuracy 0.125.\n",
      "Training epoch 3 batch 1037 with loss 1.729524850845337, accuracy 0.210317462682724.\n",
      "Training epoch 3 batch 1038 with loss 1.823293924331665, accuracy 0.20000001788139343.\n",
      "Training epoch 3 batch 1039 with loss 1.804377555847168, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 1040 with loss 1.7416785955429077, accuracy 0.2420634925365448.\n",
      "Training epoch 3 batch 1041 with loss 1.8090461492538452, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 1042 with loss 1.6963475942611694, accuracy 0.29722222685813904.\n",
      "Training epoch 3 batch 1043 with loss 1.7611687183380127, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 1044 with loss 1.771353006362915, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 1045 with loss 1.7739136219024658, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 1046 with loss 1.8117454051971436, accuracy 0.125.\n",
      "Training epoch 3 batch 1047 with loss 1.8486900329589844, accuracy 0.03333333507180214.\n",
      "Training epoch 3 batch 1048 with loss 1.7959868907928467, accuracy 0.2666666805744171.\n",
      "Training epoch 3 batch 1049 with loss 1.7945520877838135, accuracy 0.28333333134651184.\n",
      "Training epoch 3 batch 1050 with loss 1.7667570114135742, accuracy 0.222222238779068.\n",
      "Training epoch 3 batch 1051 with loss 1.809804916381836, accuracy 0.2182539701461792.\n",
      "Training epoch 3 batch 1052 with loss 1.7150163650512695, accuracy 0.2698412835597992.\n",
      "Training epoch 3 batch 1053 with loss 1.8157720565795898, accuracy 0.2750000059604645.\n",
      "Training epoch 3 batch 1054 with loss 1.8062604665756226, accuracy 0.3194444477558136.\n",
      "Training epoch 3 batch 1055 with loss 1.8060137033462524, accuracy 0.36666667461395264.\n",
      "Training epoch 3 batch 1056 with loss 1.7613885402679443, accuracy 0.2638888955116272.\n",
      "Training epoch 3 batch 1057 with loss 1.8467938899993896, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 1058 with loss 1.8965177536010742, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 1059 with loss 1.746485710144043, accuracy 0.1805555522441864.\n",
      "Training epoch 3 batch 1060 with loss 1.834656000137329, accuracy 0.33888888359069824.\n",
      "Training epoch 3 batch 1061 with loss 1.8564319610595703, accuracy 0.2083333283662796.\n",
      "Training epoch 3 batch 1062 with loss 1.85713791847229, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 1063 with loss 1.7148059606552124, accuracy 0.25.\n",
      "Training epoch 3 batch 1064 with loss 1.7880160808563232, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 1065 with loss 1.8658173084259033, accuracy 0.15833333134651184.\n",
      "Training epoch 3 batch 1066 with loss 1.721112608909607, accuracy 0.24722224473953247.\n",
      "Training epoch 3 batch 1067 with loss 1.8822139501571655, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 1068 with loss 1.7664482593536377, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 1069 with loss 1.7161791324615479, accuracy 0.29629629850387573.\n",
      "Training epoch 3 batch 1070 with loss 1.7583023309707642, accuracy 0.10000000894069672.\n",
      "Training epoch 3 batch 1071 with loss 1.778286337852478, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 1072 with loss 1.898676872253418, accuracy 0.02777777798473835.\n",
      "Training epoch 3 batch 1073 with loss 1.821612000465393, accuracy 0.125.\n",
      "Training epoch 3 batch 1074 with loss 1.8181613683700562, accuracy 0.13055555522441864.\n",
      "Training epoch 3 batch 1075 with loss 1.8392654657363892, accuracy 0.06111111491918564.\n",
      "Training epoch 3 batch 1076 with loss 1.7813184261322021, accuracy 0.2083333283662796.\n",
      "Training epoch 3 batch 1077 with loss 1.8814767599105835, accuracy 0.1458333283662796.\n",
      "Training epoch 3 batch 1078 with loss 1.7438297271728516, accuracy 0.21388888359069824.\n",
      "Training epoch 3 batch 1079 with loss 1.8990345001220703, accuracy 0.23333334922790527.\n",
      "Training epoch 3 batch 1080 with loss 1.7822363376617432, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 1081 with loss 1.7386901378631592, accuracy 0.1825396865606308.\n",
      "Training epoch 3 batch 1082 with loss 1.7571817636489868, accuracy 0.19166666269302368.\n",
      "Training epoch 3 batch 1083 with loss 1.8898084163665771, accuracy 0.0793650820851326.\n",
      "Training epoch 3 batch 1084 with loss 1.748358130455017, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 1085 with loss 1.9339698553085327, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 1086 with loss 1.791672945022583, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 1087 with loss 1.8252872228622437, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 1088 with loss 1.7763683795928955, accuracy 0.3888888955116272.\n",
      "Training epoch 3 batch 1089 with loss 1.817568063735962, accuracy 0.125.\n",
      "Training epoch 3 batch 1090 with loss 1.8621565103530884, accuracy 0.125.\n",
      "Training epoch 3 batch 1091 with loss 1.8976821899414062, accuracy 0.0694444477558136.\n",
      "Training epoch 3 batch 1092 with loss 1.8241395950317383, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 1093 with loss 1.8301633596420288, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 1094 with loss 1.7489814758300781, accuracy 0.30277779698371887.\n",
      "Training epoch 3 batch 1095 with loss 1.7500864267349243, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 1096 with loss 1.7543138265609741, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 1097 with loss 1.8553533554077148, accuracy 0.18611110746860504.\n",
      "Training epoch 3 batch 1098 with loss 1.7365214824676514, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 1099 with loss 1.9098663330078125, accuracy 0.13055555522441864.\n",
      "Training epoch 3 batch 1100 with loss 1.900539755821228, accuracy 0.05416666716337204.\n",
      "Training epoch 3 batch 1101 with loss 1.8542200326919556, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 1102 with loss 1.8283758163452148, accuracy 0.14603175222873688.\n",
      "Training epoch 3 batch 1103 with loss 1.810821533203125, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 1104 with loss 1.7692590951919556, accuracy 0.2638888955116272.\n",
      "Training epoch 3 batch 1105 with loss 1.7843258380889893, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 1106 with loss 1.7236506938934326, accuracy 0.4166666865348816.\n",
      "Training epoch 3 batch 1107 with loss 1.7218940258026123, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 1108 with loss 1.7839505672454834, accuracy 0.3571428656578064.\n",
      "Training epoch 3 batch 1109 with loss 1.6747411489486694, accuracy 0.2916666865348816.\n",
      "Training epoch 3 batch 1110 with loss 1.7985811233520508, accuracy 0.24761904776096344.\n",
      "Training epoch 3 batch 1111 with loss 1.8505945205688477, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 1112 with loss 1.780857801437378, accuracy 0.3055555820465088.\n",
      "Training epoch 3 batch 1113 with loss 1.7786880731582642, accuracy 0.24444444477558136.\n",
      "Training epoch 3 batch 1114 with loss 1.7988979816436768, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 1115 with loss 1.6850624084472656, accuracy 0.319444477558136.\n",
      "Training epoch 3 batch 1116 with loss 1.7605817317962646, accuracy 0.3166666626930237.\n",
      "Training epoch 3 batch 1117 with loss 1.748815894126892, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 1118 with loss 1.878610610961914, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 1119 with loss 1.7163515090942383, accuracy 0.14166668057441711.\n",
      "Training epoch 3 batch 1120 with loss 1.8102422952651978, accuracy 0.3305555582046509.\n",
      "Training epoch 3 batch 1121 with loss 1.7226346731185913, accuracy 0.3670634925365448.\n",
      "Training epoch 3 batch 1122 with loss 1.7081642150878906, accuracy 0.144841268658638.\n",
      "Training epoch 3 batch 1123 with loss 1.8545923233032227, accuracy 0.05714286118745804.\n",
      "Training epoch 3 batch 1124 with loss 1.7972118854522705, accuracy 0.17777778208255768.\n",
      "Training epoch 3 batch 1125 with loss 1.8771520853042603, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 1126 with loss 1.8155956268310547, accuracy 0.25.\n",
      "Training epoch 3 batch 1127 with loss 1.7625356912612915, accuracy 0.28333333134651184.\n",
      "Training epoch 3 batch 1128 with loss 1.7347373962402344, accuracy 0.13055555522441864.\n",
      "Training epoch 3 batch 1129 with loss 1.8631824254989624, accuracy 0.03333333507180214.\n",
      "Training epoch 3 batch 1130 with loss 1.7743219137191772, accuracy 0.1587301641702652.\n",
      "Training epoch 3 batch 1131 with loss 1.7129697799682617, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 1132 with loss 1.840250015258789, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 1133 with loss 1.753375768661499, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 1134 with loss 1.7807369232177734, accuracy 0.14761905372142792.\n",
      "Training epoch 3 batch 1135 with loss 1.6975399255752563, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 1136 with loss 1.7614762783050537, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 1137 with loss 1.8120380640029907, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 1138 with loss 1.7566096782684326, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 1139 with loss 1.892660140991211, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 1140 with loss 1.7662160396575928, accuracy 0.1071428582072258.\n",
      "Training epoch 3 batch 1141 with loss 1.7121856212615967, accuracy 0.28333333134651184.\n",
      "Training epoch 3 batch 1142 with loss 1.8302828073501587, accuracy 0.19603174924850464.\n",
      "Training epoch 3 batch 1143 with loss 1.7623990774154663, accuracy 0.21111111342906952.\n",
      "Training epoch 3 batch 1144 with loss 1.7951568365097046, accuracy 0.222222238779068.\n",
      "Training epoch 3 batch 1145 with loss 1.7930545806884766, accuracy 0.15555556118488312.\n",
      "Training epoch 3 batch 1146 with loss 1.9107242822647095, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 1147 with loss 1.7354656457901, accuracy 0.23888888955116272.\n",
      "Training epoch 3 batch 1148 with loss 1.869236707687378, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 1149 with loss 1.7299010753631592, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 1150 with loss 1.811303734779358, accuracy 0.22499999403953552.\n",
      "Training epoch 3 batch 1151 with loss 1.816239595413208, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 1152 with loss 1.7701473236083984, accuracy 0.22727273404598236.\n",
      "Training epoch 3 batch 1153 with loss 1.8281831741333008, accuracy 0.31111112236976624.\n",
      "Training epoch 3 batch 1154 with loss 1.8705542087554932, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 1155 with loss 1.8749157190322876, accuracy 0.0.\n",
      "Training epoch 3 batch 1156 with loss 1.8491147756576538, accuracy 0.16428571939468384.\n",
      "Training epoch 3 batch 1157 with loss 1.8424545526504517, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 1158 with loss 1.7346251010894775, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 1159 with loss 1.714777946472168, accuracy 0.25833335518836975.\n",
      "Training epoch 3 batch 1160 with loss 1.871931791305542, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 1161 with loss 1.7185392379760742, accuracy 0.4138889014720917.\n",
      "Training epoch 3 batch 1162 with loss 1.8403329849243164, accuracy 0.28333333134651184.\n",
      "Training epoch 3 batch 1163 with loss 1.737811803817749, accuracy 0.22500000894069672.\n",
      "Training epoch 3 batch 1164 with loss 1.7610464096069336, accuracy 0.2857142984867096.\n",
      "Training epoch 3 batch 1165 with loss 1.702901840209961, accuracy 0.4305555820465088.\n",
      "Training epoch 3 batch 1166 with loss 1.7037633657455444, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 1167 with loss 1.8602374792099, accuracy 0.10833333432674408.\n",
      "Training epoch 3 batch 1168 with loss 1.6365547180175781, accuracy 0.4424603283405304.\n",
      "Training epoch 3 batch 1169 with loss 1.756879210472107, accuracy 0.3611111044883728.\n",
      "Training epoch 3 batch 1170 with loss 1.6713920831680298, accuracy 0.4888889193534851.\n",
      "Training epoch 3 batch 1171 with loss 1.7229058742523193, accuracy 0.3361110985279083.\n",
      "Training epoch 3 batch 1172 with loss 1.6879791021347046, accuracy 0.3888888657093048.\n",
      "Training epoch 3 batch 1173 with loss 1.7106271982192993, accuracy 0.3402777910232544.\n",
      "Training epoch 3 batch 1174 with loss 1.8056144714355469, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 1175 with loss 1.8283395767211914, accuracy 0.08888889104127884.\n",
      "Training epoch 3 batch 1176 with loss 1.689602255821228, accuracy 0.375.\n",
      "Training epoch 3 batch 1177 with loss 1.7008672952651978, accuracy 0.25.\n",
      "Training epoch 3 batch 1178 with loss 1.872066855430603, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 1179 with loss 1.831454873085022, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 1180 with loss 1.8558533191680908, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 1181 with loss 1.8075300455093384, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 1182 with loss 1.8665597438812256, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 1183 with loss 1.7542402744293213, accuracy 0.08888889104127884.\n",
      "Training epoch 3 batch 1184 with loss 1.7859312295913696, accuracy 0.3166666626930237.\n",
      "Training epoch 3 batch 1185 with loss 1.867990255355835, accuracy 0.06111111491918564.\n",
      "Training epoch 3 batch 1186 with loss 1.7316162586212158, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 1187 with loss 1.746556043624878, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 1188 with loss 1.8724225759506226, accuracy 0.10833333432674408.\n",
      "Training epoch 3 batch 1189 with loss 1.8502719402313232, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 1190 with loss 1.7674338817596436, accuracy 0.20555555820465088.\n",
      "Training epoch 3 batch 1191 with loss 1.7769695520401, accuracy 0.10833333432674408.\n",
      "Training epoch 3 batch 1192 with loss 1.8194249868392944, accuracy 0.144841268658638.\n",
      "Training epoch 3 batch 1193 with loss 1.815382719039917, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 1194 with loss 1.7176580429077148, accuracy 0.3444444537162781.\n",
      "Training epoch 3 batch 1195 with loss 1.8475345373153687, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 1196 with loss 1.8772964477539062, accuracy 0.125.\n",
      "Training epoch 3 batch 1197 with loss 1.85906183719635, accuracy 0.18333333730697632.\n",
      "Training epoch 3 batch 1198 with loss 1.757232904434204, accuracy 0.08095238357782364.\n",
      "Training epoch 3 batch 1199 with loss 1.7067043781280518, accuracy 0.4000000059604645.\n",
      "Training epoch 3 batch 1200 with loss 1.8492622375488281, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 1201 with loss 1.7321974039077759, accuracy 0.3333333432674408.\n",
      "Training epoch 3 batch 1202 with loss 1.761387586593628, accuracy 0.10277777910232544.\n",
      "Training epoch 3 batch 1203 with loss 1.8084694147109985, accuracy 0.12222222983837128.\n",
      "Training epoch 3 batch 1204 with loss 1.8191579580307007, accuracy 0.2361111044883728.\n",
      "Training epoch 3 batch 1205 with loss 1.9180835485458374, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 1206 with loss 1.7851861715316772, accuracy 0.21111111342906952.\n",
      "Training epoch 3 batch 1207 with loss 1.7830841541290283, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 1208 with loss 1.8448617458343506, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 1209 with loss 1.7675434350967407, accuracy 0.21388889849185944.\n",
      "Training epoch 3 batch 1210 with loss 1.7207624912261963, accuracy 0.22777777910232544.\n",
      "Training epoch 3 batch 1211 with loss 1.708728551864624, accuracy 0.319444477558136.\n",
      "Training epoch 3 batch 1212 with loss 1.7597742080688477, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 1213 with loss 1.683908462524414, accuracy 0.3333333432674408.\n",
      "Training epoch 3 batch 1214 with loss 1.7973573207855225, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 1215 with loss 1.8788913488388062, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 1216 with loss 1.6970192193984985, accuracy 0.347222238779068.\n",
      "Training epoch 3 batch 1217 with loss 1.836007833480835, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 1218 with loss 1.8239123821258545, accuracy 0.0476190485060215.\n",
      "Training epoch 3 batch 1219 with loss 1.8582820892333984, accuracy 0.2361111044883728.\n",
      "Training epoch 3 batch 1220 with loss 1.7759157419204712, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 1221 with loss 1.7256759405136108, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 1222 with loss 1.8629436492919922, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 1223 with loss 1.7379690408706665, accuracy 0.21388888359069824.\n",
      "Training epoch 3 batch 1224 with loss 1.8569133281707764, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 1225 with loss 1.813800573348999, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 1226 with loss 1.790034532546997, accuracy 0.2142857164144516.\n",
      "Training epoch 3 batch 1227 with loss 1.8042045831680298, accuracy 0.06111111491918564.\n",
      "Training epoch 3 batch 1228 with loss 1.7541040182113647, accuracy 0.2361111044883728.\n",
      "Training epoch 3 batch 1229 with loss 1.8762861490249634, accuracy 0.02777777798473835.\n",
      "Training epoch 3 batch 1230 with loss 1.8527615070343018, accuracy 0.0.\n",
      "Training epoch 3 batch 1231 with loss 1.7288408279418945, accuracy 0.13055555522441864.\n",
      "Training epoch 3 batch 1232 with loss 1.81259024143219, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 1233 with loss 1.7637701034545898, accuracy 0.125.\n",
      "Training epoch 3 batch 1234 with loss 1.7992591857910156, accuracy 0.2460317462682724.\n",
      "Training epoch 3 batch 1235 with loss 1.8133121728897095, accuracy 0.17777778208255768.\n",
      "Training epoch 3 batch 1236 with loss 1.835953950881958, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 1237 with loss 1.7807743549346924, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 1238 with loss 1.790299654006958, accuracy 0.2750000059604645.\n",
      "Training epoch 3 batch 1239 with loss 1.813295602798462, accuracy 0.111111119389534.\n",
      "Training epoch 3 batch 1240 with loss 1.7458873987197876, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 1241 with loss 1.8125135898590088, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 1242 with loss 1.7733981609344482, accuracy 0.20555555820465088.\n",
      "Training epoch 3 batch 1243 with loss 1.9151617288589478, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 1244 with loss 1.7919847965240479, accuracy 0.03333333507180214.\n",
      "Training epoch 3 batch 1245 with loss 1.800084114074707, accuracy 0.329365074634552.\n",
      "Training epoch 3 batch 1246 with loss 1.6985981464385986, accuracy 0.4000000059604645.\n",
      "Training epoch 3 batch 1247 with loss 1.7721986770629883, accuracy 0.2888889014720917.\n",
      "Training epoch 3 batch 1248 with loss 1.7859094142913818, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 1249 with loss 1.769060492515564, accuracy 0.25.\n",
      "Training epoch 3 batch 1250 with loss 1.7228227853775024, accuracy 0.32499998807907104.\n",
      "Training epoch 3 batch 1251 with loss 1.8354394435882568, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 1252 with loss 1.8771575689315796, accuracy 0.15833333134651184.\n",
      "Training epoch 3 batch 1253 with loss 1.7691318988800049, accuracy 0.21388888359069824.\n",
      "Training epoch 3 batch 1254 with loss 1.7943700551986694, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 1255 with loss 1.900048017501831, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 1256 with loss 1.7534778118133545, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 1257 with loss 1.8075687885284424, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 1258 with loss 1.845393180847168, accuracy 0.1031746044754982.\n",
      "Training epoch 3 batch 1259 with loss 1.7893708944320679, accuracy 0.28333333134651184.\n",
      "Training epoch 3 batch 1260 with loss 1.9331010580062866, accuracy 0.125.\n",
      "Training epoch 3 batch 1261 with loss 1.7785695791244507, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 1262 with loss 1.8634525537490845, accuracy 0.1805555671453476.\n",
      "Training epoch 3 batch 1263 with loss 1.823144555091858, accuracy 0.09583333134651184.\n",
      "Training epoch 3 batch 1264 with loss 1.8286733627319336, accuracy 0.19166666269302368.\n",
      "Training epoch 3 batch 1265 with loss 1.881666898727417, accuracy 0.125.\n",
      "Training epoch 3 batch 1266 with loss 1.6214383840560913, accuracy 0.43611112236976624.\n",
      "Training epoch 3 batch 1267 with loss 1.8368993997573853, accuracy 0.12222222983837128.\n",
      "Training epoch 3 batch 1268 with loss 1.799443244934082, accuracy 0.2361111044883728.\n",
      "Training epoch 3 batch 1269 with loss 1.7875295877456665, accuracy 0.2916666567325592.\n",
      "Training epoch 3 batch 1270 with loss 1.7301524877548218, accuracy 0.24166667461395264.\n",
      "Training epoch 3 batch 1271 with loss 1.8508119583129883, accuracy 0.12222222983837128.\n",
      "Training epoch 3 batch 1272 with loss 1.9217621088027954, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 1273 with loss 1.74581778049469, accuracy 0.25.\n",
      "Training epoch 3 batch 1274 with loss 1.6787941455841064, accuracy 0.22685185074806213.\n",
      "Training epoch 3 batch 1275 with loss 1.765856385231018, accuracy 0.222222238779068.\n",
      "Training epoch 3 batch 1276 with loss 1.7811781167984009, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 1277 with loss 1.8159058094024658, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 1278 with loss 1.9099668264389038, accuracy 0.1071428582072258.\n",
      "Training epoch 3 batch 1279 with loss 1.7277114391326904, accuracy 0.2666666507720947.\n",
      "Training epoch 3 batch 1280 with loss 1.8746750354766846, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 1281 with loss 1.7892497777938843, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 1282 with loss 1.8337723016738892, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 1283 with loss 1.7975019216537476, accuracy 0.10000000149011612.\n",
      "Training epoch 3 batch 1284 with loss 1.9019298553466797, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 1285 with loss 1.8891664743423462, accuracy 0.2380952537059784.\n",
      "Training epoch 3 batch 1286 with loss 1.836920142173767, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 1287 with loss 1.8853771686553955, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 1288 with loss 1.854073166847229, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 1289 with loss 1.7873566150665283, accuracy 0.3541666865348816.\n",
      "Training epoch 3 batch 1290 with loss 1.7769289016723633, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 1291 with loss 1.7555522918701172, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 1292 with loss 1.877246618270874, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 1293 with loss 1.7553199529647827, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 1294 with loss 1.8524255752563477, accuracy 0.2916666567325592.\n",
      "Training epoch 3 batch 1295 with loss 1.848860502243042, accuracy 0.12222222983837128.\n",
      "Training epoch 3 batch 1296 with loss 1.8738081455230713, accuracy 0.3055555522441864.\n",
      "Training epoch 3 batch 1297 with loss 1.7196565866470337, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 1298 with loss 1.764998435974121, accuracy 0.18333333730697632.\n",
      "Training epoch 3 batch 1299 with loss 1.9455054998397827, accuracy 0.02777777798473835.\n",
      "Training epoch 3 batch 1300 with loss 1.8527052402496338, accuracy 0.13055555522441864.\n",
      "Training epoch 3 batch 1301 with loss 1.8280118703842163, accuracy 0.21111111342906952.\n",
      "Training epoch 3 batch 1302 with loss 1.806727647781372, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 1303 with loss 1.7302592992782593, accuracy 0.20714285969734192.\n",
      "Training epoch 3 batch 1304 with loss 1.871577501296997, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 1305 with loss 1.8480043411254883, accuracy 0.13055555522441864.\n",
      "Training epoch 3 batch 1306 with loss 1.8886016607284546, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 1307 with loss 1.7065455913543701, accuracy 0.1726190447807312.\n",
      "Training epoch 3 batch 1308 with loss 1.8861134052276611, accuracy 0.0.\n",
      "Training epoch 3 batch 1309 with loss 1.7794373035430908, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 1310 with loss 1.8241243362426758, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 1311 with loss 1.7457793951034546, accuracy 0.3055555522441864.\n",
      "Training epoch 3 batch 1312 with loss 1.8351075649261475, accuracy 0.0892857164144516.\n",
      "Training epoch 3 batch 1313 with loss 1.7104825973510742, accuracy 0.2083333283662796.\n",
      "Training epoch 3 batch 1314 with loss 1.8568305969238281, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 1315 with loss 1.822105050086975, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 1316 with loss 1.7095155715942383, accuracy 0.25.\n",
      "Training epoch 3 batch 1317 with loss 1.6940200328826904, accuracy 0.0892857164144516.\n",
      "Training epoch 3 batch 1318 with loss 1.777435064315796, accuracy 0.125.\n",
      "Training epoch 3 batch 1319 with loss 1.7649469375610352, accuracy 0.3432539701461792.\n",
      "Training epoch 3 batch 1320 with loss 1.8687286376953125, accuracy 0.15833333134651184.\n",
      "Training epoch 3 batch 1321 with loss 1.6773170232772827, accuracy 0.472222238779068.\n",
      "Training epoch 3 batch 1322 with loss 1.8691028356552124, accuracy 0.10277777910232544.\n",
      "Training epoch 3 batch 1323 with loss 1.8759231567382812, accuracy 0.1805555522441864.\n",
      "Training epoch 3 batch 1324 with loss 1.80801522731781, accuracy 0.1527777910232544.\n",
      "Training epoch 3 batch 1325 with loss 1.8478370904922485, accuracy 0.10833333432674408.\n",
      "Training epoch 3 batch 1326 with loss 1.865928292274475, accuracy 0.3055555522441864.\n",
      "Training epoch 3 batch 1327 with loss 1.752298355102539, accuracy 0.3194444477558136.\n",
      "Training epoch 3 batch 1328 with loss 1.6664797067642212, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 1329 with loss 1.7568985223770142, accuracy 0.1626984179019928.\n",
      "Training epoch 3 batch 1330 with loss 1.8847379684448242, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 1331 with loss 1.792949914932251, accuracy 0.22499999403953552.\n",
      "Training epoch 3 batch 1332 with loss 1.8294941186904907, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 1333 with loss 1.6800096035003662, accuracy 0.2638888955116272.\n",
      "Training epoch 3 batch 1334 with loss 1.7750202417373657, accuracy 0.3611111342906952.\n",
      "Training epoch 3 batch 1335 with loss 1.8613157272338867, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 1336 with loss 1.8523836135864258, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 1337 with loss 1.7528393268585205, accuracy 0.2750000059604645.\n",
      "Training epoch 3 batch 1338 with loss 1.781087875366211, accuracy 0.1130952388048172.\n",
      "Training epoch 3 batch 1339 with loss 1.7709089517593384, accuracy 0.3444444537162781.\n",
      "Training epoch 3 batch 1340 with loss 1.7220872640609741, accuracy 0.21111111342906952.\n",
      "Training epoch 3 batch 1341 with loss 1.8043272495269775, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 1342 with loss 1.795914888381958, accuracy 0.1349206417798996.\n",
      "Training epoch 3 batch 1343 with loss 1.9164451360702515, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 1344 with loss 1.7381064891815186, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 1345 with loss 1.885656714439392, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 1346 with loss 1.7871503829956055, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 1347 with loss 1.7924190759658813, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 1348 with loss 1.8682305812835693, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 1349 with loss 1.729465126991272, accuracy 0.3861111104488373.\n",
      "Training epoch 3 batch 1350 with loss 1.834280252456665, accuracy 0.18888889253139496.\n",
      "Training epoch 3 batch 1351 with loss 1.7491445541381836, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 1352 with loss 1.8922710418701172, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 1353 with loss 1.857177495956421, accuracy 0.2777777910232544.\n",
      "Training epoch 3 batch 1354 with loss 1.740478754043579, accuracy 0.1458333432674408.\n",
      "Training epoch 3 batch 1355 with loss 1.688709020614624, accuracy 0.5416666865348816.\n",
      "Training epoch 3 batch 1356 with loss 1.850053071975708, accuracy 0.20000000298023224.\n",
      "Training epoch 3 batch 1357 with loss 1.7790600061416626, accuracy 0.2666666805744171.\n",
      "Training epoch 3 batch 1358 with loss 1.7475255727767944, accuracy 0.2281745970249176.\n",
      "Training epoch 3 batch 1359 with loss 1.7635482549667358, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 1360 with loss 1.8421424627304077, accuracy 0.2638888955116272.\n",
      "Training epoch 3 batch 1361 with loss 1.678789496421814, accuracy 0.32500001788139343.\n",
      "Training epoch 3 batch 1362 with loss 1.7909772396087646, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 1363 with loss 1.8992000818252563, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 1364 with loss 1.7883402109146118, accuracy 0.19166666269302368.\n",
      "Training epoch 3 batch 1365 with loss 1.769806146621704, accuracy 0.1388888955116272.\n",
      "Training epoch 3 batch 1366 with loss 1.8271465301513672, accuracy 0.1805555522441864.\n",
      "Training epoch 3 batch 1367 with loss 1.74066162109375, accuracy 0.3055555522441864.\n",
      "Training epoch 3 batch 1368 with loss 1.8757950067520142, accuracy 0.1527777910232544.\n",
      "Training epoch 3 batch 1369 with loss 1.809340476989746, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 1370 with loss 1.7626781463623047, accuracy 0.236111119389534.\n",
      "Training epoch 3 batch 1371 with loss 1.731000304222107, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 1372 with loss 1.8064262866973877, accuracy 0.31111112236976624.\n",
      "Training epoch 3 batch 1373 with loss 1.7117481231689453, accuracy 0.2527777850627899.\n",
      "Training epoch 3 batch 1374 with loss 1.9061639308929443, accuracy 0.23333333432674408.\n",
      "Training epoch 3 batch 1375 with loss 1.7514870166778564, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 1376 with loss 1.8480517864227295, accuracy 0.21388888359069824.\n",
      "Training epoch 3 batch 1377 with loss 1.6732356548309326, accuracy 0.2944444417953491.\n",
      "Training epoch 3 batch 1378 with loss 1.8347320556640625, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 1379 with loss 1.8020992279052734, accuracy 0.14603175222873688.\n",
      "Training epoch 3 batch 1380 with loss 1.8611862659454346, accuracy 0.1666666865348816.\n",
      "Training epoch 3 batch 1381 with loss 1.868024230003357, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 1382 with loss 1.8156757354736328, accuracy 0.30158731341362.\n",
      "Training epoch 3 batch 1383 with loss 1.8148698806762695, accuracy 0.15833334624767303.\n",
      "Training epoch 3 batch 1384 with loss 1.8430652618408203, accuracy 0.1031746044754982.\n",
      "Training epoch 3 batch 1385 with loss 1.8236579895019531, accuracy 0.31111112236976624.\n",
      "Training epoch 3 batch 1386 with loss 1.7639652490615845, accuracy 0.2916666865348816.\n",
      "Training epoch 3 batch 1387 with loss 1.950553297996521, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 1388 with loss 1.8054955005645752, accuracy 0.08888889104127884.\n",
      "Training epoch 3 batch 1389 with loss 1.8563482761383057, accuracy 0.2638888955116272.\n",
      "Training epoch 3 batch 1390 with loss 1.9796040058135986, accuracy 0.0.\n",
      "Training epoch 3 batch 1391 with loss 1.8020375967025757, accuracy 0.19722223281860352.\n",
      "Training epoch 3 batch 1392 with loss 1.7477144002914429, accuracy 0.2500000298023224.\n",
      "Training epoch 3 batch 1393 with loss 1.7274326086044312, accuracy 0.3583333492279053.\n",
      "Training epoch 3 batch 1394 with loss 1.751501441001892, accuracy 0.1875.\n",
      "Training epoch 3 batch 1395 with loss 1.798553228378296, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 1396 with loss 1.8749034404754639, accuracy 0.0793650820851326.\n",
      "Training epoch 3 batch 1397 with loss 1.8082977533340454, accuracy 0.1597222238779068.\n",
      "Training epoch 3 batch 1398 with loss 1.7501624822616577, accuracy 0.1488095223903656.\n",
      "Training epoch 3 batch 1399 with loss 1.8284937143325806, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 1400 with loss 1.7524223327636719, accuracy 0.3055555820465088.\n",
      "Training epoch 3 batch 1401 with loss 1.8162415027618408, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 1402 with loss 1.7990297079086304, accuracy 0.18611110746860504.\n",
      "Training epoch 3 batch 1403 with loss 1.9595136642456055, accuracy 0.02777777798473835.\n",
      "Training epoch 3 batch 1404 with loss 1.6895256042480469, accuracy 0.21388888359069824.\n",
      "Training epoch 3 batch 1405 with loss 1.7567358016967773, accuracy 0.15833333134651184.\n",
      "Training epoch 3 batch 1406 with loss 1.7522512674331665, accuracy 0.25.\n",
      "Training epoch 3 batch 1407 with loss 1.797065019607544, accuracy 0.0972222238779068.\n",
      "Training epoch 3 batch 1408 with loss 1.7619001865386963, accuracy 0.20555555820465088.\n",
      "Training epoch 3 batch 1409 with loss 1.7966365814208984, accuracy 0.0476190485060215.\n",
      "Training epoch 3 batch 1410 with loss 1.8666845560073853, accuracy 0.16388888657093048.\n",
      "Training epoch 3 batch 1411 with loss 1.8733994960784912, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 1412 with loss 1.749935507774353, accuracy 0.2750000059604645.\n",
      "Training epoch 3 batch 1413 with loss 1.876161813735962, accuracy 0.11269842088222504.\n",
      "Training epoch 3 batch 1414 with loss 1.7887637615203857, accuracy 0.1805555671453476.\n",
      "Training epoch 3 batch 1415 with loss 1.8666391372680664, accuracy 0.3055555522441864.\n",
      "Training epoch 3 batch 1416 with loss 1.734468698501587, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 1417 with loss 1.7723560333251953, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 1418 with loss 1.7779009342193604, accuracy 0.13214285671710968.\n",
      "Training epoch 3 batch 1419 with loss 1.8325040340423584, accuracy 0.125.\n",
      "Training epoch 3 batch 1420 with loss 1.7762025594711304, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 1421 with loss 1.8017174005508423, accuracy 0.1765872985124588.\n",
      "Training epoch 3 batch 1422 with loss 1.89670729637146, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 1423 with loss 1.7720005512237549, accuracy 0.12222222983837128.\n",
      "Training epoch 3 batch 1424 with loss 1.7344815731048584, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 1425 with loss 1.7019634246826172, accuracy 0.25555557012557983.\n",
      "Training epoch 3 batch 1426 with loss 1.7428350448608398, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 1427 with loss 1.733138084411621, accuracy 0.3638889193534851.\n",
      "Training epoch 3 batch 1428 with loss 1.7158071994781494, accuracy 0.22777777910232544.\n",
      "Training epoch 3 batch 1429 with loss 1.9018453359603882, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 1430 with loss 1.8666908740997314, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 1431 with loss 1.7720867395401, accuracy 0.25.\n",
      "Training epoch 3 batch 1432 with loss 1.7603371143341064, accuracy 0.11666667461395264.\n",
      "Training epoch 3 batch 1433 with loss 1.8342689275741577, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 1434 with loss 1.865390419960022, accuracy 0.19722223281860352.\n",
      "Training epoch 3 batch 1435 with loss 1.8874073028564453, accuracy 0.0555555559694767.\n",
      "Training epoch 3 batch 1436 with loss 1.7944892644882202, accuracy 0.3611111342906952.\n",
      "Training epoch 3 batch 1437 with loss 1.797734260559082, accuracy 0.25.\n",
      "Training epoch 3 batch 1438 with loss 1.8126647472381592, accuracy 0.24722222983837128.\n",
      "Training epoch 3 batch 1439 with loss 1.7202908992767334, accuracy 0.0833333358168602.\n",
      "Training epoch 3 batch 1440 with loss 1.8561464548110962, accuracy 0.1041666716337204.\n",
      "Training epoch 3 batch 1441 with loss 1.7520086765289307, accuracy 0.2611111104488373.\n",
      "Training epoch 3 batch 1442 with loss 1.7682440280914307, accuracy 0.19722221791744232.\n",
      "Training epoch 3 batch 1443 with loss 1.7753674983978271, accuracy 0.15833334624767303.\n",
      "Training epoch 3 batch 1444 with loss 1.829904317855835, accuracy 0.02380952425301075.\n",
      "Training epoch 3 batch 1445 with loss 1.766037940979004, accuracy 0.2638888955116272.\n",
      "Training epoch 3 batch 1446 with loss 1.8278789520263672, accuracy 0.15833333134651184.\n",
      "Training epoch 3 batch 1447 with loss 1.90696120262146, accuracy 0.125.\n",
      "Training epoch 3 batch 1448 with loss 1.8365100622177124, accuracy 0.14444445073604584.\n",
      "Training epoch 3 batch 1449 with loss 1.8106273412704468, accuracy 0.12777778506278992.\n",
      "Training epoch 3 batch 1450 with loss 1.8126871585845947, accuracy 0.4166666567325592.\n",
      "Training epoch 3 batch 1451 with loss 1.8183425664901733, accuracy 0.125.\n",
      "Training epoch 3 batch 1452 with loss 1.7440319061279297, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 1453 with loss 1.7040221691131592, accuracy 0.26944443583488464.\n",
      "Training epoch 3 batch 1454 with loss 1.865229606628418, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 1455 with loss 1.7506418228149414, accuracy 0.3861111104488373.\n",
      "Training epoch 3 batch 1456 with loss 1.8245315551757812, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 1457 with loss 1.772059679031372, accuracy 0.222222238779068.\n",
      "Training epoch 3 batch 1458 with loss 1.8308181762695312, accuracy 0.21388888359069824.\n",
      "Training epoch 3 batch 1459 with loss 1.8243582248687744, accuracy 0.2222222238779068.\n",
      "Training epoch 3 batch 1460 with loss 1.8294645547866821, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 1461 with loss 1.697135329246521, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 1462 with loss 1.861802101135254, accuracy 0.0.\n",
      "Training epoch 3 batch 1463 with loss 1.719614028930664, accuracy 0.1666666716337204.\n",
      "Training epoch 3 batch 1464 with loss 1.7921949625015259, accuracy 0.2666666805744171.\n",
      "Training epoch 3 batch 1465 with loss 1.8170255422592163, accuracy 0.204365074634552.\n",
      "Training epoch 3 batch 1466 with loss 1.7866566181182861, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 1467 with loss 1.7941917181015015, accuracy 0.1527777761220932.\n",
      "Training epoch 3 batch 1468 with loss 1.7160078287124634, accuracy 0.28333336114883423.\n",
      "Training epoch 3 batch 1469 with loss 1.7821649312973022, accuracy 0.05714286118745804.\n",
      "Training epoch 3 batch 1470 with loss 1.7121225595474243, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 1471 with loss 1.7932288646697998, accuracy 0.20555555820465088.\n",
      "Training epoch 3 batch 1472 with loss 1.8609193563461304, accuracy 0.1527777910232544.\n",
      "Training epoch 3 batch 1473 with loss 1.7905489206314087, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 1474 with loss 1.8381128311157227, accuracy 0.236111119389534.\n",
      "Training epoch 3 batch 1475 with loss 1.791293740272522, accuracy 0.18611110746860504.\n",
      "Training epoch 3 batch 1476 with loss 1.8139894008636475, accuracy 0.24761904776096344.\n",
      "Training epoch 3 batch 1477 with loss 1.8335622549057007, accuracy 0.1944444477558136.\n",
      "Training epoch 3 batch 1478 with loss 1.8188835382461548, accuracy 0.1031746044754982.\n",
      "Training epoch 3 batch 1479 with loss 1.7832673788070679, accuracy 0.2083333432674408.\n",
      "Training epoch 3 batch 1480 with loss 1.8231875896453857, accuracy 0.18611112236976624.\n",
      "Training epoch 3 batch 1481 with loss 1.7573955059051514, accuracy 0.2916666865348816.\n",
      "Training epoch 3 batch 1482 with loss 1.7020971775054932, accuracy 0.18611110746860504.\n",
      "Training epoch 3 batch 1483 with loss 1.6914774179458618, accuracy 0.3166666626930237.\n",
      "Training epoch 3 batch 1484 with loss 1.810042142868042, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 1485 with loss 1.8715028762817383, accuracy 0.06666667014360428.\n",
      "Training epoch 3 batch 1486 with loss 1.8320598602294922, accuracy 0.0.\n",
      "Training epoch 3 batch 1487 with loss 1.7628523111343384, accuracy 0.329365074634552.\n",
      "Training epoch 3 batch 1488 with loss 1.7383617162704468, accuracy 0.17936508357524872.\n",
      "Training epoch 3 batch 1489 with loss 1.8209835290908813, accuracy 0.17222222685813904.\n",
      "Training epoch 3 batch 1490 with loss 1.7346770763397217, accuracy 0.3055555522441864.\n",
      "Training epoch 3 batch 1491 with loss 1.8492600917816162, accuracy 0.1111111119389534.\n",
      "Training epoch 3 batch 1492 with loss 1.8466678857803345, accuracy 0.15000000596046448.\n",
      "Training epoch 3 batch 1493 with loss 1.7377233505249023, accuracy 0.0416666679084301.\n",
      "Training epoch 3 batch 1494 with loss 1.8764053583145142, accuracy 0.09444444626569748.\n",
      "Training epoch 3 batch 1495 with loss 1.6701500415802002, accuracy 0.4015873074531555.\n",
      "Training epoch 3 batch 1496 with loss 1.7930055856704712, accuracy 0.22499999403953552.\n",
      "Training epoch 3 batch 1497 with loss 1.8953838348388672, accuracy 0.07500000298023224.\n",
      "Training epoch 3 batch 1498 with loss 1.8632723093032837, accuracy 0.1458333432674408.\n",
      "Training epoch 3 batch 1499 with loss 1.7286193370819092, accuracy 0.4166666865348816.\n",
      "Test batch 0 with loss 1.8852275609970093 and accuracy 0.0416666679084301.\n",
      "Test batch 1 with loss 1.7985397577285767 and accuracy 0.2460317462682724.\n",
      "Test batch 2 with loss 1.9589955806732178 and accuracy 0.02380952425301075.\n",
      "Test batch 3 with loss 1.7965662479400635 and accuracy 0.0833333358168602.\n",
      "Test batch 4 with loss 1.9651257991790771 and accuracy 0.2599206566810608.\n",
      "Test batch 5 with loss 1.730320692062378 and accuracy 0.1666666716337204.\n",
      "Test batch 6 with loss 1.8589576482772827 and accuracy 0.0625.\n",
      "Test batch 7 with loss 1.7698867321014404 and accuracy 0.18333333730697632.\n",
      "Test batch 8 with loss 1.8759781122207642 and accuracy 0.0833333358168602.\n",
      "Test batch 9 with loss 1.8426660299301147 and accuracy 0.1349206417798996.\n",
      "Test batch 10 with loss 1.900573968887329 and accuracy 0.08888889104127884.\n",
      "Test batch 11 with loss 1.82724928855896 and accuracy 0.1875000149011612.\n",
      "Test batch 12 with loss 1.9164844751358032 and accuracy 0.20555555820465088.\n",
      "Test batch 13 with loss 1.9302984476089478 and accuracy 0.0555555559694767.\n",
      "Test batch 14 with loss 1.8718513250350952 and accuracy 0.18333333730697632.\n",
      "Test batch 15 with loss 1.7403360605239868 and accuracy 0.2777777910232544.\n",
      "Test batch 16 with loss 1.7890903949737549 and accuracy 0.08888889104127884.\n",
      "Test batch 17 with loss 1.8027315139770508 and accuracy 0.26944446563720703.\n",
      "Test batch 18 with loss 1.7430346012115479 and accuracy 0.12261904776096344.\n",
      "Test batch 19 with loss 1.8118044137954712 and accuracy 0.10000000894069672.\n",
      "Test batch 20 with loss 1.7176759243011475 and accuracy 0.4861111342906952.\n",
      "Test batch 21 with loss 1.7925870418548584 and accuracy 0.1944444477558136.\n",
      "Test batch 22 with loss 1.752503752708435 and accuracy 0.33888888359069824.\n",
      "Test batch 23 with loss 1.9153079986572266 and accuracy 0.02777777798473835.\n",
      "Test batch 24 with loss 1.9097257852554321 and accuracy 0.03333333507180214.\n",
      "Test batch 25 with loss 1.8339202404022217 and accuracy 0.07500000298023224.\n",
      "Test batch 26 with loss 1.7551069259643555 and accuracy 0.24166667461395264.\n",
      "Test batch 27 with loss 1.8109979629516602 and accuracy 0.0833333358168602.\n",
      "Test batch 28 with loss 1.8165283203125 and accuracy 0.236111119389534.\n",
      "Test batch 29 with loss 1.8069190979003906 and accuracy 0.15000000596046448.\n",
      "Test batch 30 with loss 1.9488109350204468 and accuracy 0.02777777798473835.\n",
      "Test batch 31 with loss 1.8863170146942139 and accuracy 0.2222222238779068.\n",
      "Test batch 32 with loss 1.8475879430770874 and accuracy 0.1805555671453476.\n",
      "Test batch 33 with loss 1.798309564590454 and accuracy 0.03703703731298447.\n",
      "Test batch 34 with loss 1.7801227569580078 and accuracy 0.13055555522441864.\n",
      "Test batch 35 with loss 1.9142554998397827 and accuracy 0.1388888955116272.\n",
      "Test batch 36 with loss 1.8908131122589111 and accuracy 0.1388888955116272.\n",
      "Test batch 37 with loss 1.8023090362548828 and accuracy 0.1527777761220932.\n",
      "Test batch 38 with loss 1.6975867748260498 and accuracy 0.3126984238624573.\n",
      "Test batch 39 with loss 1.8268769979476929 and accuracy 0.1527777761220932.\n",
      "Test batch 40 with loss 1.9044110774993896 and accuracy 0.03333333507180214.\n",
      "Test batch 41 with loss 1.8225561380386353 and accuracy 0.10277777910232544.\n",
      "Test batch 42 with loss 1.8273969888687134 and accuracy 0.15000000596046448.\n",
      "Test batch 43 with loss 1.7356154918670654 and accuracy 0.07500000298023224.\n",
      "Test batch 44 with loss 1.8430063724517822 and accuracy 0.0416666679084301.\n",
      "Test batch 45 with loss 1.8346961736679077 and accuracy 0.1527777761220932.\n",
      "Test batch 46 with loss 1.8859684467315674 and accuracy 0.0694444477558136.\n",
      "Test batch 47 with loss 1.7582893371582031 and accuracy 0.20000000298023224.\n",
      "Test batch 48 with loss 1.8574234247207642 and accuracy 0.1666666716337204.\n",
      "Test batch 49 with loss 1.8731403350830078 and accuracy 0.125.\n",
      "Test batch 50 with loss 1.8127092123031616 and accuracy 0.13055555522441864.\n",
      "Test batch 51 with loss 1.9371627569198608 and accuracy 0.17222222685813904.\n",
      "Test batch 52 with loss 1.9753786325454712 and accuracy 0.0.\n",
      "Test batch 53 with loss 1.7991310358047485 and accuracy 0.21944445371627808.\n",
      "Test batch 54 with loss 1.8620531558990479 and accuracy 0.11666667461395264.\n",
      "Test batch 55 with loss 1.832209587097168 and accuracy 0.02380952425301075.\n",
      "Test batch 56 with loss 1.7854061126708984 and accuracy 0.2460317611694336.\n",
      "Test batch 57 with loss 1.8529319763183594 and accuracy 0.0555555559694767.\n",
      "Test batch 58 with loss 1.9583766460418701 and accuracy 0.0.\n",
      "Test batch 59 with loss 1.818159818649292 and accuracy 0.222222238779068.\n",
      "Test batch 60 with loss 1.9398071765899658 and accuracy 0.1111111119389534.\n",
      "Test batch 61 with loss 1.6319401264190674 and accuracy 0.3730158805847168.\n",
      "Test batch 62 with loss 1.76535964012146 and accuracy 0.0972222238779068.\n",
      "Test batch 63 with loss 1.7970975637435913 and accuracy 0.05714286118745804.\n",
      "Test batch 64 with loss 1.811865210533142 and accuracy 0.06666667014360428.\n",
      "Test batch 65 with loss 1.821496605873108 and accuracy 0.2916666865348816.\n",
      "Test batch 66 with loss 1.8406121730804443 and accuracy 0.0416666679084301.\n",
      "Test batch 67 with loss 1.7767871618270874 and accuracy 0.125.\n",
      "Test batch 68 with loss 1.8532798290252686 and accuracy 0.0555555559694767.\n",
      "Test batch 69 with loss 1.9076929092407227 and accuracy 0.0416666679084301.\n",
      "Test batch 70 with loss 1.793683648109436 and accuracy 0.2777777910232544.\n",
      "Test batch 71 with loss 1.8265819549560547 and accuracy 0.10833333432674408.\n",
      "Test batch 72 with loss 1.837029218673706 and accuracy 0.1666666716337204.\n",
      "Test batch 73 with loss 1.7320654392242432 and accuracy 0.3154761791229248.\n",
      "Test batch 74 with loss 1.72853684425354 and accuracy 0.13055555522441864.\n",
      "Test batch 75 with loss 1.7723525762557983 and accuracy 0.21388888359069824.\n",
      "Test batch 76 with loss 1.817230463027954 and accuracy 0.1527777761220932.\n",
      "Test batch 77 with loss 1.8182909488677979 and accuracy 0.2777777910232544.\n",
      "Test batch 78 with loss 1.9008386135101318 and accuracy 0.02380952425301075.\n",
      "Test batch 79 with loss 1.7612714767456055 and accuracy 0.24722222983837128.\n",
      "Test batch 80 with loss 1.8607709407806396 and accuracy 0.130952388048172.\n",
      "Test batch 81 with loss 1.9060167074203491 and accuracy 0.0416666679084301.\n",
      "Test batch 82 with loss 1.9076570272445679 and accuracy 0.06666667014360428.\n",
      "Test batch 83 with loss 1.815940499305725 and accuracy 0.0416666679084301.\n",
      "Test batch 84 with loss 1.809959053993225 and accuracy 0.16388890147209167.\n",
      "Test batch 85 with loss 1.7624486684799194 and accuracy 0.17222222685813904.\n",
      "Test batch 86 with loss 1.858512282371521 and accuracy 0.07500000298023224.\n",
      "Test batch 87 with loss 1.8039878606796265 and accuracy 0.17777778208255768.\n",
      "Test batch 88 with loss 1.8330951929092407 and accuracy 0.125.\n",
      "Test batch 89 with loss 1.9185060262680054 and accuracy 0.1111111119389534.\n",
      "Test batch 90 with loss 1.76249098777771 and accuracy 0.125.\n",
      "Test batch 91 with loss 1.7924449443817139 and accuracy 0.23333334922790527.\n",
      "Test batch 92 with loss 1.7514896392822266 and accuracy 0.40000003576278687.\n",
      "Test batch 93 with loss 1.8970226049423218 and accuracy 0.2083333432674408.\n",
      "Test batch 94 with loss 1.8362858295440674 and accuracy 0.125.\n",
      "Test batch 95 with loss 1.8866037130355835 and accuracy 0.3583333492279053.\n",
      "Test batch 96 with loss 1.9135338068008423 and accuracy 0.06666667014360428.\n",
      "Test batch 97 with loss 1.810573935508728 and accuracy 0.0833333358168602.\n",
      "Test batch 98 with loss 1.8477370738983154 and accuracy 0.0476190485060215.\n",
      "Test batch 99 with loss 1.9064762592315674 and accuracy 0.0833333358168602.\n",
      "Test batch 100 with loss 1.8504791259765625 and accuracy 0.1805555522441864.\n",
      "Test batch 101 with loss 1.8112417459487915 and accuracy 0.26547619700431824.\n",
      "Test batch 102 with loss 1.7887802124023438 and accuracy 0.28928571939468384.\n",
      "Test batch 103 with loss 1.8698339462280273 and accuracy 0.0694444477558136.\n",
      "Test batch 104 with loss 1.8527963161468506 and accuracy 0.4000000059604645.\n",
      "Test batch 105 with loss 1.8025985956192017 and accuracy 0.0972222238779068.\n",
      "Test batch 106 with loss 1.7489103078842163 and accuracy 0.2944444417953491.\n",
      "Test batch 107 with loss 1.7973817586898804 and accuracy 0.09047619253396988.\n",
      "Test batch 108 with loss 1.902385950088501 and accuracy 0.19166666269302368.\n",
      "Test batch 109 with loss 1.7686984539031982 and accuracy 0.1349206417798996.\n",
      "Test batch 110 with loss 1.9662126302719116 and accuracy 0.0.\n",
      "Test batch 111 with loss 1.7307780981063843 and accuracy 0.2142857164144516.\n",
      "Test batch 112 with loss 1.8239294290542603 and accuracy 0.0972222238779068.\n",
      "Test batch 113 with loss 1.7809035778045654 and accuracy 0.42777782678604126.\n",
      "Test batch 114 with loss 1.757922887802124 and accuracy 0.3083333373069763.\n",
      "Test batch 115 with loss 1.8148934841156006 and accuracy 0.08888889104127884.\n",
      "Test batch 116 with loss 1.8419967889785767 and accuracy 0.0833333358168602.\n",
      "Test batch 117 with loss 1.8000648021697998 and accuracy 0.1944444477558136.\n",
      "Test batch 118 with loss 1.8679001331329346 and accuracy 0.11666667461395264.\n",
      "Test batch 119 with loss 1.698301911354065 and accuracy 0.5349206924438477.\n",
      "Test batch 120 with loss 1.831787347793579 and accuracy 0.0833333358168602.\n",
      "Test batch 121 with loss 1.772316575050354 and accuracy 0.24166667461395264.\n",
      "Test batch 122 with loss 1.7432388067245483 and accuracy 0.20555555820465088.\n",
      "Test batch 123 with loss 1.7550780773162842 and accuracy 0.1319444477558136.\n",
      "Test batch 124 with loss 1.671087622642517 and accuracy 0.33888891339302063.\n",
      "Test batch 125 with loss 1.8748538494110107 and accuracy 0.125.\n",
      "Test batch 126 with loss 1.7426141500473022 and accuracy 0.2460317462682724.\n",
      "Test batch 127 with loss 1.8208239078521729 and accuracy 0.17500001192092896.\n",
      "Test batch 128 with loss 1.7248623371124268 and accuracy 0.1666666716337204.\n",
      "Test batch 129 with loss 1.8141425848007202 and accuracy 0.09047619253396988.\n",
      "Test batch 130 with loss 1.9561662673950195 and accuracy 0.06666667014360428.\n",
      "Test batch 131 with loss 1.9373706579208374 and accuracy 0.14444445073604584.\n",
      "Test batch 132 with loss 1.790329933166504 and accuracy 0.1805555522441864.\n",
      "Test batch 133 with loss 1.7780048847198486 and accuracy 0.15833333134651184.\n",
      "Test batch 134 with loss 1.8583614826202393 and accuracy 0.02777777798473835.\n",
      "Test batch 135 with loss 1.798312783241272 and accuracy 0.0416666679084301.\n",
      "Test batch 136 with loss 1.829026460647583 and accuracy 0.17777778208255768.\n",
      "Test batch 137 with loss 1.7972297668457031 and accuracy 0.20000000298023224.\n",
      "Test batch 138 with loss 1.803747534751892 and accuracy 0.0833333358168602.\n",
      "Test batch 139 with loss 1.8828916549682617 and accuracy 0.11666667461395264.\n",
      "Test batch 140 with loss 1.7940948009490967 and accuracy 0.1944444477558136.\n",
      "Test batch 141 with loss 1.8214582204818726 and accuracy 0.15000000596046448.\n",
      "Test batch 142 with loss 1.900003433227539 and accuracy 0.18888889253139496.\n",
      "Test batch 143 with loss 1.8397918939590454 and accuracy 0.0793650820851326.\n",
      "Test batch 144 with loss 1.7554937601089478 and accuracy 0.2797619104385376.\n",
      "Test batch 145 with loss 1.8637351989746094 and accuracy 0.2222222238779068.\n",
      "Test batch 146 with loss 1.9538204669952393 and accuracy 0.03333333507180214.\n",
      "Test batch 147 with loss 1.9522020816802979 and accuracy 0.11666667461395264.\n",
      "Test batch 148 with loss 1.8487964868545532 and accuracy 0.32500001788139343.\n",
      "Test batch 149 with loss 1.7810144424438477 and accuracy 0.24722222983837128.\n",
      "Test batch 150 with loss 1.8972469568252563 and accuracy 0.0793650820851326.\n",
      "Test batch 151 with loss 1.9244296550750732 and accuracy 0.0416666679084301.\n",
      "Test batch 152 with loss 1.8240658044815063 and accuracy 0.1944444477558136.\n",
      "Test batch 153 with loss 1.8307650089263916 and accuracy 0.2043650895357132.\n",
      "Test batch 154 with loss 1.778045415878296 and accuracy 0.1527777761220932.\n",
      "Test batch 155 with loss 1.7965062856674194 and accuracy 0.1875.\n",
      "Test batch 156 with loss 1.7384048700332642 and accuracy 0.11666667461395264.\n",
      "Test batch 157 with loss 1.8584600687026978 and accuracy 0.0416666679084301.\n",
      "Test batch 158 with loss 1.7958484888076782 and accuracy 0.23333334922790527.\n",
      "Test batch 159 with loss 1.7894859313964844 and accuracy 0.144841268658638.\n",
      "Test batch 160 with loss 1.8541676998138428 and accuracy 0.4027777910232544.\n",
      "Test batch 161 with loss 1.69515061378479 and accuracy 0.30277779698371887.\n",
      "Test batch 162 with loss 1.7834265232086182 and accuracy 0.0972222238779068.\n",
      "Test batch 163 with loss 1.7612581253051758 and accuracy 0.2083333432674408.\n",
      "Test batch 164 with loss 1.8017656803131104 and accuracy 0.14444445073604584.\n",
      "Test batch 165 with loss 1.8237531185150146 and accuracy 0.190476194024086.\n",
      "Test batch 166 with loss 1.7695951461791992 and accuracy 0.2321428656578064.\n",
      "Test batch 167 with loss 1.8697223663330078 and accuracy 0.2738095223903656.\n",
      "Test batch 168 with loss 1.890242576599121 and accuracy 0.236111119389534.\n",
      "Test batch 169 with loss 1.8014215230941772 and accuracy 0.16388888657093048.\n",
      "Test batch 170 with loss 1.8253278732299805 and accuracy 0.03333333507180214.\n",
      "Test batch 171 with loss 1.8322641849517822 and accuracy 0.02380952425301075.\n",
      "Test batch 172 with loss 1.9353443384170532 and accuracy 0.0.\n",
      "Test batch 173 with loss 1.7630411386489868 and accuracy 0.16388890147209167.\n",
      "Test batch 174 with loss 1.737635850906372 and accuracy 0.2976190447807312.\n",
      "Test batch 175 with loss 1.856015920639038 and accuracy 0.222222238779068.\n",
      "Test batch 176 with loss 1.8966572284698486 and accuracy 0.15000000596046448.\n",
      "Test batch 177 with loss 1.9032386541366577 and accuracy 0.0972222238779068.\n",
      "Test batch 178 with loss 1.8448925018310547 and accuracy 0.03333333507180214.\n",
      "Test batch 179 with loss 1.7903388738632202 and accuracy 0.27638891339302063.\n",
      "Test batch 180 with loss 1.811913251876831 and accuracy 0.1111111119389534.\n",
      "Test batch 181 with loss 1.774609923362732 and accuracy 0.2777777910232544.\n",
      "Test batch 182 with loss 1.8158107995986938 and accuracy 0.10000000149011612.\n",
      "Test batch 183 with loss 1.7670671939849854 and accuracy 0.30158731341362.\n",
      "Test batch 184 with loss 1.787013292312622 and accuracy 0.1388888955116272.\n",
      "Test batch 185 with loss 1.8744285106658936 and accuracy 0.13055555522441864.\n",
      "Test batch 186 with loss 1.7935879230499268 and accuracy 0.2430555522441864.\n",
      "Test batch 187 with loss 1.817848563194275 and accuracy 0.1388888955116272.\n",
      "Test batch 188 with loss 1.8396022319793701 and accuracy 0.1944444477558136.\n",
      "Test batch 189 with loss 1.7439191341400146 and accuracy 0.3333333432674408.\n",
      "Test batch 190 with loss 1.8611698150634766 and accuracy 0.1666666716337204.\n",
      "Test batch 191 with loss 1.8340187072753906 and accuracy 0.24074074625968933.\n",
      "Test batch 192 with loss 1.7902072668075562 and accuracy 0.21984127163887024.\n",
      "Test batch 193 with loss 1.8809295892715454 and accuracy 0.347222238779068.\n",
      "Test batch 194 with loss 1.7517732381820679 and accuracy 0.4375000298023224.\n",
      "Test batch 195 with loss 1.862762212753296 and accuracy 0.0793650820851326.\n",
      "Test batch 196 with loss 1.7933330535888672 and accuracy 0.03333333507180214.\n",
      "Test batch 197 with loss 1.860106110572815 and accuracy 0.13055555522441864.\n",
      "Test batch 198 with loss 1.8176183700561523 and accuracy 0.21388888359069824.\n",
      "Test batch 199 with loss 1.8600828647613525 and accuracy 0.06666667014360428.\n",
      "Test batch 200 with loss 1.7466169595718384 and accuracy 0.125.\n",
      "Test batch 201 with loss 1.7980070114135742 and accuracy 0.2888889014720917.\n",
      "Test batch 202 with loss 1.7624229192733765 and accuracy 0.2638888955116272.\n",
      "Test batch 203 with loss 1.840735673904419 and accuracy 0.21851852536201477.\n",
      "Test batch 204 with loss 1.8394826650619507 and accuracy 0.12222222983837128.\n",
      "Test batch 205 with loss 1.7836225032806396 and accuracy 0.20555555820465088.\n",
      "Test batch 206 with loss 1.7997690439224243 and accuracy 0.0476190485060215.\n",
      "Test batch 207 with loss 1.8038885593414307 and accuracy 0.28333333134651184.\n",
      "Test batch 208 with loss 1.7651348114013672 and accuracy 0.25.\n",
      "Test batch 209 with loss 1.8218930959701538 and accuracy 0.1944444477558136.\n",
      "Test batch 210 with loss 1.8037731647491455 and accuracy 0.46388888359069824.\n",
      "Test batch 211 with loss 1.8219560384750366 and accuracy 0.2083333432674408.\n",
      "Test batch 212 with loss 1.7602131366729736 and accuracy 0.1111111119389534.\n",
      "Test batch 213 with loss 1.9543888568878174 and accuracy 0.07500000298023224.\n",
      "Test batch 214 with loss 1.8041322231292725 and accuracy 0.0.\n",
      "Test batch 215 with loss 1.8789176940917969 and accuracy 0.20000001788139343.\n",
      "Test batch 216 with loss 1.8013572692871094 and accuracy 0.1666666716337204.\n",
      "Test batch 217 with loss 1.7928301095962524 and accuracy 0.1666666716337204.\n",
      "Test batch 218 with loss 1.8036813735961914 and accuracy 0.125.\n",
      "Test batch 219 with loss 1.8314663171768188 and accuracy 0.1071428656578064.\n",
      "Test batch 220 with loss 1.7577670812606812 and accuracy 0.2531746029853821.\n",
      "Test batch 221 with loss 1.7847236394882202 and accuracy 0.2888889014720917.\n",
      "Test batch 222 with loss 1.8399512767791748 and accuracy 0.08888889104127884.\n",
      "Test batch 223 with loss 1.805426836013794 and accuracy 0.25.\n",
      "Test batch 224 with loss 1.8327171802520752 and accuracy 0.1805555671453476.\n",
      "Test batch 225 with loss 1.8916800022125244 and accuracy 0.1805555522441864.\n",
      "Test batch 226 with loss 1.7312145233154297 and accuracy 0.15555556118488312.\n",
      "Test batch 227 with loss 1.7775274515151978 and accuracy 0.1388888955116272.\n",
      "Test batch 228 with loss 1.7438207864761353 and accuracy 0.2916666865348816.\n",
      "Test batch 229 with loss 1.7866967916488647 and accuracy 0.222222238779068.\n",
      "Test batch 230 with loss 1.8402454853057861 and accuracy 0.130952388048172.\n",
      "Test batch 231 with loss 1.8274211883544922 and accuracy 0.12222222983837128.\n",
      "Test batch 232 with loss 1.8131930828094482 and accuracy 0.24166667461395264.\n",
      "Test batch 233 with loss 1.8306105136871338 and accuracy 0.11666667461395264.\n",
      "Test batch 234 with loss 1.8418891429901123 and accuracy 0.11269842088222504.\n",
      "Test batch 235 with loss 1.8399709463119507 and accuracy 0.14047619700431824.\n",
      "Test batch 236 with loss 1.8285468816757202 and accuracy 0.0555555559694767.\n",
      "Test batch 237 with loss 1.8348678350448608 and accuracy 0.24166667461395264.\n",
      "Test batch 238 with loss 1.7879902124404907 and accuracy 0.1666666716337204.\n",
      "Test batch 239 with loss 1.8617918491363525 and accuracy 0.22685186564922333.\n",
      "Test batch 240 with loss 1.8843929767608643 and accuracy 0.1666666716337204.\n",
      "Test batch 241 with loss 1.8930730819702148 and accuracy 0.0416666679084301.\n",
      "Test batch 242 with loss 1.8840665817260742 and accuracy 0.1666666716337204.\n",
      "Test batch 243 with loss 1.8471662998199463 and accuracy 0.0833333358168602.\n",
      "Test batch 244 with loss 1.8984229564666748 and accuracy 0.0416666679084301.\n",
      "Test batch 245 with loss 1.831559181213379 and accuracy 0.07500000298023224.\n",
      "Test batch 246 with loss 1.8110326528549194 and accuracy 0.1388888955116272.\n",
      "Test batch 247 with loss 1.8367458581924438 and accuracy 0.0416666679084301.\n",
      "Test batch 248 with loss 1.8416748046875 and accuracy 0.2083333432674408.\n",
      "Test batch 249 with loss 1.8431358337402344 and accuracy 0.20370370149612427.\n",
      "Test batch 250 with loss 1.9791991710662842 and accuracy 0.1071428582072258.\n",
      "Test batch 251 with loss 1.7670958042144775 and accuracy 0.03333333507180214.\n",
      "Test batch 252 with loss 1.7652400732040405 and accuracy 0.22777777910232544.\n",
      "Test batch 253 with loss 1.8366981744766235 and accuracy 0.21666666865348816.\n",
      "Test batch 254 with loss 1.89752197265625 and accuracy 0.1527777761220932.\n",
      "Test batch 255 with loss 1.890856146812439 and accuracy 0.0833333358168602.\n",
      "Test batch 256 with loss 1.7366478443145752 and accuracy 0.1944444477558136.\n",
      "Test batch 257 with loss 1.9120328426361084 and accuracy 0.0.\n",
      "Test batch 258 with loss 1.8276703357696533 and accuracy 0.15000000596046448.\n",
      "Test batch 259 with loss 1.8832248449325562 and accuracy 0.1111111119389534.\n",
      "Test batch 260 with loss 1.7695566415786743 and accuracy 0.2133333384990692.\n",
      "Test batch 261 with loss 1.7711560726165771 and accuracy 0.10833333432674408.\n",
      "Test batch 262 with loss 1.809547781944275 and accuracy 0.125.\n",
      "Test batch 263 with loss 1.921352744102478 and accuracy 0.1666666716337204.\n",
      "Test batch 264 with loss 1.7956498861312866 and accuracy 0.14444445073604584.\n",
      "Test batch 265 with loss 1.830260992050171 and accuracy 0.28333336114883423.\n",
      "Test batch 266 with loss 1.9749685525894165 and accuracy 0.0416666679084301.\n",
      "Test batch 267 with loss 1.8157987594604492 and accuracy 0.125.\n",
      "Test batch 268 with loss 1.9442484378814697 and accuracy 0.1111111119389534.\n",
      "Test batch 269 with loss 1.9566526412963867 and accuracy 0.0416666679084301.\n",
      "Test batch 270 with loss 1.9131485223770142 and accuracy 0.0555555559694767.\n",
      "Test batch 271 with loss 1.8118788003921509 and accuracy 0.15833334624767303.\n",
      "Test batch 272 with loss 1.9661338329315186 and accuracy 0.07500000298023224.\n",
      "Test batch 273 with loss 1.7706247568130493 and accuracy 0.25158730149269104.\n",
      "Test batch 274 with loss 1.8052394390106201 and accuracy 0.18888889253139496.\n",
      "Test batch 275 with loss 1.9309022426605225 and accuracy 0.0.\n",
      "Test batch 276 with loss 1.8804203271865845 and accuracy 0.2222222238779068.\n",
      "Test batch 277 with loss 1.8619918823242188 and accuracy 0.02777777798473835.\n",
      "Test batch 278 with loss 1.8507808446884155 and accuracy 0.15555556118488312.\n",
      "Test batch 279 with loss 1.73873770236969 and accuracy 0.222222238779068.\n",
      "Test batch 280 with loss 1.805590271949768 and accuracy 0.0833333358168602.\n",
      "Test batch 281 with loss 1.7557891607284546 and accuracy 0.1111111119389534.\n",
      "Test batch 282 with loss 1.866119384765625 and accuracy 0.25.\n",
      "Test batch 283 with loss 1.8621978759765625 and accuracy 0.08888889104127884.\n",
      "Test batch 284 with loss 1.8888673782348633 and accuracy 0.0833333358168602.\n",
      "Test batch 285 with loss 1.7374862432479858 and accuracy 0.2611111104488373.\n",
      "Test batch 286 with loss 1.9264490604400635 and accuracy 0.0.\n",
      "Test batch 287 with loss 1.7880995273590088 and accuracy 0.10277777910232544.\n",
      "Test batch 288 with loss 1.6658971309661865 and accuracy 0.2611111104488373.\n",
      "Test batch 289 with loss 1.883439064025879 and accuracy 0.20000000298023224.\n",
      "Test batch 290 with loss 1.8588531017303467 and accuracy 0.1527777761220932.\n",
      "Test batch 291 with loss 1.7577762603759766 and accuracy 0.1388888955116272.\n",
      "Test batch 292 with loss 1.7445192337036133 and accuracy 0.21111111342906952.\n",
      "Test batch 293 with loss 1.810092568397522 and accuracy 0.20000000298023224.\n",
      "Test batch 294 with loss 1.8085426092147827 and accuracy 0.065476194024086.\n",
      "Test batch 295 with loss 1.8374465703964233 and accuracy 0.190476194024086.\n",
      "Test batch 296 with loss 1.8662712574005127 and accuracy 0.2777777910232544.\n",
      "Test batch 297 with loss 1.7064142227172852 and accuracy 0.19166666269302368.\n",
      "Test batch 298 with loss 1.754302978515625 and accuracy 0.25.\n",
      "Test batch 299 with loss 1.8822463750839233 and accuracy 0.2738095223903656.\n",
      "Test batch 300 with loss 1.7766212224960327 and accuracy 0.0793650820851326.\n",
      "Test batch 301 with loss 1.7739315032958984 and accuracy 0.2698412835597992.\n",
      "Test batch 302 with loss 1.8700826168060303 and accuracy 0.20666667819023132.\n",
      "Test batch 303 with loss 1.8338009119033813 and accuracy 0.07500000298023224.\n",
      "Test batch 304 with loss 1.879472017288208 and accuracy 0.329365074634552.\n",
      "Test batch 305 with loss 1.8249924182891846 and accuracy 0.25555557012557983.\n",
      "Test batch 306 with loss 1.8532698154449463 and accuracy 0.0476190485060215.\n",
      "Test batch 307 with loss 1.8328454494476318 and accuracy 0.31111112236976624.\n",
      "Test batch 308 with loss 1.9208158254623413 and accuracy 0.11666667461395264.\n",
      "Test batch 309 with loss 1.8437330722808838 and accuracy 0.2599206268787384.\n",
      "Test batch 310 with loss 1.8764865398406982 and accuracy 0.0476190485060215.\n",
      "Test batch 311 with loss 1.7668243646621704 and accuracy 0.20000000298023224.\n",
      "Test batch 312 with loss 1.6671085357666016 and accuracy 0.2777777910232544.\n",
      "Test batch 313 with loss 1.7991664409637451 and accuracy 0.1944444477558136.\n",
      "Test batch 314 with loss 1.8007488250732422 and accuracy 0.2222222238779068.\n",
      "Test batch 315 with loss 1.8375623226165771 and accuracy 0.2666666805744171.\n",
      "Test batch 316 with loss 1.7835168838500977 and accuracy 0.21388889849185944.\n",
      "Test batch 317 with loss 1.8364979028701782 and accuracy 0.3166666626930237.\n",
      "Test batch 318 with loss 1.8240896463394165 and accuracy 0.25555557012557983.\n",
      "Test batch 319 with loss 1.8788576126098633 and accuracy 0.2083333432674408.\n",
      "Test batch 320 with loss 1.8130228519439697 and accuracy 0.1666666716337204.\n",
      "Test batch 321 with loss 1.8122785091400146 and accuracy 0.0793650820851326.\n",
      "Test batch 322 with loss 1.7318079471588135 and accuracy 0.22499999403953552.\n",
      "Test batch 323 with loss 1.8027427196502686 and accuracy 0.3166666626930237.\n",
      "Test batch 324 with loss 1.8754236698150635 and accuracy 0.125.\n",
      "Test batch 325 with loss 1.799993872642517 and accuracy 0.0833333358168602.\n",
      "Test batch 326 with loss 1.845688819885254 and accuracy 0.0833333358168602.\n",
      "Test batch 327 with loss 1.8523149490356445 and accuracy 0.1111111119389534.\n",
      "Test batch 328 with loss 1.8192428350448608 and accuracy 0.1944444477558136.\n",
      "Test batch 329 with loss 1.8489739894866943 and accuracy 0.0892857164144516.\n",
      "Test batch 330 with loss 1.7296028137207031 and accuracy 0.15833333134651184.\n",
      "Test batch 331 with loss 1.8206348419189453 and accuracy 0.1666666716337204.\n",
      "Test batch 332 with loss 1.884117841720581 and accuracy 0.1805555522441864.\n",
      "Test batch 333 with loss 1.9030479192733765 and accuracy 0.0833333358168602.\n",
      "Test batch 334 with loss 1.7601966857910156 and accuracy 0.2222222238779068.\n",
      "Test batch 335 with loss 1.8159263134002686 and accuracy 0.11666666716337204.\n",
      "Test batch 336 with loss 1.802385687828064 and accuracy 0.25.\n",
      "Test batch 337 with loss 1.830569863319397 and accuracy 0.0555555559694767.\n",
      "Test batch 338 with loss 1.7455766201019287 and accuracy 0.25.\n",
      "Test batch 339 with loss 1.7298122644424438 and accuracy 0.28333333134651184.\n",
      "Test batch 340 with loss 1.8442232608795166 and accuracy 0.1666666716337204.\n",
      "Test batch 341 with loss 1.8293346166610718 and accuracy 0.3444444537162781.\n",
      "Test batch 342 with loss 1.835418462753296 and accuracy 0.12222222983837128.\n",
      "Test batch 343 with loss 1.8847776651382446 and accuracy 0.0972222238779068.\n",
      "Test batch 344 with loss 1.789520263671875 and accuracy 0.1527777761220932.\n",
      "Test batch 345 with loss 1.8439161777496338 and accuracy 0.1388888955116272.\n",
      "Test batch 346 with loss 1.8983453512191772 and accuracy 0.1666666716337204.\n",
      "Test batch 347 with loss 1.9455257654190063 and accuracy 0.0625.\n",
      "Test batch 348 with loss 1.8053067922592163 and accuracy 0.236111119389534.\n",
      "Test batch 349 with loss 1.9108736515045166 and accuracy 0.0.\n",
      "Test batch 350 with loss 1.731318473815918 and accuracy 0.21111111342906952.\n",
      "Test batch 351 with loss 1.7128480672836304 and accuracy 0.15833333134651184.\n",
      "Test batch 352 with loss 1.776850938796997 and accuracy 0.2904762029647827.\n",
      "Test batch 353 with loss 1.8312256336212158 and accuracy 0.125.\n",
      "Test batch 354 with loss 1.8273134231567383 and accuracy 0.3333333432674408.\n",
      "Test batch 355 with loss 1.8288624286651611 and accuracy 0.07870370149612427.\n",
      "Test batch 356 with loss 1.696221113204956 and accuracy 0.222222238779068.\n",
      "Test batch 357 with loss 1.900753378868103 and accuracy 0.14444445073604584.\n",
      "Test batch 358 with loss 1.8963544368743896 and accuracy 0.0416666679084301.\n",
      "Test batch 359 with loss 1.886655569076538 and accuracy 0.06666667014360428.\n",
      "Test batch 360 with loss 1.8028385639190674 and accuracy 0.1388888955116272.\n",
      "Test batch 361 with loss 1.8054720163345337 and accuracy 0.25555557012557983.\n",
      "Test batch 362 with loss 1.756799340248108 and accuracy 0.20555555820465088.\n",
      "Test batch 363 with loss 1.739640235900879 and accuracy 0.1111111119389534.\n",
      "Test batch 364 with loss 1.816992163658142 and accuracy 0.1597222238779068.\n",
      "Test batch 365 with loss 1.836554765701294 and accuracy 0.1666666716337204.\n",
      "Test batch 366 with loss 1.8258283138275146 and accuracy 0.0416666679084301.\n",
      "Test batch 367 with loss 1.7865617275238037 and accuracy 0.20000001788139343.\n",
      "Test batch 368 with loss 1.8725353479385376 and accuracy 0.125.\n",
      "Test batch 369 with loss 1.653062105178833 and accuracy 0.3263888955116272.\n",
      "Test batch 370 with loss 1.8778603076934814 and accuracy 0.0555555559694767.\n",
      "Test batch 371 with loss 1.9436359405517578 and accuracy 0.0833333358168602.\n",
      "Test batch 372 with loss 1.8438804149627686 and accuracy 0.15000000596046448.\n",
      "Test batch 373 with loss 1.8563194274902344 and accuracy 0.14444445073604584.\n",
      "Test batch 374 with loss 1.8841569423675537 and accuracy 0.2222222238779068.\n",
      "Test batch 375 with loss 1.7833101749420166 and accuracy 0.15000000596046448.\n",
      "Test batch 376 with loss 1.8359737396240234 and accuracy 0.25.\n",
      "Test batch 377 with loss 1.8400843143463135 and accuracy 0.15833333134651184.\n",
      "Test batch 378 with loss 1.8370215892791748 and accuracy 0.03333333507180214.\n",
      "Test batch 379 with loss 1.7487589120864868 and accuracy 0.4027777910232544.\n",
      "Test batch 380 with loss 1.871301293373108 and accuracy 0.0972222238779068.\n",
      "Test batch 381 with loss 1.9245001077651978 and accuracy 0.1388888955116272.\n",
      "Test batch 382 with loss 1.8547248840332031 and accuracy 0.2611111104488373.\n",
      "Test batch 383 with loss 1.7713086605072021 and accuracy 0.16825397312641144.\n",
      "Test batch 384 with loss 1.7539770603179932 and accuracy 0.1944444477558136.\n",
      "Test batch 385 with loss 1.7855291366577148 and accuracy 0.1527777761220932.\n",
      "Test batch 386 with loss 1.876312017440796 and accuracy 0.0694444477558136.\n",
      "Test batch 387 with loss 1.9287872314453125 and accuracy 0.0.\n",
      "Test batch 388 with loss 1.843794584274292 and accuracy 0.2083333432674408.\n",
      "Test batch 389 with loss 1.885367751121521 and accuracy 0.11666667461395264.\n",
      "Test batch 390 with loss 1.7464323043823242 and accuracy 0.24166667461395264.\n",
      "Test batch 391 with loss 1.77737295627594 and accuracy 0.10833333432674408.\n",
      "Test batch 392 with loss 1.9722309112548828 and accuracy 0.0.\n",
      "Test batch 393 with loss 1.8456655740737915 and accuracy 0.0694444477558136.\n",
      "Test batch 394 with loss 1.8004348278045654 and accuracy 0.1626984179019928.\n",
      "Test batch 395 with loss 1.8259140253067017 and accuracy 0.14166666567325592.\n",
      "Test batch 396 with loss 1.8289254903793335 and accuracy 0.15833334624767303.\n",
      "Test batch 397 with loss 1.9583473205566406 and accuracy 0.1875.\n",
      "Test batch 398 with loss 1.8617875576019287 and accuracy 0.1388888955116272.\n",
      "Test batch 399 with loss 1.8225212097167969 and accuracy 0.13055555522441864.\n",
      "Test batch 400 with loss 1.9138253927230835 and accuracy 0.12222222983837128.\n",
      "Test batch 401 with loss 1.7721550464630127 and accuracy 0.25555557012557983.\n",
      "Test batch 402 with loss 1.755491018295288 and accuracy 0.2291666716337204.\n",
      "Test batch 403 with loss 1.7615054845809937 and accuracy 0.3888889253139496.\n",
      "Test batch 404 with loss 1.7941852807998657 and accuracy 0.2083333283662796.\n",
      "Test batch 405 with loss 1.82949697971344 and accuracy 0.0416666679084301.\n",
      "Test batch 406 with loss 1.7169075012207031 and accuracy 0.2666666805744171.\n",
      "Test batch 407 with loss 1.8146240711212158 and accuracy 0.0793650820851326.\n",
      "Test batch 408 with loss 1.9126393795013428 and accuracy 0.1944444477558136.\n",
      "Test batch 409 with loss 1.9503376483917236 and accuracy 0.07500000298023224.\n",
      "Test batch 410 with loss 1.8798253536224365 and accuracy 0.29722222685813904.\n",
      "Test batch 411 with loss 1.8097622394561768 and accuracy 0.2222222238779068.\n",
      "Test batch 412 with loss 1.8852344751358032 and accuracy 0.0416666679084301.\n",
      "Test batch 413 with loss 1.6911077499389648 and accuracy 0.25.\n",
      "Test batch 414 with loss 1.8425390720367432 and accuracy 0.0714285746216774.\n",
      "Test batch 415 with loss 1.8254131078720093 and accuracy 0.1111111119389534.\n",
      "Test batch 416 with loss 1.7991663217544556 and accuracy 0.25555557012557983.\n",
      "Test batch 417 with loss 1.9468342065811157 and accuracy 0.1388888955116272.\n",
      "Test batch 418 with loss 1.7156221866607666 and accuracy 0.3333333432674408.\n",
      "Test batch 419 with loss 1.664852499961853 and accuracy 0.38333332538604736.\n",
      "Test batch 420 with loss 1.7795366048812866 and accuracy 0.1388888955116272.\n",
      "Test batch 421 with loss 1.9076793193817139 and accuracy 0.0972222238779068.\n",
      "Test batch 422 with loss 1.791796088218689 and accuracy 0.1111111119389534.\n",
      "Test batch 423 with loss 1.8541911840438843 and accuracy 0.21388889849185944.\n",
      "Test batch 424 with loss 1.8031256198883057 and accuracy 0.2888889014720917.\n",
      "Test batch 425 with loss 1.865261435508728 and accuracy 0.125.\n",
      "Test batch 426 with loss 1.9118807315826416 and accuracy 0.0.\n",
      "Test batch 427 with loss 1.7930972576141357 and accuracy 0.3638888895511627.\n",
      "Test batch 428 with loss 1.837874174118042 and accuracy 0.14444445073604584.\n",
      "Test batch 429 with loss 1.8998769521713257 and accuracy 0.1111111119389534.\n",
      "Test batch 430 with loss 1.8594526052474976 and accuracy 0.05714286118745804.\n",
      "Test batch 431 with loss 1.8922576904296875 and accuracy 0.20000000298023224.\n",
      "Test batch 432 with loss 1.8484504222869873 and accuracy 0.255952388048172.\n",
      "Test batch 433 with loss 1.763674020767212 and accuracy 0.347222238779068.\n",
      "Test batch 434 with loss 1.7364660501480103 and accuracy 0.222222238779068.\n",
      "Test batch 435 with loss 1.7536792755126953 and accuracy 0.1527777910232544.\n",
      "Test batch 436 with loss 1.7867927551269531 and accuracy 0.2152777910232544.\n",
      "Test batch 437 with loss 1.793053388595581 and accuracy 0.2003968209028244.\n",
      "Test batch 438 with loss 1.8400949239730835 and accuracy 0.1458333283662796.\n",
      "Test batch 439 with loss 1.7944339513778687 and accuracy 0.20555555820465088.\n",
      "Test batch 440 with loss 1.788353681564331 and accuracy 0.1527777761220932.\n",
      "Test batch 441 with loss 1.7928882837295532 and accuracy 0.17222222685813904.\n",
      "Test batch 442 with loss 1.7543184757232666 and accuracy 0.33888888359069824.\n",
      "Test batch 443 with loss 1.9445645809173584 and accuracy 0.11666667461395264.\n",
      "Test batch 444 with loss 1.8131844997406006 and accuracy 0.1666666716337204.\n",
      "Test batch 445 with loss 1.7751678228378296 and accuracy 0.1944444477558136.\n",
      "Test batch 446 with loss 1.8205921649932861 and accuracy 0.0833333358168602.\n",
      "Test batch 447 with loss 1.7932525873184204 and accuracy 0.15000000596046448.\n",
      "Test batch 448 with loss 1.795245885848999 and accuracy 0.07500000298023224.\n",
      "Test batch 449 with loss 1.7369970083236694 and accuracy 0.2638888955116272.\n",
      "Test batch 450 with loss 1.771636962890625 and accuracy 0.18214285373687744.\n",
      "Test batch 451 with loss 1.8773088455200195 and accuracy 0.15833334624767303.\n",
      "Test batch 452 with loss 1.8382536172866821 and accuracy 0.28333336114883423.\n",
      "Test batch 453 with loss 1.7988812923431396 and accuracy 0.1666666716337204.\n",
      "Test batch 454 with loss 1.9027493000030518 and accuracy 0.21944445371627808.\n",
      "Test batch 455 with loss 1.7009398937225342 and accuracy 0.28333333134651184.\n",
      "Test batch 456 with loss 1.8979828357696533 and accuracy 0.12222222983837128.\n",
      "Test batch 457 with loss 1.780264139175415 and accuracy 0.1111111119389534.\n",
      "Test batch 458 with loss 1.9074474573135376 and accuracy 0.1111111119389534.\n",
      "Test batch 459 with loss 1.8776943683624268 and accuracy 0.1071428582072258.\n",
      "Test batch 460 with loss 1.7934000492095947 and accuracy 0.125.\n",
      "Test batch 461 with loss 1.8207569122314453 and accuracy 0.0555555559694767.\n",
      "Test batch 462 with loss 1.7951314449310303 and accuracy 0.1944444477558136.\n",
      "Test batch 463 with loss 1.9296493530273438 and accuracy 0.02083333395421505.\n",
      "Test batch 464 with loss 1.8810739517211914 and accuracy 0.1527777761220932.\n",
      "Test batch 465 with loss 1.8636316061019897 and accuracy 0.0.\n",
      "Test batch 466 with loss 1.8667329549789429 and accuracy 0.0972222238779068.\n",
      "Test batch 467 with loss 1.7048397064208984 and accuracy 0.1944444477558136.\n",
      "Test batch 468 with loss 1.770735502243042 and accuracy 0.1805555522441864.\n",
      "Test batch 469 with loss 1.9613878726959229 and accuracy 0.0.\n",
      "Test batch 470 with loss 1.90094792842865 and accuracy 0.0555555559694767.\n",
      "Test batch 471 with loss 1.8050683736801147 and accuracy 0.2916666865348816.\n",
      "Test batch 472 with loss 1.8176454305648804 and accuracy 0.1944444477558136.\n",
      "Test batch 473 with loss 1.8532758951187134 and accuracy 0.08888889104127884.\n",
      "Test batch 474 with loss 1.9224151372909546 and accuracy 0.1805555522441864.\n",
      "Test batch 475 with loss 1.8411962985992432 and accuracy 0.1527777761220932.\n",
      "Test batch 476 with loss 1.8599926233291626 and accuracy 0.13055555522441864.\n",
      "Test batch 477 with loss 1.8128387928009033 and accuracy 0.17222222685813904.\n",
      "Test batch 478 with loss 1.7283439636230469 and accuracy 0.2847222089767456.\n",
      "Test batch 479 with loss 1.7324879169464111 and accuracy 0.15555556118488312.\n",
      "Test batch 480 with loss 1.806799292564392 and accuracy 0.1388888955116272.\n",
      "Test batch 481 with loss 1.8713607788085938 and accuracy 0.1587301641702652.\n",
      "Test batch 482 with loss 1.7696523666381836 and accuracy 0.1666666716337204.\n",
      "Test batch 483 with loss 1.9211403131484985 and accuracy 0.08095238357782364.\n",
      "Test batch 484 with loss 1.743407964706421 and accuracy 0.1944444477558136.\n",
      "Test batch 485 with loss 1.817586898803711 and accuracy 0.14444445073604584.\n",
      "Test batch 486 with loss 1.9486312866210938 and accuracy 0.10833333432674408.\n",
      "Test batch 487 with loss 1.8546720743179321 and accuracy 0.15833333134651184.\n",
      "Test batch 488 with loss 1.8838822841644287 and accuracy 0.06111111491918564.\n",
      "Test batch 489 with loss 1.8397728204727173 and accuracy 0.12261904776096344.\n",
      "Test batch 490 with loss 1.6773135662078857 and accuracy 0.25833335518836975.\n",
      "Test batch 491 with loss 1.7305870056152344 and accuracy 0.3492063581943512.\n",
      "Test batch 492 with loss 1.8030589818954468 and accuracy 0.1111111119389534.\n",
      "Test batch 493 with loss 1.9627463817596436 and accuracy 0.07500000298023224.\n",
      "Test batch 494 with loss 1.8581708669662476 and accuracy 0.03333333507180214.\n",
      "Test batch 495 with loss 1.7849451303482056 and accuracy 0.1875.\n",
      "Test batch 496 with loss 1.8108888864517212 and accuracy 0.22777777910232544.\n",
      "Test batch 497 with loss 1.9508838653564453 and accuracy 0.20000000298023224.\n",
      "Test batch 498 with loss 1.826200246810913 and accuracy 0.22777777910232544.\n",
      "Test batch 499 with loss 1.8360340595245361 and accuracy 0.2321428656578064.\n",
      "Training epoch 4 batch 0 with loss 1.8375356197357178, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 1 with loss 1.8439970016479492, accuracy 0.06666667014360428.\n",
      "Training epoch 4 batch 2 with loss 1.7403984069824219, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 3 with loss 1.8424304723739624, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 4 with loss 1.6992347240447998, accuracy 0.2611111104488373.\n",
      "Training epoch 4 batch 5 with loss 1.7590205669403076, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 6 with loss 1.815725564956665, accuracy 0.20000001788139343.\n",
      "Training epoch 4 batch 7 with loss 1.773406982421875, accuracy 0.125.\n",
      "Training epoch 4 batch 8 with loss 1.744560956954956, accuracy 0.18888889253139496.\n",
      "Training epoch 4 batch 9 with loss 1.7635953426361084, accuracy 0.15833333134651184.\n",
      "Training epoch 4 batch 10 with loss 1.76633620262146, accuracy 0.23888888955116272.\n",
      "Training epoch 4 batch 11 with loss 1.782954216003418, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 12 with loss 1.6638625860214233, accuracy 0.38611114025115967.\n",
      "Training epoch 4 batch 13 with loss 1.7709846496582031, accuracy 0.2043650895357132.\n",
      "Training epoch 4 batch 14 with loss 1.6989967823028564, accuracy 0.3333333432674408.\n",
      "Training epoch 4 batch 15 with loss 1.7097461223602295, accuracy 0.1458333432674408.\n",
      "Training epoch 4 batch 16 with loss 1.7977371215820312, accuracy 0.2750000059604645.\n",
      "Training epoch 4 batch 17 with loss 1.8698291778564453, accuracy 0.17083334922790527.\n",
      "Training epoch 4 batch 18 with loss 1.6777921915054321, accuracy 0.22777777910232544.\n",
      "Training epoch 4 batch 19 with loss 1.7786823511123657, accuracy 0.28333333134651184.\n",
      "Training epoch 4 batch 20 with loss 1.7608022689819336, accuracy 0.2222222238779068.\n",
      "Training epoch 4 batch 21 with loss 1.664920449256897, accuracy 0.402777761220932.\n",
      "Training epoch 4 batch 22 with loss 1.8046573400497437, accuracy 0.1805555671453476.\n",
      "Training epoch 4 batch 23 with loss 1.8041057586669922, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 24 with loss 1.6464145183563232, accuracy 0.35277777910232544.\n",
      "Training epoch 4 batch 25 with loss 1.787287950515747, accuracy 0.18611112236976624.\n",
      "Training epoch 4 batch 26 with loss 1.7372157573699951, accuracy 0.3333333432674408.\n",
      "Training epoch 4 batch 27 with loss 1.775122046470642, accuracy 0.24166667461395264.\n",
      "Training epoch 4 batch 28 with loss 1.72882080078125, accuracy 0.15555556118488312.\n",
      "Training epoch 4 batch 29 with loss 1.7236160039901733, accuracy 0.19166666269302368.\n",
      "Training epoch 4 batch 30 with loss 1.700246810913086, accuracy 0.3499999940395355.\n",
      "Training epoch 4 batch 31 with loss 1.7220115661621094, accuracy 0.2321428656578064.\n",
      "Training epoch 4 batch 32 with loss 1.7125076055526733, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 33 with loss 1.6843845844268799, accuracy 0.46666669845581055.\n",
      "Training epoch 4 batch 34 with loss 1.6789071559906006, accuracy 0.3638889193534851.\n",
      "Training epoch 4 batch 35 with loss 1.8108106851577759, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 36 with loss 1.6307575702667236, accuracy 0.20000000298023224.\n",
      "Training epoch 4 batch 37 with loss 1.6647809743881226, accuracy 0.5277777910232544.\n",
      "Training epoch 4 batch 38 with loss 1.7593791484832764, accuracy 0.19722221791744232.\n",
      "Training epoch 4 batch 39 with loss 1.7575935125350952, accuracy 0.06666667014360428.\n",
      "Training epoch 4 batch 40 with loss 1.7406702041625977, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 41 with loss 1.6735597848892212, accuracy 0.2083333283662796.\n",
      "Training epoch 4 batch 42 with loss 1.8258053064346313, accuracy 0.24722222983837128.\n",
      "Training epoch 4 batch 43 with loss 1.7463493347167969, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 44 with loss 1.8872005939483643, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 45 with loss 1.7268445491790771, accuracy 0.3531745970249176.\n",
      "Training epoch 4 batch 46 with loss 1.8801189661026, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 47 with loss 1.789838433265686, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 48 with loss 1.783355712890625, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 49 with loss 1.7662826776504517, accuracy 0.27936509251594543.\n",
      "Training epoch 4 batch 50 with loss 1.7581336498260498, accuracy 0.22499999403953552.\n",
      "Training epoch 4 batch 51 with loss 1.7386634349822998, accuracy 0.24722224473953247.\n",
      "Training epoch 4 batch 52 with loss 1.7306588888168335, accuracy 0.2361111044883728.\n",
      "Training epoch 4 batch 53 with loss 1.8430559635162354, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 54 with loss 1.7218888998031616, accuracy 0.2750000059604645.\n",
      "Training epoch 4 batch 55 with loss 1.797472596168518, accuracy 0.125.\n",
      "Training epoch 4 batch 56 with loss 1.939703345298767, accuracy 0.03333333507180214.\n",
      "Training epoch 4 batch 57 with loss 1.7823737859725952, accuracy 0.125.\n",
      "Training epoch 4 batch 58 with loss 1.6841999292373657, accuracy 0.32777777314186096.\n",
      "Training epoch 4 batch 59 with loss 1.804461121559143, accuracy 0.3611111342906952.\n",
      "Training epoch 4 batch 60 with loss 1.7656800746917725, accuracy 0.14444445073604584.\n",
      "Training epoch 4 batch 61 with loss 1.7534030675888062, accuracy 0.10833333432674408.\n",
      "Training epoch 4 batch 62 with loss 1.6788831949234009, accuracy 0.3055555820465088.\n",
      "Training epoch 4 batch 63 with loss 1.7947397232055664, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 64 with loss 1.8236455917358398, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 65 with loss 1.7910528182983398, accuracy 0.18888889253139496.\n",
      "Training epoch 4 batch 66 with loss 1.7918736934661865, accuracy 0.20555555820465088.\n",
      "Training epoch 4 batch 67 with loss 1.7424808740615845, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 68 with loss 1.6303179264068604, accuracy 0.35277777910232544.\n",
      "Training epoch 4 batch 69 with loss 1.728737473487854, accuracy 0.11269842088222504.\n",
      "Training epoch 4 batch 70 with loss 1.7726342678070068, accuracy 0.02777777798473835.\n",
      "Training epoch 4 batch 71 with loss 1.7918542623519897, accuracy 0.125.\n",
      "Training epoch 4 batch 72 with loss 1.7834129333496094, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 73 with loss 1.7796289920806885, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 74 with loss 1.8532402515411377, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 75 with loss 1.8360137939453125, accuracy 0.125.\n",
      "Training epoch 4 batch 76 with loss 1.86648428440094, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 77 with loss 1.7628824710845947, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 78 with loss 1.7269814014434814, accuracy 0.24722221493721008.\n",
      "Training epoch 4 batch 79 with loss 1.8067710399627686, accuracy 0.16944445669651031.\n",
      "Training epoch 4 batch 80 with loss 1.7555599212646484, accuracy 0.24166667461395264.\n",
      "Training epoch 4 batch 81 with loss 1.7885125875473022, accuracy 0.1924603283405304.\n",
      "Training epoch 4 batch 82 with loss 1.7536426782608032, accuracy 0.3194444477558136.\n",
      "Training epoch 4 batch 83 with loss 1.6811755895614624, accuracy 0.2968254089355469.\n",
      "Training epoch 4 batch 84 with loss 1.8325294256210327, accuracy 0.222222238779068.\n",
      "Training epoch 4 batch 85 with loss 1.8007389307022095, accuracy 0.2142857164144516.\n",
      "Training epoch 4 batch 86 with loss 1.7164535522460938, accuracy 0.3583333492279053.\n",
      "Training epoch 4 batch 87 with loss 1.7443504333496094, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 88 with loss 1.719948410987854, accuracy 0.22777777910232544.\n",
      "Training epoch 4 batch 89 with loss 1.8218915462493896, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 90 with loss 1.753928542137146, accuracy 0.3154761791229248.\n",
      "Training epoch 4 batch 91 with loss 1.6771198511123657, accuracy 0.2916666567325592.\n",
      "Training epoch 4 batch 92 with loss 1.7534717321395874, accuracy 0.23333334922790527.\n",
      "Training epoch 4 batch 93 with loss 1.7201429605484009, accuracy 0.3055555522441864.\n",
      "Training epoch 4 batch 94 with loss 1.6934254169464111, accuracy 0.19166666269302368.\n",
      "Training epoch 4 batch 95 with loss 1.8154741525650024, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 96 with loss 1.7272932529449463, accuracy 0.38055557012557983.\n",
      "Training epoch 4 batch 97 with loss 1.7242485284805298, accuracy 0.32222220301628113.\n",
      "Training epoch 4 batch 98 with loss 1.822117805480957, accuracy 0.1805555671453476.\n",
      "Training epoch 4 batch 99 with loss 1.7747924327850342, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 100 with loss 1.692784309387207, accuracy 0.1269841343164444.\n",
      "Training epoch 4 batch 101 with loss 1.7891439199447632, accuracy 0.10000000149011612.\n",
      "Training epoch 4 batch 102 with loss 1.7836774587631226, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 103 with loss 1.8761358261108398, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 104 with loss 1.7641156911849976, accuracy 0.3333333432674408.\n",
      "Training epoch 4 batch 105 with loss 1.6649500131607056, accuracy 0.3499999940395355.\n",
      "Training epoch 4 batch 106 with loss 1.815511703491211, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 107 with loss 1.7147642374038696, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 108 with loss 1.6801769733428955, accuracy 0.23888888955116272.\n",
      "Training epoch 4 batch 109 with loss 1.8227949142456055, accuracy 0.15555556118488312.\n",
      "Training epoch 4 batch 110 with loss 1.7690271139144897, accuracy 0.20000000298023224.\n",
      "Training epoch 4 batch 111 with loss 1.712781310081482, accuracy 0.21481481194496155.\n",
      "Training epoch 4 batch 112 with loss 1.8090665340423584, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 113 with loss 1.719032883644104, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 114 with loss 1.7838704586029053, accuracy 0.21111111342906952.\n",
      "Training epoch 4 batch 115 with loss 1.7907600402832031, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 116 with loss 1.8123964071273804, accuracy 0.0.\n",
      "Training epoch 4 batch 117 with loss 1.730676293373108, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 118 with loss 1.72186279296875, accuracy 0.222222238779068.\n",
      "Training epoch 4 batch 119 with loss 1.7957607507705688, accuracy 0.17500001192092896.\n",
      "Training epoch 4 batch 120 with loss 1.6652681827545166, accuracy 0.32499998807907104.\n",
      "Training epoch 4 batch 121 with loss 1.9384607076644897, accuracy 0.0.\n",
      "Training epoch 4 batch 122 with loss 1.7191721200942993, accuracy 0.10000000149011612.\n",
      "Training epoch 4 batch 123 with loss 1.6673572063446045, accuracy 0.3063492178916931.\n",
      "Training epoch 4 batch 124 with loss 1.7611232995986938, accuracy 0.2738095223903656.\n",
      "Training epoch 4 batch 125 with loss 1.7505725622177124, accuracy 0.28333333134651184.\n",
      "Training epoch 4 batch 126 with loss 1.813326120376587, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 127 with loss 1.8540432453155518, accuracy 0.14444445073604584.\n",
      "Training epoch 4 batch 128 with loss 1.7738927602767944, accuracy 0.22500000894069672.\n",
      "Training epoch 4 batch 129 with loss 1.7856521606445312, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 130 with loss 1.8157062530517578, accuracy 0.22777779400348663.\n",
      "Training epoch 4 batch 131 with loss 1.8004909753799438, accuracy 0.1210317462682724.\n",
      "Training epoch 4 batch 132 with loss 1.6952126026153564, accuracy 0.17777778208255768.\n",
      "Training epoch 4 batch 133 with loss 1.801709771156311, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 134 with loss 1.7103601694107056, accuracy 0.19166666269302368.\n",
      "Training epoch 4 batch 135 with loss 1.838181495666504, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 136 with loss 1.6908899545669556, accuracy 0.3361111283302307.\n",
      "Training epoch 4 batch 137 with loss 1.669028639793396, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 138 with loss 1.7558023929595947, accuracy 0.375.\n",
      "Training epoch 4 batch 139 with loss 1.7617769241333008, accuracy 0.12222222983837128.\n",
      "Training epoch 4 batch 140 with loss 1.8130241632461548, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 141 with loss 1.7173984050750732, accuracy 0.4027777910232544.\n",
      "Training epoch 4 batch 142 with loss 1.818469762802124, accuracy 0.02777777798473835.\n",
      "Training epoch 4 batch 143 with loss 1.8135664463043213, accuracy 0.0.\n",
      "Training epoch 4 batch 144 with loss 1.7013276815414429, accuracy 0.21388888359069824.\n",
      "Training epoch 4 batch 145 with loss 1.762695550918579, accuracy 0.32500001788139343.\n",
      "Training epoch 4 batch 146 with loss 1.7704927921295166, accuracy 0.25833332538604736.\n",
      "Training epoch 4 batch 147 with loss 1.9688360691070557, accuracy 0.03333333507180214.\n",
      "Training epoch 4 batch 148 with loss 1.8787720203399658, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 149 with loss 1.7871129512786865, accuracy 0.0446428582072258.\n",
      "Training epoch 4 batch 150 with loss 1.7674773931503296, accuracy 0.18611110746860504.\n",
      "Training epoch 4 batch 151 with loss 1.9117259979248047, accuracy 0.1587301641702652.\n",
      "Training epoch 4 batch 152 with loss 1.7751401662826538, accuracy 0.06666667014360428.\n",
      "Training epoch 4 batch 153 with loss 1.6800353527069092, accuracy 0.3333333432674408.\n",
      "Training epoch 4 batch 154 with loss 1.763007402420044, accuracy 0.3611111044883728.\n",
      "Training epoch 4 batch 155 with loss 1.7618166208267212, accuracy 0.0793650820851326.\n",
      "Training epoch 4 batch 156 with loss 1.7081811428070068, accuracy 0.3333333432674408.\n",
      "Training epoch 4 batch 157 with loss 1.733170747756958, accuracy 0.3055555522441864.\n",
      "Training epoch 4 batch 158 with loss 1.7371419668197632, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 159 with loss 1.7209806442260742, accuracy 0.1805555671453476.\n",
      "Training epoch 4 batch 160 with loss 1.8090810775756836, accuracy 0.3055555820465088.\n",
      "Training epoch 4 batch 161 with loss 1.8136276006698608, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 162 with loss 1.7472789287567139, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 163 with loss 1.7559692859649658, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 164 with loss 1.6644130945205688, accuracy 0.30277779698371887.\n",
      "Training epoch 4 batch 165 with loss 1.6151574850082397, accuracy 0.4888889193534851.\n",
      "Training epoch 4 batch 166 with loss 1.8037328720092773, accuracy 0.17777778208255768.\n",
      "Training epoch 4 batch 167 with loss 1.8088566064834595, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 168 with loss 1.6934372186660767, accuracy 0.47777777910232544.\n",
      "Training epoch 4 batch 169 with loss 1.7706495523452759, accuracy 0.2916666567325592.\n",
      "Training epoch 4 batch 170 with loss 1.7872915267944336, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 171 with loss 1.8711392879486084, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 172 with loss 1.7913191318511963, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 173 with loss 1.7462564706802368, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 174 with loss 1.7088924646377563, accuracy 0.18611110746860504.\n",
      "Training epoch 4 batch 175 with loss 1.8298444747924805, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 176 with loss 1.8067277669906616, accuracy 0.2888889014720917.\n",
      "Training epoch 4 batch 177 with loss 1.8631891012191772, accuracy 0.21388888359069824.\n",
      "Training epoch 4 batch 178 with loss 1.8669805526733398, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 179 with loss 1.7754828929901123, accuracy 0.43611112236976624.\n",
      "Training epoch 4 batch 180 with loss 1.8326421976089478, accuracy 0.236111119389534.\n",
      "Training epoch 4 batch 181 with loss 1.7073678970336914, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 182 with loss 1.7472572326660156, accuracy 0.2361111044883728.\n",
      "Training epoch 4 batch 183 with loss 1.799790620803833, accuracy 0.0694444477558136.\n",
      "Training epoch 4 batch 184 with loss 1.8493292331695557, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 185 with loss 1.7370704412460327, accuracy 0.4305555522441864.\n",
      "Training epoch 4 batch 186 with loss 1.8109853267669678, accuracy 0.3333333432674408.\n",
      "Training epoch 4 batch 187 with loss 1.6498451232910156, accuracy 0.45000001788139343.\n",
      "Training epoch 4 batch 188 with loss 1.815945029258728, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 189 with loss 1.7683141231536865, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 190 with loss 1.6877191066741943, accuracy 0.2799999713897705.\n",
      "Training epoch 4 batch 191 with loss 1.7042417526245117, accuracy 0.31111112236976624.\n",
      "Training epoch 4 batch 192 with loss 1.8723068237304688, accuracy 0.20000000298023224.\n",
      "Training epoch 4 batch 193 with loss 1.7055118083953857, accuracy 0.4333333373069763.\n",
      "Training epoch 4 batch 194 with loss 1.6908413171768188, accuracy 0.19166666269302368.\n",
      "Training epoch 4 batch 195 with loss 1.8043127059936523, accuracy 0.0694444477558136.\n",
      "Training epoch 4 batch 196 with loss 1.7182222604751587, accuracy 0.3777777850627899.\n",
      "Training epoch 4 batch 197 with loss 1.865624189376831, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 198 with loss 1.7391173839569092, accuracy 0.2013888955116272.\n",
      "Training epoch 4 batch 199 with loss 1.7311614751815796, accuracy 0.3444444537162781.\n",
      "Training epoch 4 batch 200 with loss 1.8482582569122314, accuracy 0.23333334922790527.\n",
      "Training epoch 4 batch 201 with loss 1.7721620798110962, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 202 with loss 1.796677827835083, accuracy 0.10185185074806213.\n",
      "Training epoch 4 batch 203 with loss 1.749810814857483, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 204 with loss 1.8602256774902344, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 205 with loss 1.7553682327270508, accuracy 0.24722222983837128.\n",
      "Training epoch 4 batch 206 with loss 1.7910597324371338, accuracy 0.347222238779068.\n",
      "Training epoch 4 batch 207 with loss 1.8562885522842407, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 208 with loss 1.7100169658660889, accuracy 0.3115079402923584.\n",
      "Training epoch 4 batch 209 with loss 1.8316383361816406, accuracy 0.11574074625968933.\n",
      "Training epoch 4 batch 210 with loss 1.8379032611846924, accuracy 0.02083333395421505.\n",
      "Training epoch 4 batch 211 with loss 1.769057035446167, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 212 with loss 1.859596848487854, accuracy 0.2611111104488373.\n",
      "Training epoch 4 batch 213 with loss 1.9016025066375732, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 214 with loss 1.7632373571395874, accuracy 0.17777778208255768.\n",
      "Training epoch 4 batch 215 with loss 1.6835705041885376, accuracy 0.2111111283302307.\n",
      "Training epoch 4 batch 216 with loss 1.728249192237854, accuracy 0.22777777910232544.\n",
      "Training epoch 4 batch 217 with loss 1.7543662786483765, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 218 with loss 1.8869476318359375, accuracy 0.125.\n",
      "Training epoch 4 batch 219 with loss 1.8557640314102173, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 220 with loss 1.7267566919326782, accuracy 0.17500001192092896.\n",
      "Training epoch 4 batch 221 with loss 1.82614266872406, accuracy 0.0694444477558136.\n",
      "Training epoch 4 batch 222 with loss 1.7791723012924194, accuracy 0.1071428582072258.\n",
      "Training epoch 4 batch 223 with loss 1.838139295578003, accuracy 0.14444445073604584.\n",
      "Training epoch 4 batch 224 with loss 1.824629783630371, accuracy 0.25.\n",
      "Training epoch 4 batch 225 with loss 1.8203598260879517, accuracy 0.2142857164144516.\n",
      "Training epoch 4 batch 226 with loss 1.771677017211914, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 227 with loss 1.7784188985824585, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 228 with loss 1.7466074228286743, accuracy 0.3055555820465088.\n",
      "Training epoch 4 batch 229 with loss 1.754468321800232, accuracy 0.3611111044883728.\n",
      "Training epoch 4 batch 230 with loss 1.7541477680206299, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 231 with loss 1.876185655593872, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 232 with loss 1.7749723196029663, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 233 with loss 1.8169195652008057, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 234 with loss 1.7796376943588257, accuracy 0.23333334922790527.\n",
      "Training epoch 4 batch 235 with loss 1.8175404071807861, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 236 with loss 1.7679383754730225, accuracy 0.3083333373069763.\n",
      "Training epoch 4 batch 237 with loss 1.794320821762085, accuracy 0.1726190447807312.\n",
      "Training epoch 4 batch 238 with loss 1.817237138748169, accuracy 0.03333333507180214.\n",
      "Training epoch 4 batch 239 with loss 1.8030624389648438, accuracy 0.23888888955116272.\n",
      "Training epoch 4 batch 240 with loss 1.7028900384902954, accuracy 0.3055555522441864.\n",
      "Training epoch 4 batch 241 with loss 1.8077605962753296, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 242 with loss 1.7771084308624268, accuracy 0.23333334922790527.\n",
      "Training epoch 4 batch 243 with loss 1.7176936864852905, accuracy 0.1626984179019928.\n",
      "Training epoch 4 batch 244 with loss 1.8383821249008179, accuracy 0.15833333134651184.\n",
      "Training epoch 4 batch 245 with loss 1.7008860111236572, accuracy 0.130952388048172.\n",
      "Training epoch 4 batch 246 with loss 1.744468331336975, accuracy 0.1626984179019928.\n",
      "Training epoch 4 batch 247 with loss 1.815643310546875, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 248 with loss 1.7140954732894897, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 249 with loss 1.780495285987854, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 250 with loss 1.8285108804702759, accuracy 0.06666667014360428.\n",
      "Training epoch 4 batch 251 with loss 1.679225206375122, accuracy 0.375.\n",
      "Training epoch 4 batch 252 with loss 1.819443702697754, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 253 with loss 1.726792335510254, accuracy 0.1726190596818924.\n",
      "Training epoch 4 batch 254 with loss 1.73702073097229, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 255 with loss 1.8046276569366455, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 256 with loss 1.6481891870498657, accuracy 0.2611111104488373.\n",
      "Training epoch 4 batch 257 with loss 1.8240382671356201, accuracy 0.20000000298023224.\n",
      "Training epoch 4 batch 258 with loss 1.7452694177627563, accuracy 0.1805555671453476.\n",
      "Training epoch 4 batch 259 with loss 1.7404941320419312, accuracy 0.40833336114883423.\n",
      "Training epoch 4 batch 260 with loss 1.713361144065857, accuracy 0.15555556118488312.\n",
      "Training epoch 4 batch 261 with loss 1.847084403038025, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 262 with loss 1.8028723001480103, accuracy 0.3055555522441864.\n",
      "Training epoch 4 batch 263 with loss 1.8208580017089844, accuracy 0.3194444477558136.\n",
      "Training epoch 4 batch 264 with loss 1.7513148784637451, accuracy 0.2291666865348816.\n",
      "Training epoch 4 batch 265 with loss 1.7589540481567383, accuracy 0.3365079462528229.\n",
      "Training epoch 4 batch 266 with loss 1.79586660861969, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 267 with loss 1.7834078073501587, accuracy 0.3125.\n",
      "Training epoch 4 batch 268 with loss 1.6960242986679077, accuracy 0.20555555820465088.\n",
      "Training epoch 4 batch 269 with loss 1.6906616687774658, accuracy 0.3361111283302307.\n",
      "Training epoch 4 batch 270 with loss 1.7653852701187134, accuracy 0.09047619253396988.\n",
      "Training epoch 4 batch 271 with loss 1.8214092254638672, accuracy 0.06666667014360428.\n",
      "Training epoch 4 batch 272 with loss 1.850844383239746, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 273 with loss 1.697156310081482, accuracy 0.28333333134651184.\n",
      "Training epoch 4 batch 274 with loss 1.8181365728378296, accuracy 0.2222222238779068.\n",
      "Training epoch 4 batch 275 with loss 1.7773969173431396, accuracy 0.4138888716697693.\n",
      "Training epoch 4 batch 276 with loss 1.7765693664550781, accuracy 0.2222222238779068.\n",
      "Training epoch 4 batch 277 with loss 1.8439629077911377, accuracy 0.14047619700431824.\n",
      "Training epoch 4 batch 278 with loss 1.7787036895751953, accuracy 0.3611111342906952.\n",
      "Training epoch 4 batch 279 with loss 1.7816801071166992, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 280 with loss 1.8022956848144531, accuracy 0.236111119389534.\n",
      "Training epoch 4 batch 281 with loss 1.7047138214111328, accuracy 0.12222222983837128.\n",
      "Training epoch 4 batch 282 with loss 1.7758162021636963, accuracy 0.236111119389534.\n",
      "Training epoch 4 batch 283 with loss 1.6819686889648438, accuracy 0.39444443583488464.\n",
      "Training epoch 4 batch 284 with loss 1.8711128234863281, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 285 with loss 1.8079583644866943, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 286 with loss 1.7814162969589233, accuracy 0.2750000059604645.\n",
      "Training epoch 4 batch 287 with loss 1.7853819131851196, accuracy 0.4027777910232544.\n",
      "Training epoch 4 batch 288 with loss 1.826777458190918, accuracy 0.0476190485060215.\n",
      "Training epoch 4 batch 289 with loss 1.6806676387786865, accuracy 0.38055557012557983.\n",
      "Training epoch 4 batch 290 with loss 1.7357170581817627, accuracy 0.21388888359069824.\n",
      "Training epoch 4 batch 291 with loss 1.6452205181121826, accuracy 0.4166666865348816.\n",
      "Training epoch 4 batch 292 with loss 1.8134574890136719, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 293 with loss 1.835960030555725, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 294 with loss 1.7351939678192139, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 295 with loss 1.7556155920028687, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 296 with loss 1.7873623371124268, accuracy 0.20000001788139343.\n",
      "Training epoch 4 batch 297 with loss 1.761069655418396, accuracy 0.3115079402923584.\n",
      "Training epoch 4 batch 298 with loss 1.785292387008667, accuracy 0.2222222238779068.\n",
      "Training epoch 4 batch 299 with loss 1.8006004095077515, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 300 with loss 1.8417298793792725, accuracy 0.21111111342906952.\n",
      "Training epoch 4 batch 301 with loss 1.80474054813385, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 302 with loss 1.746443510055542, accuracy 0.20000000298023224.\n",
      "Training epoch 4 batch 303 with loss 1.6736685037612915, accuracy 0.3611111342906952.\n",
      "Training epoch 4 batch 304 with loss 1.8431451320648193, accuracy 0.13611111044883728.\n",
      "Training epoch 4 batch 305 with loss 1.7861011028289795, accuracy 0.1527777910232544.\n",
      "Training epoch 4 batch 306 with loss 1.9304466247558594, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 307 with loss 1.7395353317260742, accuracy 0.3916666805744171.\n",
      "Training epoch 4 batch 308 with loss 1.9053070545196533, accuracy 0.1875.\n",
      "Training epoch 4 batch 309 with loss 1.7686269283294678, accuracy 0.46666669845581055.\n",
      "Training epoch 4 batch 310 with loss 1.8072564601898193, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 311 with loss 1.7288625240325928, accuracy 0.14444445073604584.\n",
      "Training epoch 4 batch 312 with loss 1.7453558444976807, accuracy 0.17777778208255768.\n",
      "Training epoch 4 batch 313 with loss 1.6681735515594482, accuracy 0.45000001788139343.\n",
      "Training epoch 4 batch 314 with loss 1.8391602039337158, accuracy 0.14444445073604584.\n",
      "Training epoch 4 batch 315 with loss 1.8044360876083374, accuracy 0.18333333730697632.\n",
      "Training epoch 4 batch 316 with loss 1.841422438621521, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 317 with loss 1.8547704219818115, accuracy 0.0.\n",
      "Training epoch 4 batch 318 with loss 1.852373719215393, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 319 with loss 1.7995307445526123, accuracy 0.13333334028720856.\n",
      "Training epoch 4 batch 320 with loss 1.68893563747406, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 321 with loss 1.8048450946807861, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 322 with loss 1.7556854486465454, accuracy 0.3305555582046509.\n",
      "Training epoch 4 batch 323 with loss 1.8404209613800049, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 324 with loss 1.850116491317749, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 325 with loss 1.7386474609375, accuracy 0.28333333134651184.\n",
      "Training epoch 4 batch 326 with loss 1.8012065887451172, accuracy 0.3392857313156128.\n",
      "Training epoch 4 batch 327 with loss 1.7194324731826782, accuracy 0.3055555820465088.\n",
      "Training epoch 4 batch 328 with loss 1.8667482137680054, accuracy 0.15833333134651184.\n",
      "Training epoch 4 batch 329 with loss 1.7606827020645142, accuracy 0.25.\n",
      "Training epoch 4 batch 330 with loss 1.6678053140640259, accuracy 0.20000001788139343.\n",
      "Training epoch 4 batch 331 with loss 1.766547441482544, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 332 with loss 1.770740270614624, accuracy 0.190476194024086.\n",
      "Training epoch 4 batch 333 with loss 1.726712942123413, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 334 with loss 1.8607063293457031, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 335 with loss 1.7730461359024048, accuracy 0.10000000894069672.\n",
      "Training epoch 4 batch 336 with loss 1.7531087398529053, accuracy 0.3849206566810608.\n",
      "Training epoch 4 batch 337 with loss 1.7187585830688477, accuracy 0.3888888955116272.\n",
      "Training epoch 4 batch 338 with loss 1.78622567653656, accuracy 0.08095238357782364.\n",
      "Training epoch 4 batch 339 with loss 1.7093254327774048, accuracy 0.3333333432674408.\n",
      "Training epoch 4 batch 340 with loss 1.8543132543563843, accuracy 0.14444445073604584.\n",
      "Training epoch 4 batch 341 with loss 1.6395320892333984, accuracy 0.3166666626930237.\n",
      "Training epoch 4 batch 342 with loss 1.8077157735824585, accuracy 0.10000000149011612.\n",
      "Training epoch 4 batch 343 with loss 1.8018392324447632, accuracy 0.1071428582072258.\n",
      "Training epoch 4 batch 344 with loss 1.7450567483901978, accuracy 0.4027777910232544.\n",
      "Training epoch 4 batch 345 with loss 1.7475826740264893, accuracy 0.22936508059501648.\n",
      "Training epoch 4 batch 346 with loss 1.847041368484497, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 347 with loss 1.776706337928772, accuracy 0.18333333730697632.\n",
      "Training epoch 4 batch 348 with loss 1.7199344635009766, accuracy 0.26944443583488464.\n",
      "Training epoch 4 batch 349 with loss 1.7909005880355835, accuracy 0.10833333432674408.\n",
      "Training epoch 4 batch 350 with loss 1.6676782369613647, accuracy 0.31666669249534607.\n",
      "Training epoch 4 batch 351 with loss 1.8420684337615967, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 352 with loss 1.7422645092010498, accuracy 0.18333333730697632.\n",
      "Training epoch 4 batch 353 with loss 1.6779762506484985, accuracy 0.12222222983837128.\n",
      "Training epoch 4 batch 354 with loss 1.8332315683364868, accuracy 0.12222222983837128.\n",
      "Training epoch 4 batch 355 with loss 1.7038180828094482, accuracy 0.15833333134651184.\n",
      "Training epoch 4 batch 356 with loss 1.868987798690796, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 357 with loss 1.7752084732055664, accuracy 0.3948412537574768.\n",
      "Training epoch 4 batch 358 with loss 1.7436838150024414, accuracy 0.32499998807907104.\n",
      "Training epoch 4 batch 359 with loss 1.7768646478652954, accuracy 0.38055557012557983.\n",
      "Training epoch 4 batch 360 with loss 1.8087295293807983, accuracy 0.2361111044883728.\n",
      "Training epoch 4 batch 361 with loss 1.7321573495864868, accuracy 0.20555555820465088.\n",
      "Training epoch 4 batch 362 with loss 1.6714967489242554, accuracy 0.222222238779068.\n",
      "Training epoch 4 batch 363 with loss 1.8572051525115967, accuracy 0.444444477558136.\n",
      "Training epoch 4 batch 364 with loss 1.8427460193634033, accuracy 0.125.\n",
      "Training epoch 4 batch 365 with loss 1.8209298849105835, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 366 with loss 1.75423264503479, accuracy 0.3027777671813965.\n",
      "Training epoch 4 batch 367 with loss 1.821734070777893, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 368 with loss 1.7376983165740967, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 369 with loss 1.7484569549560547, accuracy 0.03333333507180214.\n",
      "Training epoch 4 batch 370 with loss 1.8232122659683228, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 371 with loss 1.9480243921279907, accuracy 0.3333333432674408.\n",
      "Training epoch 4 batch 372 with loss 1.7842013835906982, accuracy 0.25333333015441895.\n",
      "Training epoch 4 batch 373 with loss 1.895668387413025, accuracy 0.25.\n",
      "Training epoch 4 batch 374 with loss 1.675307035446167, accuracy 0.45000001788139343.\n",
      "Training epoch 4 batch 375 with loss 1.750417709350586, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 376 with loss 1.7002604007720947, accuracy 0.38055557012557983.\n",
      "Training epoch 4 batch 377 with loss 1.780068039894104, accuracy 0.23055556416511536.\n",
      "Training epoch 4 batch 378 with loss 1.662441611289978, accuracy 0.33888888359069824.\n",
      "Training epoch 4 batch 379 with loss 1.8346550464630127, accuracy 0.17777778208255768.\n",
      "Training epoch 4 batch 380 with loss 1.8033517599105835, accuracy 0.24166667461395264.\n",
      "Training epoch 4 batch 381 with loss 1.7254533767700195, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 382 with loss 1.8832805156707764, accuracy 0.11269841343164444.\n",
      "Training epoch 4 batch 383 with loss 1.8132683038711548, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 384 with loss 1.7839488983154297, accuracy 0.125.\n",
      "Training epoch 4 batch 385 with loss 1.6957876682281494, accuracy 0.24444445967674255.\n",
      "Training epoch 4 batch 386 with loss 1.7961761951446533, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 387 with loss 1.7286932468414307, accuracy 0.2420634925365448.\n",
      "Training epoch 4 batch 388 with loss 1.9266014099121094, accuracy 0.0694444477558136.\n",
      "Training epoch 4 batch 389 with loss 1.8477370738983154, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 390 with loss 1.7118498086929321, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 391 with loss 1.75008225440979, accuracy 0.42222222685813904.\n",
      "Training epoch 4 batch 392 with loss 1.7887251377105713, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 393 with loss 1.87480890750885, accuracy 0.28333336114883423.\n",
      "Training epoch 4 batch 394 with loss 1.7361886501312256, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 395 with loss 1.8534997701644897, accuracy 0.111111119389534.\n",
      "Training epoch 4 batch 396 with loss 1.8256076574325562, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 397 with loss 1.7414602041244507, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 398 with loss 1.8158867359161377, accuracy 0.130952388048172.\n",
      "Training epoch 4 batch 399 with loss 1.8638677597045898, accuracy 0.130952388048172.\n",
      "Training epoch 4 batch 400 with loss 1.7031952142715454, accuracy 0.3861111104488373.\n",
      "Training epoch 4 batch 401 with loss 1.8065242767333984, accuracy 0.2083333283662796.\n",
      "Training epoch 4 batch 402 with loss 1.808971643447876, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 403 with loss 1.6762707233428955, accuracy 0.2666666805744171.\n",
      "Training epoch 4 batch 404 with loss 1.7944555282592773, accuracy 0.10277777910232544.\n",
      "Training epoch 4 batch 405 with loss 1.6930522918701172, accuracy 0.23333334922790527.\n",
      "Training epoch 4 batch 406 with loss 1.8351686000823975, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 407 with loss 1.7621784210205078, accuracy 0.0793650820851326.\n",
      "Training epoch 4 batch 408 with loss 1.8049157857894897, accuracy 0.20000000298023224.\n",
      "Training epoch 4 batch 409 with loss 1.8285611867904663, accuracy 0.222222238779068.\n",
      "Training epoch 4 batch 410 with loss 1.7833296060562134, accuracy 0.22777777910232544.\n",
      "Training epoch 4 batch 411 with loss 1.646188735961914, accuracy 0.4055555760860443.\n",
      "Training epoch 4 batch 412 with loss 1.793461561203003, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 413 with loss 1.6795313358306885, accuracy 0.3888888955116272.\n",
      "Training epoch 4 batch 414 with loss 1.8452370166778564, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 415 with loss 1.7983516454696655, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 416 with loss 1.7237884998321533, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 417 with loss 1.7215635776519775, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 418 with loss 1.698345422744751, accuracy 0.25.\n",
      "Training epoch 4 batch 419 with loss 1.7442525625228882, accuracy 0.2500000298023224.\n",
      "Training epoch 4 batch 420 with loss 1.7150474786758423, accuracy 0.24722222983837128.\n",
      "Training epoch 4 batch 421 with loss 1.7420591115951538, accuracy 0.210317462682724.\n",
      "Training epoch 4 batch 422 with loss 1.7170593738555908, accuracy 0.21944445371627808.\n",
      "Training epoch 4 batch 423 with loss 1.6999998092651367, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 424 with loss 1.7689920663833618, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 425 with loss 1.7198587656021118, accuracy 0.4444444477558136.\n",
      "Training epoch 4 batch 426 with loss 1.8052047491073608, accuracy 0.236111119389534.\n",
      "Training epoch 4 batch 427 with loss 1.731130599975586, accuracy 0.1825396865606308.\n",
      "Training epoch 4 batch 428 with loss 1.677465796470642, accuracy 0.2500000298023224.\n",
      "Training epoch 4 batch 429 with loss 1.8271684646606445, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 430 with loss 1.7430566549301147, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 431 with loss 1.7372360229492188, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 432 with loss 1.769200325012207, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 433 with loss 1.841210126876831, accuracy 0.13611111044883728.\n",
      "Training epoch 4 batch 434 with loss 1.719190001487732, accuracy 0.3638888895511627.\n",
      "Training epoch 4 batch 435 with loss 1.7657935619354248, accuracy 0.3263888657093048.\n",
      "Training epoch 4 batch 436 with loss 1.8194820880889893, accuracy 0.14444445073604584.\n",
      "Training epoch 4 batch 437 with loss 1.6720294952392578, accuracy 0.3958333432674408.\n",
      "Training epoch 4 batch 438 with loss 1.873382329940796, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 439 with loss 1.759264588356018, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 440 with loss 1.7564022541046143, accuracy 0.29722222685813904.\n",
      "Training epoch 4 batch 441 with loss 1.7585136890411377, accuracy 0.22380952537059784.\n",
      "Training epoch 4 batch 442 with loss 1.8059536218643188, accuracy 0.18888889253139496.\n",
      "Training epoch 4 batch 443 with loss 1.799121618270874, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 444 with loss 1.805930733680725, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 445 with loss 1.7852680683135986, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 446 with loss 1.8341987133026123, accuracy 0.19761905074119568.\n",
      "Training epoch 4 batch 447 with loss 1.8483893871307373, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 448 with loss 1.7693592309951782, accuracy 0.1805555671453476.\n",
      "Training epoch 4 batch 449 with loss 1.648406744003296, accuracy 0.3222222328186035.\n",
      "Training epoch 4 batch 450 with loss 1.6648712158203125, accuracy 0.3777777850627899.\n",
      "Training epoch 4 batch 451 with loss 1.9289716482162476, accuracy 0.10833333432674408.\n",
      "Training epoch 4 batch 452 with loss 1.726495385169983, accuracy 0.1210317462682724.\n",
      "Training epoch 4 batch 453 with loss 1.8217319250106812, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 454 with loss 1.7348512411117554, accuracy 0.26111114025115967.\n",
      "Training epoch 4 batch 455 with loss 1.7718942165374756, accuracy 0.19166666269302368.\n",
      "Training epoch 4 batch 456 with loss 1.7982594966888428, accuracy 0.3333333432674408.\n",
      "Training epoch 4 batch 457 with loss 1.7558501958847046, accuracy 0.4722222685813904.\n",
      "Training epoch 4 batch 458 with loss 1.8702675104141235, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 459 with loss 1.7156479358673096, accuracy 0.14444445073604584.\n",
      "Training epoch 4 batch 460 with loss 1.7775987386703491, accuracy 0.18611110746860504.\n",
      "Training epoch 4 batch 461 with loss 1.7502622604370117, accuracy 0.24722224473953247.\n",
      "Training epoch 4 batch 462 with loss 1.7347943782806396, accuracy 0.38055554032325745.\n",
      "Training epoch 4 batch 463 with loss 1.779197335243225, accuracy 0.20000000298023224.\n",
      "Training epoch 4 batch 464 with loss 1.7887338399887085, accuracy 0.16388888657093048.\n",
      "Training epoch 4 batch 465 with loss 1.7420190572738647, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 466 with loss 1.7308213710784912, accuracy 0.28333336114883423.\n",
      "Training epoch 4 batch 467 with loss 1.7920007705688477, accuracy 0.2805555462837219.\n",
      "Training epoch 4 batch 468 with loss 1.6144222021102905, accuracy 0.2888889014720917.\n",
      "Training epoch 4 batch 469 with loss 1.874427080154419, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 470 with loss 1.703503966331482, accuracy 0.3055555820465088.\n",
      "Training epoch 4 batch 471 with loss 1.7169969081878662, accuracy 0.1686508059501648.\n",
      "Training epoch 4 batch 472 with loss 1.8497817516326904, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 473 with loss 1.7680089473724365, accuracy 0.2916666567325592.\n",
      "Training epoch 4 batch 474 with loss 1.7267630100250244, accuracy 0.31111112236976624.\n",
      "Training epoch 4 batch 475 with loss 1.7617018222808838, accuracy 0.3055555522441864.\n",
      "Training epoch 4 batch 476 with loss 1.7607721090316772, accuracy 0.23333334922790527.\n",
      "Training epoch 4 batch 477 with loss 1.7433592081069946, accuracy 0.20000001788139343.\n",
      "Training epoch 4 batch 478 with loss 1.7425546646118164, accuracy 0.2222222238779068.\n",
      "Training epoch 4 batch 479 with loss 1.7149251699447632, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 480 with loss 1.8191230297088623, accuracy 0.20555555820465088.\n",
      "Training epoch 4 batch 481 with loss 1.815049409866333, accuracy 0.08750000596046448.\n",
      "Training epoch 4 batch 482 with loss 1.7983648777008057, accuracy 0.05714286118745804.\n",
      "Training epoch 4 batch 483 with loss 1.7673046588897705, accuracy 0.4722222089767456.\n",
      "Training epoch 4 batch 484 with loss 1.8183879852294922, accuracy 0.2083333283662796.\n",
      "Training epoch 4 batch 485 with loss 1.7368104457855225, accuracy 0.3035714328289032.\n",
      "Training epoch 4 batch 486 with loss 1.6831419467926025, accuracy 0.24861112236976624.\n",
      "Training epoch 4 batch 487 with loss 1.625241994857788, accuracy 0.4574074149131775.\n",
      "Training epoch 4 batch 488 with loss 1.8060681819915771, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 489 with loss 1.7862460613250732, accuracy 0.10833333432674408.\n",
      "Training epoch 4 batch 490 with loss 1.694053292274475, accuracy 0.375.\n",
      "Training epoch 4 batch 491 with loss 1.794693946838379, accuracy 0.2976190447807312.\n",
      "Training epoch 4 batch 492 with loss 1.786449670791626, accuracy 0.2182539701461792.\n",
      "Training epoch 4 batch 493 with loss 1.6818206310272217, accuracy 0.24722224473953247.\n",
      "Training epoch 4 batch 494 with loss 1.847719430923462, accuracy 0.20000000298023224.\n",
      "Training epoch 4 batch 495 with loss 1.8369983434677124, accuracy 0.03703703731298447.\n",
      "Training epoch 4 batch 496 with loss 1.8011335134506226, accuracy 0.12777778506278992.\n",
      "Training epoch 4 batch 497 with loss 1.717607855796814, accuracy 0.2003968209028244.\n",
      "Training epoch 4 batch 498 with loss 1.741302728652954, accuracy 0.1597222238779068.\n",
      "Training epoch 4 batch 499 with loss 1.8477747440338135, accuracy 0.1865079402923584.\n",
      "Training epoch 4 batch 500 with loss 1.6908395290374756, accuracy 0.3333333432674408.\n",
      "Training epoch 4 batch 501 with loss 1.8644092082977295, accuracy 0.2916666567325592.\n",
      "Training epoch 4 batch 502 with loss 1.8060373067855835, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 503 with loss 1.8525934219360352, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 504 with loss 1.9239953756332397, accuracy 0.1071428582072258.\n",
      "Training epoch 4 batch 505 with loss 1.7523705959320068, accuracy 0.2611111104488373.\n",
      "Training epoch 4 batch 506 with loss 1.7721388339996338, accuracy 0.20000000298023224.\n",
      "Training epoch 4 batch 507 with loss 1.728147268295288, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 508 with loss 1.718529462814331, accuracy 0.18888889253139496.\n",
      "Training epoch 4 batch 509 with loss 1.6934211254119873, accuracy 0.3611111044883728.\n",
      "Training epoch 4 batch 510 with loss 1.703574538230896, accuracy 0.38333335518836975.\n",
      "Training epoch 4 batch 511 with loss 1.831744909286499, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 512 with loss 1.6419556140899658, accuracy 0.37222224473953247.\n",
      "Training epoch 4 batch 513 with loss 1.803755760192871, accuracy 0.25.\n",
      "Training epoch 4 batch 514 with loss 1.7519220113754272, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 515 with loss 1.7100801467895508, accuracy 0.2611111104488373.\n",
      "Training epoch 4 batch 516 with loss 1.8680378198623657, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 517 with loss 1.8408558368682861, accuracy 0.03333333507180214.\n",
      "Training epoch 4 batch 518 with loss 1.8406814336776733, accuracy 0.18333333730697632.\n",
      "Training epoch 4 batch 519 with loss 1.8608458042144775, accuracy 0.1626984179019928.\n",
      "Training epoch 4 batch 520 with loss 1.8593909740447998, accuracy 0.0694444477558136.\n",
      "Training epoch 4 batch 521 with loss 1.863433837890625, accuracy 0.236111119389534.\n",
      "Training epoch 4 batch 522 with loss 1.8683722019195557, accuracy 0.0476190485060215.\n",
      "Training epoch 4 batch 523 with loss 1.8391374349594116, accuracy 0.0763888880610466.\n",
      "Training epoch 4 batch 524 with loss 1.7673343420028687, accuracy 0.28333336114883423.\n",
      "Training epoch 4 batch 525 with loss 1.810951828956604, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 526 with loss 1.8250465393066406, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 527 with loss 1.7891292572021484, accuracy 0.23333334922790527.\n",
      "Training epoch 4 batch 528 with loss 1.8805599212646484, accuracy 0.20555555820465088.\n",
      "Training epoch 4 batch 529 with loss 1.7892557382583618, accuracy 0.22777777910232544.\n",
      "Training epoch 4 batch 530 with loss 1.6771615743637085, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 531 with loss 1.759476661682129, accuracy 0.2888889014720917.\n",
      "Training epoch 4 batch 532 with loss 1.7078487873077393, accuracy 0.20555555820465088.\n",
      "Training epoch 4 batch 533 with loss 1.7360613346099854, accuracy 0.125.\n",
      "Training epoch 4 batch 534 with loss 1.738285779953003, accuracy 0.13650794327259064.\n",
      "Training epoch 4 batch 535 with loss 1.7266464233398438, accuracy 0.3055555522441864.\n",
      "Training epoch 4 batch 536 with loss 1.7987430095672607, accuracy 0.27142858505249023.\n",
      "Training epoch 4 batch 537 with loss 1.6074920892715454, accuracy 0.2916666567325592.\n",
      "Training epoch 4 batch 538 with loss 1.80373215675354, accuracy 0.15079365670681.\n",
      "Training epoch 4 batch 539 with loss 1.785679817199707, accuracy 0.1765872985124588.\n",
      "Training epoch 4 batch 540 with loss 1.764585256576538, accuracy 0.12222222983837128.\n",
      "Training epoch 4 batch 541 with loss 1.690102219581604, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 542 with loss 1.815374732017517, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 543 with loss 1.7833458185195923, accuracy 0.16388890147209167.\n",
      "Training epoch 4 batch 544 with loss 1.8270734548568726, accuracy 0.16388890147209167.\n",
      "Training epoch 4 batch 545 with loss 1.7878696918487549, accuracy 0.0793650820851326.\n",
      "Training epoch 4 batch 546 with loss 1.7605979442596436, accuracy 0.28928571939468384.\n",
      "Training epoch 4 batch 547 with loss 1.7522045373916626, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 548 with loss 1.8094393014907837, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 549 with loss 1.8493788242340088, accuracy 0.18333333730697632.\n",
      "Training epoch 4 batch 550 with loss 1.7200835943222046, accuracy 0.2321428656578064.\n",
      "Training epoch 4 batch 551 with loss 1.7536996603012085, accuracy 0.2916666865348816.\n",
      "Training epoch 4 batch 552 with loss 1.9068164825439453, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 553 with loss 1.8729912042617798, accuracy 0.3888888955116272.\n",
      "Training epoch 4 batch 554 with loss 1.7102200984954834, accuracy 0.4027777910232544.\n",
      "Training epoch 4 batch 555 with loss 1.8089450597763062, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 556 with loss 1.833188772201538, accuracy 0.1349206417798996.\n",
      "Training epoch 4 batch 557 with loss 1.809198021888733, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 558 with loss 1.7768579721450806, accuracy 0.39444446563720703.\n",
      "Training epoch 4 batch 559 with loss 1.8098564147949219, accuracy 0.23055556416511536.\n",
      "Training epoch 4 batch 560 with loss 1.7980215549468994, accuracy 0.15833334624767303.\n",
      "Training epoch 4 batch 561 with loss 1.8587276935577393, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 562 with loss 1.8325614929199219, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 563 with loss 1.7490370273590088, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 564 with loss 1.7148460149765015, accuracy 0.3888888955116272.\n",
      "Training epoch 4 batch 565 with loss 1.7957513332366943, accuracy 0.125.\n",
      "Training epoch 4 batch 566 with loss 1.6863601207733154, accuracy 0.20000001788139343.\n",
      "Training epoch 4 batch 567 with loss 1.7693507671356201, accuracy 0.09444444626569748.\n",
      "Training epoch 4 batch 568 with loss 1.7217638492584229, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 569 with loss 1.6637601852416992, accuracy 0.21944445371627808.\n",
      "Training epoch 4 batch 570 with loss 1.7231881618499756, accuracy 0.3194444477558136.\n",
      "Training epoch 4 batch 571 with loss 1.691248893737793, accuracy 0.13425926864147186.\n",
      "Training epoch 4 batch 572 with loss 1.7768243551254272, accuracy 0.125.\n",
      "Training epoch 4 batch 573 with loss 1.7349984645843506, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 574 with loss 1.8184223175048828, accuracy 0.2916666567325592.\n",
      "Training epoch 4 batch 575 with loss 1.7681783437728882, accuracy 0.2976190447807312.\n",
      "Training epoch 4 batch 576 with loss 1.8251501321792603, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 577 with loss 1.8152410984039307, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 578 with loss 1.8492803573608398, accuracy 0.17777778208255768.\n",
      "Training epoch 4 batch 579 with loss 1.686023473739624, accuracy 0.3888888955116272.\n",
      "Training epoch 4 batch 580 with loss 1.8128948211669922, accuracy 0.25.\n",
      "Training epoch 4 batch 581 with loss 1.8097915649414062, accuracy 0.1825396865606308.\n",
      "Training epoch 4 batch 582 with loss 1.7804445028305054, accuracy 0.125.\n",
      "Training epoch 4 batch 583 with loss 1.8167505264282227, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 584 with loss 1.7646480798721313, accuracy 0.20555555820465088.\n",
      "Training epoch 4 batch 585 with loss 1.7916147708892822, accuracy 0.02777777798473835.\n",
      "Training epoch 4 batch 586 with loss 1.802783727645874, accuracy 0.3222222328186035.\n",
      "Training epoch 4 batch 587 with loss 1.7909854650497437, accuracy 0.3305555582046509.\n",
      "Training epoch 4 batch 588 with loss 1.8027900457382202, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 589 with loss 1.6997077465057373, accuracy 0.4027777910232544.\n",
      "Training epoch 4 batch 590 with loss 1.7625398635864258, accuracy 0.23333334922790527.\n",
      "Training epoch 4 batch 591 with loss 1.8219585418701172, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 592 with loss 1.7656986713409424, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 593 with loss 1.7906084060668945, accuracy 0.2916666567325592.\n",
      "Training epoch 4 batch 594 with loss 1.7281805276870728, accuracy 0.21944443881511688.\n",
      "Training epoch 4 batch 595 with loss 1.7939698696136475, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 596 with loss 1.9092432260513306, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 597 with loss 1.7616045475006104, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 598 with loss 1.7678207159042358, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 599 with loss 1.6682466268539429, accuracy 0.43809524178504944.\n",
      "Training epoch 4 batch 600 with loss 1.7380964756011963, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 601 with loss 1.7019981145858765, accuracy 0.18611110746860504.\n",
      "Training epoch 4 batch 602 with loss 1.7932952642440796, accuracy 0.24722222983837128.\n",
      "Training epoch 4 batch 603 with loss 1.8832660913467407, accuracy 0.20555555820465088.\n",
      "Training epoch 4 batch 604 with loss 1.888327956199646, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 605 with loss 1.7943060398101807, accuracy 0.24166667461395264.\n",
      "Training epoch 4 batch 606 with loss 1.6957601308822632, accuracy 0.2916666567325592.\n",
      "Training epoch 4 batch 607 with loss 1.8067184686660767, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 608 with loss 1.7501742839813232, accuracy 0.29722222685813904.\n",
      "Training epoch 4 batch 609 with loss 1.8592166900634766, accuracy 0.190476194024086.\n",
      "Training epoch 4 batch 610 with loss 1.808346152305603, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 611 with loss 1.8298438787460327, accuracy 0.0694444477558136.\n",
      "Training epoch 4 batch 612 with loss 1.80767023563385, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 613 with loss 1.9182554483413696, accuracy 0.02777777798473835.\n",
      "Training epoch 4 batch 614 with loss 1.7042125463485718, accuracy 0.34166666865348816.\n",
      "Training epoch 4 batch 615 with loss 1.72563898563385, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 616 with loss 1.8918392658233643, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 617 with loss 1.8189916610717773, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 618 with loss 1.7859885692596436, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 619 with loss 1.879868507385254, accuracy 0.236111119389534.\n",
      "Training epoch 4 batch 620 with loss 1.8054883480072021, accuracy 0.25.\n",
      "Training epoch 4 batch 621 with loss 1.6971794366836548, accuracy 0.15833333134651184.\n",
      "Training epoch 4 batch 622 with loss 1.786778211593628, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 623 with loss 1.8095741271972656, accuracy 0.21666666865348816.\n",
      "Training epoch 4 batch 624 with loss 1.7937238216400146, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 625 with loss 1.886453628540039, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 626 with loss 1.8100827932357788, accuracy 0.15833333134651184.\n",
      "Training epoch 4 batch 627 with loss 1.7816097736358643, accuracy 0.31666669249534607.\n",
      "Training epoch 4 batch 628 with loss 1.6959006786346436, accuracy 0.3055555522441864.\n",
      "Training epoch 4 batch 629 with loss 1.8812618255615234, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 630 with loss 1.7750266790390015, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 631 with loss 1.7753623723983765, accuracy 0.25.\n",
      "Training epoch 4 batch 632 with loss 1.857265830039978, accuracy 0.14047619700431824.\n",
      "Training epoch 4 batch 633 with loss 1.7956571578979492, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 634 with loss 1.7409216165542603, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 635 with loss 1.7974932193756104, accuracy 0.19166666269302368.\n",
      "Training epoch 4 batch 636 with loss 1.7407375574111938, accuracy 0.2666666805744171.\n",
      "Training epoch 4 batch 637 with loss 1.751643419265747, accuracy 0.2222222238779068.\n",
      "Training epoch 4 batch 638 with loss 1.7943416833877563, accuracy 0.10000000149011612.\n",
      "Training epoch 4 batch 639 with loss 1.7071605920791626, accuracy 0.2666666805744171.\n",
      "Training epoch 4 batch 640 with loss 1.7757066488265991, accuracy 0.18518519401550293.\n",
      "Training epoch 4 batch 641 with loss 1.7606016397476196, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 642 with loss 1.8867257833480835, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 643 with loss 1.8591266870498657, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 644 with loss 1.7825559377670288, accuracy 0.3305555582046509.\n",
      "Training epoch 4 batch 645 with loss 1.6762313842773438, accuracy 0.3375000059604645.\n",
      "Training epoch 4 batch 646 with loss 1.7451865673065186, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 647 with loss 1.7528949975967407, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 648 with loss 1.7826379537582397, accuracy 0.3499999940395355.\n",
      "Training epoch 4 batch 649 with loss 1.8264490365982056, accuracy 0.14444445073604584.\n",
      "Training epoch 4 batch 650 with loss 1.6997053623199463, accuracy 0.3492063581943512.\n",
      "Training epoch 4 batch 651 with loss 1.7748886346817017, accuracy 0.25.\n",
      "Training epoch 4 batch 652 with loss 1.8812261819839478, accuracy 0.25.\n",
      "Training epoch 4 batch 653 with loss 1.7712757587432861, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 654 with loss 1.8414809703826904, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 655 with loss 1.751866102218628, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 656 with loss 1.764094352722168, accuracy 0.3055555820465088.\n",
      "Training epoch 4 batch 657 with loss 1.8504375219345093, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 658 with loss 1.6923917531967163, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 659 with loss 1.7624633312225342, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 660 with loss 1.6421871185302734, accuracy 0.6013888716697693.\n",
      "Training epoch 4 batch 661 with loss 1.8528547286987305, accuracy 0.1180555522441864.\n",
      "Training epoch 4 batch 662 with loss 1.7382627725601196, accuracy 0.1626984179019928.\n",
      "Training epoch 4 batch 663 with loss 1.8840163946151733, accuracy 0.2013888955116272.\n",
      "Training epoch 4 batch 664 with loss 1.7206380367279053, accuracy 0.3888888955116272.\n",
      "Training epoch 4 batch 665 with loss 1.7573974132537842, accuracy 0.3888888955116272.\n",
      "Training epoch 4 batch 666 with loss 1.6802606582641602, accuracy 0.31111112236976624.\n",
      "Training epoch 4 batch 667 with loss 1.759438157081604, accuracy 0.3055555522441864.\n",
      "Training epoch 4 batch 668 with loss 1.7170692682266235, accuracy 0.2916666865348816.\n",
      "Training epoch 4 batch 669 with loss 1.7924989461898804, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 670 with loss 1.8252780437469482, accuracy 0.366666704416275.\n",
      "Training epoch 4 batch 671 with loss 1.7153441905975342, accuracy 0.14444445073604584.\n",
      "Training epoch 4 batch 672 with loss 1.7997665405273438, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 673 with loss 1.8426363468170166, accuracy 0.10000000149011612.\n",
      "Training epoch 4 batch 674 with loss 1.7705650329589844, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 675 with loss 1.7484848499298096, accuracy 0.20555555820465088.\n",
      "Training epoch 4 batch 676 with loss 1.8478130102157593, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 677 with loss 1.7651551961898804, accuracy 0.18611112236976624.\n",
      "Training epoch 4 batch 678 with loss 1.7915828227996826, accuracy 0.17777778208255768.\n",
      "Training epoch 4 batch 679 with loss 1.6931473016738892, accuracy 0.2847222089767456.\n",
      "Training epoch 4 batch 680 with loss 1.841518759727478, accuracy 0.09880952537059784.\n",
      "Training epoch 4 batch 681 with loss 1.7799705266952515, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 682 with loss 1.7788947820663452, accuracy 0.1597222238779068.\n",
      "Training epoch 4 batch 683 with loss 1.690673828125, accuracy 0.22380952537059784.\n",
      "Training epoch 4 batch 684 with loss 1.7265621423721313, accuracy 0.35555556416511536.\n",
      "Training epoch 4 batch 685 with loss 1.7945725917816162, accuracy 0.23333334922790527.\n",
      "Training epoch 4 batch 686 with loss 1.7610585689544678, accuracy 0.15833334624767303.\n",
      "Training epoch 4 batch 687 with loss 1.8536007404327393, accuracy 0.2666666805744171.\n",
      "Training epoch 4 batch 688 with loss 1.9240652322769165, accuracy 0.0694444477558136.\n",
      "Training epoch 4 batch 689 with loss 1.7549562454223633, accuracy 0.22777777910232544.\n",
      "Training epoch 4 batch 690 with loss 1.7858774662017822, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 691 with loss 1.8714790344238281, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 692 with loss 1.8781074285507202, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 693 with loss 1.752423644065857, accuracy 0.3583333492279053.\n",
      "Training epoch 4 batch 694 with loss 1.6810150146484375, accuracy 0.41388893127441406.\n",
      "Training epoch 4 batch 695 with loss 1.727616310119629, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 696 with loss 1.864977240562439, accuracy 0.14047619700431824.\n",
      "Training epoch 4 batch 697 with loss 1.7117738723754883, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 698 with loss 1.7693636417388916, accuracy 0.25.\n",
      "Training epoch 4 batch 699 with loss 1.7742738723754883, accuracy 0.1269841343164444.\n",
      "Training epoch 4 batch 700 with loss 1.7925710678100586, accuracy 0.2916666567325592.\n",
      "Training epoch 4 batch 701 with loss 1.755312204360962, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 702 with loss 1.7641992568969727, accuracy 0.2222222238779068.\n",
      "Training epoch 4 batch 703 with loss 1.8571287393569946, accuracy 0.15833333134651184.\n",
      "Training epoch 4 batch 704 with loss 1.8009474277496338, accuracy 0.09444444626569748.\n",
      "Training epoch 4 batch 705 with loss 1.7905523777008057, accuracy 0.13611111044883728.\n",
      "Training epoch 4 batch 706 with loss 1.6776440143585205, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 707 with loss 1.8052749633789062, accuracy 0.25555554032325745.\n",
      "Training epoch 4 batch 708 with loss 1.8623244762420654, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 709 with loss 1.7507245540618896, accuracy 0.4027777910232544.\n",
      "Training epoch 4 batch 710 with loss 1.8087078332901, accuracy 0.125.\n",
      "Training epoch 4 batch 711 with loss 1.704411268234253, accuracy 0.17777778208255768.\n",
      "Training epoch 4 batch 712 with loss 1.9072325229644775, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 713 with loss 1.8628652095794678, accuracy 0.1458333432674408.\n",
      "Training epoch 4 batch 714 with loss 1.9082279205322266, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 715 with loss 1.7164891958236694, accuracy 0.29722222685813904.\n",
      "Training epoch 4 batch 716 with loss 1.8436791896820068, accuracy 0.190476194024086.\n",
      "Training epoch 4 batch 717 with loss 1.795037031173706, accuracy 0.31666669249534607.\n",
      "Training epoch 4 batch 718 with loss 1.9006383419036865, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 719 with loss 1.8762165307998657, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 720 with loss 1.6618942022323608, accuracy 0.3194444477558136.\n",
      "Training epoch 4 batch 721 with loss 1.8139503002166748, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 722 with loss 1.842407464981079, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 723 with loss 1.8606996536254883, accuracy 0.0476190485060215.\n",
      "Training epoch 4 batch 724 with loss 1.6684051752090454, accuracy 0.36666667461395264.\n",
      "Training epoch 4 batch 725 with loss 1.7778384685516357, accuracy 0.125.\n",
      "Training epoch 4 batch 726 with loss 1.7955522537231445, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 727 with loss 1.782056450843811, accuracy 0.06111111491918564.\n",
      "Training epoch 4 batch 728 with loss 1.7190139293670654, accuracy 0.2222222238779068.\n",
      "Training epoch 4 batch 729 with loss 1.735046148300171, accuracy 0.18611110746860504.\n",
      "Training epoch 4 batch 730 with loss 1.8509867191314697, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 731 with loss 1.7030823230743408, accuracy 0.222222238779068.\n",
      "Training epoch 4 batch 732 with loss 1.7181326150894165, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 733 with loss 1.7898772954940796, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 734 with loss 1.8353445529937744, accuracy 0.21388888359069824.\n",
      "Training epoch 4 batch 735 with loss 1.7592277526855469, accuracy 0.45000001788139343.\n",
      "Training epoch 4 batch 736 with loss 1.7859458923339844, accuracy 0.31111112236976624.\n",
      "Training epoch 4 batch 737 with loss 1.817873239517212, accuracy 0.19206349551677704.\n",
      "Training epoch 4 batch 738 with loss 1.812077283859253, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 739 with loss 1.8170877695083618, accuracy 0.1547619104385376.\n",
      "Training epoch 4 batch 740 with loss 1.7333139181137085, accuracy 0.35317462682724.\n",
      "Training epoch 4 batch 741 with loss 1.871405839920044, accuracy 0.0.\n",
      "Training epoch 4 batch 742 with loss 1.9178295135498047, accuracy 0.25.\n",
      "Training epoch 4 batch 743 with loss 1.810675024986267, accuracy 0.20000001788139343.\n",
      "Training epoch 4 batch 744 with loss 1.7543224096298218, accuracy 0.26111114025115967.\n",
      "Training epoch 4 batch 745 with loss 1.7850191593170166, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 746 with loss 1.7770748138427734, accuracy 0.1865079402923584.\n",
      "Training epoch 4 batch 747 with loss 1.7797343730926514, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 748 with loss 1.8196580410003662, accuracy 0.06666667014360428.\n",
      "Training epoch 4 batch 749 with loss 1.8936971426010132, accuracy 0.1805555671453476.\n",
      "Training epoch 4 batch 750 with loss 1.7315337657928467, accuracy 0.15833334624767303.\n",
      "Training epoch 4 batch 751 with loss 1.7391693592071533, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 752 with loss 1.784611463546753, accuracy 0.20000000298023224.\n",
      "Training epoch 4 batch 753 with loss 1.7943490743637085, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 754 with loss 1.7919857501983643, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 755 with loss 1.7271496057510376, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 756 with loss 1.773135781288147, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 757 with loss 1.8001638650894165, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 758 with loss 1.7093794345855713, accuracy 0.3055555522441864.\n",
      "Training epoch 4 batch 759 with loss 1.7565761804580688, accuracy 0.10833333432674408.\n",
      "Training epoch 4 batch 760 with loss 1.812734603881836, accuracy 0.12261904776096344.\n",
      "Training epoch 4 batch 761 with loss 1.8184516429901123, accuracy 0.236111119389534.\n",
      "Training epoch 4 batch 762 with loss 1.6932004690170288, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 763 with loss 1.737216591835022, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 764 with loss 1.7364733219146729, accuracy 0.236111119389534.\n",
      "Training epoch 4 batch 765 with loss 1.8019492626190186, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 766 with loss 1.8551435470581055, accuracy 0.2750000059604645.\n",
      "Training epoch 4 batch 767 with loss 1.7711122035980225, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 768 with loss 1.7911990880966187, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 769 with loss 1.8415178060531616, accuracy 0.1071428582072258.\n",
      "Training epoch 4 batch 770 with loss 1.7865970134735107, accuracy 0.2876984179019928.\n",
      "Training epoch 4 batch 771 with loss 1.6051876544952393, accuracy 0.6333333253860474.\n",
      "Training epoch 4 batch 772 with loss 1.6570625305175781, accuracy 0.3444444537162781.\n",
      "Training epoch 4 batch 773 with loss 1.7791351079940796, accuracy 0.20000001788139343.\n",
      "Training epoch 4 batch 774 with loss 1.7419837713241577, accuracy 0.3531745970249176.\n",
      "Training epoch 4 batch 775 with loss 1.8426058292388916, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 776 with loss 1.866898775100708, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 777 with loss 1.7298448085784912, accuracy 0.2361111044883728.\n",
      "Training epoch 4 batch 778 with loss 1.7561657428741455, accuracy 0.14047619700431824.\n",
      "Training epoch 4 batch 779 with loss 1.8186581134796143, accuracy 0.14444445073604584.\n",
      "Training epoch 4 batch 780 with loss 1.7009646892547607, accuracy 0.3055555522441864.\n",
      "Training epoch 4 batch 781 with loss 1.748159408569336, accuracy 0.3305555582046509.\n",
      "Training epoch 4 batch 782 with loss 1.7816905975341797, accuracy 0.0763888880610466.\n",
      "Training epoch 4 batch 783 with loss 1.7864961624145508, accuracy 0.24166667461395264.\n",
      "Training epoch 4 batch 784 with loss 1.762006163597107, accuracy 0.26944446563720703.\n",
      "Training epoch 4 batch 785 with loss 1.6781377792358398, accuracy 0.43611109256744385.\n",
      "Training epoch 4 batch 786 with loss 1.792937994003296, accuracy 0.06111111491918564.\n",
      "Training epoch 4 batch 787 with loss 1.8194429874420166, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 788 with loss 1.7652390003204346, accuracy 0.2976190447807312.\n",
      "Training epoch 4 batch 789 with loss 1.7166904211044312, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 790 with loss 1.831789255142212, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 791 with loss 1.7288490533828735, accuracy 0.35277777910232544.\n",
      "Training epoch 4 batch 792 with loss 1.7805259227752686, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 793 with loss 1.9358339309692383, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 794 with loss 1.8206313848495483, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 795 with loss 1.822988510131836, accuracy 0.26944443583488464.\n",
      "Training epoch 4 batch 796 with loss 1.862419843673706, accuracy 0.0.\n",
      "Training epoch 4 batch 797 with loss 1.7464559078216553, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 798 with loss 1.759935975074768, accuracy 0.25.\n",
      "Training epoch 4 batch 799 with loss 1.7971856594085693, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 800 with loss 1.6818469762802124, accuracy 0.31481483578681946.\n",
      "Training epoch 4 batch 801 with loss 1.8189427852630615, accuracy 0.02777777798473835.\n",
      "Training epoch 4 batch 802 with loss 1.8160568475723267, accuracy 0.0714285746216774.\n",
      "Training epoch 4 batch 803 with loss 1.7617132663726807, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 804 with loss 1.8148815631866455, accuracy 0.25.\n",
      "Training epoch 4 batch 805 with loss 1.8698081970214844, accuracy 0.02777777798473835.\n",
      "Training epoch 4 batch 806 with loss 1.809557557106018, accuracy 0.2321428656578064.\n",
      "Training epoch 4 batch 807 with loss 1.6412856578826904, accuracy 0.347222238779068.\n",
      "Training epoch 4 batch 808 with loss 1.8680009841918945, accuracy 0.10833333432674408.\n",
      "Training epoch 4 batch 809 with loss 1.8565410375595093, accuracy 0.25.\n",
      "Training epoch 4 batch 810 with loss 1.6468538045883179, accuracy 0.3680555522441864.\n",
      "Training epoch 4 batch 811 with loss 1.7993381023406982, accuracy 0.3472222089767456.\n",
      "Training epoch 4 batch 812 with loss 1.834935188293457, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 813 with loss 1.8619451522827148, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 814 with loss 1.7782179117202759, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 815 with loss 1.8834028244018555, accuracy 0.2083333283662796.\n",
      "Training epoch 4 batch 816 with loss 1.7931528091430664, accuracy 0.23333334922790527.\n",
      "Training epoch 4 batch 817 with loss 1.7751613855361938, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 818 with loss 1.7124783992767334, accuracy 0.4365079402923584.\n",
      "Training epoch 4 batch 819 with loss 1.7337353229522705, accuracy 0.12222222983837128.\n",
      "Training epoch 4 batch 820 with loss 1.8661487102508545, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 821 with loss 1.8744337558746338, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 822 with loss 1.8749778270721436, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 823 with loss 1.889691710472107, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 824 with loss 1.7657463550567627, accuracy 0.1825396865606308.\n",
      "Training epoch 4 batch 825 with loss 1.7995035648345947, accuracy 0.2103174775838852.\n",
      "Training epoch 4 batch 826 with loss 1.6928294897079468, accuracy 0.2797619104385376.\n",
      "Training epoch 4 batch 827 with loss 1.7389739751815796, accuracy 0.15555556118488312.\n",
      "Training epoch 4 batch 828 with loss 1.68069589138031, accuracy 0.13214285671710968.\n",
      "Training epoch 4 batch 829 with loss 1.7607189416885376, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 830 with loss 1.736864686012268, accuracy 0.125.\n",
      "Training epoch 4 batch 831 with loss 1.8544877767562866, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 832 with loss 1.7336721420288086, accuracy 0.35555559396743774.\n",
      "Training epoch 4 batch 833 with loss 1.8092254400253296, accuracy 0.14166666567325592.\n",
      "Training epoch 4 batch 834 with loss 1.8516098260879517, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 835 with loss 1.833820104598999, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 836 with loss 1.8721460103988647, accuracy 0.20000001788139343.\n",
      "Training epoch 4 batch 837 with loss 1.8997423648834229, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 838 with loss 1.7743057012557983, accuracy 0.2361111044883728.\n",
      "Training epoch 4 batch 839 with loss 1.7494062185287476, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 840 with loss 1.7389532327651978, accuracy 0.0892857164144516.\n",
      "Training epoch 4 batch 841 with loss 1.8245935440063477, accuracy 0.15833333134651184.\n",
      "Training epoch 4 batch 842 with loss 1.7700906991958618, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 843 with loss 1.7875020503997803, accuracy 0.1805555671453476.\n",
      "Training epoch 4 batch 844 with loss 1.820507287979126, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 845 with loss 1.7901477813720703, accuracy 0.2569444477558136.\n",
      "Training epoch 4 batch 846 with loss 1.817989706993103, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 847 with loss 1.814485788345337, accuracy 0.2361111044883728.\n",
      "Training epoch 4 batch 848 with loss 1.7709577083587646, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 849 with loss 1.7793781757354736, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 850 with loss 1.7136011123657227, accuracy 0.3750000298023224.\n",
      "Training epoch 4 batch 851 with loss 1.7369530200958252, accuracy 0.3333333432674408.\n",
      "Training epoch 4 batch 852 with loss 1.8032386302947998, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 853 with loss 1.7064441442489624, accuracy 0.38333332538604736.\n",
      "Training epoch 4 batch 854 with loss 1.7892086505889893, accuracy 0.14603175222873688.\n",
      "Training epoch 4 batch 855 with loss 1.8338502645492554, accuracy 0.12222222983837128.\n",
      "Training epoch 4 batch 856 with loss 1.9581382274627686, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 857 with loss 1.7586333751678467, accuracy 0.190476194024086.\n",
      "Training epoch 4 batch 858 with loss 1.7003093957901, accuracy 0.236111119389534.\n",
      "Training epoch 4 batch 859 with loss 1.8145086765289307, accuracy 0.03333333507180214.\n",
      "Training epoch 4 batch 860 with loss 1.7432258129119873, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 861 with loss 1.8200099468231201, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 862 with loss 1.876481294631958, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 863 with loss 1.8479639291763306, accuracy 0.125.\n",
      "Training epoch 4 batch 864 with loss 1.8280467987060547, accuracy 0.2291666716337204.\n",
      "Training epoch 4 batch 865 with loss 1.8001861572265625, accuracy 0.14444445073604584.\n",
      "Training epoch 4 batch 866 with loss 1.8045295476913452, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 867 with loss 1.8158948421478271, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 868 with loss 1.7756855487823486, accuracy 0.2182539701461792.\n",
      "Training epoch 4 batch 869 with loss 1.8229577541351318, accuracy 0.22499999403953552.\n",
      "Training epoch 4 batch 870 with loss 1.7295958995819092, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 871 with loss 1.8675416707992554, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 872 with loss 1.7758289575576782, accuracy 0.33888891339302063.\n",
      "Training epoch 4 batch 873 with loss 1.8295695781707764, accuracy 0.222222238779068.\n",
      "Training epoch 4 batch 874 with loss 1.7821547985076904, accuracy 0.21111111342906952.\n",
      "Training epoch 4 batch 875 with loss 1.8116099834442139, accuracy 0.125.\n",
      "Training epoch 4 batch 876 with loss 1.7085075378417969, accuracy 0.5.\n",
      "Training epoch 4 batch 877 with loss 1.8417896032333374, accuracy 0.1458333432674408.\n",
      "Training epoch 4 batch 878 with loss 1.8375781774520874, accuracy 0.2527777850627899.\n",
      "Training epoch 4 batch 879 with loss 1.7261295318603516, accuracy 0.15555556118488312.\n",
      "Training epoch 4 batch 880 with loss 1.8299745321273804, accuracy 0.125.\n",
      "Training epoch 4 batch 881 with loss 1.7528330087661743, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 882 with loss 1.8921639919281006, accuracy 0.13333334028720856.\n",
      "Training epoch 4 batch 883 with loss 1.8537925481796265, accuracy 0.0.\n",
      "Training epoch 4 batch 884 with loss 1.7007877826690674, accuracy 0.2361111044883728.\n",
      "Training epoch 4 batch 885 with loss 1.9422985315322876, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 886 with loss 1.7378326654434204, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 887 with loss 1.7200555801391602, accuracy 0.2944444417953491.\n",
      "Training epoch 4 batch 888 with loss 1.7917076349258423, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 889 with loss 1.7854993343353271, accuracy 0.25.\n",
      "Training epoch 4 batch 890 with loss 1.822854995727539, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 891 with loss 1.7906007766723633, accuracy 0.3194444477558136.\n",
      "Training epoch 4 batch 892 with loss 1.8567087650299072, accuracy 0.2222222238779068.\n",
      "Training epoch 4 batch 893 with loss 1.8034826517105103, accuracy 0.3055555522441864.\n",
      "Training epoch 4 batch 894 with loss 1.7495635747909546, accuracy 0.2142857164144516.\n",
      "Training epoch 4 batch 895 with loss 1.8068735599517822, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 896 with loss 1.7388769388198853, accuracy 0.2291666716337204.\n",
      "Training epoch 4 batch 897 with loss 1.879410982131958, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 898 with loss 1.7628570795059204, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 899 with loss 1.8319778442382812, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 900 with loss 1.7446467876434326, accuracy 0.17592594027519226.\n",
      "Training epoch 4 batch 901 with loss 1.716165542602539, accuracy 0.4138888716697693.\n",
      "Training epoch 4 batch 902 with loss 1.7019189596176147, accuracy 0.222222238779068.\n",
      "Training epoch 4 batch 903 with loss 1.7590863704681396, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 904 with loss 1.764529824256897, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 905 with loss 1.8263450860977173, accuracy 0.125.\n",
      "Training epoch 4 batch 906 with loss 1.8067699670791626, accuracy 0.2182539701461792.\n",
      "Training epoch 4 batch 907 with loss 1.819797158241272, accuracy 0.28333336114883423.\n",
      "Training epoch 4 batch 908 with loss 1.6874332427978516, accuracy 0.375.\n",
      "Training epoch 4 batch 909 with loss 1.8297611474990845, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 910 with loss 1.741651177406311, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 911 with loss 1.8216698169708252, accuracy 0.03333333507180214.\n",
      "Training epoch 4 batch 912 with loss 1.8458473682403564, accuracy 0.0694444477558136.\n",
      "Training epoch 4 batch 913 with loss 1.7784461975097656, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 914 with loss 1.7984459400177002, accuracy 0.2666666805744171.\n",
      "Training epoch 4 batch 915 with loss 1.843592643737793, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 916 with loss 1.7543220520019531, accuracy 0.1805555671453476.\n",
      "Training epoch 4 batch 917 with loss 1.8180639743804932, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 918 with loss 1.7295303344726562, accuracy 0.125.\n",
      "Training epoch 4 batch 919 with loss 1.7289059162139893, accuracy 0.21666666865348816.\n",
      "Training epoch 4 batch 920 with loss 1.7686374187469482, accuracy 0.15833333134651184.\n",
      "Training epoch 4 batch 921 with loss 1.7665774822235107, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 922 with loss 1.8735367059707642, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 923 with loss 1.8066136837005615, accuracy 0.1626984179019928.\n",
      "Training epoch 4 batch 924 with loss 1.899793028831482, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 925 with loss 1.7005493640899658, accuracy 0.38611114025115967.\n",
      "Training epoch 4 batch 926 with loss 1.7870054244995117, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 927 with loss 1.8245149850845337, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 928 with loss 1.8813345432281494, accuracy 0.03333333507180214.\n",
      "Training epoch 4 batch 929 with loss 1.8467096090316772, accuracy 0.17777778208255768.\n",
      "Training epoch 4 batch 930 with loss 1.7881038188934326, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 931 with loss 1.7213798761367798, accuracy 0.2222222238779068.\n",
      "Training epoch 4 batch 932 with loss 1.7409741878509521, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 933 with loss 1.7408969402313232, accuracy 0.1964285671710968.\n",
      "Training epoch 4 batch 934 with loss 1.7742178440093994, accuracy 0.3055555820465088.\n",
      "Training epoch 4 batch 935 with loss 1.8056328296661377, accuracy 0.0892857164144516.\n",
      "Training epoch 4 batch 936 with loss 1.7133607864379883, accuracy 0.23055556416511536.\n",
      "Training epoch 4 batch 937 with loss 1.823288917541504, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 938 with loss 1.7746632099151611, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 939 with loss 1.7074267864227295, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 940 with loss 1.8342278003692627, accuracy 0.2361111044883728.\n",
      "Training epoch 4 batch 941 with loss 1.6957037448883057, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 942 with loss 1.6538959741592407, accuracy 0.3583333194255829.\n",
      "Training epoch 4 batch 943 with loss 1.78158700466156, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 944 with loss 1.7583625316619873, accuracy 0.2361111044883728.\n",
      "Training epoch 4 batch 945 with loss 1.8941562175750732, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 946 with loss 1.9012644290924072, accuracy 0.125.\n",
      "Training epoch 4 batch 947 with loss 1.805659294128418, accuracy 0.18333333730697632.\n",
      "Training epoch 4 batch 948 with loss 1.80430006980896, accuracy 0.22857144474983215.\n",
      "Training epoch 4 batch 949 with loss 1.7276971340179443, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 950 with loss 1.730873703956604, accuracy 0.2142857313156128.\n",
      "Training epoch 4 batch 951 with loss 1.8435999155044556, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 952 with loss 1.7895395755767822, accuracy 0.21111111342906952.\n",
      "Training epoch 4 batch 953 with loss 1.7045093774795532, accuracy 0.2222222238779068.\n",
      "Training epoch 4 batch 954 with loss 1.8157294988632202, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 955 with loss 1.8459618091583252, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 956 with loss 1.8336875438690186, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 957 with loss 1.7621701955795288, accuracy 0.28333333134651184.\n",
      "Training epoch 4 batch 958 with loss 1.8164170980453491, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 959 with loss 1.8559948205947876, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 960 with loss 1.7692657709121704, accuracy 0.1805555671453476.\n",
      "Training epoch 4 batch 961 with loss 1.7519781589508057, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 962 with loss 1.800360918045044, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 963 with loss 1.752004861831665, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 964 with loss 1.7325503826141357, accuracy 0.1130952388048172.\n",
      "Training epoch 4 batch 965 with loss 1.7791560888290405, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 966 with loss 1.7550147771835327, accuracy 0.21111111342906952.\n",
      "Training epoch 4 batch 967 with loss 1.8554319143295288, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 968 with loss 1.8132600784301758, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 969 with loss 1.797157645225525, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 970 with loss 1.780577301979065, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 971 with loss 1.8455852270126343, accuracy 0.02777777798473835.\n",
      "Training epoch 4 batch 972 with loss 1.7976890802383423, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 973 with loss 1.7337620258331299, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 974 with loss 1.7569580078125, accuracy 0.25.\n",
      "Training epoch 4 batch 975 with loss 1.85602605342865, accuracy 0.1319444477558136.\n",
      "Training epoch 4 batch 976 with loss 1.7246887683868408, accuracy 0.32500001788139343.\n",
      "Training epoch 4 batch 977 with loss 1.6832557916641235, accuracy 0.2666666805744171.\n",
      "Training epoch 4 batch 978 with loss 1.8941571712493896, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 979 with loss 1.6859737634658813, accuracy 0.21111111342906952.\n",
      "Training epoch 4 batch 980 with loss 1.7356698513031006, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 981 with loss 1.7261282205581665, accuracy 0.08095238357782364.\n",
      "Training epoch 4 batch 982 with loss 1.7944549322128296, accuracy 0.3432539701461792.\n",
      "Training epoch 4 batch 983 with loss 1.7392488718032837, accuracy 0.23888888955116272.\n",
      "Training epoch 4 batch 984 with loss 1.7819935083389282, accuracy 0.1041666641831398.\n",
      "Training epoch 4 batch 985 with loss 1.7811130285263062, accuracy 0.20555555820465088.\n",
      "Training epoch 4 batch 986 with loss 1.7791645526885986, accuracy 0.22500000894069672.\n",
      "Training epoch 4 batch 987 with loss 1.6997449398040771, accuracy 0.15833333134651184.\n",
      "Training epoch 4 batch 988 with loss 1.7858890295028687, accuracy 0.12222222983837128.\n",
      "Training epoch 4 batch 989 with loss 1.818645715713501, accuracy 0.255952388048172.\n",
      "Training epoch 4 batch 990 with loss 1.7746961116790771, accuracy 0.4833333492279053.\n",
      "Training epoch 4 batch 991 with loss 1.8125203847885132, accuracy 0.0694444477558136.\n",
      "Training epoch 4 batch 992 with loss 1.9144752025604248, accuracy 0.1210317462682724.\n",
      "Training epoch 4 batch 993 with loss 1.8633753061294556, accuracy 0.25.\n",
      "Training epoch 4 batch 994 with loss 1.8194587230682373, accuracy 0.3333333432674408.\n",
      "Training epoch 4 batch 995 with loss 1.7785570621490479, accuracy 0.3333333730697632.\n",
      "Training epoch 4 batch 996 with loss 1.855467438697815, accuracy 0.02083333395421505.\n",
      "Training epoch 4 batch 997 with loss 1.8136104345321655, accuracy 0.20555555820465088.\n",
      "Training epoch 4 batch 998 with loss 1.7910140752792358, accuracy 0.14166666567325592.\n",
      "Training epoch 4 batch 999 with loss 1.7360519170761108, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1000 with loss 1.84147047996521, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 1001 with loss 1.8947193622589111, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 1002 with loss 1.7683908939361572, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1003 with loss 1.7163286209106445, accuracy 0.2750000059604645.\n",
      "Training epoch 4 batch 1004 with loss 1.7748314142227173, accuracy 0.30714285373687744.\n",
      "Training epoch 4 batch 1005 with loss 1.7142221927642822, accuracy 0.16388890147209167.\n",
      "Training epoch 4 batch 1006 with loss 1.8437464237213135, accuracy 0.06666667014360428.\n",
      "Training epoch 4 batch 1007 with loss 1.902048110961914, accuracy 0.07870370149612427.\n",
      "Training epoch 4 batch 1008 with loss 1.7983096837997437, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 1009 with loss 1.7873871326446533, accuracy 0.1071428582072258.\n",
      "Training epoch 4 batch 1010 with loss 1.8376712799072266, accuracy 0.25.\n",
      "Training epoch 4 batch 1011 with loss 1.8000293970108032, accuracy 0.236111119389534.\n",
      "Training epoch 4 batch 1012 with loss 1.741222620010376, accuracy 0.12222222983837128.\n",
      "Training epoch 4 batch 1013 with loss 1.8646453619003296, accuracy 0.18333333730697632.\n",
      "Training epoch 4 batch 1014 with loss 1.82950758934021, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1015 with loss 1.8357099294662476, accuracy 0.20555555820465088.\n",
      "Training epoch 4 batch 1016 with loss 1.7450700998306274, accuracy 0.3222222328186035.\n",
      "Training epoch 4 batch 1017 with loss 1.802080512046814, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 1018 with loss 1.7831922769546509, accuracy 0.2460317611694336.\n",
      "Training epoch 4 batch 1019 with loss 1.7375141382217407, accuracy 0.1488095223903656.\n",
      "Training epoch 4 batch 1020 with loss 1.726112961769104, accuracy 0.28333333134651184.\n",
      "Training epoch 4 batch 1021 with loss 1.8574209213256836, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1022 with loss 1.7729527950286865, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1023 with loss 1.8278379440307617, accuracy 0.18611112236976624.\n",
      "Training epoch 4 batch 1024 with loss 1.8181092739105225, accuracy 0.15833333134651184.\n",
      "Training epoch 4 batch 1025 with loss 1.847913146018982, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1026 with loss 1.689854383468628, accuracy 0.2916666567325592.\n",
      "Training epoch 4 batch 1027 with loss 1.8219553232192993, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 1028 with loss 1.9133840799331665, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 1029 with loss 1.7717640399932861, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 1030 with loss 1.8568017482757568, accuracy 0.0793650820851326.\n",
      "Training epoch 4 batch 1031 with loss 1.8412396907806396, accuracy 0.26944443583488464.\n",
      "Training epoch 4 batch 1032 with loss 1.7638463973999023, accuracy 0.16388890147209167.\n",
      "Training epoch 4 batch 1033 with loss 1.7803866863250732, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 1034 with loss 1.6398338079452515, accuracy 0.4555555284023285.\n",
      "Training epoch 4 batch 1035 with loss 1.7984775304794312, accuracy 0.2380952537059784.\n",
      "Training epoch 4 batch 1036 with loss 1.7516214847564697, accuracy 0.20555555820465088.\n",
      "Training epoch 4 batch 1037 with loss 1.7572215795516968, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1038 with loss 1.8017734289169312, accuracy 0.3166666626930237.\n",
      "Training epoch 4 batch 1039 with loss 1.8281961679458618, accuracy 0.1825396865606308.\n",
      "Training epoch 4 batch 1040 with loss 1.8617902994155884, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1041 with loss 1.6874754428863525, accuracy 0.4027777910232544.\n",
      "Training epoch 4 batch 1042 with loss 1.7541269063949585, accuracy 0.38055554032325745.\n",
      "Training epoch 4 batch 1043 with loss 1.7456105947494507, accuracy 0.2666666805744171.\n",
      "Training epoch 4 batch 1044 with loss 1.7983556985855103, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 1045 with loss 1.8317254781723022, accuracy 0.125.\n",
      "Training epoch 4 batch 1046 with loss 1.811213493347168, accuracy 0.0.\n",
      "Training epoch 4 batch 1047 with loss 1.7179021835327148, accuracy 0.42222219705581665.\n",
      "Training epoch 4 batch 1048 with loss 1.7048885822296143, accuracy 0.3611111044883728.\n",
      "Training epoch 4 batch 1049 with loss 1.8034824132919312, accuracy 0.2611111104488373.\n",
      "Training epoch 4 batch 1050 with loss 1.862851858139038, accuracy 0.2666666805744171.\n",
      "Training epoch 4 batch 1051 with loss 1.7946557998657227, accuracy 0.4642857313156128.\n",
      "Training epoch 4 batch 1052 with loss 1.9079554080963135, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1053 with loss 1.7887630462646484, accuracy 0.25.\n",
      "Training epoch 4 batch 1054 with loss 1.7154638767242432, accuracy 0.29722222685813904.\n",
      "Training epoch 4 batch 1055 with loss 1.850298523902893, accuracy 0.10277777910232544.\n",
      "Training epoch 4 batch 1056 with loss 1.8361972570419312, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1057 with loss 1.8030790090560913, accuracy 0.19166666269302368.\n",
      "Training epoch 4 batch 1058 with loss 1.6846990585327148, accuracy 0.5166667103767395.\n",
      "Training epoch 4 batch 1059 with loss 1.6650073528289795, accuracy 0.32777777314186096.\n",
      "Training epoch 4 batch 1060 with loss 1.7906430959701538, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 1061 with loss 1.8346080780029297, accuracy 0.42222222685813904.\n",
      "Training epoch 4 batch 1062 with loss 1.7352020740509033, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 1063 with loss 1.7402219772338867, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1064 with loss 1.6648929119110107, accuracy 0.28333336114883423.\n",
      "Training epoch 4 batch 1065 with loss 1.8092994689941406, accuracy 0.0476190485060215.\n",
      "Training epoch 4 batch 1066 with loss 1.7163184881210327, accuracy 0.4226190447807312.\n",
      "Training epoch 4 batch 1067 with loss 1.7888084650039673, accuracy 0.3583333492279053.\n",
      "Training epoch 4 batch 1068 with loss 1.6635583639144897, accuracy 0.366666704416275.\n",
      "Training epoch 4 batch 1069 with loss 1.765237808227539, accuracy 0.3472222089767456.\n",
      "Training epoch 4 batch 1070 with loss 1.7260011434555054, accuracy 0.23888888955116272.\n",
      "Training epoch 4 batch 1071 with loss 1.844069242477417, accuracy 0.25.\n",
      "Training epoch 4 batch 1072 with loss 1.764337182044983, accuracy 0.03333333507180214.\n",
      "Training epoch 4 batch 1073 with loss 1.7939698696136475, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 1074 with loss 1.7936980724334717, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 1075 with loss 1.7183605432510376, accuracy 0.28333333134651184.\n",
      "Training epoch 4 batch 1076 with loss 1.8899154663085938, accuracy 0.09880952537059784.\n",
      "Training epoch 4 batch 1077 with loss 1.7476707696914673, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 1078 with loss 1.791373610496521, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 1079 with loss 1.7962188720703125, accuracy 0.0.\n",
      "Training epoch 4 batch 1080 with loss 1.744513750076294, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1081 with loss 1.8909142017364502, accuracy 0.03333333507180214.\n",
      "Training epoch 4 batch 1082 with loss 1.7566194534301758, accuracy 0.2876984179019928.\n",
      "Training epoch 4 batch 1083 with loss 1.8963069915771484, accuracy 0.06666667014360428.\n",
      "Training epoch 4 batch 1084 with loss 1.7744252681732178, accuracy 0.2460317462682724.\n",
      "Training epoch 4 batch 1085 with loss 1.6509147882461548, accuracy 0.3444444537162781.\n",
      "Training epoch 4 batch 1086 with loss 1.7998363971710205, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 1087 with loss 1.8021084070205688, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1088 with loss 1.7423028945922852, accuracy 0.40833333134651184.\n",
      "Training epoch 4 batch 1089 with loss 1.905766248703003, accuracy 0.0.\n",
      "Training epoch 4 batch 1090 with loss 1.874799370765686, accuracy 0.1031746044754982.\n",
      "Training epoch 4 batch 1091 with loss 1.7833925485610962, accuracy 0.20555555820465088.\n",
      "Training epoch 4 batch 1092 with loss 1.8260217905044556, accuracy 0.125.\n",
      "Training epoch 4 batch 1093 with loss 1.855809211730957, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1094 with loss 1.8226906061172485, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1095 with loss 1.7935240268707275, accuracy 0.12222222983837128.\n",
      "Training epoch 4 batch 1096 with loss 1.7936241626739502, accuracy 0.23333334922790527.\n",
      "Training epoch 4 batch 1097 with loss 1.8094055652618408, accuracy 0.0694444477558136.\n",
      "Training epoch 4 batch 1098 with loss 1.7419853210449219, accuracy 0.3571428656578064.\n",
      "Training epoch 4 batch 1099 with loss 1.8292442560195923, accuracy 0.2599206268787384.\n",
      "Training epoch 4 batch 1100 with loss 1.7824443578720093, accuracy 0.18611110746860504.\n",
      "Training epoch 4 batch 1101 with loss 1.771101951599121, accuracy 0.3531745970249176.\n",
      "Training epoch 4 batch 1102 with loss 1.82427179813385, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1103 with loss 1.7495222091674805, accuracy 0.15833333134651184.\n",
      "Training epoch 4 batch 1104 with loss 1.7730019092559814, accuracy 0.17500001192092896.\n",
      "Training epoch 4 batch 1105 with loss 1.7368015050888062, accuracy 0.20555557310581207.\n",
      "Training epoch 4 batch 1106 with loss 1.9553321599960327, accuracy 0.06666667014360428.\n",
      "Training epoch 4 batch 1107 with loss 1.772534966468811, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 1108 with loss 1.7708241939544678, accuracy 0.2518518567085266.\n",
      "Training epoch 4 batch 1109 with loss 1.8080158233642578, accuracy 0.2291666716337204.\n",
      "Training epoch 4 batch 1110 with loss 1.7941253185272217, accuracy 0.13333334028720856.\n",
      "Training epoch 4 batch 1111 with loss 1.783200979232788, accuracy 0.4277777671813965.\n",
      "Training epoch 4 batch 1112 with loss 1.8891710042953491, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 1113 with loss 1.816285490989685, accuracy 0.2916666567325592.\n",
      "Training epoch 4 batch 1114 with loss 1.8874574899673462, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 1115 with loss 1.8367998600006104, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 1116 with loss 1.7340385913848877, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 1117 with loss 1.6528294086456299, accuracy 0.4138889014720917.\n",
      "Training epoch 4 batch 1118 with loss 1.8864657878875732, accuracy 0.02083333395421505.\n",
      "Training epoch 4 batch 1119 with loss 1.693678617477417, accuracy 0.3730158805847168.\n",
      "Training epoch 4 batch 1120 with loss 1.820319414138794, accuracy 0.2738095223903656.\n",
      "Training epoch 4 batch 1121 with loss 1.7354570627212524, accuracy 0.1805555671453476.\n",
      "Training epoch 4 batch 1122 with loss 1.7991523742675781, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 1123 with loss 1.9017833471298218, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 1124 with loss 1.8514130115509033, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 1125 with loss 1.7939313650131226, accuracy 0.1071428582072258.\n",
      "Training epoch 4 batch 1126 with loss 1.8380743265151978, accuracy 0.20000000298023224.\n",
      "Training epoch 4 batch 1127 with loss 1.7959496974945068, accuracy 0.14761905372142792.\n",
      "Training epoch 4 batch 1128 with loss 1.8529726266860962, accuracy 0.0694444477558136.\n",
      "Training epoch 4 batch 1129 with loss 1.7540308237075806, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1130 with loss 1.92449152469635, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 1131 with loss 1.7501853704452515, accuracy 0.2916666865348816.\n",
      "Training epoch 4 batch 1132 with loss 1.6573140621185303, accuracy 0.32777777314186096.\n",
      "Training epoch 4 batch 1133 with loss 1.761596441268921, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1134 with loss 1.7823412418365479, accuracy 0.2750000059604645.\n",
      "Training epoch 4 batch 1135 with loss 1.828194260597229, accuracy 0.26944443583488464.\n",
      "Training epoch 4 batch 1136 with loss 1.8220150470733643, accuracy 0.14444445073604584.\n",
      "Training epoch 4 batch 1137 with loss 1.76188063621521, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 1138 with loss 1.8067944049835205, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1139 with loss 1.7479221820831299, accuracy 0.20000001788139343.\n",
      "Training epoch 4 batch 1140 with loss 1.8240419626235962, accuracy 0.14166666567325592.\n",
      "Training epoch 4 batch 1141 with loss 1.7587217092514038, accuracy 0.0714285746216774.\n",
      "Training epoch 4 batch 1142 with loss 1.7398269176483154, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1143 with loss 1.7815005779266357, accuracy 0.22777777910232544.\n",
      "Training epoch 4 batch 1144 with loss 1.842200517654419, accuracy 0.15555556118488312.\n",
      "Training epoch 4 batch 1145 with loss 1.9065968990325928, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 1146 with loss 1.7922264337539673, accuracy 0.2222222238779068.\n",
      "Training epoch 4 batch 1147 with loss 1.741016149520874, accuracy 0.33888888359069824.\n",
      "Training epoch 4 batch 1148 with loss 1.8721246719360352, accuracy 0.09444444626569748.\n",
      "Training epoch 4 batch 1149 with loss 1.8986917734146118, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1150 with loss 1.896174669265747, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 1151 with loss 1.8022196292877197, accuracy 0.125.\n",
      "Training epoch 4 batch 1152 with loss 1.7694793939590454, accuracy 0.03333333507180214.\n",
      "Training epoch 4 batch 1153 with loss 1.8138277530670166, accuracy 0.31111112236976624.\n",
      "Training epoch 4 batch 1154 with loss 1.827617883682251, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 1155 with loss 1.8201707601547241, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 1156 with loss 1.8583523035049438, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 1157 with loss 1.865764856338501, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1158 with loss 1.8188841342926025, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 1159 with loss 1.8082796335220337, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 1160 with loss 1.7574846744537354, accuracy 0.25.\n",
      "Training epoch 4 batch 1161 with loss 1.7749242782592773, accuracy 0.1210317462682724.\n",
      "Training epoch 4 batch 1162 with loss 1.7836042642593384, accuracy 0.1825396865606308.\n",
      "Training epoch 4 batch 1163 with loss 1.8024272918701172, accuracy 0.2611111104488373.\n",
      "Training epoch 4 batch 1164 with loss 1.7934516668319702, accuracy 0.17777778208255768.\n",
      "Training epoch 4 batch 1165 with loss 1.8131163120269775, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1166 with loss 1.7298568487167358, accuracy 0.10277777910232544.\n",
      "Training epoch 4 batch 1167 with loss 1.89944589138031, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1168 with loss 1.8549916744232178, accuracy 0.0.\n",
      "Training epoch 4 batch 1169 with loss 1.8406867980957031, accuracy 0.2888889014720917.\n",
      "Training epoch 4 batch 1170 with loss 1.8281548023223877, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 1171 with loss 1.826521873474121, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 1172 with loss 1.8208410739898682, accuracy 0.0714285746216774.\n",
      "Training epoch 4 batch 1173 with loss 1.6789169311523438, accuracy 0.3500000238418579.\n",
      "Training epoch 4 batch 1174 with loss 1.769733190536499, accuracy 0.23333334922790527.\n",
      "Training epoch 4 batch 1175 with loss 1.8958747386932373, accuracy 0.0.\n",
      "Training epoch 4 batch 1176 with loss 1.7867082357406616, accuracy 0.2222222238779068.\n",
      "Training epoch 4 batch 1177 with loss 1.7885898351669312, accuracy 0.0.\n",
      "Training epoch 4 batch 1178 with loss 1.8081657886505127, accuracy 0.1210317462682724.\n",
      "Training epoch 4 batch 1179 with loss 1.7633988857269287, accuracy 0.24166667461395264.\n",
      "Training epoch 4 batch 1180 with loss 1.8289321660995483, accuracy 0.190476194024086.\n",
      "Training epoch 4 batch 1181 with loss 1.7554104328155518, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 1182 with loss 1.8422634601593018, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1183 with loss 1.9250566959381104, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 1184 with loss 1.8554607629776, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 1185 with loss 1.7389748096466064, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 1186 with loss 1.7389519214630127, accuracy 0.4305555522441864.\n",
      "Training epoch 4 batch 1187 with loss 1.6784002780914307, accuracy 0.4166666567325592.\n",
      "Training epoch 4 batch 1188 with loss 1.7906248569488525, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 1189 with loss 1.7236101627349854, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 1190 with loss 1.8124573230743408, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 1191 with loss 1.7916991710662842, accuracy 0.210317462682724.\n",
      "Training epoch 4 batch 1192 with loss 1.7295398712158203, accuracy 0.261904776096344.\n",
      "Training epoch 4 batch 1193 with loss 1.7857640981674194, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1194 with loss 1.8356139659881592, accuracy 0.2916666567325592.\n",
      "Training epoch 4 batch 1195 with loss 1.713329553604126, accuracy 0.3571428656578064.\n",
      "Training epoch 4 batch 1196 with loss 1.8443437814712524, accuracy 0.0694444477558136.\n",
      "Training epoch 4 batch 1197 with loss 1.8292791843414307, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 1198 with loss 1.819178581237793, accuracy 0.190476194024086.\n",
      "Training epoch 4 batch 1199 with loss 1.7642602920532227, accuracy 0.23888888955116272.\n",
      "Training epoch 4 batch 1200 with loss 1.7924388647079468, accuracy 0.11666666716337204.\n",
      "Training epoch 4 batch 1201 with loss 1.7534475326538086, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 1202 with loss 1.792549729347229, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 1203 with loss 1.7586486339569092, accuracy 0.16388890147209167.\n",
      "Training epoch 4 batch 1204 with loss 1.7667970657348633, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 1205 with loss 1.744016408920288, accuracy 0.25.\n",
      "Training epoch 4 batch 1206 with loss 1.7892858982086182, accuracy 0.3083333373069763.\n",
      "Training epoch 4 batch 1207 with loss 1.8299236297607422, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 1208 with loss 1.8857030868530273, accuracy 0.111111119389534.\n",
      "Training epoch 4 batch 1209 with loss 1.7720731496810913, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1210 with loss 1.837281584739685, accuracy 0.2666666805744171.\n",
      "Training epoch 4 batch 1211 with loss 1.750588059425354, accuracy 0.20000000298023224.\n",
      "Training epoch 4 batch 1212 with loss 1.8122838735580444, accuracy 0.1071428656578064.\n",
      "Training epoch 4 batch 1213 with loss 1.78232741355896, accuracy 0.22777777910232544.\n",
      "Training epoch 4 batch 1214 with loss 1.8142917156219482, accuracy 0.18333333730697632.\n",
      "Training epoch 4 batch 1215 with loss 1.8108184337615967, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 1216 with loss 1.8108117580413818, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1217 with loss 1.8753242492675781, accuracy 0.0.\n",
      "Training epoch 4 batch 1218 with loss 1.795540452003479, accuracy 0.130952388048172.\n",
      "Training epoch 4 batch 1219 with loss 1.7962623834609985, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 1220 with loss 1.8571348190307617, accuracy 0.1626984179019928.\n",
      "Training epoch 4 batch 1221 with loss 1.7772945165634155, accuracy 0.19166666269302368.\n",
      "Training epoch 4 batch 1222 with loss 1.7760292291641235, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1223 with loss 1.7478761672973633, accuracy 0.10000000894069672.\n",
      "Training epoch 4 batch 1224 with loss 1.8965305089950562, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1225 with loss 1.7810790538787842, accuracy 0.23888888955116272.\n",
      "Training epoch 4 batch 1226 with loss 1.892957329750061, accuracy 0.0.\n",
      "Training epoch 4 batch 1227 with loss 1.7653834819793701, accuracy 0.15555556118488312.\n",
      "Training epoch 4 batch 1228 with loss 1.7462513446807861, accuracy 0.3055555820465088.\n",
      "Training epoch 4 batch 1229 with loss 1.7839899063110352, accuracy 0.21388889849185944.\n",
      "Training epoch 4 batch 1230 with loss 1.762054443359375, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1231 with loss 1.8477439880371094, accuracy 0.11666666716337204.\n",
      "Training epoch 4 batch 1232 with loss 1.8024803400039673, accuracy 0.2361111044883728.\n",
      "Training epoch 4 batch 1233 with loss 1.746607780456543, accuracy 0.125.\n",
      "Training epoch 4 batch 1234 with loss 1.8533575534820557, accuracy 0.20555555820465088.\n",
      "Training epoch 4 batch 1235 with loss 1.7733583450317383, accuracy 0.1865079402923584.\n",
      "Training epoch 4 batch 1236 with loss 1.7736791372299194, accuracy 0.33888888359069824.\n",
      "Training epoch 4 batch 1237 with loss 1.721279501914978, accuracy 0.16388890147209167.\n",
      "Training epoch 4 batch 1238 with loss 1.7411603927612305, accuracy 0.4583333432674408.\n",
      "Training epoch 4 batch 1239 with loss 1.8565919399261475, accuracy 0.2738095223903656.\n",
      "Training epoch 4 batch 1240 with loss 1.8410475254058838, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1241 with loss 1.7771244049072266, accuracy 0.20000001788139343.\n",
      "Training epoch 4 batch 1242 with loss 1.8047266006469727, accuracy 0.18611110746860504.\n",
      "Training epoch 4 batch 1243 with loss 1.8525266647338867, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 1244 with loss 1.8012726306915283, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 1245 with loss 1.9453378915786743, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1246 with loss 1.8399240970611572, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 1247 with loss 1.8572849035263062, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1248 with loss 1.8591161966323853, accuracy 0.125.\n",
      "Training epoch 4 batch 1249 with loss 1.9496986865997314, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1250 with loss 1.7450923919677734, accuracy 0.21388888359069824.\n",
      "Training epoch 4 batch 1251 with loss 1.8192561864852905, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 1252 with loss 1.7432092428207397, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 1253 with loss 1.9176490306854248, accuracy 0.14444445073604584.\n",
      "Training epoch 4 batch 1254 with loss 1.9342849254608154, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1255 with loss 1.7471386194229126, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1256 with loss 1.7456022500991821, accuracy 0.3611111044883728.\n",
      "Training epoch 4 batch 1257 with loss 1.8095184564590454, accuracy 0.12222222983837128.\n",
      "Training epoch 4 batch 1258 with loss 1.6888303756713867, accuracy 0.30000001192092896.\n",
      "Training epoch 4 batch 1259 with loss 1.8575292825698853, accuracy 0.0703703761100769.\n",
      "Training epoch 4 batch 1260 with loss 1.8540544509887695, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 1261 with loss 1.8670690059661865, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 1262 with loss 1.7159357070922852, accuracy 0.3253968358039856.\n",
      "Training epoch 4 batch 1263 with loss 1.7046363353729248, accuracy 0.2611111104488373.\n",
      "Training epoch 4 batch 1264 with loss 1.7349522113800049, accuracy 0.22380952537059784.\n",
      "Training epoch 4 batch 1265 with loss 1.907463788986206, accuracy 0.0.\n",
      "Training epoch 4 batch 1266 with loss 1.693603754043579, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 1267 with loss 1.8742481470108032, accuracy 0.125.\n",
      "Training epoch 4 batch 1268 with loss 1.7679493427276611, accuracy 0.35555556416511536.\n",
      "Training epoch 4 batch 1269 with loss 1.717228889465332, accuracy 0.3194444477558136.\n",
      "Training epoch 4 batch 1270 with loss 1.6897233724594116, accuracy 0.2984127104282379.\n",
      "Training epoch 4 batch 1271 with loss 1.8033002614974976, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1272 with loss 1.7551244497299194, accuracy 0.2916666567325592.\n",
      "Training epoch 4 batch 1273 with loss 1.8373911380767822, accuracy 0.17777778208255768.\n",
      "Training epoch 4 batch 1274 with loss 1.783887505531311, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1275 with loss 1.8239259719848633, accuracy 0.14444445073604584.\n",
      "Training epoch 4 batch 1276 with loss 1.818731665611267, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 1277 with loss 1.8997443914413452, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 1278 with loss 1.8819509744644165, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 1279 with loss 1.8761221170425415, accuracy 0.15833334624767303.\n",
      "Training epoch 4 batch 1280 with loss 1.799072504043579, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 1281 with loss 1.800127625465393, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 1282 with loss 1.8432241678237915, accuracy 0.125.\n",
      "Training epoch 4 batch 1283 with loss 1.8670406341552734, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1284 with loss 1.680925965309143, accuracy 0.24722224473953247.\n",
      "Training epoch 4 batch 1285 with loss 1.815546989440918, accuracy 0.125.\n",
      "Training epoch 4 batch 1286 with loss 1.865386962890625, accuracy 0.0476190485060215.\n",
      "Training epoch 4 batch 1287 with loss 1.8978252410888672, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1288 with loss 1.7552802562713623, accuracy 0.3472222089767456.\n",
      "Training epoch 4 batch 1289 with loss 1.8193633556365967, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 1290 with loss 1.7046711444854736, accuracy 0.3194444477558136.\n",
      "Training epoch 4 batch 1291 with loss 1.8712389469146729, accuracy 0.0793650820851326.\n",
      "Training epoch 4 batch 1292 with loss 1.7867265939712524, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 1293 with loss 1.90610671043396, accuracy 0.1805555671453476.\n",
      "Training epoch 4 batch 1294 with loss 1.8035434484481812, accuracy 0.20000000298023224.\n",
      "Training epoch 4 batch 1295 with loss 1.8339446783065796, accuracy 0.1527777761220932.\n",
      "Training epoch 4 batch 1296 with loss 1.8262202739715576, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 1297 with loss 1.930959939956665, accuracy 0.02380952425301075.\n",
      "Training epoch 4 batch 1298 with loss 1.8097741603851318, accuracy 0.29722222685813904.\n",
      "Training epoch 4 batch 1299 with loss 1.8643426895141602, accuracy 0.1865079402923584.\n",
      "Training epoch 4 batch 1300 with loss 1.7641435861587524, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 1301 with loss 1.8301607370376587, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 1302 with loss 1.8083699941635132, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1303 with loss 1.8065311908721924, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 1304 with loss 1.8269208669662476, accuracy 0.0.\n",
      "Training epoch 4 batch 1305 with loss 1.751771330833435, accuracy 0.21944445371627808.\n",
      "Training epoch 4 batch 1306 with loss 1.9197056293487549, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 1307 with loss 1.9086246490478516, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1308 with loss 1.7869634628295898, accuracy 0.347222238779068.\n",
      "Training epoch 4 batch 1309 with loss 1.6812002658843994, accuracy 0.2750000059604645.\n",
      "Training epoch 4 batch 1310 with loss 1.8804982900619507, accuracy 0.111111119389534.\n",
      "Training epoch 4 batch 1311 with loss 1.8550039529800415, accuracy 0.0694444477558136.\n",
      "Training epoch 4 batch 1312 with loss 1.815455675125122, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 1313 with loss 1.8216807842254639, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 1314 with loss 1.8055881261825562, accuracy 0.2916666865348816.\n",
      "Training epoch 4 batch 1315 with loss 1.7153327465057373, accuracy 0.236111119389534.\n",
      "Training epoch 4 batch 1316 with loss 1.8146207332611084, accuracy 0.26944443583488464.\n",
      "Training epoch 4 batch 1317 with loss 1.878811240196228, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1318 with loss 1.7320098876953125, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 1319 with loss 1.772753119468689, accuracy 0.25.\n",
      "Training epoch 4 batch 1320 with loss 1.8692249059677124, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1321 with loss 1.8779271841049194, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1322 with loss 1.7682974338531494, accuracy 0.25.\n",
      "Training epoch 4 batch 1323 with loss 1.7469199895858765, accuracy 0.3115079402923584.\n",
      "Training epoch 4 batch 1324 with loss 1.7888351678848267, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 1325 with loss 1.841347336769104, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 1326 with loss 1.7413933277130127, accuracy 0.14047619700431824.\n",
      "Training epoch 4 batch 1327 with loss 1.8766977787017822, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1328 with loss 1.6845684051513672, accuracy 0.2611111104488373.\n",
      "Training epoch 4 batch 1329 with loss 1.774754524230957, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1330 with loss 1.7626922130584717, accuracy 0.30000001192092896.\n",
      "Training epoch 4 batch 1331 with loss 1.788923978805542, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1332 with loss 1.7835477590560913, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 1333 with loss 1.7987807989120483, accuracy 0.2936508059501648.\n",
      "Training epoch 4 batch 1334 with loss 1.8018839359283447, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 1335 with loss 1.7584235668182373, accuracy 0.375.\n",
      "Training epoch 4 batch 1336 with loss 1.8608806133270264, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 1337 with loss 1.8159027099609375, accuracy 0.0694444477558136.\n",
      "Training epoch 4 batch 1338 with loss 1.7847200632095337, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1339 with loss 1.787332534790039, accuracy 0.1825396865606308.\n",
      "Training epoch 4 batch 1340 with loss 1.8391773700714111, accuracy 0.2708333432674408.\n",
      "Training epoch 4 batch 1341 with loss 1.8748191595077515, accuracy 0.20000000298023224.\n",
      "Training epoch 4 batch 1342 with loss 1.7984230518341064, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1343 with loss 1.825791358947754, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1344 with loss 1.8290035724639893, accuracy 0.10833333432674408.\n",
      "Training epoch 4 batch 1345 with loss 1.7754217386245728, accuracy 0.39047619700431824.\n",
      "Training epoch 4 batch 1346 with loss 1.8457787036895752, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1347 with loss 1.83856201171875, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 1348 with loss 1.8015724420547485, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 1349 with loss 1.7176752090454102, accuracy 0.1349206417798996.\n",
      "Training epoch 4 batch 1350 with loss 1.7962186336517334, accuracy 0.0476190485060215.\n",
      "Training epoch 4 batch 1351 with loss 1.9059314727783203, accuracy 0.125.\n",
      "Training epoch 4 batch 1352 with loss 1.7777111530303955, accuracy 0.25.\n",
      "Training epoch 4 batch 1353 with loss 1.732183814048767, accuracy 0.18333333730697632.\n",
      "Training epoch 4 batch 1354 with loss 1.7373650074005127, accuracy 0.3154761791229248.\n",
      "Training epoch 4 batch 1355 with loss 1.745840311050415, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1356 with loss 1.8121004104614258, accuracy 0.0694444477558136.\n",
      "Training epoch 4 batch 1357 with loss 1.7108356952667236, accuracy 0.2805555462837219.\n",
      "Training epoch 4 batch 1358 with loss 1.7702910900115967, accuracy 0.2936508059501648.\n",
      "Training epoch 4 batch 1359 with loss 1.7393478155136108, accuracy 0.24166667461395264.\n",
      "Training epoch 4 batch 1360 with loss 1.8588708639144897, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1361 with loss 1.8903706073760986, accuracy 0.14166668057441711.\n",
      "Training epoch 4 batch 1362 with loss 1.8239519596099854, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 1363 with loss 1.8359792232513428, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1364 with loss 1.7909657955169678, accuracy 0.26944446563720703.\n",
      "Training epoch 4 batch 1365 with loss 1.8841636180877686, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 1366 with loss 1.8650197982788086, accuracy 0.11666667461395264.\n",
      "Training epoch 4 batch 1367 with loss 1.7141706943511963, accuracy 0.19166667759418488.\n",
      "Training epoch 4 batch 1368 with loss 1.8065416812896729, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 1369 with loss 1.7861058712005615, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 1370 with loss 1.8039602041244507, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1371 with loss 1.738004446029663, accuracy 0.31111112236976624.\n",
      "Training epoch 4 batch 1372 with loss 1.886033296585083, accuracy 0.13055555522441864.\n",
      "Training epoch 4 batch 1373 with loss 1.8260225057601929, accuracy 0.2666666805744171.\n",
      "Training epoch 4 batch 1374 with loss 1.9258315563201904, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 1375 with loss 1.8031495809555054, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 1376 with loss 1.8661750555038452, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 1377 with loss 1.7264049053192139, accuracy 0.2291666865348816.\n",
      "Training epoch 4 batch 1378 with loss 1.7751888036727905, accuracy 0.402777761220932.\n",
      "Training epoch 4 batch 1379 with loss 1.8036344051361084, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1380 with loss 1.7423614263534546, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 1381 with loss 1.7811224460601807, accuracy 0.2976190447807312.\n",
      "Training epoch 4 batch 1382 with loss 1.7988981008529663, accuracy 0.21547619998455048.\n",
      "Training epoch 4 batch 1383 with loss 1.7529083490371704, accuracy 0.07500000298023224.\n",
      "Training epoch 4 batch 1384 with loss 1.7871116399765015, accuracy 0.0793650820851326.\n",
      "Training epoch 4 batch 1385 with loss 1.785642385482788, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 1386 with loss 1.8320999145507812, accuracy 0.03333333507180214.\n",
      "Training epoch 4 batch 1387 with loss 1.6868098974227905, accuracy 0.21111111342906952.\n",
      "Training epoch 4 batch 1388 with loss 1.803480863571167, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1389 with loss 1.8006763458251953, accuracy 0.18333333730697632.\n",
      "Training epoch 4 batch 1390 with loss 1.735958456993103, accuracy 0.24722224473953247.\n",
      "Training epoch 4 batch 1391 with loss 1.771977186203003, accuracy 0.20992064476013184.\n",
      "Training epoch 4 batch 1392 with loss 1.7240225076675415, accuracy 0.30416667461395264.\n",
      "Training epoch 4 batch 1393 with loss 1.7600882053375244, accuracy 0.33888891339302063.\n",
      "Training epoch 4 batch 1394 with loss 1.7973995208740234, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 1395 with loss 1.665846824645996, accuracy 0.2888889014720917.\n",
      "Training epoch 4 batch 1396 with loss 1.7883241176605225, accuracy 0.0694444477558136.\n",
      "Training epoch 4 batch 1397 with loss 1.7565629482269287, accuracy 0.2916666567325592.\n",
      "Training epoch 4 batch 1398 with loss 1.8440001010894775, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 1399 with loss 1.885746955871582, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1400 with loss 1.8908929824829102, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 1401 with loss 1.7040693759918213, accuracy 0.2361111044883728.\n",
      "Training epoch 4 batch 1402 with loss 1.8655109405517578, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 1403 with loss 1.9041919708251953, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1404 with loss 1.7730268239974976, accuracy 0.30714285373687744.\n",
      "Training epoch 4 batch 1405 with loss 1.7481966018676758, accuracy 0.17777778208255768.\n",
      "Training epoch 4 batch 1406 with loss 1.8412096500396729, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1407 with loss 1.7594518661499023, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 1408 with loss 1.8166662454605103, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1409 with loss 1.763938307762146, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 1410 with loss 1.834124207496643, accuracy 0.1071428582072258.\n",
      "Training epoch 4 batch 1411 with loss 1.807843804359436, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 1412 with loss 1.7840076684951782, accuracy 0.17222222685813904.\n",
      "Training epoch 4 batch 1413 with loss 1.750518798828125, accuracy 0.255952388048172.\n",
      "Training epoch 4 batch 1414 with loss 1.9177494049072266, accuracy 0.0.\n",
      "Training epoch 4 batch 1415 with loss 1.7190158367156982, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1416 with loss 1.7295347452163696, accuracy 0.1805555671453476.\n",
      "Training epoch 4 batch 1417 with loss 1.821637511253357, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1418 with loss 1.8296949863433838, accuracy 0.0555555559694767.\n",
      "Training epoch 4 batch 1419 with loss 1.8824714422225952, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1420 with loss 1.8319190740585327, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1421 with loss 1.78545343875885, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 1422 with loss 1.737318992614746, accuracy 0.30000001192092896.\n",
      "Training epoch 4 batch 1423 with loss 1.753554344177246, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 1424 with loss 1.8580067157745361, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 1425 with loss 1.750353217124939, accuracy 0.3027777671813965.\n",
      "Training epoch 4 batch 1426 with loss 1.9019027948379517, accuracy 0.0416666679084301.\n",
      "Training epoch 4 batch 1427 with loss 1.75459885597229, accuracy 0.2936508059501648.\n",
      "Training epoch 4 batch 1428 with loss 1.7850977182388306, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 1429 with loss 1.7689415216445923, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 1430 with loss 1.8898741006851196, accuracy 0.125.\n",
      "Training epoch 4 batch 1431 with loss 1.8213573694229126, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1432 with loss 1.824829339981079, accuracy 0.1626984179019928.\n",
      "Training epoch 4 batch 1433 with loss 1.7614825963974, accuracy 0.18333333730697632.\n",
      "Training epoch 4 batch 1434 with loss 1.775842308998108, accuracy 0.34047621488571167.\n",
      "Training epoch 4 batch 1435 with loss 1.8308378458023071, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1436 with loss 1.851319670677185, accuracy 0.06666667014360428.\n",
      "Training epoch 4 batch 1437 with loss 1.8826911449432373, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1438 with loss 1.85418701171875, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1439 with loss 1.7951757907867432, accuracy 0.2083333283662796.\n",
      "Training epoch 4 batch 1440 with loss 1.7414867877960205, accuracy 0.10277777910232544.\n",
      "Training epoch 4 batch 1441 with loss 1.8701934814453125, accuracy 0.065476194024086.\n",
      "Training epoch 4 batch 1442 with loss 1.7444572448730469, accuracy 0.210317462682724.\n",
      "Training epoch 4 batch 1443 with loss 1.8454786539077759, accuracy 0.20000000298023224.\n",
      "Training epoch 4 batch 1444 with loss 1.6744306087493896, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 1445 with loss 1.840550422668457, accuracy 0.21666666865348816.\n",
      "Training epoch 4 batch 1446 with loss 1.7819089889526367, accuracy 0.17777778208255768.\n",
      "Training epoch 4 batch 1447 with loss 1.8573499917984009, accuracy 0.1944444477558136.\n",
      "Training epoch 4 batch 1448 with loss 1.8530210256576538, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1449 with loss 1.851629614830017, accuracy 0.2638888955116272.\n",
      "Training epoch 4 batch 1450 with loss 1.7889127731323242, accuracy 0.18333333730697632.\n",
      "Training epoch 4 batch 1451 with loss 1.817591667175293, accuracy 0.17777778208255768.\n",
      "Training epoch 4 batch 1452 with loss 1.8494503498077393, accuracy 0.0972222238779068.\n",
      "Training epoch 4 batch 1453 with loss 1.8762620687484741, accuracy 0.0.\n",
      "Training epoch 4 batch 1454 with loss 1.7410777807235718, accuracy 0.1626984179019928.\n",
      "Training epoch 4 batch 1455 with loss 1.8250162601470947, accuracy 0.125.\n",
      "Training epoch 4 batch 1456 with loss 1.8790733814239502, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1457 with loss 1.7959562540054321, accuracy 0.1805555522441864.\n",
      "Training epoch 4 batch 1458 with loss 1.7126119136810303, accuracy 0.24166667461395264.\n",
      "Training epoch 4 batch 1459 with loss 1.816536545753479, accuracy 0.18333333730697632.\n",
      "Training epoch 4 batch 1460 with loss 1.7727222442626953, accuracy 0.3055555820465088.\n",
      "Training epoch 4 batch 1461 with loss 1.8297488689422607, accuracy 0.3611111044883728.\n",
      "Training epoch 4 batch 1462 with loss 1.7189077138900757, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1463 with loss 1.7133057117462158, accuracy 0.25.\n",
      "Training epoch 4 batch 1464 with loss 1.8757394552230835, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1465 with loss 1.8389437198638916, accuracy 0.21111111342906952.\n",
      "Training epoch 4 batch 1466 with loss 1.8193870782852173, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1467 with loss 1.7414404153823853, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1468 with loss 1.7003637552261353, accuracy 0.4166666865348816.\n",
      "Training epoch 4 batch 1469 with loss 1.813198447227478, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 1470 with loss 1.8955574035644531, accuracy 0.0.\n",
      "Training epoch 4 batch 1471 with loss 1.8136844635009766, accuracy 0.222222238779068.\n",
      "Training epoch 4 batch 1472 with loss 1.7649040222167969, accuracy 0.1875.\n",
      "Training epoch 4 batch 1473 with loss 1.7620881795883179, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 1474 with loss 1.7405811548233032, accuracy 0.22499999403953552.\n",
      "Training epoch 4 batch 1475 with loss 1.7531750202178955, accuracy 0.18611112236976624.\n",
      "Training epoch 4 batch 1476 with loss 1.8894017934799194, accuracy 0.0833333358168602.\n",
      "Training epoch 4 batch 1477 with loss 1.7457863092422485, accuracy 0.31388887763023376.\n",
      "Training epoch 4 batch 1478 with loss 1.7901430130004883, accuracy 0.25.\n",
      "Training epoch 4 batch 1479 with loss 1.8346891403198242, accuracy 0.2222222238779068.\n",
      "Training epoch 4 batch 1480 with loss 1.8198953866958618, accuracy 0.20000000298023224.\n",
      "Training epoch 4 batch 1481 with loss 1.9091355800628662, accuracy 0.1111111119389534.\n",
      "Training epoch 4 batch 1482 with loss 1.6860418319702148, accuracy 0.23333334922790527.\n",
      "Training epoch 4 batch 1483 with loss 1.828338384628296, accuracy 0.2083333432674408.\n",
      "Training epoch 4 batch 1484 with loss 1.759804368019104, accuracy 0.25555557012557983.\n",
      "Training epoch 4 batch 1485 with loss 1.7709484100341797, accuracy 0.3055555820465088.\n",
      "Training epoch 4 batch 1486 with loss 1.8104816675186157, accuracy 0.14444445073604584.\n",
      "Training epoch 4 batch 1487 with loss 1.7088521718978882, accuracy 0.15000000596046448.\n",
      "Training epoch 4 batch 1488 with loss 1.7327502965927124, accuracy 0.4166666865348816.\n",
      "Training epoch 4 batch 1489 with loss 1.823108434677124, accuracy 0.09047619253396988.\n",
      "Training epoch 4 batch 1490 with loss 1.7971681356430054, accuracy 0.08888889104127884.\n",
      "Training epoch 4 batch 1491 with loss 1.9018560647964478, accuracy 0.24722224473953247.\n",
      "Training epoch 4 batch 1492 with loss 1.7699806690216064, accuracy 0.2222222238779068.\n",
      "Training epoch 4 batch 1493 with loss 1.765260934829712, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1494 with loss 1.9605741500854492, accuracy 0.1666666716337204.\n",
      "Training epoch 4 batch 1495 with loss 1.7534481287002563, accuracy 0.24166668951511383.\n",
      "Training epoch 4 batch 1496 with loss 1.8030914068222046, accuracy 0.1388888955116272.\n",
      "Training epoch 4 batch 1497 with loss 1.7712905406951904, accuracy 0.3888888955116272.\n",
      "Training epoch 4 batch 1498 with loss 1.756736397743225, accuracy 0.2777777910232544.\n",
      "Training epoch 4 batch 1499 with loss 1.758845329284668, accuracy 0.0416666679084301.\n",
      "Test batch 0 with loss 1.836881399154663 and accuracy 0.0555555559694767.\n",
      "Test batch 1 with loss 1.8365777730941772 and accuracy 0.1111111119389534.\n",
      "Test batch 2 with loss 1.8758890628814697 and accuracy 0.0416666679084301.\n",
      "Test batch 3 with loss 1.8887951374053955 and accuracy 0.23333334922790527.\n",
      "Test batch 4 with loss 1.7895513772964478 and accuracy 0.2698412835597992.\n",
      "Test batch 5 with loss 1.9056211709976196 and accuracy 0.0416666679084301.\n",
      "Test batch 6 with loss 1.8767225742340088 and accuracy 0.25555557012557983.\n",
      "Test batch 7 with loss 1.8653672933578491 and accuracy 0.0555555559694767.\n",
      "Test batch 8 with loss 1.9353930950164795 and accuracy 0.065476194024086.\n",
      "Test batch 9 with loss 1.8502862453460693 and accuracy 0.06666667014360428.\n",
      "Test batch 10 with loss 1.8959354162216187 and accuracy 0.0555555559694767.\n",
      "Test batch 11 with loss 1.7928787469863892 and accuracy 0.1805555522441864.\n",
      "Test batch 12 with loss 1.8201360702514648 and accuracy 0.08888889104127884.\n",
      "Test batch 13 with loss 1.841692566871643 and accuracy 0.1805555671453476.\n",
      "Test batch 14 with loss 1.857008695602417 and accuracy 0.1388888955116272.\n",
      "Test batch 15 with loss 1.7763965129852295 and accuracy 0.20000001788139343.\n",
      "Test batch 16 with loss 1.7314443588256836 and accuracy 0.03333333507180214.\n",
      "Test batch 17 with loss 1.8800932168960571 and accuracy 0.03333333507180214.\n",
      "Test batch 18 with loss 1.8348712921142578 and accuracy 0.0416666679084301.\n",
      "Test batch 19 with loss 1.8479686975479126 and accuracy 0.08888889104127884.\n",
      "Test batch 20 with loss 1.852434515953064 and accuracy 0.15833334624767303.\n",
      "Test batch 21 with loss 1.8159167766571045 and accuracy 0.4222222566604614.\n",
      "Test batch 22 with loss 1.8784126043319702 and accuracy 0.0416666679084301.\n",
      "Test batch 23 with loss 1.9288917779922485 and accuracy 0.02380952425301075.\n",
      "Test batch 24 with loss 1.819881796836853 and accuracy 0.32500001788139343.\n",
      "Test batch 25 with loss 1.7813684940338135 and accuracy 0.0555555559694767.\n",
      "Test batch 26 with loss 1.7351758480072021 and accuracy 0.35277777910232544.\n",
      "Test batch 27 with loss 1.89511239528656 and accuracy 0.25.\n",
      "Test batch 28 with loss 1.7854474782943726 and accuracy 0.210317462682724.\n",
      "Test batch 29 with loss 1.8568947315216064 and accuracy 0.06666667014360428.\n",
      "Test batch 30 with loss 1.6663545370101929 and accuracy 0.4297618865966797.\n",
      "Test batch 31 with loss 1.8223257064819336 and accuracy 0.31111112236976624.\n",
      "Test batch 32 with loss 1.884070634841919 and accuracy 0.06666667014360428.\n",
      "Test batch 33 with loss 1.843435287475586 and accuracy 0.14166666567325592.\n",
      "Test batch 34 with loss 1.8562355041503906 and accuracy 0.0833333358168602.\n",
      "Test batch 35 with loss 1.8550865650177002 and accuracy 0.1111111119389534.\n",
      "Test batch 36 with loss 1.839268684387207 and accuracy 0.08888889104127884.\n",
      "Test batch 37 with loss 1.7999597787857056 and accuracy 0.15000000596046448.\n",
      "Test batch 38 with loss 1.8986732959747314 and accuracy 0.03333333507180214.\n",
      "Test batch 39 with loss 1.8245939016342163 and accuracy 0.1388888955116272.\n",
      "Test batch 40 with loss 1.7899551391601562 and accuracy 0.1111111119389534.\n",
      "Test batch 41 with loss 1.7904880046844482 and accuracy 0.18333333730697632.\n",
      "Test batch 42 with loss 1.8851760625839233 and accuracy 0.03333333507180214.\n",
      "Test batch 43 with loss 1.916434645652771 and accuracy 0.0694444477558136.\n",
      "Test batch 44 with loss 1.815879464149475 and accuracy 0.0555555559694767.\n",
      "Test batch 45 with loss 1.814294457435608 and accuracy 0.3115079402923584.\n",
      "Test batch 46 with loss 1.7831703424453735 and accuracy 0.28333333134651184.\n",
      "Test batch 47 with loss 1.7630577087402344 and accuracy 0.25555557012557983.\n",
      "Test batch 48 with loss 1.8855491876602173 and accuracy 0.11666667461395264.\n",
      "Test batch 49 with loss 1.8448209762573242 and accuracy 0.2083333432674408.\n",
      "Test batch 50 with loss 1.8192870616912842 and accuracy 0.14444445073604584.\n",
      "Test batch 51 with loss 1.9384782314300537 and accuracy 0.0.\n",
      "Test batch 52 with loss 1.8805065155029297 and accuracy 0.09444444626569748.\n",
      "Test batch 53 with loss 1.8703861236572266 and accuracy 0.0833333358168602.\n",
      "Test batch 54 with loss 1.9678821563720703 and accuracy 0.0833333358168602.\n",
      "Test batch 55 with loss 1.9110374450683594 and accuracy 0.2083333432674408.\n",
      "Test batch 56 with loss 1.8506481647491455 and accuracy 0.07500000298023224.\n",
      "Test batch 57 with loss 1.7515977621078491 and accuracy 0.2777777910232544.\n",
      "Test batch 58 with loss 1.8522770404815674 and accuracy 0.17222222685813904.\n",
      "Test batch 59 with loss 1.8451204299926758 and accuracy 0.14444445073604584.\n",
      "Test batch 60 with loss 1.7899415493011475 and accuracy 0.20000000298023224.\n",
      "Test batch 61 with loss 1.901620864868164 and accuracy 0.125.\n",
      "Test batch 62 with loss 1.7395141124725342 and accuracy 0.1666666716337204.\n",
      "Test batch 63 with loss 1.814379096031189 and accuracy 0.13333334028720856.\n",
      "Test batch 64 with loss 1.8067728281021118 and accuracy 0.125.\n",
      "Test batch 65 with loss 1.851158857345581 and accuracy 0.1805555522441864.\n",
      "Test batch 66 with loss 1.8010509014129639 and accuracy 0.130952388048172.\n",
      "Test batch 67 with loss 1.7910559177398682 and accuracy 0.15833334624767303.\n",
      "Test batch 68 with loss 1.84579598903656 and accuracy 0.15000000596046448.\n",
      "Test batch 69 with loss 1.8050222396850586 and accuracy 0.02777777798473835.\n",
      "Test batch 70 with loss 1.77934992313385 and accuracy 0.1527777761220932.\n",
      "Test batch 71 with loss 1.83309805393219 and accuracy 0.1666666716337204.\n",
      "Test batch 72 with loss 1.800994634628296 and accuracy 0.222222238779068.\n",
      "Test batch 73 with loss 1.9184925556182861 and accuracy 0.08888889104127884.\n",
      "Test batch 74 with loss 1.8331407308578491 and accuracy 0.2222222238779068.\n",
      "Test batch 75 with loss 1.7436809539794922 and accuracy 0.3194444477558136.\n",
      "Test batch 76 with loss 1.9466129541397095 and accuracy 0.2916666567325592.\n",
      "Test batch 77 with loss 1.8254324197769165 and accuracy 0.1944444477558136.\n",
      "Test batch 78 with loss 1.8412622213363647 and accuracy 0.02777777798473835.\n",
      "Test batch 79 with loss 1.8668702840805054 and accuracy 0.125.\n",
      "Test batch 80 with loss 1.9347937107086182 and accuracy 0.23333334922790527.\n",
      "Test batch 81 with loss 1.8256031274795532 and accuracy 0.1130952388048172.\n",
      "Test batch 82 with loss 1.7687498331069946 and accuracy 0.347222238779068.\n",
      "Test batch 83 with loss 1.9270185232162476 and accuracy 0.0416666679084301.\n",
      "Test batch 84 with loss 1.811941385269165 and accuracy 0.1388888955116272.\n",
      "Test batch 85 with loss 1.8304325342178345 and accuracy 0.10833333432674408.\n",
      "Test batch 86 with loss 1.785535216331482 and accuracy 0.2611111104488373.\n",
      "Test batch 87 with loss 1.8931163549423218 and accuracy 0.0714285746216774.\n",
      "Test batch 88 with loss 1.7347482442855835 and accuracy 0.2152777761220932.\n",
      "Test batch 89 with loss 1.8703958988189697 and accuracy 0.13055555522441864.\n",
      "Test batch 90 with loss 1.83918035030365 and accuracy 0.20000000298023224.\n",
      "Test batch 91 with loss 1.878281831741333 and accuracy 0.125.\n",
      "Test batch 92 with loss 1.7231290340423584 and accuracy 0.3948412537574768.\n",
      "Test batch 93 with loss 1.7999451160430908 and accuracy 0.125.\n",
      "Test batch 94 with loss 1.8841689825057983 and accuracy 0.2750000059604645.\n",
      "Test batch 95 with loss 1.725801706314087 and accuracy 0.2222222238779068.\n",
      "Test batch 96 with loss 1.8877391815185547 and accuracy 0.0972222238779068.\n",
      "Test batch 97 with loss 1.7872264385223389 and accuracy 0.17777778208255768.\n",
      "Test batch 98 with loss 1.7585468292236328 and accuracy 0.3083333373069763.\n",
      "Test batch 99 with loss 1.891122817993164 and accuracy 0.1527777761220932.\n",
      "Test batch 100 with loss 1.7450193166732788 and accuracy 0.1388888955116272.\n",
      "Test batch 101 with loss 1.8297637701034546 and accuracy 0.2083333432674408.\n",
      "Test batch 102 with loss 1.8432872295379639 and accuracy 0.125.\n",
      "Test batch 103 with loss 1.8377702236175537 and accuracy 0.0476190485060215.\n",
      "Test batch 104 with loss 1.7831947803497314 and accuracy 0.21984127163887024.\n",
      "Test batch 105 with loss 1.8366544246673584 and accuracy 0.0833333358168602.\n",
      "Test batch 106 with loss 1.8337745666503906 and accuracy 0.125.\n",
      "Test batch 107 with loss 1.8557837009429932 and accuracy 0.06666667014360428.\n",
      "Test batch 108 with loss 1.7395355701446533 and accuracy 0.24074074625968933.\n",
      "Test batch 109 with loss 1.82882821559906 and accuracy 0.2638888955116272.\n",
      "Test batch 110 with loss 1.8684393167495728 and accuracy 0.0555555559694767.\n",
      "Test batch 111 with loss 1.8670012950897217 and accuracy 0.0833333358168602.\n",
      "Test batch 112 with loss 1.8951505422592163 and accuracy 0.1388888955116272.\n",
      "Test batch 113 with loss 1.7356653213500977 and accuracy 0.236111119389534.\n",
      "Test batch 114 with loss 1.8247101306915283 and accuracy 0.1944444477558136.\n",
      "Test batch 115 with loss 1.725064992904663 and accuracy 0.3333333432674408.\n",
      "Test batch 116 with loss 1.842566728591919 and accuracy 0.23333333432674408.\n",
      "Test batch 117 with loss 1.851934790611267 and accuracy 0.0972222238779068.\n",
      "Test batch 118 with loss 1.8621642589569092 and accuracy 0.07500000298023224.\n",
      "Test batch 119 with loss 1.796810507774353 and accuracy 0.1805555522441864.\n",
      "Test batch 120 with loss 1.8688385486602783 and accuracy 0.1111111119389534.\n",
      "Test batch 121 with loss 1.837881326675415 and accuracy 0.125.\n",
      "Test batch 122 with loss 1.8224222660064697 and accuracy 0.17222222685813904.\n",
      "Test batch 123 with loss 1.8484516143798828 and accuracy 0.1666666716337204.\n",
      "Test batch 124 with loss 1.7562793493270874 and accuracy 0.1388888955116272.\n",
      "Test batch 125 with loss 1.9277637004852295 and accuracy 0.03333333507180214.\n",
      "Test batch 126 with loss 1.9097347259521484 and accuracy 0.0416666679084301.\n",
      "Test batch 127 with loss 1.86172616481781 and accuracy 0.0972222238779068.\n",
      "Test batch 128 with loss 1.8148729801177979 and accuracy 0.11666667461395264.\n",
      "Test batch 129 with loss 1.871270775794983 and accuracy 0.0416666679084301.\n",
      "Test batch 130 with loss 1.8896620273590088 and accuracy 0.0416666679084301.\n",
      "Test batch 131 with loss 1.8706989288330078 and accuracy 0.31111112236976624.\n",
      "Test batch 132 with loss 1.7885115146636963 and accuracy 0.1666666716337204.\n",
      "Test batch 133 with loss 1.795941948890686 and accuracy 0.125.\n",
      "Test batch 134 with loss 1.821051001548767 and accuracy 0.25.\n",
      "Test batch 135 with loss 1.7037169933319092 and accuracy 0.3960317373275757.\n",
      "Test batch 136 with loss 1.9586222171783447 and accuracy 0.02777777798473835.\n",
      "Test batch 137 with loss 1.8215439319610596 and accuracy 0.05416666716337204.\n",
      "Test batch 138 with loss 1.847362756729126 and accuracy 0.07500000298023224.\n",
      "Test batch 139 with loss 1.817779541015625 and accuracy 0.3055555522441864.\n",
      "Test batch 140 with loss 1.7918586730957031 and accuracy 0.3166666626930237.\n",
      "Test batch 141 with loss 1.771519422531128 and accuracy 0.2361111044883728.\n",
      "Test batch 142 with loss 1.8439209461212158 and accuracy 0.07500000298023224.\n",
      "Test batch 143 with loss 1.803593635559082 and accuracy 0.07500000298023224.\n",
      "Test batch 144 with loss 1.742963433265686 and accuracy 0.19722221791744232.\n",
      "Test batch 145 with loss 1.79997980594635 and accuracy 0.08888889104127884.\n",
      "Test batch 146 with loss 1.98481023311615 and accuracy 0.0555555559694767.\n",
      "Test batch 147 with loss 1.795100212097168 and accuracy 0.2611111104488373.\n",
      "Test batch 148 with loss 1.6933796405792236 and accuracy 0.2222222238779068.\n",
      "Test batch 149 with loss 1.774385690689087 and accuracy 0.1408730149269104.\n",
      "Test batch 150 with loss 1.9204292297363281 and accuracy 0.0416666679084301.\n",
      "Test batch 151 with loss 1.8024345636367798 and accuracy 0.10833333432674408.\n",
      "Test batch 152 with loss 1.8172504901885986 and accuracy 0.33888891339302063.\n",
      "Test batch 153 with loss 1.9060595035552979 and accuracy 0.11666667461395264.\n",
      "Test batch 154 with loss 1.7837384939193726 and accuracy 0.11666667461395264.\n",
      "Test batch 155 with loss 1.8501207828521729 and accuracy 0.15833334624767303.\n",
      "Test batch 156 with loss 1.8031842708587646 and accuracy 0.3777777850627899.\n",
      "Test batch 157 with loss 1.8563207387924194 and accuracy 0.11666667461395264.\n",
      "Test batch 158 with loss 1.8119977712631226 and accuracy 0.1666666716337204.\n",
      "Test batch 159 with loss 1.8607670068740845 and accuracy 0.20000000298023224.\n",
      "Test batch 160 with loss 1.7508795261383057 and accuracy 0.15000000596046448.\n",
      "Test batch 161 with loss 1.77691650390625 and accuracy 0.1111111119389534.\n",
      "Test batch 162 with loss 1.7815297842025757 and accuracy 0.28333333134651184.\n",
      "Test batch 163 with loss 1.8436224460601807 and accuracy 0.0625.\n",
      "Test batch 164 with loss 1.7575671672821045 and accuracy 0.2083333432674408.\n",
      "Test batch 165 with loss 1.8834329843521118 and accuracy 0.0793650820851326.\n",
      "Test batch 166 with loss 1.852556586265564 and accuracy 0.1666666716337204.\n",
      "Test batch 167 with loss 1.8967983722686768 and accuracy 0.26944443583488464.\n",
      "Test batch 168 with loss 1.8125860691070557 and accuracy 0.2182539701461792.\n",
      "Test batch 169 with loss 1.8519662618637085 and accuracy 0.1527777761220932.\n",
      "Test batch 170 with loss 1.8468704223632812 and accuracy 0.24166667461395264.\n",
      "Test batch 171 with loss 1.8340362310409546 and accuracy 0.0555555559694767.\n",
      "Test batch 172 with loss 1.7826112508773804 and accuracy 0.3710317611694336.\n",
      "Test batch 173 with loss 1.7308721542358398 and accuracy 0.1805555522441864.\n",
      "Test batch 174 with loss 1.8061418533325195 and accuracy 0.1765872985124588.\n",
      "Test batch 175 with loss 1.6993887424468994 and accuracy 0.1130952388048172.\n",
      "Test batch 176 with loss 1.7733882665634155 and accuracy 0.1587301641702652.\n",
      "Test batch 177 with loss 1.8979203701019287 and accuracy 0.1666666716337204.\n",
      "Test batch 178 with loss 1.882866621017456 and accuracy 0.0416666679084301.\n",
      "Test batch 179 with loss 1.8575811386108398 and accuracy 0.1785714328289032.\n",
      "Test batch 180 with loss 1.9739071130752563 and accuracy 0.0.\n",
      "Test batch 181 with loss 1.7920610904693604 and accuracy 0.3055555522441864.\n",
      "Test batch 182 with loss 1.7736870050430298 and accuracy 0.1527777761220932.\n",
      "Test batch 183 with loss 1.8565280437469482 and accuracy 0.0972222238779068.\n",
      "Test batch 184 with loss 1.9268884658813477 and accuracy 0.0555555559694767.\n",
      "Test batch 185 with loss 1.725482702255249 and accuracy 0.3194444477558136.\n",
      "Test batch 186 with loss 1.8530489206314087 and accuracy 0.236111119389534.\n",
      "Test batch 187 with loss 1.8490877151489258 and accuracy 0.03333333507180214.\n",
      "Test batch 188 with loss 1.8884773254394531 and accuracy 0.0793650820851326.\n",
      "Test batch 189 with loss 1.8522371053695679 and accuracy 0.11666667461395264.\n",
      "Test batch 190 with loss 1.820704698562622 and accuracy 0.40833333134651184.\n",
      "Test batch 191 with loss 1.8521406650543213 and accuracy 0.1666666716337204.\n",
      "Test batch 192 with loss 1.844696044921875 and accuracy 0.125.\n",
      "Test batch 193 with loss 1.9532725811004639 and accuracy 0.03333333507180214.\n",
      "Test batch 194 with loss 1.8082212209701538 and accuracy 0.3333333432674408.\n",
      "Test batch 195 with loss 1.807554006576538 and accuracy 0.2321428656578064.\n",
      "Test batch 196 with loss 1.8733174800872803 and accuracy 0.1111111119389534.\n",
      "Test batch 197 with loss 1.828721284866333 and accuracy 0.0833333358168602.\n",
      "Test batch 198 with loss 1.8365577459335327 and accuracy 0.12222222983837128.\n",
      "Test batch 199 with loss 1.7722032070159912 and accuracy 0.4027777910232544.\n",
      "Test batch 200 with loss 1.8607193231582642 and accuracy 0.22777777910232544.\n",
      "Test batch 201 with loss 1.788161277770996 and accuracy 0.36666667461395264.\n",
      "Test batch 202 with loss 1.8840221166610718 and accuracy 0.1666666716337204.\n",
      "Test batch 203 with loss 1.8073463439941406 and accuracy 0.31111112236976624.\n",
      "Test batch 204 with loss 1.7679046392440796 and accuracy 0.17222222685813904.\n",
      "Test batch 205 with loss 1.846472978591919 and accuracy 0.1944444477558136.\n",
      "Test batch 206 with loss 1.899627447128296 and accuracy 0.1805555522441864.\n",
      "Test batch 207 with loss 1.902235746383667 and accuracy 0.0833333358168602.\n",
      "Test batch 208 with loss 1.8576730489730835 and accuracy 0.15000000596046448.\n",
      "Test batch 209 with loss 1.7911341190338135 and accuracy 0.0416666679084301.\n",
      "Test batch 210 with loss 1.8135955333709717 and accuracy 0.20000000298023224.\n",
      "Test batch 211 with loss 1.7064965963363647 and accuracy 0.23888888955116272.\n",
      "Test batch 212 with loss 1.822885513305664 and accuracy 0.3333333432674408.\n",
      "Test batch 213 with loss 1.8050190210342407 and accuracy 0.1805555522441864.\n",
      "Test batch 214 with loss 1.8765836954116821 and accuracy 0.36666667461395264.\n",
      "Test batch 215 with loss 1.8526923656463623 and accuracy 0.0555555559694767.\n",
      "Test batch 216 with loss 1.8957656621932983 and accuracy 0.0833333358168602.\n",
      "Test batch 217 with loss 1.7124032974243164 and accuracy 0.1944444477558136.\n",
      "Test batch 218 with loss 1.7587779760360718 and accuracy 0.3472222089767456.\n",
      "Test batch 219 with loss 1.8697307109832764 and accuracy 0.1349206417798996.\n",
      "Test batch 220 with loss 1.9034878015518188 and accuracy 0.2182539701461792.\n",
      "Test batch 221 with loss 1.9121137857437134 and accuracy 0.0555555559694767.\n",
      "Test batch 222 with loss 1.8077526092529297 and accuracy 0.236111119389534.\n",
      "Test batch 223 with loss 1.7638616561889648 and accuracy 0.22777777910232544.\n",
      "Test batch 224 with loss 1.8118617534637451 and accuracy 0.19166666269302368.\n",
      "Test batch 225 with loss 1.8278777599334717 and accuracy 0.14444445073604584.\n",
      "Test batch 226 with loss 1.7690391540527344 and accuracy 0.1666666716337204.\n",
      "Test batch 227 with loss 1.7444102764129639 and accuracy 0.20555557310581207.\n",
      "Test batch 228 with loss 1.9374603033065796 and accuracy 0.0833333358168602.\n",
      "Test batch 229 with loss 1.7561231851577759 and accuracy 0.2222222238779068.\n",
      "Test batch 230 with loss 1.8549444675445557 and accuracy 0.1805555522441864.\n",
      "Test batch 231 with loss 1.7783327102661133 and accuracy 0.130952388048172.\n",
      "Test batch 232 with loss 1.8734681606292725 and accuracy 0.14444445073604584.\n",
      "Test batch 233 with loss 1.7143385410308838 and accuracy 0.1865079402923584.\n",
      "Test batch 234 with loss 1.8758649826049805 and accuracy 0.0694444477558136.\n",
      "Test batch 235 with loss 1.8678741455078125 and accuracy 0.1111111119389534.\n",
      "Test batch 236 with loss 1.8034439086914062 and accuracy 0.14166668057441711.\n",
      "Test batch 237 with loss 1.8320016860961914 and accuracy 0.1666666716337204.\n",
      "Test batch 238 with loss 1.7644659280776978 and accuracy 0.2013888955116272.\n",
      "Test batch 239 with loss 1.7965946197509766 and accuracy 0.3571428656578064.\n",
      "Test batch 240 with loss 1.8474403619766235 and accuracy 0.08888889104127884.\n",
      "Test batch 241 with loss 1.8627926111221313 and accuracy 0.17222222685813904.\n",
      "Test batch 242 with loss 1.8098104000091553 and accuracy 0.25.\n",
      "Test batch 243 with loss 1.8043098449707031 and accuracy 0.2043650895357132.\n",
      "Test batch 244 with loss 1.8864660263061523 and accuracy 0.02777777798473835.\n",
      "Test batch 245 with loss 1.7010389566421509 and accuracy 0.30158731341362.\n",
      "Test batch 246 with loss 1.8341007232666016 and accuracy 0.22777777910232544.\n",
      "Test batch 247 with loss 1.7314341068267822 and accuracy 0.2222222238779068.\n",
      "Test batch 248 with loss 1.8010263442993164 and accuracy 0.1666666716337204.\n",
      "Test batch 249 with loss 1.6586897373199463 and accuracy 0.3829365372657776.\n",
      "Test batch 250 with loss 1.695901870727539 and accuracy 0.3472222089767456.\n",
      "Test batch 251 with loss 1.8537442684173584 and accuracy 0.18333333730697632.\n",
      "Test batch 252 with loss 1.762325644493103 and accuracy 0.21388889849185944.\n",
      "Test batch 253 with loss 1.802556037902832 and accuracy 0.236111119389534.\n",
      "Test batch 254 with loss 1.7762256860733032 and accuracy 0.17777778208255768.\n",
      "Test batch 255 with loss 1.8810322284698486 and accuracy 0.0694444477558136.\n",
      "Test batch 256 with loss 1.796915054321289 and accuracy 0.1666666716337204.\n",
      "Test batch 257 with loss 1.831491470336914 and accuracy 0.25.\n",
      "Test batch 258 with loss 1.8401734828948975 and accuracy 0.2083333432674408.\n",
      "Test batch 259 with loss 1.6906057596206665 and accuracy 0.2916666865348816.\n",
      "Test batch 260 with loss 1.8335978984832764 and accuracy 0.0892857164144516.\n",
      "Test batch 261 with loss 1.861197829246521 and accuracy 0.0.\n",
      "Test batch 262 with loss 1.8173272609710693 and accuracy 0.20666667819023132.\n",
      "Test batch 263 with loss 1.8205187320709229 and accuracy 0.1111111119389534.\n",
      "Test batch 264 with loss 1.7093709707260132 and accuracy 0.2916666567325592.\n",
      "Test batch 265 with loss 1.7973527908325195 and accuracy 0.3222222328186035.\n",
      "Test batch 266 with loss 1.7827260494232178 and accuracy 0.2361111044883728.\n",
      "Test batch 267 with loss 1.8280881643295288 and accuracy 0.02380952425301075.\n",
      "Test batch 268 with loss 1.7260463237762451 and accuracy 0.4305555820465088.\n",
      "Test batch 269 with loss 1.8591079711914062 and accuracy 0.2888889014720917.\n",
      "Test batch 270 with loss 1.877608060836792 and accuracy 0.0.\n",
      "Test batch 271 with loss 1.7690410614013672 and accuracy 0.19166666269302368.\n",
      "Test batch 272 with loss 1.8576984405517578 and accuracy 0.18611112236976624.\n",
      "Test batch 273 with loss 1.8313722610473633 and accuracy 0.25.\n",
      "Test batch 274 with loss 1.8954098224639893 and accuracy 0.31111112236976624.\n",
      "Test batch 275 with loss 1.8201395273208618 and accuracy 0.0.\n",
      "Test batch 276 with loss 1.8946020603179932 and accuracy 0.0833333358168602.\n",
      "Test batch 277 with loss 1.7027219533920288 and accuracy 0.375.\n",
      "Test batch 278 with loss 1.8527532815933228 and accuracy 0.1527777910232544.\n",
      "Test batch 279 with loss 1.7259258031845093 and accuracy 0.20555555820465088.\n",
      "Test batch 280 with loss 1.8656851053237915 and accuracy 0.2222222238779068.\n",
      "Test batch 281 with loss 1.954223394393921 and accuracy 0.1111111119389534.\n",
      "Test batch 282 with loss 1.8479797840118408 and accuracy 0.0476190485060215.\n",
      "Test batch 283 with loss 1.7698628902435303 and accuracy 0.13333334028720856.\n",
      "Test batch 284 with loss 1.833343505859375 and accuracy 0.1666666716337204.\n",
      "Test batch 285 with loss 1.8388500213623047 and accuracy 0.1527777761220932.\n",
      "Test batch 286 with loss 1.789396047592163 and accuracy 0.3055555522441864.\n",
      "Test batch 287 with loss 1.8245973587036133 and accuracy 0.3055555522441864.\n",
      "Test batch 288 with loss 1.8557252883911133 and accuracy 0.1388888955116272.\n",
      "Test batch 289 with loss 1.7662372589111328 and accuracy 0.11428572237491608.\n",
      "Test batch 290 with loss 1.8369019031524658 and accuracy 0.125.\n",
      "Test batch 291 with loss 1.7882179021835327 and accuracy 0.1805555522441864.\n",
      "Test batch 292 with loss 1.7826496362686157 and accuracy 0.3444444537162781.\n",
      "Test batch 293 with loss 1.8204857110977173 and accuracy 0.25.\n",
      "Test batch 294 with loss 1.8197866678237915 and accuracy 0.06666667014360428.\n",
      "Test batch 295 with loss 1.7493976354599 and accuracy 0.25.\n",
      "Test batch 296 with loss 1.8218005895614624 and accuracy 0.23333334922790527.\n",
      "Test batch 297 with loss 1.763658881187439 and accuracy 0.17777778208255768.\n",
      "Test batch 298 with loss 1.9154398441314697 and accuracy 0.08888889104127884.\n",
      "Test batch 299 with loss 1.7843959331512451 and accuracy 0.3611111342906952.\n",
      "Test batch 300 with loss 1.7507994174957275 and accuracy 0.2291666865348816.\n",
      "Test batch 301 with loss 1.914229154586792 and accuracy 0.03333333507180214.\n",
      "Test batch 302 with loss 1.7055065631866455 and accuracy 0.2777777910232544.\n",
      "Test batch 303 with loss 1.824392557144165 and accuracy 0.1944444477558136.\n",
      "Test batch 304 with loss 1.8501355648040771 and accuracy 0.1527777761220932.\n",
      "Test batch 305 with loss 1.8819631338119507 and accuracy 0.0972222238779068.\n",
      "Test batch 306 with loss 1.7528873682022095 and accuracy 0.130952388048172.\n",
      "Test batch 307 with loss 1.7895736694335938 and accuracy 0.25.\n",
      "Test batch 308 with loss 1.9003387689590454 and accuracy 0.02380952425301075.\n",
      "Test batch 309 with loss 1.8337901830673218 and accuracy 0.1587301641702652.\n",
      "Test batch 310 with loss 1.8744556903839111 and accuracy 0.0416666679084301.\n",
      "Test batch 311 with loss 1.870083212852478 and accuracy 0.06666667014360428.\n",
      "Test batch 312 with loss 1.7382161617279053 and accuracy 0.5031746029853821.\n",
      "Test batch 313 with loss 1.7492256164550781 and accuracy 0.20555555820465088.\n",
      "Test batch 314 with loss 1.7705589532852173 and accuracy 0.1666666716337204.\n",
      "Test batch 315 with loss 1.8341096639633179 and accuracy 0.1388888955116272.\n",
      "Test batch 316 with loss 1.8860857486724854 and accuracy 0.08888889104127884.\n",
      "Test batch 317 with loss 1.824200987815857 and accuracy 0.1527777761220932.\n",
      "Test batch 318 with loss 1.9008516073226929 and accuracy 0.0833333358168602.\n",
      "Test batch 319 with loss 1.8007371425628662 and accuracy 0.065476194024086.\n",
      "Test batch 320 with loss 1.8472731113433838 and accuracy 0.13055555522441864.\n",
      "Test batch 321 with loss 1.8551225662231445 and accuracy 0.1666666716337204.\n",
      "Test batch 322 with loss 1.834509253501892 and accuracy 0.1388888955116272.\n",
      "Test batch 323 with loss 1.840838074684143 and accuracy 0.06666667014360428.\n",
      "Test batch 324 with loss 1.9328924417495728 and accuracy 0.1210317462682724.\n",
      "Test batch 325 with loss 1.7604129314422607 and accuracy 0.32500001788139343.\n",
      "Test batch 326 with loss 1.8580729961395264 and accuracy 0.0972222238779068.\n",
      "Test batch 327 with loss 1.7314609289169312 and accuracy 0.25555554032325745.\n",
      "Test batch 328 with loss 1.8597885370254517 and accuracy 0.125.\n",
      "Test batch 329 with loss 1.8553647994995117 and accuracy 0.0555555559694767.\n",
      "Test batch 330 with loss 1.7410939931869507 and accuracy 0.1944444477558136.\n",
      "Test batch 331 with loss 1.7318782806396484 and accuracy 0.18888889253139496.\n",
      "Test batch 332 with loss 1.8124622106552124 and accuracy 0.14444445073604584.\n",
      "Test batch 333 with loss 1.7615960836410522 and accuracy 0.1944444477558136.\n",
      "Test batch 334 with loss 1.7417188882827759 and accuracy 0.08095238357782364.\n",
      "Test batch 335 with loss 1.8203842639923096 and accuracy 0.15000000596046448.\n",
      "Test batch 336 with loss 1.7035770416259766 and accuracy 0.3888888955116272.\n",
      "Test batch 337 with loss 1.784767746925354 and accuracy 0.28333333134651184.\n",
      "Test batch 338 with loss 1.8026336431503296 and accuracy 0.07500000298023224.\n",
      "Test batch 339 with loss 1.9170143604278564 and accuracy 0.0555555559694767.\n",
      "Test batch 340 with loss 1.8474994897842407 and accuracy 0.0694444477558136.\n",
      "Test batch 341 with loss 1.906299352645874 and accuracy 0.11666667461395264.\n",
      "Test batch 342 with loss 1.8028545379638672 and accuracy 0.3263888955116272.\n",
      "Test batch 343 with loss 1.7829290628433228 and accuracy 0.3611111342906952.\n",
      "Test batch 344 with loss 1.8824350833892822 and accuracy 0.15833333134651184.\n",
      "Test batch 345 with loss 1.852739691734314 and accuracy 0.0.\n",
      "Test batch 346 with loss 1.9914939403533936 and accuracy 0.0.\n",
      "Test batch 347 with loss 1.870589256286621 and accuracy 0.18611110746860504.\n",
      "Test batch 348 with loss 1.8696315288543701 and accuracy 0.2083333432674408.\n",
      "Test batch 349 with loss 1.8862600326538086 and accuracy 0.1488095223903656.\n",
      "Test batch 350 with loss 1.8058650493621826 and accuracy 0.17817460000514984.\n",
      "Test batch 351 with loss 1.8414825201034546 and accuracy 0.1597222238779068.\n",
      "Test batch 352 with loss 1.8138115406036377 and accuracy 0.0833333358168602.\n",
      "Test batch 353 with loss 1.81570303440094 and accuracy 0.0555555559694767.\n",
      "Test batch 354 with loss 1.8750483989715576 and accuracy 0.14166666567325592.\n",
      "Test batch 355 with loss 1.8747793436050415 and accuracy 0.14351852238178253.\n",
      "Test batch 356 with loss 1.8164247274398804 and accuracy 0.0833333358168602.\n",
      "Test batch 357 with loss 1.811522126197815 and accuracy 0.14444445073604584.\n",
      "Test batch 358 with loss 1.8218839168548584 and accuracy 0.0555555559694767.\n",
      "Test batch 359 with loss 1.934332251548767 and accuracy 0.0972222238779068.\n",
      "Test batch 360 with loss 1.8963016271591187 and accuracy 0.24074074625968933.\n",
      "Test batch 361 with loss 1.7503677606582642 and accuracy 0.1944444477558136.\n",
      "Test batch 362 with loss 1.8317110538482666 and accuracy 0.0625.\n",
      "Test batch 363 with loss 1.935274362564087 and accuracy 0.1666666716337204.\n",
      "Test batch 364 with loss 1.7986719608306885 and accuracy 0.125.\n",
      "Test batch 365 with loss 1.734326720237732 and accuracy 0.2083333432674408.\n",
      "Test batch 366 with loss 1.85930597782135 and accuracy 0.3571428656578064.\n",
      "Test batch 367 with loss 1.7624988555908203 and accuracy 0.10833333432674408.\n",
      "Test batch 368 with loss 1.871726393699646 and accuracy 0.24761904776096344.\n",
      "Test batch 369 with loss 1.9062093496322632 and accuracy 0.1527777761220932.\n",
      "Test batch 370 with loss 1.8660767078399658 and accuracy 0.15833334624767303.\n",
      "Test batch 371 with loss 1.7660726308822632 and accuracy 0.18333333730697632.\n",
      "Test batch 372 with loss 1.8092933893203735 and accuracy 0.1944444477558136.\n",
      "Test batch 373 with loss 1.7840778827667236 and accuracy 0.3055555820465088.\n",
      "Test batch 374 with loss 1.7967498302459717 and accuracy 0.1388888955116272.\n",
      "Test batch 375 with loss 1.770629644393921 and accuracy 0.125.\n",
      "Test batch 376 with loss 1.7541640996932983 and accuracy 0.26944446563720703.\n",
      "Test batch 377 with loss 1.8260498046875 and accuracy 0.1111111119389534.\n",
      "Test batch 378 with loss 1.798187017440796 and accuracy 0.0892857164144516.\n",
      "Test batch 379 with loss 1.8752723932266235 and accuracy 0.16388888657093048.\n",
      "Test batch 380 with loss 1.9589154720306396 and accuracy 0.02380952425301075.\n",
      "Test batch 381 with loss 1.712659478187561 and accuracy 0.4513888955116272.\n",
      "Test batch 382 with loss 1.8378595113754272 and accuracy 0.0416666679084301.\n",
      "Test batch 383 with loss 1.903432846069336 and accuracy 0.12222222983837128.\n",
      "Test batch 384 with loss 1.8890775442123413 and accuracy 0.39444446563720703.\n",
      "Test batch 385 with loss 1.7896192073822021 and accuracy 0.09444444626569748.\n",
      "Test batch 386 with loss 1.816650629043579 and accuracy 0.29722222685813904.\n",
      "Test batch 387 with loss 1.8999512195587158 and accuracy 0.14444445073604584.\n",
      "Test batch 388 with loss 1.7557882070541382 and accuracy 0.1944444477558136.\n",
      "Test batch 389 with loss 1.695996880531311 and accuracy 0.20555555820465088.\n",
      "Test batch 390 with loss 1.8472540378570557 and accuracy 0.0555555559694767.\n",
      "Test batch 391 with loss 1.7962236404418945 and accuracy 0.12916666269302368.\n",
      "Test batch 392 with loss 1.829500436782837 and accuracy 0.3611111342906952.\n",
      "Test batch 393 with loss 1.871106743812561 and accuracy 0.0694444477558136.\n",
      "Test batch 394 with loss 1.8213424682617188 and accuracy 0.2460317611694336.\n",
      "Test batch 395 with loss 1.7817796468734741 and accuracy 0.190476194024086.\n",
      "Test batch 396 with loss 1.7732902765274048 and accuracy 0.08888889104127884.\n",
      "Test batch 397 with loss 1.7545417547225952 and accuracy 0.09880952537059784.\n",
      "Test batch 398 with loss 1.832798719406128 and accuracy 0.144841268658638.\n",
      "Test batch 399 with loss 1.8333005905151367 and accuracy 0.0416666679084301.\n",
      "Test batch 400 with loss 1.8991848230361938 and accuracy 0.0416666679084301.\n",
      "Test batch 401 with loss 1.8877060413360596 and accuracy 0.0833333358168602.\n",
      "Test batch 402 with loss 1.8891236782073975 and accuracy 0.1666666716337204.\n",
      "Test batch 403 with loss 1.8595651388168335 and accuracy 0.0972222238779068.\n",
      "Test batch 404 with loss 1.8049249649047852 and accuracy 0.06111111491918564.\n",
      "Test batch 405 with loss 1.8523772954940796 and accuracy 0.13611111044883728.\n",
      "Test batch 406 with loss 1.7620055675506592 and accuracy 0.2083333432674408.\n",
      "Test batch 407 with loss 1.8986533880233765 and accuracy 0.0833333358168602.\n",
      "Test batch 408 with loss 1.815070390701294 and accuracy 0.2182539701461792.\n",
      "Test batch 409 with loss 1.813930869102478 and accuracy 0.1111111119389534.\n",
      "Test batch 410 with loss 1.806204080581665 and accuracy 0.1388888955116272.\n",
      "Test batch 411 with loss 1.9805800914764404 and accuracy 0.0555555559694767.\n",
      "Test batch 412 with loss 1.8376010656356812 and accuracy 0.0416666679084301.\n",
      "Test batch 413 with loss 1.8228200674057007 and accuracy 0.11269842088222504.\n",
      "Test batch 414 with loss 1.8391882181167603 and accuracy 0.18333333730697632.\n",
      "Test batch 415 with loss 1.8629869222640991 and accuracy 0.08888889104127884.\n",
      "Test batch 416 with loss 1.803592324256897 and accuracy 0.02083333395421505.\n",
      "Test batch 417 with loss 1.9179255962371826 and accuracy 0.10833333432674408.\n",
      "Test batch 418 with loss 1.6831190586090088 and accuracy 0.18333333730697632.\n",
      "Test batch 419 with loss 1.8708873987197876 and accuracy 0.0555555559694767.\n",
      "Test batch 420 with loss 1.834120512008667 and accuracy 0.13333334028720856.\n",
      "Test batch 421 with loss 1.852927803993225 and accuracy 0.20000001788139343.\n",
      "Test batch 422 with loss 1.9150526523590088 and accuracy 0.3333333432674408.\n",
      "Test batch 423 with loss 1.8613687753677368 and accuracy 0.02380952425301075.\n",
      "Test batch 424 with loss 1.8480777740478516 and accuracy 0.0833333358168602.\n",
      "Test batch 425 with loss 1.7318916320800781 and accuracy 0.24166667461395264.\n",
      "Test batch 426 with loss 1.8623898029327393 and accuracy 0.1111111119389534.\n",
      "Test batch 427 with loss 1.8282968997955322 and accuracy 0.0892857164144516.\n",
      "Test batch 428 with loss 1.7997575998306274 and accuracy 0.1875.\n",
      "Test batch 429 with loss 1.8616987466812134 and accuracy 0.0416666679084301.\n",
      "Test batch 430 with loss 1.9423803091049194 and accuracy 0.0555555559694767.\n",
      "Test batch 431 with loss 1.7877734899520874 and accuracy 0.23888888955116272.\n",
      "Test batch 432 with loss 1.8020986318588257 and accuracy 0.2738095223903656.\n",
      "Test batch 433 with loss 1.781267523765564 and accuracy 0.0793650820851326.\n",
      "Test batch 434 with loss 1.8188127279281616 and accuracy 0.1944444477558136.\n",
      "Test batch 435 with loss 1.7798669338226318 and accuracy 0.3444444537162781.\n",
      "Test batch 436 with loss 1.7630866765975952 and accuracy 0.24166667461395264.\n",
      "Test batch 437 with loss 1.7911901473999023 and accuracy 0.17777778208255768.\n",
      "Test batch 438 with loss 1.7802000045776367 and accuracy 0.10000000894069672.\n",
      "Test batch 439 with loss 1.7696256637573242 and accuracy 0.09444444626569748.\n",
      "Test batch 440 with loss 1.869485855102539 and accuracy 0.1944444477558136.\n",
      "Test batch 441 with loss 1.8604503870010376 and accuracy 0.22380954027175903.\n",
      "Test batch 442 with loss 1.8356183767318726 and accuracy 0.125.\n",
      "Test batch 443 with loss 1.9189563989639282 and accuracy 0.02380952425301075.\n",
      "Test batch 444 with loss 1.8776432275772095 and accuracy 0.1111111119389534.\n",
      "Test batch 445 with loss 1.9043052196502686 and accuracy 0.17222222685813904.\n",
      "Test batch 446 with loss 1.8242607116699219 and accuracy 0.25.\n",
      "Test batch 447 with loss 1.7916147708892822 and accuracy 0.1488095223903656.\n",
      "Test batch 448 with loss 1.8995749950408936 and accuracy 0.15000000596046448.\n",
      "Test batch 449 with loss 1.8632609844207764 and accuracy 0.1388888955116272.\n",
      "Test batch 450 with loss 1.7826963663101196 and accuracy 0.21388888359069824.\n",
      "Test batch 451 with loss 1.7882760763168335 and accuracy 0.3222222328186035.\n",
      "Test batch 452 with loss 1.8213777542114258 and accuracy 0.02380952425301075.\n",
      "Test batch 453 with loss 1.7959020137786865 and accuracy 0.39444446563720703.\n",
      "Test batch 454 with loss 1.9452497959136963 and accuracy 0.125.\n",
      "Test batch 455 with loss 1.8687938451766968 and accuracy 0.0555555559694767.\n",
      "Test batch 456 with loss 1.869532823562622 and accuracy 0.03333333507180214.\n",
      "Test batch 457 with loss 1.9302246570587158 and accuracy 0.1349206417798996.\n",
      "Test batch 458 with loss 1.9003181457519531 and accuracy 0.06666667014360428.\n",
      "Test batch 459 with loss 1.8417530059814453 and accuracy 0.17222222685813904.\n",
      "Test batch 460 with loss 1.7779165506362915 and accuracy 0.21388889849185944.\n",
      "Test batch 461 with loss 1.8839857578277588 and accuracy 0.0972222238779068.\n",
      "Test batch 462 with loss 1.693542242050171 and accuracy 0.3305555582046509.\n",
      "Test batch 463 with loss 1.934828519821167 and accuracy 0.0.\n",
      "Test batch 464 with loss 1.7462854385375977 and accuracy 0.2083333432674408.\n",
      "Test batch 465 with loss 1.8302932977676392 and accuracy 0.1111111119389534.\n",
      "Test batch 466 with loss 1.8049468994140625 and accuracy 0.1666666716337204.\n",
      "Test batch 467 with loss 1.8479492664337158 and accuracy 0.0833333358168602.\n",
      "Test batch 468 with loss 1.8183908462524414 and accuracy 0.23888888955116272.\n",
      "Test batch 469 with loss 1.8667373657226562 and accuracy 0.1111111119389534.\n",
      "Test batch 470 with loss 1.7999060153961182 and accuracy 0.0932539701461792.\n",
      "Test batch 471 with loss 1.7526906728744507 and accuracy 0.28333333134651184.\n",
      "Test batch 472 with loss 1.7917715311050415 and accuracy 0.1388888955116272.\n",
      "Test batch 473 with loss 1.8731200695037842 and accuracy 0.0416666679084301.\n",
      "Test batch 474 with loss 1.7546532154083252 and accuracy 0.2611111104488373.\n",
      "Test batch 475 with loss 1.8245680332183838 and accuracy 0.02777777798473835.\n",
      "Test batch 476 with loss 1.8493022918701172 and accuracy 0.0416666679084301.\n",
      "Test batch 477 with loss 1.711181640625 and accuracy 0.38055557012557983.\n",
      "Test batch 478 with loss 1.7932054996490479 and accuracy 0.25.\n",
      "Test batch 479 with loss 1.9060585498809814 and accuracy 0.2083333432674408.\n",
      "Test batch 480 with loss 1.757110834121704 and accuracy 0.09047619253396988.\n",
      "Test batch 481 with loss 1.8696658611297607 and accuracy 0.1805555522441864.\n",
      "Test batch 482 with loss 1.880369782447815 and accuracy 0.125.\n",
      "Test batch 483 with loss 1.8364900350570679 and accuracy 0.1527777761220932.\n",
      "Test batch 484 with loss 1.8021637201309204 and accuracy 0.1805555522441864.\n",
      "Test batch 485 with loss 1.7842121124267578 and accuracy 0.118055559694767.\n",
      "Test batch 486 with loss 1.8335365056991577 and accuracy 0.0833333358168602.\n",
      "Test batch 487 with loss 1.9264447689056396 and accuracy 0.125.\n",
      "Test batch 488 with loss 1.9028466939926147 and accuracy 0.2083333432674408.\n",
      "Test batch 489 with loss 1.7188348770141602 and accuracy 0.2527777850627899.\n",
      "Test batch 490 with loss 1.8702274560928345 and accuracy 0.08888889104127884.\n",
      "Test batch 491 with loss 1.8113845586776733 and accuracy 0.17777778208255768.\n",
      "Test batch 492 with loss 1.8623476028442383 and accuracy 0.07500000298023224.\n",
      "Test batch 493 with loss 1.8342268466949463 and accuracy 0.1944444477558136.\n",
      "Test batch 494 with loss 1.7538074254989624 and accuracy 0.2638888955116272.\n",
      "Test batch 495 with loss 1.7761741876602173 and accuracy 0.19166666269302368.\n",
      "Test batch 496 with loss 1.8595266342163086 and accuracy 0.15833333134651184.\n",
      "Test batch 497 with loss 1.802991271018982 and accuracy 0.23333333432674408.\n",
      "Test batch 498 with loss 1.8193244934082031 and accuracy 0.3809524178504944.\n",
      "Test batch 499 with loss 1.881183385848999 and accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 0 with loss 1.8573023080825806, accuracy 0.14166666567325592.\n",
      "Training epoch 5 batch 1 with loss 1.7037713527679443, accuracy 0.4833333194255829.\n",
      "Training epoch 5 batch 2 with loss 1.7537380456924438, accuracy 0.33888888359069824.\n",
      "Training epoch 5 batch 3 with loss 1.7194026708602905, accuracy 0.1805555671453476.\n",
      "Training epoch 5 batch 4 with loss 1.622262954711914, accuracy 0.4333333373069763.\n",
      "Training epoch 5 batch 5 with loss 1.7930269241333008, accuracy 0.35277777910232544.\n",
      "Training epoch 5 batch 6 with loss 1.774553894996643, accuracy 0.3819444477558136.\n",
      "Training epoch 5 batch 7 with loss 1.844996452331543, accuracy 0.18888889253139496.\n",
      "Training epoch 5 batch 8 with loss 1.834119439125061, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 9 with loss 1.7118451595306396, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 10 with loss 1.6593087911605835, accuracy 0.3194444477558136.\n",
      "Training epoch 5 batch 11 with loss 1.7919260263442993, accuracy 0.14166666567325592.\n",
      "Training epoch 5 batch 12 with loss 1.7116912603378296, accuracy 0.222222238779068.\n",
      "Training epoch 5 batch 13 with loss 1.807415246963501, accuracy 0.42777782678604126.\n",
      "Training epoch 5 batch 14 with loss 1.727181077003479, accuracy 0.4472222328186035.\n",
      "Training epoch 5 batch 15 with loss 1.8262945413589478, accuracy 0.07500000298023224.\n",
      "Training epoch 5 batch 16 with loss 1.7754243612289429, accuracy 0.14166666567325592.\n",
      "Training epoch 5 batch 17 with loss 1.6269458532333374, accuracy 0.2380952537059784.\n",
      "Training epoch 5 batch 18 with loss 1.8182846307754517, accuracy 0.11574074625968933.\n",
      "Training epoch 5 batch 19 with loss 1.789020299911499, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 20 with loss 1.7917554378509521, accuracy 0.190476194024086.\n",
      "Training epoch 5 batch 21 with loss 1.8093206882476807, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 22 with loss 1.7590255737304688, accuracy 0.2361111044883728.\n",
      "Training epoch 5 batch 23 with loss 1.6952928304672241, accuracy 0.25.\n",
      "Training epoch 5 batch 24 with loss 1.7533702850341797, accuracy 0.29722222685813904.\n",
      "Training epoch 5 batch 25 with loss 1.7536710500717163, accuracy 0.2083333283662796.\n",
      "Training epoch 5 batch 26 with loss 1.7663547992706299, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 27 with loss 1.7752269506454468, accuracy 0.24166667461395264.\n",
      "Training epoch 5 batch 28 with loss 1.7415969371795654, accuracy 0.3194444477558136.\n",
      "Training epoch 5 batch 29 with loss 1.7580429315567017, accuracy 0.18333333730697632.\n",
      "Training epoch 5 batch 30 with loss 1.7057584524154663, accuracy 0.4444444477558136.\n",
      "Training epoch 5 batch 31 with loss 1.7417619228363037, accuracy 0.16428571939468384.\n",
      "Training epoch 5 batch 32 with loss 1.7392890453338623, accuracy 0.24166667461395264.\n",
      "Training epoch 5 batch 33 with loss 1.6392358541488647, accuracy 0.3888888955116272.\n",
      "Training epoch 5 batch 34 with loss 1.7642543315887451, accuracy 0.125.\n",
      "Training epoch 5 batch 35 with loss 1.8407094478607178, accuracy 0.0.\n",
      "Training epoch 5 batch 36 with loss 1.6939146518707275, accuracy 0.2888889014720917.\n",
      "Training epoch 5 batch 37 with loss 1.7068408727645874, accuracy 0.1865079402923584.\n",
      "Training epoch 5 batch 38 with loss 1.55745530128479, accuracy 0.3722222149372101.\n",
      "Training epoch 5 batch 39 with loss 1.7805545330047607, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 40 with loss 1.8240972757339478, accuracy 0.25.\n",
      "Training epoch 5 batch 41 with loss 1.7342685461044312, accuracy 0.17777778208255768.\n",
      "Training epoch 5 batch 42 with loss 1.7649238109588623, accuracy 0.3888888955116272.\n",
      "Training epoch 5 batch 43 with loss 1.6187156438827515, accuracy 0.4583333432674408.\n",
      "Training epoch 5 batch 44 with loss 1.8079423904418945, accuracy 0.2361111044883728.\n",
      "Training epoch 5 batch 45 with loss 1.843157410621643, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 46 with loss 1.8051652908325195, accuracy 0.4055555462837219.\n",
      "Training epoch 5 batch 47 with loss 1.8261487483978271, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 48 with loss 1.7970085144042969, accuracy 0.3194444477558136.\n",
      "Training epoch 5 batch 49 with loss 1.7367178201675415, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 50 with loss 1.8430402278900146, accuracy 0.20000001788139343.\n",
      "Training epoch 5 batch 51 with loss 1.8303697109222412, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 52 with loss 1.8135169744491577, accuracy 0.19166666269302368.\n",
      "Training epoch 5 batch 53 with loss 1.7646287679672241, accuracy 0.1488095223903656.\n",
      "Training epoch 5 batch 54 with loss 1.7192531824111938, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 55 with loss 1.6810352802276611, accuracy 0.5277777910232544.\n",
      "Training epoch 5 batch 56 with loss 1.7234472036361694, accuracy 0.1805555671453476.\n",
      "Training epoch 5 batch 57 with loss 1.8189417123794556, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 58 with loss 1.7081372737884521, accuracy 0.1071428582072258.\n",
      "Training epoch 5 batch 59 with loss 1.7187944650650024, accuracy 0.35277777910232544.\n",
      "Training epoch 5 batch 60 with loss 1.691204309463501, accuracy 0.3333333432674408.\n",
      "Training epoch 5 batch 61 with loss 1.6200103759765625, accuracy 0.41111111640930176.\n",
      "Training epoch 5 batch 62 with loss 1.765514612197876, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 63 with loss 1.8041250705718994, accuracy 0.2638888955116272.\n",
      "Training epoch 5 batch 64 with loss 1.7839912176132202, accuracy 0.13809524476528168.\n",
      "Training epoch 5 batch 65 with loss 1.7759106159210205, accuracy 0.20555555820465088.\n",
      "Training epoch 5 batch 66 with loss 1.7511173486709595, accuracy 0.31388887763023376.\n",
      "Training epoch 5 batch 67 with loss 1.8956559896469116, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 68 with loss 1.7208044528961182, accuracy 0.25.\n",
      "Training epoch 5 batch 69 with loss 1.6872308254241943, accuracy 0.4027777910232544.\n",
      "Training epoch 5 batch 70 with loss 1.7194406986236572, accuracy 0.38055557012557983.\n",
      "Training epoch 5 batch 71 with loss 1.7767648696899414, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 72 with loss 1.715903878211975, accuracy 0.236111119389534.\n",
      "Training epoch 5 batch 73 with loss 1.8016488552093506, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 74 with loss 1.9257303476333618, accuracy 0.0.\n",
      "Training epoch 5 batch 75 with loss 1.7034142017364502, accuracy 0.25.\n",
      "Training epoch 5 batch 76 with loss 1.7640726566314697, accuracy 0.125.\n",
      "Training epoch 5 batch 77 with loss 1.768031120300293, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 78 with loss 1.6500753164291382, accuracy 0.25555557012557983.\n",
      "Training epoch 5 batch 79 with loss 1.7825422286987305, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 80 with loss 1.695686936378479, accuracy 0.23333334922790527.\n",
      "Training epoch 5 batch 81 with loss 1.8117644786834717, accuracy 0.21111111342906952.\n",
      "Training epoch 5 batch 82 with loss 1.8401817083358765, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 83 with loss 1.7516868114471436, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 84 with loss 1.82101571559906, accuracy 0.0416666679084301.\n",
      "Training epoch 5 batch 85 with loss 1.7828102111816406, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 86 with loss 1.7501941919326782, accuracy 0.4000000059604645.\n",
      "Training epoch 5 batch 87 with loss 1.6639505624771118, accuracy 0.30000001192092896.\n",
      "Training epoch 5 batch 88 with loss 1.8225691318511963, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 89 with loss 1.9018980264663696, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 90 with loss 1.7932376861572266, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 91 with loss 1.7593047618865967, accuracy 0.1349206417798996.\n",
      "Training epoch 5 batch 92 with loss 1.7487949132919312, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 93 with loss 1.8004558086395264, accuracy 0.2291666716337204.\n",
      "Training epoch 5 batch 94 with loss 1.8389999866485596, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 95 with loss 1.9352871179580688, accuracy 0.0.\n",
      "Training epoch 5 batch 96 with loss 1.8565019369125366, accuracy 0.02777777798473835.\n",
      "Training epoch 5 batch 97 with loss 1.7939903736114502, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 98 with loss 1.744836449623108, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 99 with loss 1.7219165563583374, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 100 with loss 1.714603066444397, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 101 with loss 1.75186026096344, accuracy 0.28703704476356506.\n",
      "Training epoch 5 batch 102 with loss 1.7557146549224854, accuracy 0.222222238779068.\n",
      "Training epoch 5 batch 103 with loss 1.7895663976669312, accuracy 0.19166666269302368.\n",
      "Training epoch 5 batch 104 with loss 1.722475290298462, accuracy 0.4583333134651184.\n",
      "Training epoch 5 batch 105 with loss 1.7191851139068604, accuracy 0.25555557012557983.\n",
      "Training epoch 5 batch 106 with loss 1.7305587530136108, accuracy 0.2876984179019928.\n",
      "Training epoch 5 batch 107 with loss 1.6179920434951782, accuracy 0.375.\n",
      "Training epoch 5 batch 108 with loss 1.8211829662322998, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 109 with loss 1.7426636219024658, accuracy 0.3444444537162781.\n",
      "Training epoch 5 batch 110 with loss 1.909497618675232, accuracy 0.1597222238779068.\n",
      "Training epoch 5 batch 111 with loss 1.8036420345306396, accuracy 0.0625.\n",
      "Training epoch 5 batch 112 with loss 1.7894275188446045, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 113 with loss 1.7149184942245483, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 114 with loss 1.751043677330017, accuracy 0.2638888955116272.\n",
      "Training epoch 5 batch 115 with loss 1.726265549659729, accuracy 0.15740740299224854.\n",
      "Training epoch 5 batch 116 with loss 1.71404230594635, accuracy 0.4166666865348816.\n",
      "Training epoch 5 batch 117 with loss 1.822950005531311, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 118 with loss 1.691605567932129, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 119 with loss 1.6766817569732666, accuracy 0.222222238779068.\n",
      "Training epoch 5 batch 120 with loss 1.7642968893051147, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 121 with loss 1.7617299556732178, accuracy 0.22777777910232544.\n",
      "Training epoch 5 batch 122 with loss 1.8135207891464233, accuracy 0.30000001192092896.\n",
      "Training epoch 5 batch 123 with loss 1.824327826499939, accuracy 0.03333333507180214.\n",
      "Training epoch 5 batch 124 with loss 1.812420129776001, accuracy 0.20714285969734192.\n",
      "Training epoch 5 batch 125 with loss 1.8475008010864258, accuracy 0.22777777910232544.\n",
      "Training epoch 5 batch 126 with loss 1.764423131942749, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 127 with loss 1.8503459692001343, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 128 with loss 1.8717851638793945, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 129 with loss 1.716040849685669, accuracy 0.23333334922790527.\n",
      "Training epoch 5 batch 130 with loss 1.7429287433624268, accuracy 0.3888888955116272.\n",
      "Training epoch 5 batch 131 with loss 1.8290923833847046, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 132 with loss 1.8133203983306885, accuracy 0.1210317462682724.\n",
      "Training epoch 5 batch 133 with loss 1.7989238500595093, accuracy 0.111111119389534.\n",
      "Training epoch 5 batch 134 with loss 1.6935911178588867, accuracy 0.4166666865348816.\n",
      "Training epoch 5 batch 135 with loss 1.7874867916107178, accuracy 0.02777777798473835.\n",
      "Training epoch 5 batch 136 with loss 1.7706066370010376, accuracy 0.25.\n",
      "Training epoch 5 batch 137 with loss 1.8120425939559937, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 138 with loss 1.6484344005584717, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 139 with loss 1.6638984680175781, accuracy 0.3583333194255829.\n",
      "Training epoch 5 batch 140 with loss 1.8317161798477173, accuracy 0.02777777798473835.\n",
      "Training epoch 5 batch 141 with loss 1.7930259704589844, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 142 with loss 1.6965959072113037, accuracy 0.4333333671092987.\n",
      "Training epoch 5 batch 143 with loss 1.6559356451034546, accuracy 0.4464285671710968.\n",
      "Training epoch 5 batch 144 with loss 1.8313324451446533, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 145 with loss 1.8006795644760132, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 146 with loss 1.7516508102416992, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 147 with loss 1.715762734413147, accuracy 0.1825396865606308.\n",
      "Training epoch 5 batch 148 with loss 1.8105064630508423, accuracy 0.07500000298023224.\n",
      "Training epoch 5 batch 149 with loss 1.7870378494262695, accuracy 0.2361111044883728.\n",
      "Training epoch 5 batch 150 with loss 1.8014112710952759, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 151 with loss 1.821557641029358, accuracy 0.25.\n",
      "Training epoch 5 batch 152 with loss 1.7888826131820679, accuracy 0.1547619104385376.\n",
      "Training epoch 5 batch 153 with loss 1.7992887496948242, accuracy 0.15555556118488312.\n",
      "Training epoch 5 batch 154 with loss 1.750284194946289, accuracy 0.2888889014720917.\n",
      "Training epoch 5 batch 155 with loss 1.8012502193450928, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 156 with loss 1.8331279754638672, accuracy 0.25.\n",
      "Training epoch 5 batch 157 with loss 1.7592134475708008, accuracy 0.4333333373069763.\n",
      "Training epoch 5 batch 158 with loss 1.8364406824111938, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 159 with loss 1.7290970087051392, accuracy 0.31388890743255615.\n",
      "Training epoch 5 batch 160 with loss 1.8063865900039673, accuracy 0.0694444477558136.\n",
      "Training epoch 5 batch 161 with loss 1.68573796749115, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 162 with loss 1.7179769277572632, accuracy 0.23333334922790527.\n",
      "Training epoch 5 batch 163 with loss 1.7473373413085938, accuracy 0.2361111044883728.\n",
      "Training epoch 5 batch 164 with loss 1.7222458124160767, accuracy 0.4194444417953491.\n",
      "Training epoch 5 batch 165 with loss 1.8546768426895142, accuracy 0.0793650820851326.\n",
      "Training epoch 5 batch 166 with loss 1.8582630157470703, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 167 with loss 1.6043701171875, accuracy 0.4999999701976776.\n",
      "Training epoch 5 batch 168 with loss 1.7203786373138428, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 169 with loss 1.7642822265625, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 170 with loss 1.7807471752166748, accuracy 0.1071428582072258.\n",
      "Training epoch 5 batch 171 with loss 1.764624834060669, accuracy 0.25555557012557983.\n",
      "Training epoch 5 batch 172 with loss 1.6803274154663086, accuracy 0.17500001192092896.\n",
      "Training epoch 5 batch 173 with loss 1.812171220779419, accuracy 0.32500001788139343.\n",
      "Training epoch 5 batch 174 with loss 1.7300220727920532, accuracy 0.375.\n",
      "Training epoch 5 batch 175 with loss 1.7143980264663696, accuracy 0.3611111044883728.\n",
      "Training epoch 5 batch 176 with loss 1.7777061462402344, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 177 with loss 1.6931995153427124, accuracy 0.3611111044883728.\n",
      "Training epoch 5 batch 178 with loss 1.8218199014663696, accuracy 0.0.\n",
      "Training epoch 5 batch 179 with loss 1.7282426357269287, accuracy 0.3583333492279053.\n",
      "Training epoch 5 batch 180 with loss 1.7481679916381836, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 181 with loss 1.8036714792251587, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 182 with loss 1.7310025691986084, accuracy 0.1587301641702652.\n",
      "Training epoch 5 batch 183 with loss 1.7280794382095337, accuracy 0.36666664481163025.\n",
      "Training epoch 5 batch 184 with loss 1.8081471920013428, accuracy 0.19166667759418488.\n",
      "Training epoch 5 batch 185 with loss 1.811737060546875, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 186 with loss 1.855677843093872, accuracy 0.1488095223903656.\n",
      "Training epoch 5 batch 187 with loss 1.8892520666122437, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 188 with loss 1.77912175655365, accuracy 0.0793650820851326.\n",
      "Training epoch 5 batch 189 with loss 1.8004238605499268, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 190 with loss 1.8347221612930298, accuracy 0.02380952425301075.\n",
      "Training epoch 5 batch 191 with loss 1.7543659210205078, accuracy 0.2916666567325592.\n",
      "Training epoch 5 batch 192 with loss 1.7736365795135498, accuracy 0.32499998807907104.\n",
      "Training epoch 5 batch 193 with loss 1.695543885231018, accuracy 0.29722222685813904.\n",
      "Training epoch 5 batch 194 with loss 1.7241275310516357, accuracy 0.125.\n",
      "Training epoch 5 batch 195 with loss 1.7019636631011963, accuracy 0.2638888955116272.\n",
      "Training epoch 5 batch 196 with loss 1.7750046253204346, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 197 with loss 1.7701267004013062, accuracy 0.4791666865348816.\n",
      "Training epoch 5 batch 198 with loss 1.795210838317871, accuracy 0.14603175222873688.\n",
      "Training epoch 5 batch 199 with loss 1.8049545288085938, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 200 with loss 1.8328685760498047, accuracy 0.1865079402923584.\n",
      "Training epoch 5 batch 201 with loss 1.7771533727645874, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 202 with loss 1.7739368677139282, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 203 with loss 1.591875672340393, accuracy 0.4222222566604614.\n",
      "Training epoch 5 batch 204 with loss 1.6759284734725952, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 205 with loss 1.7770246267318726, accuracy 0.2519841194152832.\n",
      "Training epoch 5 batch 206 with loss 1.7703752517700195, accuracy 0.18888890743255615.\n",
      "Training epoch 5 batch 207 with loss 1.7736564874649048, accuracy 0.15833333134651184.\n",
      "Training epoch 5 batch 208 with loss 1.731955885887146, accuracy 0.29722222685813904.\n",
      "Training epoch 5 batch 209 with loss 1.7091286182403564, accuracy 0.2916666865348816.\n",
      "Training epoch 5 batch 210 with loss 1.7626228332519531, accuracy 0.125.\n",
      "Training epoch 5 batch 211 with loss 1.8473145961761475, accuracy 0.1865079402923584.\n",
      "Training epoch 5 batch 212 with loss 1.7491382360458374, accuracy 0.19166666269302368.\n",
      "Training epoch 5 batch 213 with loss 1.8850276470184326, accuracy 0.2142857164144516.\n",
      "Training epoch 5 batch 214 with loss 1.8321281671524048, accuracy 0.25555557012557983.\n",
      "Training epoch 5 batch 215 with loss 1.7574996948242188, accuracy 0.24166667461395264.\n",
      "Training epoch 5 batch 216 with loss 1.8196052312850952, accuracy 0.18611110746860504.\n",
      "Training epoch 5 batch 217 with loss 1.8452818393707275, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 218 with loss 1.6943769454956055, accuracy 0.2916666865348816.\n",
      "Training epoch 5 batch 219 with loss 1.8483749628067017, accuracy 0.2750000059604645.\n",
      "Training epoch 5 batch 220 with loss 1.8104197978973389, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 221 with loss 1.7059650421142578, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 222 with loss 1.778047800064087, accuracy 0.125.\n",
      "Training epoch 5 batch 223 with loss 1.815538763999939, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 224 with loss 1.7988808155059814, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 225 with loss 1.8154842853546143, accuracy 0.22777777910232544.\n",
      "Training epoch 5 batch 226 with loss 1.820718765258789, accuracy 0.2916666567325592.\n",
      "Training epoch 5 batch 227 with loss 1.816306710243225, accuracy 0.23333334922790527.\n",
      "Training epoch 5 batch 228 with loss 1.7543179988861084, accuracy 0.3083333373069763.\n",
      "Training epoch 5 batch 229 with loss 1.7943294048309326, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 230 with loss 1.9528049230575562, accuracy 0.1130952388048172.\n",
      "Training epoch 5 batch 231 with loss 1.7234214544296265, accuracy 0.11269842088222504.\n",
      "Training epoch 5 batch 232 with loss 1.8392579555511475, accuracy 0.1527777910232544.\n",
      "Training epoch 5 batch 233 with loss 1.844599962234497, accuracy 0.0.\n",
      "Training epoch 5 batch 234 with loss 1.8429787158966064, accuracy 0.347222238779068.\n",
      "Training epoch 5 batch 235 with loss 1.7087936401367188, accuracy 0.2750000059604645.\n",
      "Training epoch 5 batch 236 with loss 1.7607629299163818, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 237 with loss 1.8688284158706665, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 238 with loss 1.8346904516220093, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 239 with loss 1.6337379217147827, accuracy 0.2547619044780731.\n",
      "Training epoch 5 batch 240 with loss 1.6984832286834717, accuracy 0.4007936716079712.\n",
      "Training epoch 5 batch 241 with loss 1.7529261112213135, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 242 with loss 1.731802225112915, accuracy 0.23888888955116272.\n",
      "Training epoch 5 batch 243 with loss 1.7970107793807983, accuracy 0.3333333432674408.\n",
      "Training epoch 5 batch 244 with loss 1.690298080444336, accuracy 0.24722221493721008.\n",
      "Training epoch 5 batch 245 with loss 1.711289405822754, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 246 with loss 1.8045566082000732, accuracy 0.20000000298023224.\n",
      "Training epoch 5 batch 247 with loss 1.7175846099853516, accuracy 0.4138889014720917.\n",
      "Training epoch 5 batch 248 with loss 1.9176414012908936, accuracy 0.13611111044883728.\n",
      "Training epoch 5 batch 249 with loss 1.7952182292938232, accuracy 0.2361111044883728.\n",
      "Training epoch 5 batch 250 with loss 1.8709862232208252, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 251 with loss 1.6776797771453857, accuracy 0.5055555701255798.\n",
      "Training epoch 5 batch 252 with loss 1.6579391956329346, accuracy 0.3611111342906952.\n",
      "Training epoch 5 batch 253 with loss 1.7268892526626587, accuracy 0.3047619163990021.\n",
      "Training epoch 5 batch 254 with loss 1.6817868947982788, accuracy 0.3392857313156128.\n",
      "Training epoch 5 batch 255 with loss 1.6791152954101562, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 256 with loss 1.7936351299285889, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 257 with loss 1.740783929824829, accuracy 0.2083333283662796.\n",
      "Training epoch 5 batch 258 with loss 1.7950384616851807, accuracy 0.25.\n",
      "Training epoch 5 batch 259 with loss 1.7570695877075195, accuracy 0.28333333134651184.\n",
      "Training epoch 5 batch 260 with loss 1.7336795330047607, accuracy 0.20555555820465088.\n",
      "Training epoch 5 batch 261 with loss 1.6782581806182861, accuracy 0.25.\n",
      "Training epoch 5 batch 262 with loss 1.7396585941314697, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 263 with loss 1.7054874897003174, accuracy 0.125.\n",
      "Training epoch 5 batch 264 with loss 1.7498830556869507, accuracy 0.3166666626930237.\n",
      "Training epoch 5 batch 265 with loss 1.818057656288147, accuracy 0.24722222983837128.\n",
      "Training epoch 5 batch 266 with loss 1.674699068069458, accuracy 0.4555555582046509.\n",
      "Training epoch 5 batch 267 with loss 1.7859842777252197, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 268 with loss 1.7887804508209229, accuracy 0.23888888955116272.\n",
      "Training epoch 5 batch 269 with loss 1.7092500925064087, accuracy 0.2361111044883728.\n",
      "Training epoch 5 batch 270 with loss 1.6280351877212524, accuracy 0.4833333492279053.\n",
      "Training epoch 5 batch 271 with loss 1.8589973449707031, accuracy 0.0.\n",
      "Training epoch 5 batch 272 with loss 1.7759004831314087, accuracy 0.22777777910232544.\n",
      "Training epoch 5 batch 273 with loss 1.8123897314071655, accuracy 0.0476190485060215.\n",
      "Training epoch 5 batch 274 with loss 1.760913610458374, accuracy 0.25555557012557983.\n",
      "Training epoch 5 batch 275 with loss 1.749346375465393, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 276 with loss 1.8314955234527588, accuracy 0.2361111044883728.\n",
      "Training epoch 5 batch 277 with loss 1.750276803970337, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 278 with loss 1.7632722854614258, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 279 with loss 1.7345612049102783, accuracy 0.3888888955116272.\n",
      "Training epoch 5 batch 280 with loss 1.7436809539794922, accuracy 0.125.\n",
      "Training epoch 5 batch 281 with loss 1.7863101959228516, accuracy 0.0476190485060215.\n",
      "Training epoch 5 batch 282 with loss 1.7529674768447876, accuracy 0.31111112236976624.\n",
      "Training epoch 5 batch 283 with loss 1.7289257049560547, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 284 with loss 1.7294952869415283, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 285 with loss 1.7788387537002563, accuracy 0.28333333134651184.\n",
      "Training epoch 5 batch 286 with loss 1.8426860570907593, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 287 with loss 1.7542368173599243, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 288 with loss 1.905411958694458, accuracy 0.25.\n",
      "Training epoch 5 batch 289 with loss 1.7450244426727295, accuracy 0.20333333313465118.\n",
      "Training epoch 5 batch 290 with loss 1.796917200088501, accuracy 0.25.\n",
      "Training epoch 5 batch 291 with loss 1.7086461782455444, accuracy 0.26944443583488464.\n",
      "Training epoch 5 batch 292 with loss 1.7758753299713135, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 293 with loss 1.7365888357162476, accuracy 0.28333333134651184.\n",
      "Training epoch 5 batch 294 with loss 1.7786811590194702, accuracy 0.1527777910232544.\n",
      "Training epoch 5 batch 295 with loss 1.8582769632339478, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 296 with loss 1.707390546798706, accuracy 0.08888889104127884.\n",
      "Training epoch 5 batch 297 with loss 1.7779613733291626, accuracy 0.20000000298023224.\n",
      "Training epoch 5 batch 298 with loss 1.74141526222229, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 299 with loss 1.801749587059021, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 300 with loss 1.7702853679656982, accuracy 0.17592594027519226.\n",
      "Training epoch 5 batch 301 with loss 1.8508812189102173, accuracy 0.2321428656578064.\n",
      "Training epoch 5 batch 302 with loss 1.7172935009002686, accuracy 0.3999999761581421.\n",
      "Training epoch 5 batch 303 with loss 1.768904685974121, accuracy 0.19166667759418488.\n",
      "Training epoch 5 batch 304 with loss 1.6985641717910767, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 305 with loss 1.7923167943954468, accuracy 0.19166667759418488.\n",
      "Training epoch 5 batch 306 with loss 1.7560535669326782, accuracy 0.23333333432674408.\n",
      "Training epoch 5 batch 307 with loss 1.7917909622192383, accuracy 0.31666669249534607.\n",
      "Training epoch 5 batch 308 with loss 1.7576566934585571, accuracy 0.17500001192092896.\n",
      "Training epoch 5 batch 309 with loss 1.751133680343628, accuracy 0.4047619104385376.\n",
      "Training epoch 5 batch 310 with loss 1.7441442012786865, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 311 with loss 1.7570663690567017, accuracy 0.18809524178504944.\n",
      "Training epoch 5 batch 312 with loss 1.7862447500228882, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 313 with loss 1.7299625873565674, accuracy 0.23333334922790527.\n",
      "Training epoch 5 batch 314 with loss 1.7163982391357422, accuracy 0.28333333134651184.\n",
      "Training epoch 5 batch 315 with loss 1.8309729099273682, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 316 with loss 1.851976752281189, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 317 with loss 1.7850463390350342, accuracy 0.24444444477558136.\n",
      "Training epoch 5 batch 318 with loss 1.756284475326538, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 319 with loss 1.8174511194229126, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 320 with loss 1.843427300453186, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 321 with loss 1.9062128067016602, accuracy 0.05416666716337204.\n",
      "Training epoch 5 batch 322 with loss 1.754485845565796, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 323 with loss 1.6570402383804321, accuracy 0.28611111640930176.\n",
      "Training epoch 5 batch 324 with loss 1.6735719442367554, accuracy 0.2944444417953491.\n",
      "Training epoch 5 batch 325 with loss 1.742558240890503, accuracy 0.3472222089767456.\n",
      "Training epoch 5 batch 326 with loss 1.6885764598846436, accuracy 0.347222238779068.\n",
      "Training epoch 5 batch 327 with loss 1.7822061777114868, accuracy 0.2638888955116272.\n",
      "Training epoch 5 batch 328 with loss 1.7461150884628296, accuracy 0.1875.\n",
      "Training epoch 5 batch 329 with loss 1.7969547510147095, accuracy 0.1805555522441864.\n",
      "Training epoch 5 batch 330 with loss 1.722834825515747, accuracy 0.3888888955116272.\n",
      "Training epoch 5 batch 331 with loss 1.7003405094146729, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 332 with loss 1.6727893352508545, accuracy 0.347222238779068.\n",
      "Training epoch 5 batch 333 with loss 1.7682651281356812, accuracy 0.10833333432674408.\n",
      "Training epoch 5 batch 334 with loss 1.7673022747039795, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 335 with loss 1.7777907848358154, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 336 with loss 1.7910850048065186, accuracy 0.30158731341362.\n",
      "Training epoch 5 batch 337 with loss 1.8814691305160522, accuracy 0.15833333134651184.\n",
      "Training epoch 5 batch 338 with loss 1.5989220142364502, accuracy 0.5000000596046448.\n",
      "Training epoch 5 batch 339 with loss 1.7801412343978882, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 340 with loss 1.76689875125885, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 341 with loss 1.7231537103652954, accuracy 0.3154762089252472.\n",
      "Training epoch 5 batch 342 with loss 1.6768887042999268, accuracy 0.3888888955116272.\n",
      "Training epoch 5 batch 343 with loss 1.7808711528778076, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 344 with loss 1.8389263153076172, accuracy 0.1269841343164444.\n",
      "Training epoch 5 batch 345 with loss 1.8113206624984741, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 346 with loss 1.7760881185531616, accuracy 0.31666669249534607.\n",
      "Training epoch 5 batch 347 with loss 1.7567085027694702, accuracy 0.22777777910232544.\n",
      "Training epoch 5 batch 348 with loss 1.7272199392318726, accuracy 0.39722222089767456.\n",
      "Training epoch 5 batch 349 with loss 1.7200244665145874, accuracy 0.20000000298023224.\n",
      "Training epoch 5 batch 350 with loss 1.8448352813720703, accuracy 0.31666669249534607.\n",
      "Training epoch 5 batch 351 with loss 1.7795566320419312, accuracy 0.1547619104385376.\n",
      "Training epoch 5 batch 352 with loss 1.725389838218689, accuracy 0.30158731341362.\n",
      "Training epoch 5 batch 353 with loss 1.8354078531265259, accuracy 0.15833334624767303.\n",
      "Training epoch 5 batch 354 with loss 1.8463068008422852, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 355 with loss 1.8278573751449585, accuracy 0.02777777798473835.\n",
      "Training epoch 5 batch 356 with loss 1.6007591485977173, accuracy 0.38055557012557983.\n",
      "Training epoch 5 batch 357 with loss 1.7727292776107788, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 358 with loss 1.7413947582244873, accuracy 0.2849206328392029.\n",
      "Training epoch 5 batch 359 with loss 1.7334034442901611, accuracy 0.3611111342906952.\n",
      "Training epoch 5 batch 360 with loss 1.7840074300765991, accuracy 0.15833333134651184.\n",
      "Training epoch 5 batch 361 with loss 1.8198909759521484, accuracy 0.2916666567325592.\n",
      "Training epoch 5 batch 362 with loss 1.8377940654754639, accuracy 0.1071428582072258.\n",
      "Training epoch 5 batch 363 with loss 1.8714933395385742, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 364 with loss 1.804938554763794, accuracy 0.12777778506278992.\n",
      "Training epoch 5 batch 365 with loss 1.7702009677886963, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 366 with loss 1.7540639638900757, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 367 with loss 1.8556115627288818, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 368 with loss 1.7305529117584229, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 369 with loss 1.8129371404647827, accuracy 0.17592592537403107.\n",
      "Training epoch 5 batch 370 with loss 1.7001953125, accuracy 0.4583333730697632.\n",
      "Training epoch 5 batch 371 with loss 1.7688525915145874, accuracy 0.22777777910232544.\n",
      "Training epoch 5 batch 372 with loss 1.844395399093628, accuracy 0.02380952425301075.\n",
      "Training epoch 5 batch 373 with loss 1.8807628154754639, accuracy 0.0694444477558136.\n",
      "Training epoch 5 batch 374 with loss 1.7881981134414673, accuracy 0.2805555462837219.\n",
      "Training epoch 5 batch 375 with loss 1.806415319442749, accuracy 0.24722222983837128.\n",
      "Training epoch 5 batch 376 with loss 1.6206239461898804, accuracy 0.3333333432674408.\n",
      "Training epoch 5 batch 377 with loss 1.743656873703003, accuracy 0.24166667461395264.\n",
      "Training epoch 5 batch 378 with loss 1.6542555093765259, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 379 with loss 1.8719940185546875, accuracy 0.08888889104127884.\n",
      "Training epoch 5 batch 380 with loss 1.775829553604126, accuracy 0.3611111342906952.\n",
      "Training epoch 5 batch 381 with loss 1.7715985774993896, accuracy 0.1031746044754982.\n",
      "Training epoch 5 batch 382 with loss 1.8045507669448853, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 383 with loss 1.8414243459701538, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 384 with loss 1.7836672067642212, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 385 with loss 1.8108761310577393, accuracy 0.21111111342906952.\n",
      "Training epoch 5 batch 386 with loss 1.7326123714447021, accuracy 0.3611111342906952.\n",
      "Training epoch 5 batch 387 with loss 1.7901817560195923, accuracy 0.15833333134651184.\n",
      "Training epoch 5 batch 388 with loss 1.7239439487457275, accuracy 0.16388890147209167.\n",
      "Training epoch 5 batch 389 with loss 1.8748153448104858, accuracy 0.10476191341876984.\n",
      "Training epoch 5 batch 390 with loss 1.7064746618270874, accuracy 0.39722225069999695.\n",
      "Training epoch 5 batch 391 with loss 1.7874460220336914, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 392 with loss 1.8241599798202515, accuracy 0.17500001192092896.\n",
      "Training epoch 5 batch 393 with loss 1.8269996643066406, accuracy 0.25.\n",
      "Training epoch 5 batch 394 with loss 1.9223846197128296, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 395 with loss 1.8436040878295898, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 396 with loss 1.8734601736068726, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 397 with loss 1.7358477115631104, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 398 with loss 1.7334825992584229, accuracy 0.17777778208255768.\n",
      "Training epoch 5 batch 399 with loss 1.752123236656189, accuracy 0.26944443583488464.\n",
      "Training epoch 5 batch 400 with loss 1.7869428396224976, accuracy 0.3194444477558136.\n",
      "Training epoch 5 batch 401 with loss 1.8989330530166626, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 402 with loss 1.7207237482070923, accuracy 0.3392857015132904.\n",
      "Training epoch 5 batch 403 with loss 1.6661351919174194, accuracy 0.33888888359069824.\n",
      "Training epoch 5 batch 404 with loss 1.7197582721710205, accuracy 0.25.\n",
      "Training epoch 5 batch 405 with loss 1.7234039306640625, accuracy 0.2698412835597992.\n",
      "Training epoch 5 batch 406 with loss 1.8131828308105469, accuracy 0.18333333730697632.\n",
      "Training epoch 5 batch 407 with loss 1.7455726861953735, accuracy 0.40000003576278687.\n",
      "Training epoch 5 batch 408 with loss 1.7863410711288452, accuracy 0.1805555671453476.\n",
      "Training epoch 5 batch 409 with loss 1.6120589971542358, accuracy 0.4642857313156128.\n",
      "Training epoch 5 batch 410 with loss 1.7950356006622314, accuracy 0.3333333432674408.\n",
      "Training epoch 5 batch 411 with loss 1.6648762226104736, accuracy 0.24722221493721008.\n",
      "Training epoch 5 batch 412 with loss 1.8150384426116943, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 413 with loss 1.6311569213867188, accuracy 0.4194444715976715.\n",
      "Training epoch 5 batch 414 with loss 1.8476626873016357, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 415 with loss 1.7205865383148193, accuracy 0.36666667461395264.\n",
      "Training epoch 5 batch 416 with loss 1.811560869216919, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 417 with loss 1.8285648822784424, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 418 with loss 1.8720645904541016, accuracy 0.1805555671453476.\n",
      "Training epoch 5 batch 419 with loss 1.7157032489776611, accuracy 0.3305555582046509.\n",
      "Training epoch 5 batch 420 with loss 1.8999128341674805, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 421 with loss 1.865129828453064, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 422 with loss 1.6813256740570068, accuracy 0.335317462682724.\n",
      "Training epoch 5 batch 423 with loss 1.7621139287948608, accuracy 0.125.\n",
      "Training epoch 5 batch 424 with loss 1.7512096166610718, accuracy 0.13055555522441864.\n",
      "Training epoch 5 batch 425 with loss 1.8162257671356201, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 426 with loss 1.761964201927185, accuracy 0.3194444477558136.\n",
      "Training epoch 5 batch 427 with loss 1.7579759359359741, accuracy 0.222222238779068.\n",
      "Training epoch 5 batch 428 with loss 1.8396518230438232, accuracy 0.07500000298023224.\n",
      "Training epoch 5 batch 429 with loss 1.7822151184082031, accuracy 0.2750000059604645.\n",
      "Training epoch 5 batch 430 with loss 1.7454391717910767, accuracy 0.3333333432674408.\n",
      "Training epoch 5 batch 431 with loss 1.7761846780776978, accuracy 0.23888888955116272.\n",
      "Training epoch 5 batch 432 with loss 1.8620290756225586, accuracy 0.125.\n",
      "Training epoch 5 batch 433 with loss 1.7834465503692627, accuracy 0.2420634925365448.\n",
      "Training epoch 5 batch 434 with loss 1.723292589187622, accuracy 0.45000001788139343.\n",
      "Training epoch 5 batch 435 with loss 1.7992584705352783, accuracy 0.3541666865348816.\n",
      "Training epoch 5 batch 436 with loss 1.7798198461532593, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 437 with loss 1.6700910329818726, accuracy 0.3571428656578064.\n",
      "Training epoch 5 batch 438 with loss 1.795243263244629, accuracy 0.18000000715255737.\n",
      "Training epoch 5 batch 439 with loss 1.8076508045196533, accuracy 0.24166667461395264.\n",
      "Training epoch 5 batch 440 with loss 1.702516794204712, accuracy 0.2638888955116272.\n",
      "Training epoch 5 batch 441 with loss 1.8756065368652344, accuracy 0.2281745970249176.\n",
      "Training epoch 5 batch 442 with loss 1.7222200632095337, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 443 with loss 1.7279068231582642, accuracy 0.23888888955116272.\n",
      "Training epoch 5 batch 444 with loss 1.6994556188583374, accuracy 0.3583333492279053.\n",
      "Training epoch 5 batch 445 with loss 1.692356824874878, accuracy 0.28333333134651184.\n",
      "Training epoch 5 batch 446 with loss 1.7571914196014404, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 447 with loss 1.8776531219482422, accuracy 0.07500000298023224.\n",
      "Training epoch 5 batch 448 with loss 1.759800910949707, accuracy 0.25.\n",
      "Training epoch 5 batch 449 with loss 1.7116680145263672, accuracy 0.42777779698371887.\n",
      "Training epoch 5 batch 450 with loss 1.7833499908447266, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 451 with loss 1.683666467666626, accuracy 0.4880952537059784.\n",
      "Training epoch 5 batch 452 with loss 1.7711654901504517, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 453 with loss 1.8085359334945679, accuracy 0.095238097012043.\n",
      "Training epoch 5 batch 454 with loss 1.8340599536895752, accuracy 0.2083333283662796.\n",
      "Training epoch 5 batch 455 with loss 1.827405333518982, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 456 with loss 1.7145683765411377, accuracy 0.3055555820465088.\n",
      "Training epoch 5 batch 457 with loss 1.7520803213119507, accuracy 0.236111119389534.\n",
      "Training epoch 5 batch 458 with loss 1.8137331008911133, accuracy 0.3253968358039856.\n",
      "Training epoch 5 batch 459 with loss 1.8633105754852295, accuracy 0.125.\n",
      "Training epoch 5 batch 460 with loss 1.8644320964813232, accuracy 0.1547619104385376.\n",
      "Training epoch 5 batch 461 with loss 1.7508389949798584, accuracy 0.22499999403953552.\n",
      "Training epoch 5 batch 462 with loss 1.7045316696166992, accuracy 0.2611111104488373.\n",
      "Training epoch 5 batch 463 with loss 1.8201675415039062, accuracy 0.25925928354263306.\n",
      "Training epoch 5 batch 464 with loss 1.823944330215454, accuracy 0.23333334922790527.\n",
      "Training epoch 5 batch 465 with loss 1.633583664894104, accuracy 0.43888890743255615.\n",
      "Training epoch 5 batch 466 with loss 1.6829630136489868, accuracy 0.46666669845581055.\n",
      "Training epoch 5 batch 467 with loss 1.7846533060073853, accuracy 0.10000000894069672.\n",
      "Training epoch 5 batch 468 with loss 1.6950409412384033, accuracy 0.3333333730697632.\n",
      "Training epoch 5 batch 469 with loss 1.7412163019180298, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 470 with loss 1.7870893478393555, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 471 with loss 1.6045913696289062, accuracy 0.3777777850627899.\n",
      "Training epoch 5 batch 472 with loss 1.9002668857574463, accuracy 0.02777777798473835.\n",
      "Training epoch 5 batch 473 with loss 1.7680063247680664, accuracy 0.24166667461395264.\n",
      "Training epoch 5 batch 474 with loss 1.6902316808700562, accuracy 0.41111111640930176.\n",
      "Training epoch 5 batch 475 with loss 1.8792345523834229, accuracy 0.22777777910232544.\n",
      "Training epoch 5 batch 476 with loss 1.7881425619125366, accuracy 0.17777778208255768.\n",
      "Training epoch 5 batch 477 with loss 1.7146743535995483, accuracy 0.20000000298023224.\n",
      "Training epoch 5 batch 478 with loss 1.7315477132797241, accuracy 0.130952388048172.\n",
      "Training epoch 5 batch 479 with loss 1.761055588722229, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 480 with loss 1.875694990158081, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 481 with loss 1.7739312648773193, accuracy 0.25555557012557983.\n",
      "Training epoch 5 batch 482 with loss 1.726036787033081, accuracy 0.2805555462837219.\n",
      "Training epoch 5 batch 483 with loss 1.7792552709579468, accuracy 0.1626984179019928.\n",
      "Training epoch 5 batch 484 with loss 1.7400734424591064, accuracy 0.2916666865348816.\n",
      "Training epoch 5 batch 485 with loss 1.6646817922592163, accuracy 0.190476194024086.\n",
      "Training epoch 5 batch 486 with loss 1.7749977111816406, accuracy 0.125.\n",
      "Training epoch 5 batch 487 with loss 1.6858173608779907, accuracy 0.2638888955116272.\n",
      "Training epoch 5 batch 488 with loss 1.889517068862915, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 489 with loss 1.8977254629135132, accuracy 0.10833333432674408.\n",
      "Training epoch 5 batch 490 with loss 1.7244561910629272, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 491 with loss 1.8003603219985962, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 492 with loss 1.8266704082489014, accuracy 0.125.\n",
      "Training epoch 5 batch 493 with loss 1.8550697565078735, accuracy 0.14166666567325592.\n",
      "Training epoch 5 batch 494 with loss 1.7497644424438477, accuracy 0.125.\n",
      "Training epoch 5 batch 495 with loss 1.8087689876556396, accuracy 0.22777777910232544.\n",
      "Training epoch 5 batch 496 with loss 1.807037115097046, accuracy 0.13055555522441864.\n",
      "Training epoch 5 batch 497 with loss 1.7161909341812134, accuracy 0.28333333134651184.\n",
      "Training epoch 5 batch 498 with loss 1.680720567703247, accuracy 0.3333333432674408.\n",
      "Training epoch 5 batch 499 with loss 1.7806484699249268, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 500 with loss 1.931009292602539, accuracy 0.02777777798473835.\n",
      "Training epoch 5 batch 501 with loss 1.8796939849853516, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 502 with loss 1.6812347173690796, accuracy 0.37916669249534607.\n",
      "Training epoch 5 batch 503 with loss 1.7990268468856812, accuracy 0.20416666567325592.\n",
      "Training epoch 5 batch 504 with loss 1.8059005737304688, accuracy 0.07500000298023224.\n",
      "Training epoch 5 batch 505 with loss 1.9157917499542236, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 506 with loss 1.7778072357177734, accuracy 0.2916666865348816.\n",
      "Training epoch 5 batch 507 with loss 1.70867121219635, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 508 with loss 1.730354905128479, accuracy 0.28333333134651184.\n",
      "Training epoch 5 batch 509 with loss 1.7463678121566772, accuracy 0.32499998807907104.\n",
      "Training epoch 5 batch 510 with loss 1.7736749649047852, accuracy 0.10833333432674408.\n",
      "Training epoch 5 batch 511 with loss 1.8099254369735718, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 512 with loss 1.908565878868103, accuracy 0.0.\n",
      "Training epoch 5 batch 513 with loss 1.7457071542739868, accuracy 0.26944446563720703.\n",
      "Training epoch 5 batch 514 with loss 1.7637819051742554, accuracy 0.24166667461395264.\n",
      "Training epoch 5 batch 515 with loss 1.7184404134750366, accuracy 0.2750000059604645.\n",
      "Training epoch 5 batch 516 with loss 1.7501327991485596, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 517 with loss 1.77213454246521, accuracy 0.2291666716337204.\n",
      "Training epoch 5 batch 518 with loss 1.6957048177719116, accuracy 0.2750000059604645.\n",
      "Training epoch 5 batch 519 with loss 1.8976898193359375, accuracy 0.18333333730697632.\n",
      "Training epoch 5 batch 520 with loss 1.756488561630249, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 521 with loss 1.8519222736358643, accuracy 0.20555555820465088.\n",
      "Training epoch 5 batch 522 with loss 1.7283852100372314, accuracy 0.2944444715976715.\n",
      "Training epoch 5 batch 523 with loss 1.646249532699585, accuracy 0.25555557012557983.\n",
      "Training epoch 5 batch 524 with loss 1.7653694152832031, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 525 with loss 1.5913856029510498, accuracy 0.35277777910232544.\n",
      "Training epoch 5 batch 526 with loss 1.9008888006210327, accuracy 0.125.\n",
      "Training epoch 5 batch 527 with loss 1.6392701864242554, accuracy 0.2281746119260788.\n",
      "Training epoch 5 batch 528 with loss 1.8356263637542725, accuracy 0.22685185074806213.\n",
      "Training epoch 5 batch 529 with loss 1.7828575372695923, accuracy 0.19206349551677704.\n",
      "Training epoch 5 batch 530 with loss 1.7895898818969727, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 531 with loss 1.8515284061431885, accuracy 0.255952388048172.\n",
      "Training epoch 5 batch 532 with loss 1.7566404342651367, accuracy 0.13055555522441864.\n",
      "Training epoch 5 batch 533 with loss 1.8573334217071533, accuracy 0.0694444477558136.\n",
      "Training epoch 5 batch 534 with loss 1.6965570449829102, accuracy 0.1686507910490036.\n",
      "Training epoch 5 batch 535 with loss 1.7140967845916748, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 536 with loss 1.8110978603363037, accuracy 0.2666666805744171.\n",
      "Training epoch 5 batch 537 with loss 1.7524480819702148, accuracy 0.255952388048172.\n",
      "Training epoch 5 batch 538 with loss 1.8732035160064697, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 539 with loss 1.7398502826690674, accuracy 0.2380952537059784.\n",
      "Training epoch 5 batch 540 with loss 1.838945984840393, accuracy 0.13750000298023224.\n",
      "Training epoch 5 batch 541 with loss 1.8033311367034912, accuracy 0.15833334624767303.\n",
      "Training epoch 5 batch 542 with loss 1.7920544147491455, accuracy 0.15555556118488312.\n",
      "Training epoch 5 batch 543 with loss 1.8342376947402954, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 544 with loss 1.8092132806777954, accuracy 0.13055555522441864.\n",
      "Training epoch 5 batch 545 with loss 1.7758792638778687, accuracy 0.25.\n",
      "Training epoch 5 batch 546 with loss 1.7500988245010376, accuracy 0.35555556416511536.\n",
      "Training epoch 5 batch 547 with loss 1.7619524002075195, accuracy 0.125.\n",
      "Training epoch 5 batch 548 with loss 1.8972883224487305, accuracy 0.1805555671453476.\n",
      "Training epoch 5 batch 549 with loss 1.6929889917373657, accuracy 0.35277777910232544.\n",
      "Training epoch 5 batch 550 with loss 1.880124807357788, accuracy 0.0625.\n",
      "Training epoch 5 batch 551 with loss 1.6887239217758179, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 552 with loss 1.7762571573257446, accuracy 0.14166668057441711.\n",
      "Training epoch 5 batch 553 with loss 1.8465020656585693, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 554 with loss 1.8740314245224, accuracy 0.18611110746860504.\n",
      "Training epoch 5 batch 555 with loss 1.7835718393325806, accuracy 0.28333333134651184.\n",
      "Training epoch 5 batch 556 with loss 1.6803165674209595, accuracy 0.31666669249534607.\n",
      "Training epoch 5 batch 557 with loss 1.64936101436615, accuracy 0.4305555820465088.\n",
      "Training epoch 5 batch 558 with loss 1.7240368127822876, accuracy 0.4166666865348816.\n",
      "Training epoch 5 batch 559 with loss 1.877102255821228, accuracy 0.07500000298023224.\n",
      "Training epoch 5 batch 560 with loss 1.7405627965927124, accuracy 0.4087301790714264.\n",
      "Training epoch 5 batch 561 with loss 1.8537012338638306, accuracy 0.08888889104127884.\n",
      "Training epoch 5 batch 562 with loss 1.8982528448104858, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 563 with loss 1.791973352432251, accuracy 0.36666667461395264.\n",
      "Training epoch 5 batch 564 with loss 1.7454513311386108, accuracy 0.3880952298641205.\n",
      "Training epoch 5 batch 565 with loss 1.7192533016204834, accuracy 0.36666667461395264.\n",
      "Training epoch 5 batch 566 with loss 1.7617172002792358, accuracy 0.18611112236976624.\n",
      "Training epoch 5 batch 567 with loss 1.7657369375228882, accuracy 0.21666666865348816.\n",
      "Training epoch 5 batch 568 with loss 1.7891104221343994, accuracy 0.20000000298023224.\n",
      "Training epoch 5 batch 569 with loss 1.7462966442108154, accuracy 0.329365074634552.\n",
      "Training epoch 5 batch 570 with loss 1.7523365020751953, accuracy 0.3333333432674408.\n",
      "Training epoch 5 batch 571 with loss 1.7416489124298096, accuracy 0.40833336114883423.\n",
      "Training epoch 5 batch 572 with loss 1.818844199180603, accuracy 0.19761905074119568.\n",
      "Training epoch 5 batch 573 with loss 1.8411496877670288, accuracy 0.3333333432674408.\n",
      "Training epoch 5 batch 574 with loss 1.7217451333999634, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 575 with loss 1.7994848489761353, accuracy 0.24166667461395264.\n",
      "Training epoch 5 batch 576 with loss 1.886012077331543, accuracy 0.0793650820851326.\n",
      "Training epoch 5 batch 577 with loss 1.7425167560577393, accuracy 0.15555556118488312.\n",
      "Training epoch 5 batch 578 with loss 1.8473927974700928, accuracy 0.19166666269302368.\n",
      "Training epoch 5 batch 579 with loss 1.8337055444717407, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 580 with loss 1.6956112384796143, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 581 with loss 1.7671705484390259, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 582 with loss 1.772679090499878, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 583 with loss 1.8260266780853271, accuracy 0.15555556118488312.\n",
      "Training epoch 5 batch 584 with loss 1.8314943313598633, accuracy 0.24074074625968933.\n",
      "Training epoch 5 batch 585 with loss 1.7606747150421143, accuracy 0.28333333134651184.\n",
      "Training epoch 5 batch 586 with loss 1.737283706665039, accuracy 0.3611111044883728.\n",
      "Training epoch 5 batch 587 with loss 1.7538890838623047, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 588 with loss 1.9257701635360718, accuracy 0.03333333507180214.\n",
      "Training epoch 5 batch 589 with loss 1.7739896774291992, accuracy 0.4000000059604645.\n",
      "Training epoch 5 batch 590 with loss 1.7611989974975586, accuracy 0.2638888955116272.\n",
      "Training epoch 5 batch 591 with loss 1.7458397150039673, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 592 with loss 1.8455345630645752, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 593 with loss 1.8135395050048828, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 594 with loss 1.721775770187378, accuracy 0.13055555522441864.\n",
      "Training epoch 5 batch 595 with loss 1.7548859119415283, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 596 with loss 1.730690598487854, accuracy 0.1587301641702652.\n",
      "Training epoch 5 batch 597 with loss 1.8155443668365479, accuracy 0.14166666567325592.\n",
      "Training epoch 5 batch 598 with loss 1.8821674585342407, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 599 with loss 1.8334916830062866, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 600 with loss 1.7438606023788452, accuracy 0.24722224473953247.\n",
      "Training epoch 5 batch 601 with loss 1.927271842956543, accuracy 0.03333333507180214.\n",
      "Training epoch 5 batch 602 with loss 1.7766634225845337, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 603 with loss 1.7207708358764648, accuracy 0.28333333134651184.\n",
      "Training epoch 5 batch 604 with loss 1.683868646621704, accuracy 0.28333333134651184.\n",
      "Training epoch 5 batch 605 with loss 1.772316575050354, accuracy 0.3444444537162781.\n",
      "Training epoch 5 batch 606 with loss 1.80160391330719, accuracy 0.125.\n",
      "Training epoch 5 batch 607 with loss 1.8109140396118164, accuracy 0.18333333730697632.\n",
      "Training epoch 5 batch 608 with loss 1.8394206762313843, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 609 with loss 1.7362205982208252, accuracy 0.30000001192092896.\n",
      "Training epoch 5 batch 610 with loss 1.8319928646087646, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 611 with loss 1.730445146560669, accuracy 0.2666666805744171.\n",
      "Training epoch 5 batch 612 with loss 1.7397263050079346, accuracy 0.5079365372657776.\n",
      "Training epoch 5 batch 613 with loss 1.733380913734436, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 614 with loss 1.851340889930725, accuracy 0.2797619104385376.\n",
      "Training epoch 5 batch 615 with loss 1.8606760501861572, accuracy 0.03333333507180214.\n",
      "Training epoch 5 batch 616 with loss 1.7540651559829712, accuracy 0.2380952537059784.\n",
      "Training epoch 5 batch 617 with loss 1.8207645416259766, accuracy 0.03333333507180214.\n",
      "Training epoch 5 batch 618 with loss 1.7746940851211548, accuracy 0.3611111044883728.\n",
      "Training epoch 5 batch 619 with loss 1.8457612991333008, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 620 with loss 1.7934662103652954, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 621 with loss 1.7004684209823608, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 622 with loss 1.7565443515777588, accuracy 0.30092594027519226.\n",
      "Training epoch 5 batch 623 with loss 1.7474921941757202, accuracy 0.25.\n",
      "Training epoch 5 batch 624 with loss 1.784950613975525, accuracy 0.3333333432674408.\n",
      "Training epoch 5 batch 625 with loss 1.7252075672149658, accuracy 0.2361111044883728.\n",
      "Training epoch 5 batch 626 with loss 1.8228334188461304, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 627 with loss 1.8413769006729126, accuracy 0.329365074634552.\n",
      "Training epoch 5 batch 628 with loss 1.7494205236434937, accuracy 0.25.\n",
      "Training epoch 5 batch 629 with loss 1.8148311376571655, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 630 with loss 1.7563822269439697, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 631 with loss 1.7099103927612305, accuracy 0.4166666865348816.\n",
      "Training epoch 5 batch 632 with loss 1.7526357173919678, accuracy 0.1805555522441864.\n",
      "Training epoch 5 batch 633 with loss 1.7425966262817383, accuracy 0.3142857253551483.\n",
      "Training epoch 5 batch 634 with loss 1.7133512496948242, accuracy 0.319444477558136.\n",
      "Training epoch 5 batch 635 with loss 1.749399185180664, accuracy 0.2142857313156128.\n",
      "Training epoch 5 batch 636 with loss 1.7448781728744507, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 637 with loss 1.741360068321228, accuracy 0.11666666716337204.\n",
      "Training epoch 5 batch 638 with loss 1.7944520711898804, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 639 with loss 1.7226555347442627, accuracy 0.3888888955116272.\n",
      "Training epoch 5 batch 640 with loss 1.744213342666626, accuracy 0.25.\n",
      "Training epoch 5 batch 641 with loss 1.721056580543518, accuracy 0.35555556416511536.\n",
      "Training epoch 5 batch 642 with loss 1.7611726522445679, accuracy 0.4166666865348816.\n",
      "Training epoch 5 batch 643 with loss 1.8121732473373413, accuracy 0.31666669249534607.\n",
      "Training epoch 5 batch 644 with loss 1.7748823165893555, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 645 with loss 1.7743202447891235, accuracy 0.16388888657093048.\n",
      "Training epoch 5 batch 646 with loss 1.7740623950958252, accuracy 0.222222238779068.\n",
      "Training epoch 5 batch 647 with loss 1.8901182413101196, accuracy 0.222222238779068.\n",
      "Training epoch 5 batch 648 with loss 1.7495203018188477, accuracy 0.2611111104488373.\n",
      "Training epoch 5 batch 649 with loss 1.807720422744751, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 650 with loss 1.7714173793792725, accuracy 0.4305555522441864.\n",
      "Training epoch 5 batch 651 with loss 1.7476978302001953, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 652 with loss 1.8507893085479736, accuracy 0.20555555820465088.\n",
      "Training epoch 5 batch 653 with loss 1.7280117273330688, accuracy 0.4416666626930237.\n",
      "Training epoch 5 batch 654 with loss 1.756113052368164, accuracy 0.4277777671813965.\n",
      "Training epoch 5 batch 655 with loss 1.8515007495880127, accuracy 0.2708333432674408.\n",
      "Training epoch 5 batch 656 with loss 1.8080387115478516, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 657 with loss 1.6675161123275757, accuracy 0.33888891339302063.\n",
      "Training epoch 5 batch 658 with loss 1.7212804555892944, accuracy 0.3722222149372101.\n",
      "Training epoch 5 batch 659 with loss 1.706502914428711, accuracy 0.2361111044883728.\n",
      "Training epoch 5 batch 660 with loss 1.7309672832489014, accuracy 0.236111119389534.\n",
      "Training epoch 5 batch 661 with loss 1.8679912090301514, accuracy 0.07500000298023224.\n",
      "Training epoch 5 batch 662 with loss 1.8394279479980469, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 663 with loss 1.8724931478500366, accuracy 0.25555557012557983.\n",
      "Training epoch 5 batch 664 with loss 1.7963244915008545, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 665 with loss 1.793412446975708, accuracy 0.1031746044754982.\n",
      "Training epoch 5 batch 666 with loss 1.6999261379241943, accuracy 0.2750000059604645.\n",
      "Training epoch 5 batch 667 with loss 1.7766765356063843, accuracy 0.28333333134651184.\n",
      "Training epoch 5 batch 668 with loss 1.7754871845245361, accuracy 0.20000000298023224.\n",
      "Training epoch 5 batch 669 with loss 1.7341505289077759, accuracy 0.2083333283662796.\n",
      "Training epoch 5 batch 670 with loss 1.8230613470077515, accuracy 0.25555557012557983.\n",
      "Training epoch 5 batch 671 with loss 1.6822277307510376, accuracy 0.3194444477558136.\n",
      "Training epoch 5 batch 672 with loss 1.765710473060608, accuracy 0.08095238357782364.\n",
      "Training epoch 5 batch 673 with loss 1.751361608505249, accuracy 0.1488095223903656.\n",
      "Training epoch 5 batch 674 with loss 1.9011814594268799, accuracy 0.02777777798473835.\n",
      "Training epoch 5 batch 675 with loss 1.8909143209457397, accuracy 0.2738095223903656.\n",
      "Training epoch 5 batch 676 with loss 1.7962831258773804, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 677 with loss 1.8506286144256592, accuracy 0.2738095223903656.\n",
      "Training epoch 5 batch 678 with loss 1.809265375137329, accuracy 0.1875.\n",
      "Training epoch 5 batch 679 with loss 1.8340190649032593, accuracy 0.10000000149011612.\n",
      "Training epoch 5 batch 680 with loss 1.7551997900009155, accuracy 0.2321428656578064.\n",
      "Training epoch 5 batch 681 with loss 1.838038682937622, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 682 with loss 1.8093065023422241, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 683 with loss 1.7343711853027344, accuracy 0.20555555820465088.\n",
      "Training epoch 5 batch 684 with loss 1.6343262195587158, accuracy 0.1369047611951828.\n",
      "Training epoch 5 batch 685 with loss 1.7479732036590576, accuracy 0.402777761220932.\n",
      "Training epoch 5 batch 686 with loss 1.774865746498108, accuracy 0.222222238779068.\n",
      "Training epoch 5 batch 687 with loss 1.7342478036880493, accuracy 0.28333336114883423.\n",
      "Training epoch 5 batch 688 with loss 1.857438087463379, accuracy 0.190476194024086.\n",
      "Training epoch 5 batch 689 with loss 1.8260889053344727, accuracy 0.375.\n",
      "Training epoch 5 batch 690 with loss 1.807166337966919, accuracy 0.15833333134651184.\n",
      "Training epoch 5 batch 691 with loss 1.8216453790664673, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 692 with loss 1.842869520187378, accuracy 0.19166666269302368.\n",
      "Training epoch 5 batch 693 with loss 1.7161401510238647, accuracy 0.3402777910232544.\n",
      "Training epoch 5 batch 694 with loss 1.8217432498931885, accuracy 0.1071428582072258.\n",
      "Training epoch 5 batch 695 with loss 1.780360221862793, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 696 with loss 1.8450868129730225, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 697 with loss 1.8520063161849976, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 698 with loss 1.7926031351089478, accuracy 0.2888889014720917.\n",
      "Training epoch 5 batch 699 with loss 1.8327909708023071, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 700 with loss 1.8378007411956787, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 701 with loss 1.7694841623306274, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 702 with loss 1.8425897359848022, accuracy 0.125.\n",
      "Training epoch 5 batch 703 with loss 1.7395353317260742, accuracy 0.03333333507180214.\n",
      "Training epoch 5 batch 704 with loss 1.8668296337127686, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 705 with loss 1.7828025817871094, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 706 with loss 1.8371683359146118, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 707 with loss 1.7364400625228882, accuracy 0.3222222328186035.\n",
      "Training epoch 5 batch 708 with loss 1.8477709293365479, accuracy 0.33194446563720703.\n",
      "Training epoch 5 batch 709 with loss 1.848183035850525, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 710 with loss 1.8633606433868408, accuracy 0.125.\n",
      "Training epoch 5 batch 711 with loss 1.7026554346084595, accuracy 0.3583333492279053.\n",
      "Training epoch 5 batch 712 with loss 1.8538665771484375, accuracy 0.190476194024086.\n",
      "Training epoch 5 batch 713 with loss 1.812708854675293, accuracy 0.0763888880610466.\n",
      "Training epoch 5 batch 714 with loss 1.7590198516845703, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 715 with loss 1.8569440841674805, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 716 with loss 1.8232561349868774, accuracy 0.35277777910232544.\n",
      "Training epoch 5 batch 717 with loss 1.888627052307129, accuracy 0.25555557012557983.\n",
      "Training epoch 5 batch 718 with loss 1.7829450368881226, accuracy 0.20555555820465088.\n",
      "Training epoch 5 batch 719 with loss 1.879406213760376, accuracy 0.03333333507180214.\n",
      "Training epoch 5 batch 720 with loss 1.7885379791259766, accuracy 0.3305555582046509.\n",
      "Training epoch 5 batch 721 with loss 1.7818876504898071, accuracy 0.17777778208255768.\n",
      "Training epoch 5 batch 722 with loss 1.8685020208358765, accuracy 0.1805555522441864.\n",
      "Training epoch 5 batch 723 with loss 1.7264941930770874, accuracy 0.19722223281860352.\n",
      "Training epoch 5 batch 724 with loss 1.8080742359161377, accuracy 0.19166667759418488.\n",
      "Training epoch 5 batch 725 with loss 1.774237036705017, accuracy 0.18888889253139496.\n",
      "Training epoch 5 batch 726 with loss 1.7726843357086182, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 727 with loss 1.6621755361557007, accuracy 0.3263888657093048.\n",
      "Training epoch 5 batch 728 with loss 1.8250722885131836, accuracy 0.236111119389534.\n",
      "Training epoch 5 batch 729 with loss 1.802859902381897, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 730 with loss 1.7566627264022827, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 731 with loss 1.7450597286224365, accuracy 0.3444444537162781.\n",
      "Training epoch 5 batch 732 with loss 1.8522136211395264, accuracy 0.23333334922790527.\n",
      "Training epoch 5 batch 733 with loss 1.7966785430908203, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 734 with loss 1.9209754467010498, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 735 with loss 1.8204034566879272, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 736 with loss 1.8745791912078857, accuracy 0.0.\n",
      "Training epoch 5 batch 737 with loss 1.7012748718261719, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 738 with loss 1.7782331705093384, accuracy 0.22777777910232544.\n",
      "Training epoch 5 batch 739 with loss 1.7622034549713135, accuracy 0.1805555671453476.\n",
      "Training epoch 5 batch 740 with loss 1.6673015356063843, accuracy 0.23333334922790527.\n",
      "Training epoch 5 batch 741 with loss 1.814947485923767, accuracy 0.08888889104127884.\n",
      "Training epoch 5 batch 742 with loss 1.6729395389556885, accuracy 0.38333332538604736.\n",
      "Training epoch 5 batch 743 with loss 1.8259035348892212, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 744 with loss 1.8173288106918335, accuracy 0.125.\n",
      "Training epoch 5 batch 745 with loss 1.7925844192504883, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 746 with loss 1.7697035074234009, accuracy 0.472222238779068.\n",
      "Training epoch 5 batch 747 with loss 1.7642608880996704, accuracy 0.444444477558136.\n",
      "Training epoch 5 batch 748 with loss 1.718747854232788, accuracy 0.25.\n",
      "Training epoch 5 batch 749 with loss 1.7556095123291016, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 750 with loss 1.6959469318389893, accuracy 0.32499998807907104.\n",
      "Training epoch 5 batch 751 with loss 1.67788565158844, accuracy 0.26944443583488464.\n",
      "Training epoch 5 batch 752 with loss 1.8105942010879517, accuracy 0.2888889014720917.\n",
      "Training epoch 5 batch 753 with loss 1.6359577178955078, accuracy 0.335317462682724.\n",
      "Training epoch 5 batch 754 with loss 1.7884117364883423, accuracy 0.1071428582072258.\n",
      "Training epoch 5 batch 755 with loss 1.7716127634048462, accuracy 0.23333334922790527.\n",
      "Training epoch 5 batch 756 with loss 1.7401405572891235, accuracy 0.25555557012557983.\n",
      "Training epoch 5 batch 757 with loss 1.6942236423492432, accuracy 0.34166669845581055.\n",
      "Training epoch 5 batch 758 with loss 1.7040132284164429, accuracy 0.2666666805744171.\n",
      "Training epoch 5 batch 759 with loss 1.717841386795044, accuracy 0.37222224473953247.\n",
      "Training epoch 5 batch 760 with loss 1.7645385265350342, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 761 with loss 1.8009185791015625, accuracy 0.4027777910232544.\n",
      "Training epoch 5 batch 762 with loss 1.852932333946228, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 763 with loss 1.7488340139389038, accuracy 0.2182539701461792.\n",
      "Training epoch 5 batch 764 with loss 1.8554513454437256, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 765 with loss 1.8094393014907837, accuracy 0.3888888955116272.\n",
      "Training epoch 5 batch 766 with loss 1.8681039810180664, accuracy 0.0416666679084301.\n",
      "Training epoch 5 batch 767 with loss 1.7180368900299072, accuracy 0.3849206566810608.\n",
      "Training epoch 5 batch 768 with loss 1.7498193979263306, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 769 with loss 1.739965796470642, accuracy 0.26944446563720703.\n",
      "Training epoch 5 batch 770 with loss 1.7548472881317139, accuracy 0.21666666865348816.\n",
      "Training epoch 5 batch 771 with loss 1.7161481380462646, accuracy 0.4047619104385376.\n",
      "Training epoch 5 batch 772 with loss 1.7740103006362915, accuracy 0.1805555671453476.\n",
      "Training epoch 5 batch 773 with loss 1.7292048931121826, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 774 with loss 1.8512321710586548, accuracy 0.1805555671453476.\n",
      "Training epoch 5 batch 775 with loss 1.7783921957015991, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 776 with loss 1.812034249305725, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 777 with loss 1.8149402141571045, accuracy 0.125.\n",
      "Training epoch 5 batch 778 with loss 1.772568941116333, accuracy 0.19166666269302368.\n",
      "Training epoch 5 batch 779 with loss 1.7286790609359741, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 780 with loss 1.795270562171936, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 781 with loss 1.8335806131362915, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 782 with loss 1.7629693746566772, accuracy 0.1458333432674408.\n",
      "Training epoch 5 batch 783 with loss 1.7407087087631226, accuracy 0.27037036418914795.\n",
      "Training epoch 5 batch 784 with loss 1.8069108724594116, accuracy 0.07999999821186066.\n",
      "Training epoch 5 batch 785 with loss 1.8072803020477295, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 786 with loss 1.8191388845443726, accuracy 0.25.\n",
      "Training epoch 5 batch 787 with loss 1.6996612548828125, accuracy 0.2666666805744171.\n",
      "Training epoch 5 batch 788 with loss 1.712707281112671, accuracy 0.0416666679084301.\n",
      "Training epoch 5 batch 789 with loss 1.7852649688720703, accuracy 0.23333334922790527.\n",
      "Training epoch 5 batch 790 with loss 1.8155832290649414, accuracy 0.15833333134651184.\n",
      "Training epoch 5 batch 791 with loss 1.70953369140625, accuracy 0.3305555582046509.\n",
      "Training epoch 5 batch 792 with loss 1.733231782913208, accuracy 0.17658731341362.\n",
      "Training epoch 5 batch 793 with loss 1.7516587972640991, accuracy 0.26111114025115967.\n",
      "Training epoch 5 batch 794 with loss 1.7990585565567017, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 795 with loss 1.8789644241333008, accuracy 0.1726190447807312.\n",
      "Training epoch 5 batch 796 with loss 1.7718225717544556, accuracy 0.25.\n",
      "Training epoch 5 batch 797 with loss 1.7746156454086304, accuracy 0.1031746044754982.\n",
      "Training epoch 5 batch 798 with loss 1.809027910232544, accuracy 0.1349206417798996.\n",
      "Training epoch 5 batch 799 with loss 1.94219970703125, accuracy 0.0.\n",
      "Training epoch 5 batch 800 with loss 1.8507436513900757, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 801 with loss 1.7151893377304077, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 802 with loss 1.7419688701629639, accuracy 0.4611110985279083.\n",
      "Training epoch 5 batch 803 with loss 1.7244011163711548, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 804 with loss 1.8177063465118408, accuracy 0.31111112236976624.\n",
      "Training epoch 5 batch 805 with loss 1.7868120670318604, accuracy 0.236111119389534.\n",
      "Training epoch 5 batch 806 with loss 1.849587082862854, accuracy 0.204365074634552.\n",
      "Training epoch 5 batch 807 with loss 1.8403676748275757, accuracy 0.33492064476013184.\n",
      "Training epoch 5 batch 808 with loss 1.7484662532806396, accuracy 0.2291666865348816.\n",
      "Training epoch 5 batch 809 with loss 1.7642605304718018, accuracy 0.2361111044883728.\n",
      "Training epoch 5 batch 810 with loss 1.825775384902954, accuracy 0.4222222566604614.\n",
      "Training epoch 5 batch 811 with loss 1.702143669128418, accuracy 0.18809524178504944.\n",
      "Training epoch 5 batch 812 with loss 1.7565510272979736, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 813 with loss 1.771946668624878, accuracy 0.2944444715976715.\n",
      "Training epoch 5 batch 814 with loss 1.8740894794464111, accuracy 0.20000000298023224.\n",
      "Training epoch 5 batch 815 with loss 1.9191029071807861, accuracy 0.03333333507180214.\n",
      "Training epoch 5 batch 816 with loss 1.7692782878875732, accuracy 0.2888888716697693.\n",
      "Training epoch 5 batch 817 with loss 1.755529761314392, accuracy 0.17777778208255768.\n",
      "Training epoch 5 batch 818 with loss 1.8275477886199951, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 819 with loss 1.783540964126587, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 820 with loss 1.822678565979004, accuracy 0.36666667461395264.\n",
      "Training epoch 5 batch 821 with loss 1.699803113937378, accuracy 0.3027777671813965.\n",
      "Training epoch 5 batch 822 with loss 1.7436933517456055, accuracy 0.18888890743255615.\n",
      "Training epoch 5 batch 823 with loss 1.8896310329437256, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 824 with loss 1.9262101650238037, accuracy 0.0.\n",
      "Training epoch 5 batch 825 with loss 1.8113269805908203, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 826 with loss 1.7918729782104492, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 827 with loss 1.758968710899353, accuracy 0.23333333432674408.\n",
      "Training epoch 5 batch 828 with loss 1.834537148475647, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 829 with loss 1.7735506296157837, accuracy 0.444444477558136.\n",
      "Training epoch 5 batch 830 with loss 1.743634581565857, accuracy 0.21666666865348816.\n",
      "Training epoch 5 batch 831 with loss 1.6947336196899414, accuracy 0.3027777671813965.\n",
      "Training epoch 5 batch 832 with loss 1.7607330083847046, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 833 with loss 1.7619714736938477, accuracy 0.25.\n",
      "Training epoch 5 batch 834 with loss 1.7455049753189087, accuracy 0.1726190447807312.\n",
      "Training epoch 5 batch 835 with loss 1.6514850854873657, accuracy 0.25.\n",
      "Training epoch 5 batch 836 with loss 1.7516472339630127, accuracy 0.40833336114883423.\n",
      "Training epoch 5 batch 837 with loss 1.7398847341537476, accuracy 0.40833333134651184.\n",
      "Training epoch 5 batch 838 with loss 1.716713309288025, accuracy 0.2916666567325592.\n",
      "Training epoch 5 batch 839 with loss 1.766495943069458, accuracy 0.10277777910232544.\n",
      "Training epoch 5 batch 840 with loss 1.8487964868545532, accuracy 0.065476194024086.\n",
      "Training epoch 5 batch 841 with loss 1.9345775842666626, accuracy 0.02380952425301075.\n",
      "Training epoch 5 batch 842 with loss 1.860830307006836, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 843 with loss 1.8497596979141235, accuracy 0.125.\n",
      "Training epoch 5 batch 844 with loss 1.7742630243301392, accuracy 0.1071428582072258.\n",
      "Training epoch 5 batch 845 with loss 1.7724902629852295, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 846 with loss 1.8222023248672485, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 847 with loss 1.898552656173706, accuracy 0.0.\n",
      "Training epoch 5 batch 848 with loss 1.7214092016220093, accuracy 0.16388890147209167.\n",
      "Training epoch 5 batch 849 with loss 1.7898130416870117, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 850 with loss 1.8155057430267334, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 851 with loss 1.8239593505859375, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 852 with loss 1.867751121520996, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 853 with loss 1.8289419412612915, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 854 with loss 1.8153711557388306, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 855 with loss 1.8340543508529663, accuracy 0.03333333507180214.\n",
      "Training epoch 5 batch 856 with loss 1.8159654140472412, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 857 with loss 1.8538169860839844, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 858 with loss 1.7529408931732178, accuracy 0.2916666567325592.\n",
      "Training epoch 5 batch 859 with loss 1.848981499671936, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 860 with loss 1.9349424839019775, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 861 with loss 1.7978887557983398, accuracy 0.10833333432674408.\n",
      "Training epoch 5 batch 862 with loss 1.7979109287261963, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 863 with loss 1.8003311157226562, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 864 with loss 1.8960981369018555, accuracy 0.0.\n",
      "Training epoch 5 batch 865 with loss 1.9237838983535767, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 866 with loss 1.8401199579238892, accuracy 0.15833333134651184.\n",
      "Training epoch 5 batch 867 with loss 1.7411978244781494, accuracy 0.2888889014720917.\n",
      "Training epoch 5 batch 868 with loss 1.7994142770767212, accuracy 0.125.\n",
      "Training epoch 5 batch 869 with loss 1.6900379657745361, accuracy 0.20000000298023224.\n",
      "Training epoch 5 batch 870 with loss 1.6753584146499634, accuracy 0.5333333611488342.\n",
      "Training epoch 5 batch 871 with loss 1.661750078201294, accuracy 0.29722222685813904.\n",
      "Training epoch 5 batch 872 with loss 1.7835203409194946, accuracy 0.13650794327259064.\n",
      "Training epoch 5 batch 873 with loss 1.913513422012329, accuracy 0.08888889104127884.\n",
      "Training epoch 5 batch 874 with loss 1.7429640293121338, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 875 with loss 1.7998672723770142, accuracy 0.16428571939468384.\n",
      "Training epoch 5 batch 876 with loss 1.777769684791565, accuracy 0.10000000894069672.\n",
      "Training epoch 5 batch 877 with loss 1.7151553630828857, accuracy 0.25555557012557983.\n",
      "Training epoch 5 batch 878 with loss 1.8096482753753662, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 879 with loss 1.8776452541351318, accuracy 0.2876984179019928.\n",
      "Training epoch 5 batch 880 with loss 1.8418066501617432, accuracy 0.2916666567325592.\n",
      "Training epoch 5 batch 881 with loss 1.8651564121246338, accuracy 0.125.\n",
      "Training epoch 5 batch 882 with loss 1.7515445947647095, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 883 with loss 1.7822520732879639, accuracy 0.3499999940395355.\n",
      "Training epoch 5 batch 884 with loss 1.7568132877349854, accuracy 0.14351852238178253.\n",
      "Training epoch 5 batch 885 with loss 1.9281374216079712, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 886 with loss 1.8155596256256104, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 887 with loss 1.8855979442596436, accuracy 0.0793650820851326.\n",
      "Training epoch 5 batch 888 with loss 1.5975382328033447, accuracy 0.5222222208976746.\n",
      "Training epoch 5 batch 889 with loss 1.764237403869629, accuracy 0.4194444417953491.\n",
      "Training epoch 5 batch 890 with loss 1.8460136651992798, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 891 with loss 1.8026920557022095, accuracy 0.0416666679084301.\n",
      "Training epoch 5 batch 892 with loss 1.8812291622161865, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 893 with loss 1.788252592086792, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 894 with loss 1.7830333709716797, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 895 with loss 1.6784594058990479, accuracy 0.3083333373069763.\n",
      "Training epoch 5 batch 896 with loss 1.776007056236267, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 897 with loss 1.8463430404663086, accuracy 0.125.\n",
      "Training epoch 5 batch 898 with loss 1.7673259973526, accuracy 0.36666667461395264.\n",
      "Training epoch 5 batch 899 with loss 1.7294164896011353, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 900 with loss 1.7091480493545532, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 901 with loss 1.73139226436615, accuracy 0.2611111104488373.\n",
      "Training epoch 5 batch 902 with loss 1.7820907831192017, accuracy 0.1587301641702652.\n",
      "Training epoch 5 batch 903 with loss 1.814750075340271, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 904 with loss 1.8433500528335571, accuracy 0.19166667759418488.\n",
      "Training epoch 5 batch 905 with loss 1.8188387155532837, accuracy 0.02777777798473835.\n",
      "Training epoch 5 batch 906 with loss 1.7490345239639282, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 907 with loss 1.7780601978302002, accuracy 0.1626984179019928.\n",
      "Training epoch 5 batch 908 with loss 1.8584325313568115, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 909 with loss 1.8877198696136475, accuracy 0.0.\n",
      "Training epoch 5 batch 910 with loss 1.855068564414978, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 911 with loss 1.7880805730819702, accuracy 0.33888891339302063.\n",
      "Training epoch 5 batch 912 with loss 1.7574384212493896, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 913 with loss 1.6852995157241821, accuracy 0.2767857313156128.\n",
      "Training epoch 5 batch 914 with loss 1.8504120111465454, accuracy 0.15833334624767303.\n",
      "Training epoch 5 batch 915 with loss 1.8241231441497803, accuracy 0.12777778506278992.\n",
      "Training epoch 5 batch 916 with loss 1.7237790822982788, accuracy 0.02777777798473835.\n",
      "Training epoch 5 batch 917 with loss 1.6156011819839478, accuracy 0.3583333492279053.\n",
      "Training epoch 5 batch 918 with loss 1.8680429458618164, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 919 with loss 1.819289207458496, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 920 with loss 1.6540158987045288, accuracy 0.29722222685813904.\n",
      "Training epoch 5 batch 921 with loss 1.8359514474868774, accuracy 0.1805555522441864.\n",
      "Training epoch 5 batch 922 with loss 1.7108218669891357, accuracy 0.3611111044883728.\n",
      "Training epoch 5 batch 923 with loss 1.7954456806182861, accuracy 0.32500001788139343.\n",
      "Training epoch 5 batch 924 with loss 1.675302267074585, accuracy 0.3392857015132904.\n",
      "Training epoch 5 batch 925 with loss 1.771295189857483, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 926 with loss 1.7878299951553345, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 927 with loss 1.7930635213851929, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 928 with loss 1.8614763021469116, accuracy 0.1726190447807312.\n",
      "Training epoch 5 batch 929 with loss 1.8683626651763916, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 930 with loss 1.8077913522720337, accuracy 0.16388890147209167.\n",
      "Training epoch 5 batch 931 with loss 1.7360813617706299, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 932 with loss 1.7521111965179443, accuracy 0.222222238779068.\n",
      "Training epoch 5 batch 933 with loss 1.819339394569397, accuracy 0.1626984179019928.\n",
      "Training epoch 5 batch 934 with loss 1.8342863321304321, accuracy 0.1031746044754982.\n",
      "Training epoch 5 batch 935 with loss 1.7721999883651733, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 936 with loss 1.8153355121612549, accuracy 0.13055555522441864.\n",
      "Training epoch 5 batch 937 with loss 1.8344475030899048, accuracy 0.4166666865348816.\n",
      "Training epoch 5 batch 938 with loss 1.8149325847625732, accuracy 0.0416666679084301.\n",
      "Training epoch 5 batch 939 with loss 1.928945541381836, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 940 with loss 1.8491430282592773, accuracy 0.13055555522441864.\n",
      "Training epoch 5 batch 941 with loss 1.738226294517517, accuracy 0.2083333283662796.\n",
      "Training epoch 5 batch 942 with loss 1.7038812637329102, accuracy 0.23888888955116272.\n",
      "Training epoch 5 batch 943 with loss 1.7600948810577393, accuracy 0.2916666865348816.\n",
      "Training epoch 5 batch 944 with loss 1.8430522680282593, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 945 with loss 1.7876431941986084, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 946 with loss 1.9102060794830322, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 947 with loss 1.7237071990966797, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 948 with loss 1.7846689224243164, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 949 with loss 1.7221546173095703, accuracy 0.4293650984764099.\n",
      "Training epoch 5 batch 950 with loss 1.7304884195327759, accuracy 0.20555555820465088.\n",
      "Training epoch 5 batch 951 with loss 1.7330701351165771, accuracy 0.3500000238418579.\n",
      "Training epoch 5 batch 952 with loss 1.7930691242218018, accuracy 0.329365074634552.\n",
      "Training epoch 5 batch 953 with loss 1.7472379207611084, accuracy 0.125.\n",
      "Training epoch 5 batch 954 with loss 1.7337539196014404, accuracy 0.3638889193534851.\n",
      "Training epoch 5 batch 955 with loss 1.8087656497955322, accuracy 0.2281745970249176.\n",
      "Training epoch 5 batch 956 with loss 1.7949464321136475, accuracy 0.22777777910232544.\n",
      "Training epoch 5 batch 957 with loss 1.8970105648040771, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 958 with loss 1.7736055850982666, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 959 with loss 1.7635046243667603, accuracy 0.21944443881511688.\n",
      "Training epoch 5 batch 960 with loss 1.787732481956482, accuracy 0.25.\n",
      "Training epoch 5 batch 961 with loss 1.7507356405258179, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 962 with loss 1.705446481704712, accuracy 0.30158731341362.\n",
      "Training epoch 5 batch 963 with loss 1.8773012161254883, accuracy 0.0694444477558136.\n",
      "Training epoch 5 batch 964 with loss 1.7400232553482056, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 965 with loss 1.7460800409317017, accuracy 0.5.\n",
      "Training epoch 5 batch 966 with loss 1.7568273544311523, accuracy 0.2341269850730896.\n",
      "Training epoch 5 batch 967 with loss 1.6968166828155518, accuracy 0.3444444537162781.\n",
      "Training epoch 5 batch 968 with loss 1.7975070476531982, accuracy 0.2611111104488373.\n",
      "Training epoch 5 batch 969 with loss 1.7315568923950195, accuracy 0.21388888359069824.\n",
      "Training epoch 5 batch 970 with loss 1.8378164768218994, accuracy 0.18333333730697632.\n",
      "Training epoch 5 batch 971 with loss 1.8129713535308838, accuracy 0.03333333507180214.\n",
      "Training epoch 5 batch 972 with loss 1.7852548360824585, accuracy 0.2611111104488373.\n",
      "Training epoch 5 batch 973 with loss 1.8684829473495483, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 974 with loss 1.8588005304336548, accuracy 0.125.\n",
      "Training epoch 5 batch 975 with loss 1.7283920049667358, accuracy 0.1865079402923584.\n",
      "Training epoch 5 batch 976 with loss 1.7539856433868408, accuracy 0.065476194024086.\n",
      "Training epoch 5 batch 977 with loss 1.8189783096313477, accuracy 0.20000001788139343.\n",
      "Training epoch 5 batch 978 with loss 1.8338003158569336, accuracy 0.22380952537059784.\n",
      "Training epoch 5 batch 979 with loss 1.739837408065796, accuracy 0.2569444477558136.\n",
      "Training epoch 5 batch 980 with loss 1.7530425786972046, accuracy 0.18333333730697632.\n",
      "Training epoch 5 batch 981 with loss 1.7824220657348633, accuracy 0.3194444477558136.\n",
      "Training epoch 5 batch 982 with loss 1.7677381038665771, accuracy 0.130952388048172.\n",
      "Training epoch 5 batch 983 with loss 1.7975285053253174, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 984 with loss 1.7760355472564697, accuracy 0.19166666269302368.\n",
      "Training epoch 5 batch 985 with loss 1.784581184387207, accuracy 0.25.\n",
      "Training epoch 5 batch 986 with loss 1.6481168270111084, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 987 with loss 1.7244821786880493, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 988 with loss 1.777080774307251, accuracy 0.5158730149269104.\n",
      "Training epoch 5 batch 989 with loss 1.8395135402679443, accuracy 0.11666666716337204.\n",
      "Training epoch 5 batch 990 with loss 1.8332712650299072, accuracy 0.236111119389534.\n",
      "Training epoch 5 batch 991 with loss 1.8366031646728516, accuracy 0.18166667222976685.\n",
      "Training epoch 5 batch 992 with loss 1.8053367137908936, accuracy 0.0.\n",
      "Training epoch 5 batch 993 with loss 1.6973209381103516, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 994 with loss 1.8074111938476562, accuracy 0.15833334624767303.\n",
      "Training epoch 5 batch 995 with loss 1.6738042831420898, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 996 with loss 1.7361894845962524, accuracy 0.40833333134651184.\n",
      "Training epoch 5 batch 997 with loss 1.7185554504394531, accuracy 0.4503968358039856.\n",
      "Training epoch 5 batch 998 with loss 1.8730719089508057, accuracy 0.190476194024086.\n",
      "Training epoch 5 batch 999 with loss 1.7874162197113037, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 1000 with loss 1.8124969005584717, accuracy 0.3333333432674408.\n",
      "Training epoch 5 batch 1001 with loss 1.6646379232406616, accuracy 0.19603174924850464.\n",
      "Training epoch 5 batch 1002 with loss 1.7784992456436157, accuracy 0.13214285671710968.\n",
      "Training epoch 5 batch 1003 with loss 1.734674096107483, accuracy 0.236111119389534.\n",
      "Training epoch 5 batch 1004 with loss 1.865097999572754, accuracy 0.06111111491918564.\n",
      "Training epoch 5 batch 1005 with loss 1.7934322357177734, accuracy 0.204365074634552.\n",
      "Training epoch 5 batch 1006 with loss 1.8018382787704468, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 1007 with loss 1.7802938222885132, accuracy 0.3611111044883728.\n",
      "Training epoch 5 batch 1008 with loss 1.8956162929534912, accuracy 0.1626984179019928.\n",
      "Training epoch 5 batch 1009 with loss 1.853713035583496, accuracy 0.125.\n",
      "Training epoch 5 batch 1010 with loss 1.7216945886611938, accuracy 0.222222238779068.\n",
      "Training epoch 5 batch 1011 with loss 1.762860894203186, accuracy 0.15833333134651184.\n",
      "Training epoch 5 batch 1012 with loss 1.7957426309585571, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1013 with loss 1.8693459033966064, accuracy 0.0416666679084301.\n",
      "Training epoch 5 batch 1014 with loss 1.7241684198379517, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 1015 with loss 1.8039172887802124, accuracy 0.1349206417798996.\n",
      "Training epoch 5 batch 1016 with loss 1.7615406513214111, accuracy 0.2916666865348816.\n",
      "Training epoch 5 batch 1017 with loss 1.7437187433242798, accuracy 0.22777777910232544.\n",
      "Training epoch 5 batch 1018 with loss 1.734926462173462, accuracy 0.2750000059604645.\n",
      "Training epoch 5 batch 1019 with loss 1.8714954853057861, accuracy 0.03333333507180214.\n",
      "Training epoch 5 batch 1020 with loss 1.9228906631469727, accuracy 0.02777777798473835.\n",
      "Training epoch 5 batch 1021 with loss 1.7805436849594116, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 1022 with loss 1.8919585943222046, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 1023 with loss 1.7048652172088623, accuracy 0.38333332538604736.\n",
      "Training epoch 5 batch 1024 with loss 1.7389825582504272, accuracy 0.25.\n",
      "Training epoch 5 batch 1025 with loss 1.8265823125839233, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1026 with loss 1.8422189950942993, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 1027 with loss 1.7253862619400024, accuracy 0.125.\n",
      "Training epoch 5 batch 1028 with loss 1.7525813579559326, accuracy 0.2361111342906952.\n",
      "Training epoch 5 batch 1029 with loss 1.7526518106460571, accuracy 0.255952388048172.\n",
      "Training epoch 5 batch 1030 with loss 1.8448550701141357, accuracy 0.1031746044754982.\n",
      "Training epoch 5 batch 1031 with loss 1.7873165607452393, accuracy 0.17777778208255768.\n",
      "Training epoch 5 batch 1032 with loss 1.7188694477081299, accuracy 0.38055557012557983.\n",
      "Training epoch 5 batch 1033 with loss 1.8448282480239868, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 1034 with loss 1.8313350677490234, accuracy 0.16388888657093048.\n",
      "Training epoch 5 batch 1035 with loss 1.6673495769500732, accuracy 0.2750000059604645.\n",
      "Training epoch 5 batch 1036 with loss 1.8329664468765259, accuracy 0.130952388048172.\n",
      "Training epoch 5 batch 1037 with loss 1.9077327251434326, accuracy 0.125.\n",
      "Training epoch 5 batch 1038 with loss 1.6665672063827515, accuracy 0.47857144474983215.\n",
      "Training epoch 5 batch 1039 with loss 1.7641721963882446, accuracy 0.2611111104488373.\n",
      "Training epoch 5 batch 1040 with loss 1.7196550369262695, accuracy 0.4055555462837219.\n",
      "Training epoch 5 batch 1041 with loss 1.7437156438827515, accuracy 0.31111112236976624.\n",
      "Training epoch 5 batch 1042 with loss 1.8156026601791382, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 1043 with loss 1.771031141281128, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 1044 with loss 1.7270927429199219, accuracy 0.18611112236976624.\n",
      "Training epoch 5 batch 1045 with loss 1.7836363315582275, accuracy 0.07500000298023224.\n",
      "Training epoch 5 batch 1046 with loss 1.8287500143051147, accuracy 0.15833334624767303.\n",
      "Training epoch 5 batch 1047 with loss 1.6412074565887451, accuracy 0.2380952537059784.\n",
      "Training epoch 5 batch 1048 with loss 1.8295156955718994, accuracy 0.0694444477558136.\n",
      "Training epoch 5 batch 1049 with loss 1.8256161212921143, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 1050 with loss 1.7971769571304321, accuracy 0.03703703731298447.\n",
      "Training epoch 5 batch 1051 with loss 1.7570617198944092, accuracy 0.236111119389534.\n",
      "Training epoch 5 batch 1052 with loss 1.8080047369003296, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 1053 with loss 1.7355530261993408, accuracy 0.347222238779068.\n",
      "Training epoch 5 batch 1054 with loss 1.7849171161651611, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 1055 with loss 1.7189162969589233, accuracy 0.48055556416511536.\n",
      "Training epoch 5 batch 1056 with loss 1.785578727722168, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1057 with loss 1.7196956872940063, accuracy 0.1041666716337204.\n",
      "Training epoch 5 batch 1058 with loss 1.746793508529663, accuracy 0.2281745970249176.\n",
      "Training epoch 5 batch 1059 with loss 1.7967920303344727, accuracy 0.18888890743255615.\n",
      "Training epoch 5 batch 1060 with loss 1.6857001781463623, accuracy 0.13611111044883728.\n",
      "Training epoch 5 batch 1061 with loss 1.744807243347168, accuracy 0.28333333134651184.\n",
      "Training epoch 5 batch 1062 with loss 1.7570263147354126, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1063 with loss 1.7568187713623047, accuracy 0.15595239400863647.\n",
      "Training epoch 5 batch 1064 with loss 1.8435449600219727, accuracy 0.125.\n",
      "Training epoch 5 batch 1065 with loss 1.7716840505599976, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 1066 with loss 1.794904351234436, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 1067 with loss 1.8204965591430664, accuracy 0.15555556118488312.\n",
      "Training epoch 5 batch 1068 with loss 1.8422882556915283, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 1069 with loss 1.7867807149887085, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1070 with loss 1.7076971530914307, accuracy 0.33888888359069824.\n",
      "Training epoch 5 batch 1071 with loss 1.8763065338134766, accuracy 0.0694444477558136.\n",
      "Training epoch 5 batch 1072 with loss 1.8191492557525635, accuracy 0.1527777910232544.\n",
      "Training epoch 5 batch 1073 with loss 1.9368276596069336, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 1074 with loss 1.8048244714736938, accuracy 0.20000000298023224.\n",
      "Training epoch 5 batch 1075 with loss 1.8642266988754272, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 1076 with loss 1.668739914894104, accuracy 0.3194444477558136.\n",
      "Training epoch 5 batch 1077 with loss 1.8691192865371704, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 1078 with loss 1.8201663494110107, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 1079 with loss 1.8115730285644531, accuracy 0.21388889849185944.\n",
      "Training epoch 5 batch 1080 with loss 1.7020705938339233, accuracy 0.2944444417953491.\n",
      "Training epoch 5 batch 1081 with loss 1.803763747215271, accuracy 0.18333333730697632.\n",
      "Training epoch 5 batch 1082 with loss 1.8225305080413818, accuracy 0.2182539701461792.\n",
      "Training epoch 5 batch 1083 with loss 1.8690369129180908, accuracy 0.125.\n",
      "Training epoch 5 batch 1084 with loss 1.7255840301513672, accuracy 0.17777778208255768.\n",
      "Training epoch 5 batch 1085 with loss 1.849096655845642, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 1086 with loss 1.7151387929916382, accuracy 0.24166667461395264.\n",
      "Training epoch 5 batch 1087 with loss 1.7366445064544678, accuracy 0.21388888359069824.\n",
      "Training epoch 5 batch 1088 with loss 1.8192209005355835, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 1089 with loss 1.9086074829101562, accuracy 0.13055555522441864.\n",
      "Training epoch 5 batch 1090 with loss 1.804741621017456, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1091 with loss 1.724622130393982, accuracy 0.3948412835597992.\n",
      "Training epoch 5 batch 1092 with loss 1.8419511318206787, accuracy 0.0.\n",
      "Training epoch 5 batch 1093 with loss 1.7641757726669312, accuracy 0.0694444477558136.\n",
      "Training epoch 5 batch 1094 with loss 1.8603770732879639, accuracy 0.02777777798473835.\n",
      "Training epoch 5 batch 1095 with loss 1.7299273014068604, accuracy 0.32698413729667664.\n",
      "Training epoch 5 batch 1096 with loss 1.8014297485351562, accuracy 0.20555555820465088.\n",
      "Training epoch 5 batch 1097 with loss 1.7812950611114502, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 1098 with loss 1.7482582330703735, accuracy 0.3333333432674408.\n",
      "Training epoch 5 batch 1099 with loss 1.7764145135879517, accuracy 0.2611111104488373.\n",
      "Training epoch 5 batch 1100 with loss 1.7275511026382446, accuracy 0.15833334624767303.\n",
      "Training epoch 5 batch 1101 with loss 1.7508811950683594, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 1102 with loss 1.7866357564926147, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1103 with loss 1.9139407873153687, accuracy 0.20000000298023224.\n",
      "Training epoch 5 batch 1104 with loss 1.7673819065093994, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 1105 with loss 1.922201156616211, accuracy 0.125.\n",
      "Training epoch 5 batch 1106 with loss 1.9404033422470093, accuracy 0.0476190485060215.\n",
      "Training epoch 5 batch 1107 with loss 1.8688814640045166, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 1108 with loss 1.776482343673706, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 1109 with loss 1.7773513793945312, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1110 with loss 1.8347785472869873, accuracy 0.0416666679084301.\n",
      "Training epoch 5 batch 1111 with loss 1.9043781757354736, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 1112 with loss 1.8056936264038086, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 1113 with loss 1.8651005029678345, accuracy 0.20000000298023224.\n",
      "Training epoch 5 batch 1114 with loss 1.906550407409668, accuracy 0.125.\n",
      "Training epoch 5 batch 1115 with loss 1.8382790088653564, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 1116 with loss 1.7644075155258179, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 1117 with loss 1.917689561843872, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 1118 with loss 1.7927777767181396, accuracy 0.20000001788139343.\n",
      "Training epoch 5 batch 1119 with loss 1.714078664779663, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 1120 with loss 1.8297908306121826, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 1121 with loss 1.77792227268219, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 1122 with loss 1.7126258611679077, accuracy 0.4361111521720886.\n",
      "Training epoch 5 batch 1123 with loss 1.765816330909729, accuracy 0.3305555582046509.\n",
      "Training epoch 5 batch 1124 with loss 1.8061192035675049, accuracy 0.23333334922790527.\n",
      "Training epoch 5 batch 1125 with loss 1.695376992225647, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 1126 with loss 1.8404104709625244, accuracy 0.02777777798473835.\n",
      "Training epoch 5 batch 1127 with loss 1.8649753332138062, accuracy 0.03333333507180214.\n",
      "Training epoch 5 batch 1128 with loss 1.841191291809082, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1129 with loss 1.921276330947876, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 1130 with loss 1.7780077457427979, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 1131 with loss 1.7346426248550415, accuracy 0.21388888359069824.\n",
      "Training epoch 5 batch 1132 with loss 1.7353883981704712, accuracy 0.31111112236976624.\n",
      "Training epoch 5 batch 1133 with loss 1.707516074180603, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 1134 with loss 1.7882490158081055, accuracy 0.2152777761220932.\n",
      "Training epoch 5 batch 1135 with loss 1.7855091094970703, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1136 with loss 1.7302192449569702, accuracy 0.3611111342906952.\n",
      "Training epoch 5 batch 1137 with loss 1.916897177696228, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 1138 with loss 1.8366501331329346, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 1139 with loss 1.8629070520401, accuracy 0.08888889104127884.\n",
      "Training epoch 5 batch 1140 with loss 1.8131868839263916, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 1141 with loss 1.821489691734314, accuracy 0.0714285746216774.\n",
      "Training epoch 5 batch 1142 with loss 1.8654731512069702, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1143 with loss 1.7788957357406616, accuracy 0.0.\n",
      "Training epoch 5 batch 1144 with loss 1.7387886047363281, accuracy 0.2611111104488373.\n",
      "Training epoch 5 batch 1145 with loss 1.847240686416626, accuracy 0.02083333395421505.\n",
      "Training epoch 5 batch 1146 with loss 1.9026342630386353, accuracy 0.02083333395421505.\n",
      "Training epoch 5 batch 1147 with loss 1.7160284519195557, accuracy 0.3333333432674408.\n",
      "Training epoch 5 batch 1148 with loss 1.8848650455474854, accuracy 0.14047619700431824.\n",
      "Training epoch 5 batch 1149 with loss 1.854225516319275, accuracy 0.236111119389534.\n",
      "Training epoch 5 batch 1150 with loss 1.7771275043487549, accuracy 0.2611111104488373.\n",
      "Training epoch 5 batch 1151 with loss 1.7928447723388672, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 1152 with loss 1.8263113498687744, accuracy 0.07500000298023224.\n",
      "Training epoch 5 batch 1153 with loss 1.778709053993225, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 1154 with loss 1.8519703149795532, accuracy 0.23333334922790527.\n",
      "Training epoch 5 batch 1155 with loss 1.8636671304702759, accuracy 0.02083333395421505.\n",
      "Training epoch 5 batch 1156 with loss 1.6703075170516968, accuracy 0.3194444477558136.\n",
      "Training epoch 5 batch 1157 with loss 1.7688604593276978, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 1158 with loss 1.7583478689193726, accuracy 0.23333334922790527.\n",
      "Training epoch 5 batch 1159 with loss 1.7585557699203491, accuracy 0.19166667759418488.\n",
      "Training epoch 5 batch 1160 with loss 1.6181526184082031, accuracy 0.3222222328186035.\n",
      "Training epoch 5 batch 1161 with loss 1.84275221824646, accuracy 0.12222222983837128.\n",
      "Training epoch 5 batch 1162 with loss 1.8649870157241821, accuracy 0.25.\n",
      "Training epoch 5 batch 1163 with loss 1.8136060237884521, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 1164 with loss 1.6583276987075806, accuracy 0.4146825671195984.\n",
      "Training epoch 5 batch 1165 with loss 1.7794673442840576, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1166 with loss 1.7866023778915405, accuracy 0.29722222685813904.\n",
      "Training epoch 5 batch 1167 with loss 1.9121030569076538, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 1168 with loss 1.7724101543426514, accuracy 0.21388888359069824.\n",
      "Training epoch 5 batch 1169 with loss 1.8539882898330688, accuracy 0.15833333134651184.\n",
      "Training epoch 5 batch 1170 with loss 1.6998538970947266, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1171 with loss 1.8575149774551392, accuracy 0.4000000059604645.\n",
      "Training epoch 5 batch 1172 with loss 1.7135894298553467, accuracy 0.42777779698371887.\n",
      "Training epoch 5 batch 1173 with loss 1.6875308752059937, accuracy 0.19722223281860352.\n",
      "Training epoch 5 batch 1174 with loss 1.727063775062561, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 1175 with loss 1.766748070716858, accuracy 0.125.\n",
      "Training epoch 5 batch 1176 with loss 1.8600146770477295, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 1177 with loss 1.836607575416565, accuracy 0.0416666679084301.\n",
      "Training epoch 5 batch 1178 with loss 1.7331771850585938, accuracy 0.31111112236976624.\n",
      "Training epoch 5 batch 1179 with loss 1.804869294166565, accuracy 0.07500000298023224.\n",
      "Training epoch 5 batch 1180 with loss 1.7349326610565186, accuracy 0.25.\n",
      "Training epoch 5 batch 1181 with loss 1.763281226158142, accuracy 0.24722221493721008.\n",
      "Training epoch 5 batch 1182 with loss 1.8092483282089233, accuracy 0.09880952537059784.\n",
      "Training epoch 5 batch 1183 with loss 1.728070616722107, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1184 with loss 1.8381679058074951, accuracy 0.0.\n",
      "Training epoch 5 batch 1185 with loss 1.7743651866912842, accuracy 0.20370370149612427.\n",
      "Training epoch 5 batch 1186 with loss 1.8082962036132812, accuracy 0.3194444477558136.\n",
      "Training epoch 5 batch 1187 with loss 1.7513954639434814, accuracy 0.25.\n",
      "Training epoch 5 batch 1188 with loss 1.8149840831756592, accuracy 0.07500000298023224.\n",
      "Training epoch 5 batch 1189 with loss 1.8541622161865234, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 1190 with loss 1.7947685718536377, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 1191 with loss 2.0353264808654785, accuracy 0.0.\n",
      "Training epoch 5 batch 1192 with loss 1.883971929550171, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 1193 with loss 1.7859516143798828, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1194 with loss 1.7176716327667236, accuracy 0.21388889849185944.\n",
      "Training epoch 5 batch 1195 with loss 1.6901003122329712, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 1196 with loss 1.7787704467773438, accuracy 0.08888889104127884.\n",
      "Training epoch 5 batch 1197 with loss 1.8269140720367432, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 1198 with loss 1.7599798440933228, accuracy 0.125.\n",
      "Training epoch 5 batch 1199 with loss 1.7981278896331787, accuracy 0.347222238779068.\n",
      "Training epoch 5 batch 1200 with loss 1.790998101234436, accuracy 0.2611111104488373.\n",
      "Training epoch 5 batch 1201 with loss 1.806302785873413, accuracy 0.22777777910232544.\n",
      "Training epoch 5 batch 1202 with loss 1.8103195428848267, accuracy 0.25555557012557983.\n",
      "Training epoch 5 batch 1203 with loss 1.8150103092193604, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1204 with loss 1.8428786993026733, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1205 with loss 1.6893457174301147, accuracy 0.2083333283662796.\n",
      "Training epoch 5 batch 1206 with loss 1.832948923110962, accuracy 0.065476194024086.\n",
      "Training epoch 5 batch 1207 with loss 1.901611089706421, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1208 with loss 1.8113181591033936, accuracy 0.1597222238779068.\n",
      "Training epoch 5 batch 1209 with loss 1.7016500234603882, accuracy 0.22777777910232544.\n",
      "Training epoch 5 batch 1210 with loss 1.878576636314392, accuracy 0.13055555522441864.\n",
      "Training epoch 5 batch 1211 with loss 1.8843803405761719, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1212 with loss 1.8443682193756104, accuracy 0.1031746044754982.\n",
      "Training epoch 5 batch 1213 with loss 1.8117210865020752, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1214 with loss 1.7125381231307983, accuracy 0.39722222089767456.\n",
      "Training epoch 5 batch 1215 with loss 1.7840449810028076, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 1216 with loss 1.8158270120620728, accuracy 0.3055555522441864.\n",
      "Training epoch 5 batch 1217 with loss 1.8008549213409424, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1218 with loss 1.7351099252700806, accuracy 0.2888889014720917.\n",
      "Training epoch 5 batch 1219 with loss 1.7189735174179077, accuracy 0.1805555522441864.\n",
      "Training epoch 5 batch 1220 with loss 1.823799729347229, accuracy 0.21388888359069824.\n",
      "Training epoch 5 batch 1221 with loss 1.8123563528060913, accuracy 0.18611110746860504.\n",
      "Training epoch 5 batch 1222 with loss 1.8845231533050537, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 1223 with loss 1.799804925918579, accuracy 0.22777777910232544.\n",
      "Training epoch 5 batch 1224 with loss 1.7819862365722656, accuracy 0.29523810744285583.\n",
      "Training epoch 5 batch 1225 with loss 1.6581296920776367, accuracy 0.3500000238418579.\n",
      "Training epoch 5 batch 1226 with loss 1.9027513265609741, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 1227 with loss 1.7585780620574951, accuracy 0.15833333134651184.\n",
      "Training epoch 5 batch 1228 with loss 1.8402706384658813, accuracy 0.09047619253396988.\n",
      "Training epoch 5 batch 1229 with loss 1.8366920948028564, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1230 with loss 1.7526600360870361, accuracy 0.3611111044883728.\n",
      "Training epoch 5 batch 1231 with loss 1.7101819515228271, accuracy 0.236111119389534.\n",
      "Training epoch 5 batch 1232 with loss 1.6819709539413452, accuracy 0.36250001192092896.\n",
      "Training epoch 5 batch 1233 with loss 1.8810920715332031, accuracy 0.08888889104127884.\n",
      "Training epoch 5 batch 1234 with loss 1.7296730279922485, accuracy 0.17777778208255768.\n",
      "Training epoch 5 batch 1235 with loss 1.643770456314087, accuracy 0.3222222328186035.\n",
      "Training epoch 5 batch 1236 with loss 1.8434587717056274, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 1237 with loss 1.9361772537231445, accuracy 0.2083333283662796.\n",
      "Training epoch 5 batch 1238 with loss 1.8694429397583008, accuracy 0.125.\n",
      "Training epoch 5 batch 1239 with loss 1.730889916419983, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 1240 with loss 1.7551205158233643, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 1241 with loss 1.8028723001480103, accuracy 0.10000000894069672.\n",
      "Training epoch 5 batch 1242 with loss 1.8175487518310547, accuracy 0.2738095223903656.\n",
      "Training epoch 5 batch 1243 with loss 1.8849132061004639, accuracy 0.2083333283662796.\n",
      "Training epoch 5 batch 1244 with loss 1.7996221780776978, accuracy 0.26944443583488464.\n",
      "Training epoch 5 batch 1245 with loss 1.6930593252182007, accuracy 0.4027777910232544.\n",
      "Training epoch 5 batch 1246 with loss 1.8041378259658813, accuracy 0.22500000894069672.\n",
      "Training epoch 5 batch 1247 with loss 1.7748949527740479, accuracy 0.31111112236976624.\n",
      "Training epoch 5 batch 1248 with loss 1.8569399118423462, accuracy 0.06111111491918564.\n",
      "Training epoch 5 batch 1249 with loss 1.8627828359603882, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 1250 with loss 1.791357398033142, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 1251 with loss 1.9035457372665405, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 1252 with loss 1.7132465839385986, accuracy 0.31111112236976624.\n",
      "Training epoch 5 batch 1253 with loss 1.7576744556427002, accuracy 0.1527777910232544.\n",
      "Training epoch 5 batch 1254 with loss 1.7722651958465576, accuracy 0.36666667461395264.\n",
      "Training epoch 5 batch 1255 with loss 1.8522504568099976, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 1256 with loss 1.7322628498077393, accuracy 0.2916666567325592.\n",
      "Training epoch 5 batch 1257 with loss 1.828131914138794, accuracy 0.2638888955116272.\n",
      "Training epoch 5 batch 1258 with loss 1.8623535633087158, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 1259 with loss 1.7827619314193726, accuracy 0.25555557012557983.\n",
      "Training epoch 5 batch 1260 with loss 1.7188777923583984, accuracy 0.23333333432674408.\n",
      "Training epoch 5 batch 1261 with loss 1.6758050918579102, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 1262 with loss 1.8155597448349, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 1263 with loss 1.8477483987808228, accuracy 0.06111111491918564.\n",
      "Training epoch 5 batch 1264 with loss 1.8366358280181885, accuracy 0.07500000298023224.\n",
      "Training epoch 5 batch 1265 with loss 1.7334951162338257, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 1266 with loss 1.8520746231079102, accuracy 0.10000000894069672.\n",
      "Training epoch 5 batch 1267 with loss 1.7505731582641602, accuracy 0.15555556118488312.\n",
      "Training epoch 5 batch 1268 with loss 1.7713056802749634, accuracy 0.2083333283662796.\n",
      "Training epoch 5 batch 1269 with loss 1.806753158569336, accuracy 0.0763888880610466.\n",
      "Training epoch 5 batch 1270 with loss 1.9464517831802368, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 1271 with loss 1.8272072076797485, accuracy 0.07500000298023224.\n",
      "Training epoch 5 batch 1272 with loss 1.7077449560165405, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 1273 with loss 1.796384572982788, accuracy 0.02777777798473835.\n",
      "Training epoch 5 batch 1274 with loss 1.8761078119277954, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 1275 with loss 1.719129204750061, accuracy 0.46666669845581055.\n",
      "Training epoch 5 batch 1276 with loss 1.8434488773345947, accuracy 0.15833334624767303.\n",
      "Training epoch 5 batch 1277 with loss 1.9513896703720093, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 1278 with loss 1.860913634300232, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1279 with loss 1.8367999792099, accuracy 0.26944443583488464.\n",
      "Training epoch 5 batch 1280 with loss 1.7621335983276367, accuracy 0.3611111044883728.\n",
      "Training epoch 5 batch 1281 with loss 1.8361961841583252, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1282 with loss 1.8415100574493408, accuracy 0.07407407462596893.\n",
      "Training epoch 5 batch 1283 with loss 1.8381245136260986, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 1284 with loss 1.7660176753997803, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 1285 with loss 1.9216978549957275, accuracy 0.0.\n",
      "Training epoch 5 batch 1286 with loss 1.8286120891571045, accuracy 0.02380952425301075.\n",
      "Training epoch 5 batch 1287 with loss 1.7998554706573486, accuracy 0.26250001788139343.\n",
      "Training epoch 5 batch 1288 with loss 1.8473554849624634, accuracy 0.13055555522441864.\n",
      "Training epoch 5 batch 1289 with loss 1.7712475061416626, accuracy 0.222222238779068.\n",
      "Training epoch 5 batch 1290 with loss 1.766577124595642, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1291 with loss 1.8740415573120117, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1292 with loss 1.8667608499526978, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1293 with loss 1.8893539905548096, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 1294 with loss 1.7894216775894165, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 1295 with loss 1.7901594638824463, accuracy 0.3541666865348816.\n",
      "Training epoch 5 batch 1296 with loss 1.937208890914917, accuracy 0.0.\n",
      "Training epoch 5 batch 1297 with loss 1.6976683139801025, accuracy 0.23055556416511536.\n",
      "Training epoch 5 batch 1298 with loss 1.8555721044540405, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 1299 with loss 1.8656431436538696, accuracy 0.125.\n",
      "Training epoch 5 batch 1300 with loss 1.810116171836853, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 1301 with loss 1.7424110174179077, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 1302 with loss 1.8652563095092773, accuracy 0.10833333432674408.\n",
      "Training epoch 5 batch 1303 with loss 1.700845718383789, accuracy 0.4027777910232544.\n",
      "Training epoch 5 batch 1304 with loss 1.8529733419418335, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 1305 with loss 1.835959792137146, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 1306 with loss 1.8360140323638916, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 1307 with loss 1.8443762063980103, accuracy 0.0416666679084301.\n",
      "Training epoch 5 batch 1308 with loss 1.9018865823745728, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 1309 with loss 1.6954708099365234, accuracy 0.4027777910232544.\n",
      "Training epoch 5 batch 1310 with loss 1.9180068969726562, accuracy 0.1319444477558136.\n",
      "Training epoch 5 batch 1311 with loss 1.851148247718811, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 1312 with loss 1.868337631225586, accuracy 0.0476190485060215.\n",
      "Training epoch 5 batch 1313 with loss 1.8724788427352905, accuracy 0.08888889104127884.\n",
      "Training epoch 5 batch 1314 with loss 1.7457635402679443, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 1315 with loss 1.811332106590271, accuracy 0.10833333432674408.\n",
      "Training epoch 5 batch 1316 with loss 1.815224289894104, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1317 with loss 1.8326832056045532, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 1318 with loss 1.7066810131072998, accuracy 0.4138889014720917.\n",
      "Training epoch 5 batch 1319 with loss 1.7355735301971436, accuracy 0.4027777910232544.\n",
      "Training epoch 5 batch 1320 with loss 1.765281319618225, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 1321 with loss 1.7630910873413086, accuracy 0.24166667461395264.\n",
      "Training epoch 5 batch 1322 with loss 1.857642412185669, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 1323 with loss 1.8215663433074951, accuracy 0.03333333507180214.\n",
      "Training epoch 5 batch 1324 with loss 1.7417875528335571, accuracy 0.2976190745830536.\n",
      "Training epoch 5 batch 1325 with loss 1.6559693813323975, accuracy 0.46666669845581055.\n",
      "Training epoch 5 batch 1326 with loss 1.759430170059204, accuracy 0.2599206566810608.\n",
      "Training epoch 5 batch 1327 with loss 1.8406946659088135, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 1328 with loss 1.6715024709701538, accuracy 0.20000000298023224.\n",
      "Training epoch 5 batch 1329 with loss 1.8406823873519897, accuracy 0.075396828353405.\n",
      "Training epoch 5 batch 1330 with loss 1.790854811668396, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 1331 with loss 1.8430917263031006, accuracy 0.0625.\n",
      "Training epoch 5 batch 1332 with loss 1.7631568908691406, accuracy 0.2888889014720917.\n",
      "Training epoch 5 batch 1333 with loss 1.806610107421875, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 1334 with loss 1.6604366302490234, accuracy 0.38333335518836975.\n",
      "Training epoch 5 batch 1335 with loss 1.842674970626831, accuracy 0.21944445371627808.\n",
      "Training epoch 5 batch 1336 with loss 1.688497543334961, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 1337 with loss 1.721323013305664, accuracy 0.375.\n",
      "Training epoch 5 batch 1338 with loss 1.7658014297485352, accuracy 0.125.\n",
      "Training epoch 5 batch 1339 with loss 1.8124122619628906, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 1340 with loss 1.8474308252334595, accuracy 0.0.\n",
      "Training epoch 5 batch 1341 with loss 1.7710225582122803, accuracy 0.20555555820465088.\n",
      "Training epoch 5 batch 1342 with loss 1.7298355102539062, accuracy 0.4277777671813965.\n",
      "Training epoch 5 batch 1343 with loss 1.8549350500106812, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1344 with loss 1.8761894702911377, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 1345 with loss 1.9215447902679443, accuracy 0.03333333507180214.\n",
      "Training epoch 5 batch 1346 with loss 1.8510338068008423, accuracy 0.07500000298023224.\n",
      "Training epoch 5 batch 1347 with loss 1.846430778503418, accuracy 0.17777778208255768.\n",
      "Training epoch 5 batch 1348 with loss 1.8074054718017578, accuracy 0.2638888955116272.\n",
      "Training epoch 5 batch 1349 with loss 1.882948875427246, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 1350 with loss 1.785352110862732, accuracy 0.10833333432674408.\n",
      "Training epoch 5 batch 1351 with loss 1.9288787841796875, accuracy 0.02380952425301075.\n",
      "Training epoch 5 batch 1352 with loss 1.8444488048553467, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 1353 with loss 1.8701750040054321, accuracy 0.03333333507180214.\n",
      "Training epoch 5 batch 1354 with loss 1.7557384967803955, accuracy 0.2083333283662796.\n",
      "Training epoch 5 batch 1355 with loss 1.8919956684112549, accuracy 0.1805555522441864.\n",
      "Training epoch 5 batch 1356 with loss 1.800386667251587, accuracy 0.1527777910232544.\n",
      "Training epoch 5 batch 1357 with loss 1.7737420797348022, accuracy 0.3888888955116272.\n",
      "Training epoch 5 batch 1358 with loss 1.8150714635849, accuracy 0.1805555522441864.\n",
      "Training epoch 5 batch 1359 with loss 1.8224337100982666, accuracy 0.25555557012557983.\n",
      "Training epoch 5 batch 1360 with loss 1.8281018733978271, accuracy 0.25555557012557983.\n",
      "Training epoch 5 batch 1361 with loss 1.867746114730835, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1362 with loss 1.8219677209854126, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 1363 with loss 1.864366888999939, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 1364 with loss 1.8194453716278076, accuracy 0.210317462682724.\n",
      "Training epoch 5 batch 1365 with loss 1.8532012701034546, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 1366 with loss 1.8162248134613037, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 1367 with loss 1.8297055959701538, accuracy 0.0694444477558136.\n",
      "Training epoch 5 batch 1368 with loss 1.8397340774536133, accuracy 0.08888889104127884.\n",
      "Training epoch 5 batch 1369 with loss 1.7586994171142578, accuracy 0.19166666269302368.\n",
      "Training epoch 5 batch 1370 with loss 1.8215444087982178, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 1371 with loss 1.7044423818588257, accuracy 0.2976190447807312.\n",
      "Training epoch 5 batch 1372 with loss 1.8556503057479858, accuracy 0.0476190485060215.\n",
      "Training epoch 5 batch 1373 with loss 1.8878984451293945, accuracy 0.10277777910232544.\n",
      "Training epoch 5 batch 1374 with loss 1.8370168209075928, accuracy 0.2805555462837219.\n",
      "Training epoch 5 batch 1375 with loss 1.7229993343353271, accuracy 0.13055555522441864.\n",
      "Training epoch 5 batch 1376 with loss 1.7316656112670898, accuracy 0.222222238779068.\n",
      "Training epoch 5 batch 1377 with loss 1.7688257694244385, accuracy 0.2638888955116272.\n",
      "Training epoch 5 batch 1378 with loss 1.8249543905258179, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 1379 with loss 1.8688409328460693, accuracy 0.06481481343507767.\n",
      "Training epoch 5 batch 1380 with loss 1.745727777481079, accuracy 0.2638888955116272.\n",
      "Training epoch 5 batch 1381 with loss 1.8645639419555664, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 1382 with loss 1.6746898889541626, accuracy 0.2133333384990692.\n",
      "Training epoch 5 batch 1383 with loss 1.7929617166519165, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 1384 with loss 1.8589502573013306, accuracy 0.130952388048172.\n",
      "Training epoch 5 batch 1385 with loss 1.8554785251617432, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 1386 with loss 1.8355993032455444, accuracy 0.17222222685813904.\n",
      "Training epoch 5 batch 1387 with loss 1.8483150005340576, accuracy 0.125.\n",
      "Training epoch 5 batch 1388 with loss 1.6883232593536377, accuracy 0.2916666567325592.\n",
      "Training epoch 5 batch 1389 with loss 1.8024327754974365, accuracy 0.3444444537162781.\n",
      "Training epoch 5 batch 1390 with loss 1.7700284719467163, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 1391 with loss 1.7339932918548584, accuracy 0.3583333492279053.\n",
      "Training epoch 5 batch 1392 with loss 1.7501853704452515, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 1393 with loss 1.784000039100647, accuracy 0.1944444477558136.\n",
      "Training epoch 5 batch 1394 with loss 1.7967582941055298, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 1395 with loss 1.7140964269638062, accuracy 0.4444444477558136.\n",
      "Training epoch 5 batch 1396 with loss 1.7575790882110596, accuracy 0.30000001192092896.\n",
      "Training epoch 5 batch 1397 with loss 1.7531774044036865, accuracy 0.35277777910232544.\n",
      "Training epoch 5 batch 1398 with loss 1.8655242919921875, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1399 with loss 1.7709178924560547, accuracy 0.29722222685813904.\n",
      "Training epoch 5 batch 1400 with loss 1.8433395624160767, accuracy 0.0.\n",
      "Training epoch 5 batch 1401 with loss 1.883507490158081, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1402 with loss 1.8635437488555908, accuracy 0.0416666679084301.\n",
      "Training epoch 5 batch 1403 with loss 1.8606691360473633, accuracy 0.09444444626569748.\n",
      "Training epoch 5 batch 1404 with loss 1.866273283958435, accuracy 0.065476194024086.\n",
      "Training epoch 5 batch 1405 with loss 1.8538858890533447, accuracy 0.236111119389534.\n",
      "Training epoch 5 batch 1406 with loss 1.761770248413086, accuracy 0.14166668057441711.\n",
      "Training epoch 5 batch 1407 with loss 1.8103755712509155, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 1408 with loss 1.822522759437561, accuracy 0.13055555522441864.\n",
      "Training epoch 5 batch 1409 with loss 1.8280109167099, accuracy 0.21984127163887024.\n",
      "Training epoch 5 batch 1410 with loss 1.8027360439300537, accuracy 0.22777777910232544.\n",
      "Training epoch 5 batch 1411 with loss 1.7287511825561523, accuracy 0.14444445073604584.\n",
      "Training epoch 5 batch 1412 with loss 1.849794626235962, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 1413 with loss 1.8522882461547852, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 1414 with loss 1.8758642673492432, accuracy 0.125.\n",
      "Training epoch 5 batch 1415 with loss 1.898876428604126, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1416 with loss 1.807676076889038, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 1417 with loss 1.7748744487762451, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1418 with loss 1.7996896505355835, accuracy 0.1805555522441864.\n",
      "Training epoch 5 batch 1419 with loss 1.7621822357177734, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 1420 with loss 1.781200647354126, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1421 with loss 1.7605565786361694, accuracy 0.125.\n",
      "Training epoch 5 batch 1422 with loss 1.8636547327041626, accuracy 0.125.\n",
      "Training epoch 5 batch 1423 with loss 1.8183238506317139, accuracy 0.125.\n",
      "Training epoch 5 batch 1424 with loss 1.724841833114624, accuracy 0.4652777910232544.\n",
      "Training epoch 5 batch 1425 with loss 1.8902666568756104, accuracy 0.02777777798473835.\n",
      "Training epoch 5 batch 1426 with loss 1.8462177515029907, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 1427 with loss 1.911665916442871, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1428 with loss 1.8199527263641357, accuracy 0.0694444477558136.\n",
      "Training epoch 5 batch 1429 with loss 1.8965747356414795, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 1430 with loss 1.8235464096069336, accuracy 0.0416666679084301.\n",
      "Training epoch 5 batch 1431 with loss 1.824737787246704, accuracy 0.0555555559694767.\n",
      "Training epoch 5 batch 1432 with loss 1.8482881784439087, accuracy 0.2916666567325592.\n",
      "Training epoch 5 batch 1433 with loss 1.7032406330108643, accuracy 0.19166667759418488.\n",
      "Training epoch 5 batch 1434 with loss 1.8658630847930908, accuracy 0.06018518656492233.\n",
      "Training epoch 5 batch 1435 with loss 1.751537561416626, accuracy 0.1458333432674408.\n",
      "Training epoch 5 batch 1436 with loss 1.8725545406341553, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1437 with loss 1.88242506980896, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 1438 with loss 1.8740469217300415, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 1439 with loss 1.7821919918060303, accuracy 0.125.\n",
      "Training epoch 5 batch 1440 with loss 1.7960097789764404, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 1441 with loss 1.8058803081512451, accuracy 0.21111111342906952.\n",
      "Training epoch 5 batch 1442 with loss 1.8360693454742432, accuracy 0.1111111119389534.\n",
      "Training epoch 5 batch 1443 with loss 1.9013640880584717, accuracy 0.02380952425301075.\n",
      "Training epoch 5 batch 1444 with loss 1.7743635177612305, accuracy 0.20555555820465088.\n",
      "Training epoch 5 batch 1445 with loss 1.826886773109436, accuracy 0.1488095223903656.\n",
      "Training epoch 5 batch 1446 with loss 1.7833507061004639, accuracy 0.22777779400348663.\n",
      "Training epoch 5 batch 1447 with loss 1.932705283164978, accuracy 0.06666667014360428.\n",
      "Training epoch 5 batch 1448 with loss 1.8056516647338867, accuracy 0.21388888359069824.\n",
      "Training epoch 5 batch 1449 with loss 1.7580511569976807, accuracy 0.2777777910232544.\n",
      "Training epoch 5 batch 1450 with loss 1.7973721027374268, accuracy 0.2611111104488373.\n",
      "Training epoch 5 batch 1451 with loss 1.6912075281143188, accuracy 0.31111112236976624.\n",
      "Training epoch 5 batch 1452 with loss 1.7775758504867554, accuracy 0.1805555671453476.\n",
      "Training epoch 5 batch 1453 with loss 1.6517530679702759, accuracy 0.2083333432674408.\n",
      "Training epoch 5 batch 1454 with loss 1.7166564464569092, accuracy 0.3166666626930237.\n",
      "Training epoch 5 batch 1455 with loss 1.8445236682891846, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1456 with loss 1.9524332284927368, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 1457 with loss 1.8419647216796875, accuracy 0.11666667461395264.\n",
      "Training epoch 5 batch 1458 with loss 1.7606561183929443, accuracy 0.236111119389534.\n",
      "Training epoch 5 batch 1459 with loss 1.8348448276519775, accuracy 0.07500000298023224.\n",
      "Training epoch 5 batch 1460 with loss 1.804451584815979, accuracy 0.1527777761220932.\n",
      "Training epoch 5 batch 1461 with loss 1.8739172220230103, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1462 with loss 1.830263376235962, accuracy 0.23888888955116272.\n",
      "Training epoch 5 batch 1463 with loss 1.8360275030136108, accuracy 0.02777777798473835.\n",
      "Training epoch 5 batch 1464 with loss 1.8177458047866821, accuracy 0.20555555820465088.\n",
      "Training epoch 5 batch 1465 with loss 1.8374319076538086, accuracy 0.03333333507180214.\n",
      "Training epoch 5 batch 1466 with loss 1.761936902999878, accuracy 0.25.\n",
      "Training epoch 5 batch 1467 with loss 1.8332030773162842, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 1468 with loss 1.8910669088363647, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1469 with loss 1.8262954950332642, accuracy 0.125.\n",
      "Training epoch 5 batch 1470 with loss 1.8599786758422852, accuracy 0.2750000059604645.\n",
      "Training epoch 5 batch 1471 with loss 1.890098214149475, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 1472 with loss 1.7445223331451416, accuracy 0.2388889044523239.\n",
      "Training epoch 5 batch 1473 with loss 1.7439210414886475, accuracy 0.2222222238779068.\n",
      "Training epoch 5 batch 1474 with loss 1.8295981884002686, accuracy 0.0416666679084301.\n",
      "Training epoch 5 batch 1475 with loss 1.8141428232192993, accuracy 0.14166666567325592.\n",
      "Training epoch 5 batch 1476 with loss 1.8517475128173828, accuracy 0.125.\n",
      "Training epoch 5 batch 1477 with loss 1.7289135456085205, accuracy 0.28333336114883423.\n",
      "Training epoch 5 batch 1478 with loss 1.916967749595642, accuracy 0.0416666679084301.\n",
      "Training epoch 5 batch 1479 with loss 1.770979881286621, accuracy 0.2888889014720917.\n",
      "Training epoch 5 batch 1480 with loss 1.7852205038070679, accuracy 0.1071428582072258.\n",
      "Training epoch 5 batch 1481 with loss 1.924443244934082, accuracy 0.255952388048172.\n",
      "Training epoch 5 batch 1482 with loss 1.8387229442596436, accuracy 0.13055555522441864.\n",
      "Training epoch 5 batch 1483 with loss 1.8287115097045898, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 1484 with loss 1.8633791208267212, accuracy 0.02777777798473835.\n",
      "Training epoch 5 batch 1485 with loss 1.8719991445541382, accuracy 0.1388888955116272.\n",
      "Training epoch 5 batch 1486 with loss 1.868713617324829, accuracy 0.02380952425301075.\n",
      "Training epoch 5 batch 1487 with loss 1.8270456790924072, accuracy 0.21388889849185944.\n",
      "Training epoch 5 batch 1488 with loss 1.8013871908187866, accuracy 0.1666666716337204.\n",
      "Training epoch 5 batch 1489 with loss 1.7986561059951782, accuracy 0.236111119389534.\n",
      "Training epoch 5 batch 1490 with loss 1.917392373085022, accuracy 0.15000000596046448.\n",
      "Training epoch 5 batch 1491 with loss 1.7388904094696045, accuracy 0.22103175520896912.\n",
      "Training epoch 5 batch 1492 with loss 1.7625869512557983, accuracy 0.15833334624767303.\n",
      "Training epoch 5 batch 1493 with loss 1.7654573917388916, accuracy 0.2420634925365448.\n",
      "Training epoch 5 batch 1494 with loss 1.9103237390518188, accuracy 0.0972222238779068.\n",
      "Training epoch 5 batch 1495 with loss 1.8343034982681274, accuracy 0.25.\n",
      "Training epoch 5 batch 1496 with loss 1.9005286693572998, accuracy 0.0833333358168602.\n",
      "Training epoch 5 batch 1497 with loss 1.791059136390686, accuracy 0.2638888955116272.\n",
      "Training epoch 5 batch 1498 with loss 1.939306616783142, accuracy 0.0476190485060215.\n",
      "Training epoch 5 batch 1499 with loss 1.710972547531128, accuracy 0.33888891339302063.\n",
      "Test batch 0 with loss 1.725572943687439 and accuracy 0.23055556416511536.\n",
      "Test batch 1 with loss 1.9477819204330444 and accuracy 0.0.\n",
      "Test batch 2 with loss 1.809570550918579 and accuracy 0.11666667461395264.\n",
      "Test batch 3 with loss 1.8717502355575562 and accuracy 0.25.\n",
      "Test batch 4 with loss 1.7555116415023804 and accuracy 0.2361111044883728.\n",
      "Test batch 5 with loss 1.7298085689544678 and accuracy 0.18611112236976624.\n",
      "Test batch 6 with loss 1.8563804626464844 and accuracy 0.1388888955116272.\n",
      "Test batch 7 with loss 1.8579623699188232 and accuracy 0.11666667461395264.\n",
      "Test batch 8 with loss 1.8025922775268555 and accuracy 0.14444445073604584.\n",
      "Test batch 9 with loss 1.8621292114257812 and accuracy 0.1736111044883728.\n",
      "Test batch 10 with loss 1.9040473699569702 and accuracy 0.05714286118745804.\n",
      "Test batch 11 with loss 1.9175560474395752 and accuracy 0.0892857164144516.\n",
      "Test batch 12 with loss 1.7614643573760986 and accuracy 0.42222222685813904.\n",
      "Test batch 13 with loss 1.8260629177093506 and accuracy 0.06666667014360428.\n",
      "Test batch 14 with loss 1.8072086572647095 and accuracy 0.25.\n",
      "Test batch 15 with loss 1.7078520059585571 and accuracy 0.3166666626930237.\n",
      "Test batch 16 with loss 1.8266956806182861 and accuracy 0.08888889104127884.\n",
      "Test batch 17 with loss 1.8787877559661865 and accuracy 0.02083333395421505.\n",
      "Test batch 18 with loss 1.816155195236206 and accuracy 0.1111111119389534.\n",
      "Test batch 19 with loss 1.849287986755371 and accuracy 0.2916666567325592.\n",
      "Test batch 20 with loss 1.7918975353240967 and accuracy 0.2361111044883728.\n",
      "Test batch 21 with loss 1.9109008312225342 and accuracy 0.06666667014360428.\n",
      "Test batch 22 with loss 1.881492257118225 and accuracy 0.0833333358168602.\n",
      "Test batch 23 with loss 1.8723459243774414 and accuracy 0.125.\n",
      "Test batch 24 with loss 1.7715637683868408 and accuracy 0.15436507761478424.\n",
      "Test batch 25 with loss 1.9334005117416382 and accuracy 0.12777778506278992.\n",
      "Test batch 26 with loss 1.8384771347045898 and accuracy 0.0416666679084301.\n",
      "Test batch 27 with loss 1.8770698308944702 and accuracy 0.0833333358168602.\n",
      "Test batch 28 with loss 1.8578742742538452 and accuracy 0.15833334624767303.\n",
      "Test batch 29 with loss 1.777971625328064 and accuracy 0.05714286118745804.\n",
      "Test batch 30 with loss 1.8167250156402588 and accuracy 0.02777777798473835.\n",
      "Test batch 31 with loss 1.800673246383667 and accuracy 0.2222222238779068.\n",
      "Test batch 32 with loss 1.8144209384918213 and accuracy 0.12777778506278992.\n",
      "Test batch 33 with loss 1.8214483261108398 and accuracy 0.1388888955116272.\n",
      "Test batch 34 with loss 1.849511742591858 and accuracy 0.07500000298023224.\n",
      "Test batch 35 with loss 1.926527738571167 and accuracy 0.02777777798473835.\n",
      "Test batch 36 with loss 1.782971739768982 and accuracy 0.1944444477558136.\n",
      "Test batch 37 with loss 1.8740723133087158 and accuracy 0.1944444477558136.\n",
      "Test batch 38 with loss 1.8760902881622314 and accuracy 0.2083333432674408.\n",
      "Test batch 39 with loss 1.973955512046814 and accuracy 0.0555555559694767.\n",
      "Test batch 40 with loss 1.8397595882415771 and accuracy 0.1071428656578064.\n",
      "Test batch 41 with loss 1.747259497642517 and accuracy 0.32500001788139343.\n",
      "Test batch 42 with loss 1.8502368927001953 and accuracy 0.0833333358168602.\n",
      "Test batch 43 with loss 1.8081516027450562 and accuracy 0.30000001192092896.\n",
      "Test batch 44 with loss 1.849463701248169 and accuracy 0.19166666269302368.\n",
      "Test batch 45 with loss 1.777280569076538 and accuracy 0.222222238779068.\n",
      "Test batch 46 with loss 1.8368864059448242 and accuracy 0.18611112236976624.\n",
      "Test batch 47 with loss 1.727384328842163 and accuracy 0.02380952425301075.\n",
      "Test batch 48 with loss 1.789693832397461 and accuracy 0.4158730208873749.\n",
      "Test batch 49 with loss 1.7930805683135986 and accuracy 0.1388888955116272.\n",
      "Test batch 50 with loss 1.895965337753296 and accuracy 0.1865079402923584.\n",
      "Test batch 51 with loss 1.8050167560577393 and accuracy 0.0972222238779068.\n",
      "Test batch 52 with loss 1.8182077407836914 and accuracy 0.30972224473953247.\n",
      "Test batch 53 with loss 1.7517938613891602 and accuracy 0.2043650895357132.\n",
      "Test batch 54 with loss 1.8829066753387451 and accuracy 0.0555555559694767.\n",
      "Test batch 55 with loss 1.842418909072876 and accuracy 0.21388889849185944.\n",
      "Test batch 56 with loss 1.7997982501983643 and accuracy 0.2611111104488373.\n",
      "Test batch 57 with loss 1.9174143075942993 and accuracy 0.1666666716337204.\n",
      "Test batch 58 with loss 1.866882562637329 and accuracy 0.1388888955116272.\n",
      "Test batch 59 with loss 1.7654736042022705 and accuracy 0.2222222238779068.\n",
      "Test batch 60 with loss 1.767632246017456 and accuracy 0.26944443583488464.\n",
      "Test batch 61 with loss 1.8275110721588135 and accuracy 0.1785714328289032.\n",
      "Test batch 62 with loss 1.895833969116211 and accuracy 0.0.\n",
      "Test batch 63 with loss 1.8285038471221924 and accuracy 0.125.\n",
      "Test batch 64 with loss 1.8231509923934937 and accuracy 0.1666666716337204.\n",
      "Test batch 65 with loss 1.7905718088150024 and accuracy 0.2321428656578064.\n",
      "Test batch 66 with loss 1.8511320352554321 and accuracy 0.125.\n",
      "Test batch 67 with loss 1.7560374736785889 and accuracy 0.17500001192092896.\n",
      "Test batch 68 with loss 1.7436689138412476 and accuracy 0.1527777761220932.\n",
      "Test batch 69 with loss 1.89723801612854 and accuracy 0.2083333432674408.\n",
      "Test batch 70 with loss 1.8444932699203491 and accuracy 0.15833334624767303.\n",
      "Test batch 71 with loss 1.782118558883667 and accuracy 0.2083333283662796.\n",
      "Test batch 72 with loss 1.793227195739746 and accuracy 0.1458333432674408.\n",
      "Test batch 73 with loss 1.7461531162261963 and accuracy 0.4333333373069763.\n",
      "Test batch 74 with loss 1.8554370403289795 and accuracy 0.09444444626569748.\n",
      "Test batch 75 with loss 1.7457889318466187 and accuracy 0.0833333358168602.\n",
      "Test batch 76 with loss 1.8665812015533447 and accuracy 0.25.\n",
      "Test batch 77 with loss 1.7415834665298462 and accuracy 0.1666666716337204.\n",
      "Test batch 78 with loss 1.8711259365081787 and accuracy 0.0.\n",
      "Test batch 79 with loss 1.8375041484832764 and accuracy 0.125.\n",
      "Test batch 80 with loss 1.8808467388153076 and accuracy 0.1865079402923584.\n",
      "Test batch 81 with loss 1.7802413702011108 and accuracy 0.1111111119389534.\n",
      "Test batch 82 with loss 1.9065395593643188 and accuracy 0.25.\n",
      "Test batch 83 with loss 1.7886247634887695 and accuracy 0.3958333730697632.\n",
      "Test batch 84 with loss 1.8974583148956299 and accuracy 0.1666666716337204.\n",
      "Test batch 85 with loss 1.710424780845642 and accuracy 0.2888889014720917.\n",
      "Test batch 86 with loss 1.7321815490722656 and accuracy 0.2708333432674408.\n",
      "Test batch 87 with loss 1.7737054824829102 and accuracy 0.3055555522441864.\n",
      "Test batch 88 with loss 1.8645150661468506 and accuracy 0.2361111044883728.\n",
      "Test batch 89 with loss 1.8418452739715576 and accuracy 0.2083333432674408.\n",
      "Test batch 90 with loss 1.8167450428009033 and accuracy 0.1805555522441864.\n",
      "Test batch 91 with loss 1.819318175315857 and accuracy 0.0892857164144516.\n",
      "Test batch 92 with loss 1.8084853887557983 and accuracy 0.125.\n",
      "Test batch 93 with loss 1.886993646621704 and accuracy 0.1527777761220932.\n",
      "Test batch 94 with loss 1.7770988941192627 and accuracy 0.222222238779068.\n",
      "Test batch 95 with loss 1.7459243535995483 and accuracy 0.3194444477558136.\n",
      "Test batch 96 with loss 1.8552591800689697 and accuracy 0.1111111119389534.\n",
      "Test batch 97 with loss 1.8092691898345947 and accuracy 0.222222238779068.\n",
      "Test batch 98 with loss 1.8706175088882446 and accuracy 0.11666666716337204.\n",
      "Test batch 99 with loss 1.8126331567764282 and accuracy 0.0416666679084301.\n",
      "Test batch 100 with loss 1.866506576538086 and accuracy 0.22777777910232544.\n",
      "Test batch 101 with loss 1.933733344078064 and accuracy 0.0.\n",
      "Test batch 102 with loss 1.8779802322387695 and accuracy 0.0555555559694767.\n",
      "Test batch 103 with loss 1.8770395517349243 and accuracy 0.2777777910232544.\n",
      "Test batch 104 with loss 1.7606052160263062 and accuracy 0.24166667461395264.\n",
      "Test batch 105 with loss 1.8132692575454712 and accuracy 0.08888889104127884.\n",
      "Test batch 106 with loss 1.7851454019546509 and accuracy 0.3222222328186035.\n",
      "Test batch 107 with loss 1.719156265258789 and accuracy 0.5638889074325562.\n",
      "Test batch 108 with loss 1.8799835443496704 and accuracy 0.1765872985124588.\n",
      "Test batch 109 with loss 1.815360426902771 and accuracy 0.3333333432674408.\n",
      "Test batch 110 with loss 1.7755393981933594 and accuracy 0.2916666567325592.\n",
      "Test batch 111 with loss 1.973415732383728 and accuracy 0.06666667014360428.\n",
      "Test batch 112 with loss 1.7435070276260376 and accuracy 0.28333333134651184.\n",
      "Test batch 113 with loss 1.887001633644104 and accuracy 0.1111111119389534.\n",
      "Test batch 114 with loss 1.8843399286270142 and accuracy 0.0833333358168602.\n",
      "Test batch 115 with loss 1.865576982498169 and accuracy 0.25.\n",
      "Test batch 116 with loss 1.8722741603851318 and accuracy 0.3055555522441864.\n",
      "Test batch 117 with loss 1.8745054006576538 and accuracy 0.03333333507180214.\n",
      "Test batch 118 with loss 1.8265399932861328 and accuracy 0.15833333134651184.\n",
      "Test batch 119 with loss 1.7802727222442627 and accuracy 0.1944444477558136.\n",
      "Test batch 120 with loss 1.7598752975463867 and accuracy 0.17222222685813904.\n",
      "Test batch 121 with loss 1.793368935585022 and accuracy 0.20000000298023224.\n",
      "Test batch 122 with loss 1.900568962097168 and accuracy 0.07500000298023224.\n",
      "Test batch 123 with loss 1.8424043655395508 and accuracy 0.111111119389534.\n",
      "Test batch 124 with loss 1.7942371368408203 and accuracy 0.15833333134651184.\n",
      "Test batch 125 with loss 1.8278157711029053 and accuracy 0.15833333134651184.\n",
      "Test batch 126 with loss 1.7220163345336914 and accuracy 0.32500001788139343.\n",
      "Test batch 127 with loss 2.0097784996032715 and accuracy 0.0555555559694767.\n",
      "Test batch 128 with loss 1.8377230167388916 and accuracy 0.125.\n",
      "Test batch 129 with loss 1.7260767221450806 and accuracy 0.2111111283302307.\n",
      "Test batch 130 with loss 1.8791887760162354 and accuracy 0.125.\n",
      "Test batch 131 with loss 1.84820556640625 and accuracy 0.1388888955116272.\n",
      "Test batch 132 with loss 1.844352126121521 and accuracy 0.02777777798473835.\n",
      "Test batch 133 with loss 1.7809803485870361 and accuracy 0.15000000596046448.\n",
      "Test batch 134 with loss 1.8005937337875366 and accuracy 0.0416666679084301.\n",
      "Test batch 135 with loss 1.7250463962554932 and accuracy 0.31666669249534607.\n",
      "Test batch 136 with loss 1.8231470584869385 and accuracy 0.3222222328186035.\n",
      "Test batch 137 with loss 1.858342170715332 and accuracy 0.14444445073604584.\n",
      "Test batch 138 with loss 1.8347667455673218 and accuracy 0.11666667461395264.\n",
      "Test batch 139 with loss 1.7744457721710205 and accuracy 0.130952388048172.\n",
      "Test batch 140 with loss 1.861742615699768 and accuracy 0.125.\n",
      "Test batch 141 with loss 1.8964189291000366 and accuracy 0.0972222238779068.\n",
      "Test batch 142 with loss 1.858709692955017 and accuracy 0.03333333507180214.\n",
      "Test batch 143 with loss 1.8520495891571045 and accuracy 0.1111111119389534.\n",
      "Test batch 144 with loss 1.7576805353164673 and accuracy 0.2611111104488373.\n",
      "Test batch 145 with loss 1.8090801239013672 and accuracy 0.2638888955116272.\n",
      "Test batch 146 with loss 1.6971914768218994 and accuracy 0.33000001311302185.\n",
      "Test batch 147 with loss 1.8779265880584717 and accuracy 0.0833333358168602.\n",
      "Test batch 148 with loss 1.881260633468628 and accuracy 0.2083333432674408.\n",
      "Test batch 149 with loss 1.8416074514389038 and accuracy 0.25555557012557983.\n",
      "Test batch 150 with loss 1.817138910293579 and accuracy 0.319444477558136.\n",
      "Test batch 151 with loss 1.9009456634521484 and accuracy 0.06666667014360428.\n",
      "Test batch 152 with loss 1.8461675643920898 and accuracy 0.14047619700431824.\n",
      "Test batch 153 with loss 1.8629744052886963 and accuracy 0.13750000298023224.\n",
      "Test batch 154 with loss 1.799182653427124 and accuracy 0.0555555559694767.\n",
      "Test batch 155 with loss 1.8834196329116821 and accuracy 0.03333333507180214.\n",
      "Test batch 156 with loss 1.857295036315918 and accuracy 0.1388888955116272.\n",
      "Test batch 157 with loss 1.923632025718689 and accuracy 0.25555557012557983.\n",
      "Test batch 158 with loss 1.8501325845718384 and accuracy 0.2083333283662796.\n",
      "Test batch 159 with loss 1.8021045923233032 and accuracy 0.0972222238779068.\n",
      "Test batch 160 with loss 1.8594074249267578 and accuracy 0.1111111119389534.\n",
      "Test batch 161 with loss 1.889685869216919 and accuracy 0.03333333507180214.\n",
      "Test batch 162 with loss 1.7528537511825562 and accuracy 0.0833333358168602.\n",
      "Test batch 163 with loss 1.8067028522491455 and accuracy 0.21388889849185944.\n",
      "Test batch 164 with loss 1.8510087728500366 and accuracy 0.277777761220932.\n",
      "Test batch 165 with loss 1.8264598846435547 and accuracy 0.15000000596046448.\n",
      "Test batch 166 with loss 1.8521207571029663 and accuracy 0.0833333358168602.\n",
      "Test batch 167 with loss 1.8245586156845093 and accuracy 0.1369047611951828.\n",
      "Test batch 168 with loss 1.8188196420669556 and accuracy 0.1865079402923584.\n",
      "Test batch 169 with loss 1.7068383693695068 and accuracy 0.3583333492279053.\n",
      "Test batch 170 with loss 1.8716856241226196 and accuracy 0.23333334922790527.\n",
      "Test batch 171 with loss 1.8378798961639404 and accuracy 0.1666666716337204.\n",
      "Test batch 172 with loss 1.841192603111267 and accuracy 0.4305555820465088.\n",
      "Test batch 173 with loss 1.7457361221313477 and accuracy 0.1944444477558136.\n",
      "Test batch 174 with loss 1.834413766860962 and accuracy 0.18968254327774048.\n",
      "Test batch 175 with loss 1.8218889236450195 and accuracy 0.125.\n",
      "Test batch 176 with loss 1.8135364055633545 and accuracy 0.0833333358168602.\n",
      "Test batch 177 with loss 1.789178490638733 and accuracy 0.1071428582072258.\n",
      "Test batch 178 with loss 1.9215558767318726 and accuracy 0.125.\n",
      "Test batch 179 with loss 1.8808202743530273 and accuracy 0.09259259700775146.\n",
      "Test batch 180 with loss 1.7723852396011353 and accuracy 0.19166667759418488.\n",
      "Test batch 181 with loss 1.8193775415420532 and accuracy 0.11269841343164444.\n",
      "Test batch 182 with loss 1.9040682315826416 and accuracy 0.03333333507180214.\n",
      "Test batch 183 with loss 1.968065619468689 and accuracy 0.14444445073604584.\n",
      "Test batch 184 with loss 1.7600347995758057 and accuracy 0.2638888955116272.\n",
      "Test batch 185 with loss 1.8734546899795532 and accuracy 0.065476194024086.\n",
      "Test batch 186 with loss 1.8654148578643799 and accuracy 0.02777777798473835.\n",
      "Test batch 187 with loss 1.7968404293060303 and accuracy 0.2916666567325592.\n",
      "Test batch 188 with loss 1.8250572681427002 and accuracy 0.1626984179019928.\n",
      "Test batch 189 with loss 1.8034391403198242 and accuracy 0.3333333432674408.\n",
      "Test batch 190 with loss 1.8569141626358032 and accuracy 0.2638888955116272.\n",
      "Test batch 191 with loss 1.851819396018982 and accuracy 0.3849206566810608.\n",
      "Test batch 192 with loss 1.8298124074935913 and accuracy 0.06666667014360428.\n",
      "Test batch 193 with loss 1.9051473140716553 and accuracy 0.02777777798473835.\n",
      "Test batch 194 with loss 1.8707691431045532 and accuracy 0.10000000149011612.\n",
      "Test batch 195 with loss 1.8398492336273193 and accuracy 0.17222222685813904.\n",
      "Test batch 196 with loss 1.788741111755371 and accuracy 0.1597222238779068.\n",
      "Test batch 197 with loss 1.8818695545196533 and accuracy 0.11666667461395264.\n",
      "Test batch 198 with loss 1.8870395421981812 and accuracy 0.1944444477558136.\n",
      "Test batch 199 with loss 1.7372173070907593 and accuracy 0.17222222685813904.\n",
      "Test batch 200 with loss 1.8648513555526733 and accuracy 0.1388888955116272.\n",
      "Test batch 201 with loss 1.7866798639297485 and accuracy 0.1785714328289032.\n",
      "Test batch 202 with loss 1.9077008962631226 and accuracy 0.23333334922790527.\n",
      "Test batch 203 with loss 1.8264386653900146 and accuracy 0.1805555671453476.\n",
      "Test batch 204 with loss 1.8646701574325562 and accuracy 0.284722238779068.\n",
      "Test batch 205 with loss 1.725741982460022 and accuracy 0.10000000894069672.\n",
      "Test batch 206 with loss 1.8626282215118408 and accuracy 0.24444445967674255.\n",
      "Test batch 207 with loss 1.7672345638275146 and accuracy 0.2916666865348816.\n",
      "Test batch 208 with loss 1.8596769571304321 and accuracy 0.2291666865348816.\n",
      "Test batch 209 with loss 1.8890354633331299 and accuracy 0.06666667014360428.\n",
      "Test batch 210 with loss 1.904598593711853 and accuracy 0.15000000596046448.\n",
      "Test batch 211 with loss 1.8519569635391235 and accuracy 0.11666667461395264.\n",
      "Test batch 212 with loss 1.8122934103012085 and accuracy 0.0972222238779068.\n",
      "Test batch 213 with loss 1.6990444660186768 and accuracy 0.23888888955116272.\n",
      "Test batch 214 with loss 1.8726654052734375 and accuracy 0.1388888955116272.\n",
      "Test batch 215 with loss 1.9018704891204834 and accuracy 0.0833333358168602.\n",
      "Test batch 216 with loss 1.7630655765533447 and accuracy 0.14444445073604584.\n",
      "Test batch 217 with loss 1.7793283462524414 and accuracy 0.2460317611694336.\n",
      "Test batch 218 with loss 1.7979503870010376 and accuracy 0.19722223281860352.\n",
      "Test batch 219 with loss 1.8251512050628662 and accuracy 0.21666666865348816.\n",
      "Test batch 220 with loss 1.9117799997329712 and accuracy 0.2083333432674408.\n",
      "Test batch 221 with loss 1.8047939538955688 and accuracy 0.2222222238779068.\n",
      "Test batch 222 with loss 1.8679711818695068 and accuracy 0.1031746044754982.\n",
      "Test batch 223 with loss 1.851790189743042 and accuracy 0.1319444477558136.\n",
      "Test batch 224 with loss 1.8384758234024048 and accuracy 0.0555555559694767.\n",
      "Test batch 225 with loss 1.9005028009414673 and accuracy 0.03333333507180214.\n",
      "Test batch 226 with loss 1.8444362878799438 and accuracy 0.130952388048172.\n",
      "Test batch 227 with loss 1.8176796436309814 and accuracy 0.2142857164144516.\n",
      "Test batch 228 with loss 1.8547465801239014 and accuracy 0.1488095223903656.\n",
      "Test batch 229 with loss 1.8899390697479248 and accuracy 0.1805555522441864.\n",
      "Test batch 230 with loss 1.7622623443603516 and accuracy 0.25.\n",
      "Test batch 231 with loss 1.7199771404266357 and accuracy 0.236111119389534.\n",
      "Test batch 232 with loss 1.8313716650009155 and accuracy 0.11666667461395264.\n",
      "Test batch 233 with loss 1.8780672550201416 and accuracy 0.03333333507180214.\n",
      "Test batch 234 with loss 1.843674898147583 and accuracy 0.1111111119389534.\n",
      "Test batch 235 with loss 1.7943388223648071 and accuracy 0.2888889014720917.\n",
      "Test batch 236 with loss 1.8422296047210693 and accuracy 0.2222222238779068.\n",
      "Test batch 237 with loss 1.7993316650390625 and accuracy 0.3194444477558136.\n",
      "Test batch 238 with loss 1.833827257156372 and accuracy 0.08888889104127884.\n",
      "Test batch 239 with loss 1.8568471670150757 and accuracy 0.0416666679084301.\n",
      "Test batch 240 with loss 1.7054808139801025 and accuracy 0.3888888955116272.\n",
      "Test batch 241 with loss 1.7929677963256836 and accuracy 0.21388888359069824.\n",
      "Test batch 242 with loss 1.9236888885498047 and accuracy 0.03333333507180214.\n",
      "Test batch 243 with loss 1.7676252126693726 and accuracy 0.2777777910232544.\n",
      "Test batch 244 with loss 1.8770917654037476 and accuracy 0.0.\n",
      "Test batch 245 with loss 1.8569542169570923 and accuracy 0.0476190485060215.\n",
      "Test batch 246 with loss 1.831165075302124 and accuracy 0.15000000596046448.\n",
      "Test batch 247 with loss 1.8288453817367554 and accuracy 0.16388888657093048.\n",
      "Test batch 248 with loss 1.888611078262329 and accuracy 0.0972222238779068.\n",
      "Test batch 249 with loss 1.8032982349395752 and accuracy 0.0833333358168602.\n",
      "Test batch 250 with loss 1.6957132816314697 and accuracy 0.2083333432674408.\n",
      "Test batch 251 with loss 1.8316494226455688 and accuracy 0.1388888955116272.\n",
      "Test batch 252 with loss 1.84433913230896 and accuracy 0.11666667461395264.\n",
      "Test batch 253 with loss 1.8518545627593994 and accuracy 0.17222222685813904.\n",
      "Test batch 254 with loss 1.6764377355575562 and accuracy 0.3958333432674408.\n",
      "Test batch 255 with loss 1.7276510000228882 and accuracy 0.20555555820465088.\n",
      "Test batch 256 with loss 1.7609145641326904 and accuracy 0.1388888955116272.\n",
      "Test batch 257 with loss 1.765014886856079 and accuracy 0.18888889253139496.\n",
      "Test batch 258 with loss 1.836580514907837 and accuracy 0.1805555671453476.\n",
      "Test batch 259 with loss 1.786077857017517 and accuracy 0.09444444626569748.\n",
      "Test batch 260 with loss 1.8181663751602173 and accuracy 0.2750000059604645.\n",
      "Test batch 261 with loss 1.7916953563690186 and accuracy 0.19166666269302368.\n",
      "Test batch 262 with loss 1.7651240825653076 and accuracy 0.12222222983837128.\n",
      "Test batch 263 with loss 1.6979354619979858 and accuracy 0.222222238779068.\n",
      "Test batch 264 with loss 1.8647762537002563 and accuracy 0.0833333358168602.\n",
      "Test batch 265 with loss 1.7932955026626587 and accuracy 0.13055555522441864.\n",
      "Test batch 266 with loss 1.810739278793335 and accuracy 0.1111111119389534.\n",
      "Test batch 267 with loss 1.7696517705917358 and accuracy 0.15530303120613098.\n",
      "Test batch 268 with loss 1.8659324645996094 and accuracy 0.22777777910232544.\n",
      "Test batch 269 with loss 1.796815276145935 and accuracy 0.130952388048172.\n",
      "Test batch 270 with loss 1.8084030151367188 and accuracy 0.1666666716337204.\n",
      "Test batch 271 with loss 1.8770520687103271 and accuracy 0.2222222238779068.\n",
      "Test batch 272 with loss 1.8605997562408447 and accuracy 0.0833333358168602.\n",
      "Test batch 273 with loss 1.8530349731445312 and accuracy 0.03333333507180214.\n",
      "Test batch 274 with loss 1.8016254901885986 and accuracy 0.1805555522441864.\n",
      "Test batch 275 with loss 1.789576768875122 and accuracy 0.1527777761220932.\n",
      "Test batch 276 with loss 1.8549363613128662 and accuracy 0.1666666716337204.\n",
      "Test batch 277 with loss 1.8696215152740479 and accuracy 0.10277777910232544.\n",
      "Test batch 278 with loss 1.8585697412490845 and accuracy 0.1527777761220932.\n",
      "Test batch 279 with loss 1.8042246103286743 and accuracy 0.1041666716337204.\n",
      "Test batch 280 with loss 1.8169050216674805 and accuracy 0.0.\n",
      "Test batch 281 with loss 1.8938995599746704 and accuracy 0.1388888955116272.\n",
      "Test batch 282 with loss 1.8515316247940063 and accuracy 0.10277777910232544.\n",
      "Test batch 283 with loss 1.8726098537445068 and accuracy 0.1666666716337204.\n",
      "Test batch 284 with loss 1.8003356456756592 and accuracy 0.125.\n",
      "Test batch 285 with loss 1.9222116470336914 and accuracy 0.02777777798473835.\n",
      "Test batch 286 with loss 1.8349281549453735 and accuracy 0.3166666626930237.\n",
      "Test batch 287 with loss 1.733909010887146 and accuracy 0.21944445371627808.\n",
      "Test batch 288 with loss 1.8867870569229126 and accuracy 0.0555555559694767.\n",
      "Test batch 289 with loss 1.824487328529358 and accuracy 0.10833333432674408.\n",
      "Test batch 290 with loss 1.7537168264389038 and accuracy 0.1388888955116272.\n",
      "Test batch 291 with loss 1.7843053340911865 and accuracy 0.02777777798473835.\n",
      "Test batch 292 with loss 1.9228309392929077 and accuracy 0.0.\n",
      "Test batch 293 with loss 1.7566198110580444 and accuracy 0.25.\n",
      "Test batch 294 with loss 1.8537594079971313 and accuracy 0.14444445073604584.\n",
      "Test batch 295 with loss 1.6926956176757812 and accuracy 0.35277777910232544.\n",
      "Test batch 296 with loss 1.8038747310638428 and accuracy 0.1666666716337204.\n",
      "Test batch 297 with loss 1.8027145862579346 and accuracy 0.1527777761220932.\n",
      "Test batch 298 with loss 1.7593437433242798 and accuracy 0.2013888955116272.\n",
      "Test batch 299 with loss 1.7598369121551514 and accuracy 0.20555555820465088.\n",
      "Test batch 300 with loss 1.738511323928833 and accuracy 0.3611111342906952.\n",
      "Test batch 301 with loss 1.8349632024765015 and accuracy 0.06666667014360428.\n",
      "Test batch 302 with loss 1.7993850708007812 and accuracy 0.1626984179019928.\n",
      "Test batch 303 with loss 1.7960822582244873 and accuracy 0.1041666716337204.\n",
      "Test batch 304 with loss 1.8614492416381836 and accuracy 0.1944444477558136.\n",
      "Test batch 305 with loss 1.8355087041854858 and accuracy 0.125.\n",
      "Test batch 306 with loss 1.9105920791625977 and accuracy 0.0833333358168602.\n",
      "Test batch 307 with loss 1.8084087371826172 and accuracy 0.20000000298023224.\n",
      "Test batch 308 with loss 1.8477624654769897 and accuracy 0.2361111044883728.\n",
      "Test batch 309 with loss 1.7963308095932007 and accuracy 0.11666667461395264.\n",
      "Test batch 310 with loss 1.8536062240600586 and accuracy 0.125.\n",
      "Test batch 311 with loss 1.8514991998672485 and accuracy 0.1944444477558136.\n",
      "Test batch 312 with loss 1.8151023387908936 and accuracy 0.2666666805744171.\n",
      "Test batch 313 with loss 1.831602692604065 and accuracy 0.11666667461395264.\n",
      "Test batch 314 with loss 1.7903320789337158 and accuracy 0.1319444477558136.\n",
      "Test batch 315 with loss 1.9327605962753296 and accuracy 0.0.\n",
      "Test batch 316 with loss 1.9481608867645264 and accuracy 0.0555555559694767.\n",
      "Test batch 317 with loss 1.8701181411743164 and accuracy 0.10833333432674408.\n",
      "Test batch 318 with loss 1.876225471496582 and accuracy 0.1666666716337204.\n",
      "Test batch 319 with loss 1.8707363605499268 and accuracy 0.03333333507180214.\n",
      "Test batch 320 with loss 1.7830549478530884 and accuracy 0.1626984179019928.\n",
      "Test batch 321 with loss 1.8168020248413086 and accuracy 0.125.\n",
      "Test batch 322 with loss 1.828574538230896 and accuracy 0.18611110746860504.\n",
      "Test batch 323 with loss 1.854058027267456 and accuracy 0.08888889104127884.\n",
      "Test batch 324 with loss 1.7565691471099854 and accuracy 0.3125.\n",
      "Test batch 325 with loss 1.8043839931488037 and accuracy 0.3055555820465088.\n",
      "Test batch 326 with loss 1.876121163368225 and accuracy 0.222222238779068.\n",
      "Test batch 327 with loss 1.8061965703964233 and accuracy 0.0972222238779068.\n",
      "Test batch 328 with loss 1.8332836627960205 and accuracy 0.20000001788139343.\n",
      "Test batch 329 with loss 1.856941819190979 and accuracy 0.0555555559694767.\n",
      "Test batch 330 with loss 1.6917701959609985 and accuracy 0.1944444477558136.\n",
      "Test batch 331 with loss 1.775024175643921 and accuracy 0.0833333358168602.\n",
      "Test batch 332 with loss 1.8157590627670288 and accuracy 0.1527777761220932.\n",
      "Test batch 333 with loss 1.8250362873077393 and accuracy 0.1597222238779068.\n",
      "Test batch 334 with loss 1.9041004180908203 and accuracy 0.20555555820465088.\n",
      "Test batch 335 with loss 1.9253829717636108 and accuracy 0.1666666716337204.\n",
      "Test batch 336 with loss 1.9776195287704468 and accuracy 0.1666666716337204.\n",
      "Test batch 337 with loss 1.8051742315292358 and accuracy 0.15555556118488312.\n",
      "Test batch 338 with loss 1.870822548866272 and accuracy 0.20000000298023224.\n",
      "Test batch 339 with loss 1.8892161846160889 and accuracy 0.0.\n",
      "Test batch 340 with loss 1.7225723266601562 and accuracy 0.1527777761220932.\n",
      "Test batch 341 with loss 1.9183671474456787 and accuracy 0.12222222983837128.\n",
      "Test batch 342 with loss 1.783553123474121 and accuracy 0.2916666865348816.\n",
      "Test batch 343 with loss 1.8417766094207764 and accuracy 0.20555557310581207.\n",
      "Test batch 344 with loss 1.8894367218017578 and accuracy 0.1111111119389534.\n",
      "Test batch 345 with loss 1.8133525848388672 and accuracy 0.1527777761220932.\n",
      "Test batch 346 with loss 1.918831467628479 and accuracy 0.07500000298023224.\n",
      "Test batch 347 with loss 1.8388019800186157 and accuracy 0.10833333432674408.\n",
      "Test batch 348 with loss 1.7896875143051147 and accuracy 0.13055555522441864.\n",
      "Test batch 349 with loss 1.8790487051010132 and accuracy 0.236111119389534.\n",
      "Test batch 350 with loss 1.8271548748016357 and accuracy 0.2420634925365448.\n",
      "Test batch 351 with loss 1.8235981464385986 and accuracy 0.0476190485060215.\n",
      "Test batch 352 with loss 1.8001117706298828 and accuracy 0.2083333432674408.\n",
      "Test batch 353 with loss 1.8470462560653687 and accuracy 0.125.\n",
      "Test batch 354 with loss 1.7859739065170288 and accuracy 0.17222222685813904.\n",
      "Test batch 355 with loss 1.8972088098526 and accuracy 0.1071428582072258.\n",
      "Test batch 356 with loss 1.7553832530975342 and accuracy 0.2083333432674408.\n",
      "Test batch 357 with loss 1.954106330871582 and accuracy 0.0833333358168602.\n",
      "Test batch 358 with loss 1.8367938995361328 and accuracy 0.0555555559694767.\n",
      "Test batch 359 with loss 1.8900444507598877 and accuracy 0.0555555559694767.\n",
      "Test batch 360 with loss 1.833196997642517 and accuracy 0.0892857164144516.\n",
      "Test batch 361 with loss 1.8397290706634521 and accuracy 0.125.\n",
      "Test batch 362 with loss 1.8619321584701538 and accuracy 0.0763888880610466.\n",
      "Test batch 363 with loss 1.8723245859146118 and accuracy 0.05714286118745804.\n",
      "Test batch 364 with loss 1.868410348892212 and accuracy 0.17222222685813904.\n",
      "Test batch 365 with loss 1.8318204879760742 and accuracy 0.1666666716337204.\n",
      "Test batch 366 with loss 1.853677749633789 and accuracy 0.11666667461395264.\n",
      "Test batch 367 with loss 1.7537193298339844 and accuracy 0.125.\n",
      "Test batch 368 with loss 1.7329356670379639 and accuracy 0.25.\n",
      "Test batch 369 with loss 1.7878694534301758 and accuracy 0.2460317462682724.\n",
      "Test batch 370 with loss 1.907836675643921 and accuracy 0.0555555559694767.\n",
      "Test batch 371 with loss 1.8053566217422485 and accuracy 0.39444446563720703.\n",
      "Test batch 372 with loss 1.8472833633422852 and accuracy 0.10833333432674408.\n",
      "Test batch 373 with loss 1.8971275091171265 and accuracy 0.08888889104127884.\n",
      "Test batch 374 with loss 1.8755426406860352 and accuracy 0.15000000596046448.\n",
      "Test batch 375 with loss 1.8725675344467163 and accuracy 0.0625.\n",
      "Test batch 376 with loss 1.8729660511016846 and accuracy 0.12222222983837128.\n",
      "Test batch 377 with loss 1.87483811378479 and accuracy 0.06666667014360428.\n",
      "Test batch 378 with loss 1.776055097579956 and accuracy 0.15000000596046448.\n",
      "Test batch 379 with loss 1.7369846105575562 and accuracy 0.30158731341362.\n",
      "Test batch 380 with loss 1.8917343616485596 and accuracy 0.2666666805744171.\n",
      "Test batch 381 with loss 1.818227767944336 and accuracy 0.0694444477558136.\n",
      "Test batch 382 with loss 1.9540621042251587 and accuracy 0.03333333507180214.\n",
      "Test batch 383 with loss 1.7319523096084595 and accuracy 0.5.\n",
      "Test batch 384 with loss 1.7650245428085327 and accuracy 0.3194444477558136.\n",
      "Test batch 385 with loss 1.8067843914031982 and accuracy 0.12222222983837128.\n",
      "Test batch 386 with loss 1.8664964437484741 and accuracy 0.10185185074806213.\n",
      "Test batch 387 with loss 1.7504936456680298 and accuracy 0.1944444477558136.\n",
      "Test batch 388 with loss 1.8927476406097412 and accuracy 0.0.\n",
      "Test batch 389 with loss 1.7963670492172241 and accuracy 0.20000000298023224.\n",
      "Test batch 390 with loss 1.8343347311019897 and accuracy 0.1944444477558136.\n",
      "Test batch 391 with loss 1.8561630249023438 and accuracy 0.29722222685813904.\n",
      "Test batch 392 with loss 1.760236382484436 and accuracy 0.16190476715564728.\n",
      "Test batch 393 with loss 1.8801921606063843 and accuracy 0.18333333730697632.\n",
      "Test batch 394 with loss 1.878827452659607 and accuracy 0.23333334922790527.\n",
      "Test batch 395 with loss 1.7924394607543945 and accuracy 0.0763888880610466.\n",
      "Test batch 396 with loss 1.7838947772979736 and accuracy 0.20555555820465088.\n",
      "Test batch 397 with loss 1.858268141746521 and accuracy 0.1388888955116272.\n",
      "Test batch 398 with loss 1.8629982471466064 and accuracy 0.1388888955116272.\n",
      "Test batch 399 with loss 1.7767635583877563 and accuracy 0.375.\n",
      "Test batch 400 with loss 1.79572331905365 and accuracy 0.1111111119389534.\n",
      "Test batch 401 with loss 1.8167259693145752 and accuracy 0.22777777910232544.\n",
      "Test batch 402 with loss 1.8295466899871826 and accuracy 0.11666667461395264.\n",
      "Test batch 403 with loss 1.8035471439361572 and accuracy 0.0555555559694767.\n",
      "Test batch 404 with loss 1.7252140045166016 and accuracy 0.36666667461395264.\n",
      "Test batch 405 with loss 1.7675079107284546 and accuracy 0.0972222238779068.\n",
      "Test batch 406 with loss 1.9254878759384155 and accuracy 0.1666666716337204.\n",
      "Test batch 407 with loss 1.7950499057769775 and accuracy 0.1944444477558136.\n",
      "Test batch 408 with loss 1.8170795440673828 and accuracy 0.18333333730697632.\n",
      "Test batch 409 with loss 1.8680200576782227 and accuracy 0.125.\n",
      "Test batch 410 with loss 1.8454437255859375 and accuracy 0.0694444477558136.\n",
      "Test batch 411 with loss 1.7648683786392212 and accuracy 0.2777777910232544.\n",
      "Test batch 412 with loss 1.8630164861679077 and accuracy 0.16388888657093048.\n",
      "Test batch 413 with loss 1.8082199096679688 and accuracy 0.12777778506278992.\n",
      "Test batch 414 with loss 1.8274824619293213 and accuracy 0.075396828353405.\n",
      "Test batch 415 with loss 1.8261092901229858 and accuracy 0.05416666716337204.\n",
      "Test batch 416 with loss 1.799666166305542 and accuracy 0.2460317462682724.\n",
      "Test batch 417 with loss 1.9315475225448608 and accuracy 0.0833333358168602.\n",
      "Test batch 418 with loss 1.9460906982421875 and accuracy 0.0972222238779068.\n",
      "Test batch 419 with loss 1.9819896221160889 and accuracy 0.1111111119389534.\n",
      "Test batch 420 with loss 1.7571547031402588 and accuracy 0.2888889014720917.\n",
      "Test batch 421 with loss 1.8378479480743408 and accuracy 0.190476194024086.\n",
      "Test batch 422 with loss 1.8094247579574585 and accuracy 0.22777777910232544.\n",
      "Test batch 423 with loss 1.7993638515472412 and accuracy 0.15833333134651184.\n",
      "Test batch 424 with loss 1.8501536846160889 and accuracy 0.07500000298023224.\n",
      "Test batch 425 with loss 1.9059299230575562 and accuracy 0.0416666679084301.\n",
      "Test batch 426 with loss 1.7520889043807983 and accuracy 0.3444444537162781.\n",
      "Test batch 427 with loss 1.9105207920074463 and accuracy 0.11666667461395264.\n",
      "Test batch 428 with loss 1.7008564472198486 and accuracy 0.2944444417953491.\n",
      "Test batch 429 with loss 1.9166275262832642 and accuracy 0.02777777798473835.\n",
      "Test batch 430 with loss 1.7279481887817383 and accuracy 0.1071428582072258.\n",
      "Test batch 431 with loss 1.8559017181396484 and accuracy 0.14444445073604584.\n",
      "Test batch 432 with loss 1.8043816089630127 and accuracy 0.125.\n",
      "Test batch 433 with loss 1.832256555557251 and accuracy 0.0833333358168602.\n",
      "Test batch 434 with loss 1.8609564304351807 and accuracy 0.2777777910232544.\n",
      "Test batch 435 with loss 1.851538062095642 and accuracy 0.0833333358168602.\n",
      "Test batch 436 with loss 1.7627732753753662 and accuracy 0.31111112236976624.\n",
      "Test batch 437 with loss 1.8427757024765015 and accuracy 0.2638888955116272.\n",
      "Test batch 438 with loss 1.784222960472107 and accuracy 0.23333333432674408.\n",
      "Test batch 439 with loss 1.980481743812561 and accuracy 0.02083333395421505.\n",
      "Test batch 440 with loss 1.8749338388442993 and accuracy 0.13333334028720856.\n",
      "Test batch 441 with loss 1.8613431453704834 and accuracy 0.2083333432674408.\n",
      "Test batch 442 with loss 1.866040825843811 and accuracy 0.2222222238779068.\n",
      "Test batch 443 with loss 1.762152910232544 and accuracy 0.1666666716337204.\n",
      "Test batch 444 with loss 1.893559217453003 and accuracy 0.06666667014360428.\n",
      "Test batch 445 with loss 1.751455307006836 and accuracy 0.1210317462682724.\n",
      "Test batch 446 with loss 1.7500684261322021 and accuracy 0.25555557012557983.\n",
      "Test batch 447 with loss 1.8388617038726807 and accuracy 0.02380952425301075.\n",
      "Test batch 448 with loss 1.8413231372833252 and accuracy 0.23333333432674408.\n",
      "Test batch 449 with loss 1.8422763347625732 and accuracy 0.0793650820851326.\n",
      "Test batch 450 with loss 1.7561213970184326 and accuracy 0.15000000596046448.\n",
      "Test batch 451 with loss 1.8793329000473022 and accuracy 0.25.\n",
      "Test batch 452 with loss 1.8752353191375732 and accuracy 0.06666667014360428.\n",
      "Test batch 453 with loss 1.705626130104065 and accuracy 0.39814817905426025.\n",
      "Test batch 454 with loss 1.845937728881836 and accuracy 0.0833333358168602.\n",
      "Test batch 455 with loss 1.8149192333221436 and accuracy 0.1805555522441864.\n",
      "Test batch 456 with loss 1.8357532024383545 and accuracy 0.1527777761220932.\n",
      "Test batch 457 with loss 1.8262412548065186 and accuracy 0.0.\n",
      "Test batch 458 with loss 1.752123475074768 and accuracy 0.3095238208770752.\n",
      "Test batch 459 with loss 1.9348055124282837 and accuracy 0.2916666865348816.\n",
      "Test batch 460 with loss 1.7865511178970337 and accuracy 0.35277777910232544.\n",
      "Test batch 461 with loss 1.8893406391143799 and accuracy 0.2083333432674408.\n",
      "Test batch 462 with loss 1.7866170406341553 and accuracy 0.125.\n",
      "Test batch 463 with loss 1.7480781078338623 and accuracy 0.0833333358168602.\n",
      "Test batch 464 with loss 1.790984869003296 and accuracy 0.1626984179019928.\n",
      "Test batch 465 with loss 1.7807590961456299 and accuracy 0.15555556118488312.\n",
      "Test batch 466 with loss 1.747382402420044 and accuracy 0.4444444477558136.\n",
      "Test batch 467 with loss 1.8651268482208252 and accuracy 0.0416666679084301.\n",
      "Test batch 468 with loss 1.7920297384262085 and accuracy 0.2888889014720917.\n",
      "Test batch 469 with loss 1.9237781763076782 and accuracy 0.0694444477558136.\n",
      "Test batch 470 with loss 1.8799302577972412 and accuracy 0.02777777798473835.\n",
      "Test batch 471 with loss 1.6882908344268799 and accuracy 0.4583333134651184.\n",
      "Test batch 472 with loss 1.8475453853607178 and accuracy 0.1527777761220932.\n",
      "Test batch 473 with loss 1.8248329162597656 and accuracy 0.1388888955116272.\n",
      "Test batch 474 with loss 1.7792314291000366 and accuracy 0.190476194024086.\n",
      "Test batch 475 with loss 1.799300193786621 and accuracy 0.125.\n",
      "Test batch 476 with loss 1.8381869792938232 and accuracy 0.125.\n",
      "Test batch 477 with loss 1.7650041580200195 and accuracy 0.25.\n",
      "Test batch 478 with loss 1.8412708044052124 and accuracy 0.1388888955116272.\n",
      "Test batch 479 with loss 1.8411016464233398 and accuracy 0.2083333432674408.\n",
      "Test batch 480 with loss 1.746185064315796 and accuracy 0.33194446563720703.\n",
      "Test batch 481 with loss 1.8752803802490234 and accuracy 0.1388888955116272.\n",
      "Test batch 482 with loss 1.9042961597442627 and accuracy 0.1666666716337204.\n",
      "Test batch 483 with loss 1.8220481872558594 and accuracy 0.24722222983837128.\n",
      "Test batch 484 with loss 1.7278187274932861 and accuracy 0.444444477558136.\n",
      "Test batch 485 with loss 1.910550832748413 and accuracy 0.08888889104127884.\n",
      "Test batch 486 with loss 1.8809268474578857 and accuracy 0.0833333358168602.\n",
      "Test batch 487 with loss 1.7503843307495117 and accuracy 0.2888889014720917.\n",
      "Test batch 488 with loss 1.7652899026870728 and accuracy 0.0416666679084301.\n",
      "Test batch 489 with loss 1.6655502319335938 and accuracy 0.2666666805744171.\n",
      "Test batch 490 with loss 1.7318954467773438 and accuracy 0.27222222089767456.\n",
      "Test batch 491 with loss 1.708507776260376 and accuracy 0.204365074634552.\n",
      "Test batch 492 with loss 1.899579644203186 and accuracy 0.08095238357782364.\n",
      "Test batch 493 with loss 1.7957727909088135 and accuracy 0.31666669249534607.\n",
      "Test batch 494 with loss 1.831202745437622 and accuracy 0.0625.\n",
      "Test batch 495 with loss 1.8286081552505493 and accuracy 0.07500000298023224.\n",
      "Test batch 496 with loss 1.890895128250122 and accuracy 0.03333333507180214.\n",
      "Test batch 497 with loss 1.8575875759124756 and accuracy 0.15833334624767303.\n",
      "Test batch 498 with loss 1.798957109451294 and accuracy 0.065476194024086.\n",
      "Test batch 499 with loss 1.8059766292572021 and accuracy 0.30317461490631104.\n",
      "Training epoch 6 batch 0 with loss 1.7814000844955444, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 1 with loss 1.7262275218963623, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 2 with loss 1.7628300189971924, accuracy 0.15833333134651184.\n",
      "Training epoch 6 batch 3 with loss 1.7795782089233398, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 4 with loss 1.6984895467758179, accuracy 0.3611111044883728.\n",
      "Training epoch 6 batch 5 with loss 1.8507665395736694, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 6 with loss 1.7245407104492188, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 7 with loss 1.7693674564361572, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 8 with loss 1.7461503744125366, accuracy 0.5277777910232544.\n",
      "Training epoch 6 batch 9 with loss 1.7112071514129639, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 10 with loss 1.8026260137557983, accuracy 0.2888889014720917.\n",
      "Training epoch 6 batch 11 with loss 1.714714765548706, accuracy 0.3166666626930237.\n",
      "Training epoch 6 batch 12 with loss 1.6615839004516602, accuracy 0.31111112236976624.\n",
      "Training epoch 6 batch 13 with loss 1.712039589881897, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 14 with loss 1.755164384841919, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 15 with loss 1.7442725896835327, accuracy 0.3194444477558136.\n",
      "Training epoch 6 batch 16 with loss 1.7101579904556274, accuracy 0.3305555582046509.\n",
      "Training epoch 6 batch 17 with loss 1.7581634521484375, accuracy 0.24166668951511383.\n",
      "Training epoch 6 batch 18 with loss 1.7552287578582764, accuracy 0.2321428656578064.\n",
      "Training epoch 6 batch 19 with loss 1.7891933917999268, accuracy 0.11269841343164444.\n",
      "Training epoch 6 batch 20 with loss 1.7090167999267578, accuracy 0.37222224473953247.\n",
      "Training epoch 6 batch 21 with loss 1.7874524593353271, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 22 with loss 1.7375633716583252, accuracy 0.20370370149612427.\n",
      "Training epoch 6 batch 23 with loss 1.7491748332977295, accuracy 0.12222222983837128.\n",
      "Training epoch 6 batch 24 with loss 1.711443543434143, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 25 with loss 1.763920783996582, accuracy 0.24444445967674255.\n",
      "Training epoch 6 batch 26 with loss 1.8371938467025757, accuracy 0.13055555522441864.\n",
      "Training epoch 6 batch 27 with loss 1.8580671548843384, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 28 with loss 1.7833579778671265, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 29 with loss 1.777465581893921, accuracy 0.222222238779068.\n",
      "Training epoch 6 batch 30 with loss 1.765657663345337, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 31 with loss 1.689353585243225, accuracy 0.2916666567325592.\n",
      "Training epoch 6 batch 32 with loss 1.7346057891845703, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 33 with loss 1.7587649822235107, accuracy 0.3916666805744171.\n",
      "Training epoch 6 batch 34 with loss 1.7035763263702393, accuracy 0.35277777910232544.\n",
      "Training epoch 6 batch 35 with loss 1.7890697717666626, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 36 with loss 1.7479112148284912, accuracy 0.1269841343164444.\n",
      "Training epoch 6 batch 37 with loss 1.8046716451644897, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 38 with loss 1.7085031270980835, accuracy 0.12222222983837128.\n",
      "Training epoch 6 batch 39 with loss 1.674077033996582, accuracy 0.4027777910232544.\n",
      "Training epoch 6 batch 40 with loss 1.695557951927185, accuracy 0.31111112236976624.\n",
      "Training epoch 6 batch 41 with loss 1.7641372680664062, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 42 with loss 1.7395130395889282, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 43 with loss 1.7362340688705444, accuracy 0.41388893127441406.\n",
      "Training epoch 6 batch 44 with loss 1.8106170892715454, accuracy 0.20370370149612427.\n",
      "Training epoch 6 batch 45 with loss 1.767888069152832, accuracy 0.236111119389534.\n",
      "Training epoch 6 batch 46 with loss 1.81826913356781, accuracy 0.06666667014360428.\n",
      "Training epoch 6 batch 47 with loss 1.7405033111572266, accuracy 0.2916666865348816.\n",
      "Training epoch 6 batch 48 with loss 1.763677954673767, accuracy 0.30000001192092896.\n",
      "Training epoch 6 batch 49 with loss 1.7731893062591553, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 50 with loss 1.6281945705413818, accuracy 0.444444477558136.\n",
      "Training epoch 6 batch 51 with loss 1.9070125818252563, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 52 with loss 1.7695391178131104, accuracy 0.10833333432674408.\n",
      "Training epoch 6 batch 53 with loss 1.6866852045059204, accuracy 0.2916666567325592.\n",
      "Training epoch 6 batch 54 with loss 1.6948286294937134, accuracy 0.24722221493721008.\n",
      "Training epoch 6 batch 55 with loss 1.7010606527328491, accuracy 0.3750000298023224.\n",
      "Training epoch 6 batch 56 with loss 1.8396928310394287, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 57 with loss 1.7640163898468018, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 58 with loss 1.7451961040496826, accuracy 0.23333334922790527.\n",
      "Training epoch 6 batch 59 with loss 1.7823333740234375, accuracy 0.21388889849185944.\n",
      "Training epoch 6 batch 60 with loss 1.7604954242706299, accuracy 0.4027777910232544.\n",
      "Training epoch 6 batch 61 with loss 1.776654601097107, accuracy 0.22777777910232544.\n",
      "Training epoch 6 batch 62 with loss 1.812145471572876, accuracy 0.28333333134651184.\n",
      "Training epoch 6 batch 63 with loss 1.7068331241607666, accuracy 0.1488095223903656.\n",
      "Training epoch 6 batch 64 with loss 1.7085812091827393, accuracy 0.20555555820465088.\n",
      "Training epoch 6 batch 65 with loss 1.601889967918396, accuracy 0.6666667461395264.\n",
      "Training epoch 6 batch 66 with loss 1.7784706354141235, accuracy 0.1547619104385376.\n",
      "Training epoch 6 batch 67 with loss 1.795244574546814, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 68 with loss 1.7136571407318115, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 69 with loss 1.7932403087615967, accuracy 0.13055555522441864.\n",
      "Training epoch 6 batch 70 with loss 1.6890928745269775, accuracy 0.4305555522441864.\n",
      "Training epoch 6 batch 71 with loss 1.7592302560806274, accuracy 0.3194444477558136.\n",
      "Training epoch 6 batch 72 with loss 1.8066952228546143, accuracy 0.4833333492279053.\n",
      "Training epoch 6 batch 73 with loss 1.775799036026001, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 74 with loss 1.8025583028793335, accuracy 0.236111119389534.\n",
      "Training epoch 6 batch 75 with loss 1.830797791481018, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 76 with loss 1.7522618770599365, accuracy 0.31111112236976624.\n",
      "Training epoch 6 batch 77 with loss 1.6609861850738525, accuracy 0.3444444537162781.\n",
      "Training epoch 6 batch 78 with loss 1.8475059270858765, accuracy 0.0694444477558136.\n",
      "Training epoch 6 batch 79 with loss 1.7771141529083252, accuracy 0.3333333134651184.\n",
      "Training epoch 6 batch 80 with loss 1.6429469585418701, accuracy 0.25833335518836975.\n",
      "Training epoch 6 batch 81 with loss 1.7487733364105225, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 82 with loss 1.680854082107544, accuracy 0.0714285746216774.\n",
      "Training epoch 6 batch 83 with loss 1.6289732456207275, accuracy 0.5527778267860413.\n",
      "Training epoch 6 batch 84 with loss 1.7158094644546509, accuracy 0.21944445371627808.\n",
      "Training epoch 6 batch 85 with loss 1.8217767477035522, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 86 with loss 1.753162145614624, accuracy 0.17222222685813904.\n",
      "Training epoch 6 batch 87 with loss 1.8176952600479126, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 88 with loss 1.7314313650131226, accuracy 0.21944445371627808.\n",
      "Training epoch 6 batch 89 with loss 1.7712510824203491, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 90 with loss 1.7661041021347046, accuracy 0.13055555522441864.\n",
      "Training epoch 6 batch 91 with loss 1.7522046566009521, accuracy 0.15555556118488312.\n",
      "Training epoch 6 batch 92 with loss 1.775177001953125, accuracy 0.06111111491918564.\n",
      "Training epoch 6 batch 93 with loss 1.7612178325653076, accuracy 0.4027777910232544.\n",
      "Training epoch 6 batch 94 with loss 1.690207839012146, accuracy 0.5166666507720947.\n",
      "Training epoch 6 batch 95 with loss 1.6968122720718384, accuracy 0.35555556416511536.\n",
      "Training epoch 6 batch 96 with loss 1.7547553777694702, accuracy 0.24166667461395264.\n",
      "Training epoch 6 batch 97 with loss 1.713722586631775, accuracy 0.3222222328186035.\n",
      "Training epoch 6 batch 98 with loss 1.7178680896759033, accuracy 0.27222222089767456.\n",
      "Training epoch 6 batch 99 with loss 1.8758926391601562, accuracy 0.08095238357782364.\n",
      "Training epoch 6 batch 100 with loss 1.7160533666610718, accuracy 0.3500000238418579.\n",
      "Training epoch 6 batch 101 with loss 1.8246068954467773, accuracy 0.30158731341362.\n",
      "Training epoch 6 batch 102 with loss 1.8749656677246094, accuracy 0.3333333432674408.\n",
      "Training epoch 6 batch 103 with loss 1.7749557495117188, accuracy 0.0515873022377491.\n",
      "Training epoch 6 batch 104 with loss 1.8525722026824951, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 105 with loss 1.7760127782821655, accuracy 0.2888889014720917.\n",
      "Training epoch 6 batch 106 with loss 1.7467424869537354, accuracy 0.125.\n",
      "Training epoch 6 batch 107 with loss 1.7997376918792725, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 108 with loss 1.7472597360610962, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 109 with loss 1.8313490152359009, accuracy 0.10833333432674408.\n",
      "Training epoch 6 batch 110 with loss 1.8408483266830444, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 111 with loss 1.7489213943481445, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 112 with loss 1.787980318069458, accuracy 0.2361111044883728.\n",
      "Training epoch 6 batch 113 with loss 1.8051693439483643, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 114 with loss 1.6599870920181274, accuracy 0.3033333420753479.\n",
      "Training epoch 6 batch 115 with loss 1.808990240097046, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 116 with loss 1.8849945068359375, accuracy 0.18611110746860504.\n",
      "Training epoch 6 batch 117 with loss 1.7365992069244385, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 118 with loss 1.8377206325531006, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 119 with loss 1.6776355504989624, accuracy 0.35277777910232544.\n",
      "Training epoch 6 batch 120 with loss 1.789372205734253, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 121 with loss 1.694989562034607, accuracy 0.375.\n",
      "Training epoch 6 batch 122 with loss 1.7441797256469727, accuracy 0.13055555522441864.\n",
      "Training epoch 6 batch 123 with loss 1.7851377725601196, accuracy 0.13055555522441864.\n",
      "Training epoch 6 batch 124 with loss 1.6437753438949585, accuracy 0.3499999940395355.\n",
      "Training epoch 6 batch 125 with loss 1.7913949489593506, accuracy 0.17222222685813904.\n",
      "Training epoch 6 batch 126 with loss 1.7331138849258423, accuracy 0.347222238779068.\n",
      "Training epoch 6 batch 127 with loss 1.8732045888900757, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 128 with loss 1.7895694971084595, accuracy 0.1626984179019928.\n",
      "Training epoch 6 batch 129 with loss 1.6525264978408813, accuracy 0.1686508059501648.\n",
      "Training epoch 6 batch 130 with loss 1.823650598526001, accuracy 0.06666667014360428.\n",
      "Training epoch 6 batch 131 with loss 1.6493237018585205, accuracy 0.4138889014720917.\n",
      "Training epoch 6 batch 132 with loss 1.6255409717559814, accuracy 0.3499999940395355.\n",
      "Training epoch 6 batch 133 with loss 1.690002679824829, accuracy 0.5138888955116272.\n",
      "Training epoch 6 batch 134 with loss 1.68234121799469, accuracy 0.5333333611488342.\n",
      "Training epoch 6 batch 135 with loss 1.7375491857528687, accuracy 0.2611111104488373.\n",
      "Training epoch 6 batch 136 with loss 1.6814825534820557, accuracy 0.2916666567325592.\n",
      "Training epoch 6 batch 137 with loss 1.9351587295532227, accuracy 0.0.\n",
      "Training epoch 6 batch 138 with loss 1.7177908420562744, accuracy 0.125.\n",
      "Training epoch 6 batch 139 with loss 1.7787020206451416, accuracy 0.11269842088222504.\n",
      "Training epoch 6 batch 140 with loss 1.8426563739776611, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 141 with loss 1.7838729619979858, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 142 with loss 1.6917479038238525, accuracy 0.2888889014720917.\n",
      "Training epoch 6 batch 143 with loss 1.7081167697906494, accuracy 0.2750000059604645.\n",
      "Training epoch 6 batch 144 with loss 1.7521007061004639, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 145 with loss 1.6944854259490967, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 146 with loss 1.6944574117660522, accuracy 0.30158731341362.\n",
      "Training epoch 6 batch 147 with loss 1.7895311117172241, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 148 with loss 1.690783143043518, accuracy 0.12222222983837128.\n",
      "Training epoch 6 batch 149 with loss 1.7194783687591553, accuracy 0.125.\n",
      "Training epoch 6 batch 150 with loss 1.877442717552185, accuracy 0.0793650820851326.\n",
      "Training epoch 6 batch 151 with loss 1.877440094947815, accuracy 0.27142858505249023.\n",
      "Training epoch 6 batch 152 with loss 1.7126251459121704, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 153 with loss 1.7119653224945068, accuracy 0.20555555820465088.\n",
      "Training epoch 6 batch 154 with loss 1.7237989902496338, accuracy 0.20555555820465088.\n",
      "Training epoch 6 batch 155 with loss 1.7007806301116943, accuracy 0.3055555820465088.\n",
      "Training epoch 6 batch 156 with loss 1.8248761892318726, accuracy 0.23888888955116272.\n",
      "Training epoch 6 batch 157 with loss 1.8062423467636108, accuracy 0.125.\n",
      "Training epoch 6 batch 158 with loss 1.7752147912979126, accuracy 0.12222222983837128.\n",
      "Training epoch 6 batch 159 with loss 1.6631224155426025, accuracy 0.4027777910232544.\n",
      "Training epoch 6 batch 160 with loss 1.7485296726226807, accuracy 0.3888888955116272.\n",
      "Training epoch 6 batch 161 with loss 1.6780351400375366, accuracy 0.4138889014720917.\n",
      "Training epoch 6 batch 162 with loss 1.8594621419906616, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 163 with loss 1.7940536737442017, accuracy 0.24166667461395264.\n",
      "Training epoch 6 batch 164 with loss 1.7024946212768555, accuracy 0.28333336114883423.\n",
      "Training epoch 6 batch 165 with loss 1.8611180782318115, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 166 with loss 1.6581039428710938, accuracy 0.25833332538604736.\n",
      "Training epoch 6 batch 167 with loss 1.789443016052246, accuracy 0.210317462682724.\n",
      "Training epoch 6 batch 168 with loss 1.6994997262954712, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 169 with loss 1.7748298645019531, accuracy 0.22777777910232544.\n",
      "Training epoch 6 batch 170 with loss 1.750361442565918, accuracy 0.22500000894069672.\n",
      "Training epoch 6 batch 171 with loss 1.8449128866195679, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 172 with loss 1.7151985168457031, accuracy 0.3583333492279053.\n",
      "Training epoch 6 batch 173 with loss 1.822283148765564, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 174 with loss 1.7323204278945923, accuracy 0.3333333432674408.\n",
      "Training epoch 6 batch 175 with loss 1.79327392578125, accuracy 0.22380952537059784.\n",
      "Training epoch 6 batch 176 with loss 1.8100550174713135, accuracy 0.15833333134651184.\n",
      "Training epoch 6 batch 177 with loss 1.7656053304672241, accuracy 0.2527777850627899.\n",
      "Training epoch 6 batch 178 with loss 1.7454456090927124, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 179 with loss 1.7440344095230103, accuracy 0.1805555671453476.\n",
      "Training epoch 6 batch 180 with loss 1.70005202293396, accuracy 0.3126984238624573.\n",
      "Training epoch 6 batch 181 with loss 1.6961557865142822, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 182 with loss 1.7977354526519775, accuracy 0.4777778089046478.\n",
      "Training epoch 6 batch 183 with loss 1.7412354946136475, accuracy 0.38055557012557983.\n",
      "Training epoch 6 batch 184 with loss 1.7518246173858643, accuracy 0.24166667461395264.\n",
      "Training epoch 6 batch 185 with loss 1.827549695968628, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 186 with loss 1.8383557796478271, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 187 with loss 1.7395967245101929, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 188 with loss 1.8497421741485596, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 189 with loss 1.6406276226043701, accuracy 0.4416666626930237.\n",
      "Training epoch 6 batch 190 with loss 1.7738544940948486, accuracy 0.1805555671453476.\n",
      "Training epoch 6 batch 191 with loss 1.7867403030395508, accuracy 0.1626984179019928.\n",
      "Training epoch 6 batch 192 with loss 1.9380857944488525, accuracy 0.03333333507180214.\n",
      "Training epoch 6 batch 193 with loss 1.7856642007827759, accuracy 0.2916666567325592.\n",
      "Training epoch 6 batch 194 with loss 1.8465461730957031, accuracy 0.15833333134651184.\n",
      "Training epoch 6 batch 195 with loss 1.7023357152938843, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 196 with loss 1.7897872924804688, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 197 with loss 1.7129764556884766, accuracy 0.2361111342906952.\n",
      "Training epoch 6 batch 198 with loss 1.8382165431976318, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 199 with loss 1.9179613590240479, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 200 with loss 1.8032102584838867, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 201 with loss 1.800992727279663, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 202 with loss 1.7893270254135132, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 203 with loss 1.7688140869140625, accuracy 0.25.\n",
      "Training epoch 6 batch 204 with loss 1.7741152048110962, accuracy 0.31805557012557983.\n",
      "Training epoch 6 batch 205 with loss 1.7231724262237549, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 206 with loss 1.7300927639007568, accuracy 0.3194444477558136.\n",
      "Training epoch 6 batch 207 with loss 1.7935664653778076, accuracy 0.24166668951511383.\n",
      "Training epoch 6 batch 208 with loss 1.6951357126235962, accuracy 0.6166666746139526.\n",
      "Training epoch 6 batch 209 with loss 1.686944603919983, accuracy 0.15740740299224854.\n",
      "Training epoch 6 batch 210 with loss 1.7534176111221313, accuracy 0.16388890147209167.\n",
      "Training epoch 6 batch 211 with loss 1.7974355220794678, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 212 with loss 1.6601976156234741, accuracy 0.4194444417953491.\n",
      "Training epoch 6 batch 213 with loss 1.7582252025604248, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 214 with loss 1.6479988098144531, accuracy 0.24722221493721008.\n",
      "Training epoch 6 batch 215 with loss 1.877699851989746, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 216 with loss 1.8158237934112549, accuracy 0.20158730447292328.\n",
      "Training epoch 6 batch 217 with loss 1.738417625427246, accuracy 0.17777778208255768.\n",
      "Training epoch 6 batch 218 with loss 1.7301172018051147, accuracy 0.2361111044883728.\n",
      "Training epoch 6 batch 219 with loss 1.621058464050293, accuracy 0.3777777850627899.\n",
      "Training epoch 6 batch 220 with loss 1.7644507884979248, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 221 with loss 1.7939798831939697, accuracy 0.28333333134651184.\n",
      "Training epoch 6 batch 222 with loss 1.8604274988174438, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 223 with loss 1.8010584115982056, accuracy 0.06111111491918564.\n",
      "Training epoch 6 batch 224 with loss 1.7441335916519165, accuracy 0.3472222089767456.\n",
      "Training epoch 6 batch 225 with loss 1.8206520080566406, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 226 with loss 1.7983983755111694, accuracy 0.13055555522441864.\n",
      "Training epoch 6 batch 227 with loss 1.8210210800170898, accuracy 0.2380952537059784.\n",
      "Training epoch 6 batch 228 with loss 1.8021596670150757, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 229 with loss 1.7955020666122437, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 230 with loss 1.753917932510376, accuracy 0.35317462682724.\n",
      "Training epoch 6 batch 231 with loss 1.8328977823257446, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 232 with loss 1.7068769931793213, accuracy 0.222222238779068.\n",
      "Training epoch 6 batch 233 with loss 1.7844184637069702, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 234 with loss 1.714979887008667, accuracy 0.3452380895614624.\n",
      "Training epoch 6 batch 235 with loss 1.796666145324707, accuracy 0.12222222983837128.\n",
      "Training epoch 6 batch 236 with loss 1.8222382068634033, accuracy 0.10277777910232544.\n",
      "Training epoch 6 batch 237 with loss 1.7708275318145752, accuracy 0.125.\n",
      "Training epoch 6 batch 238 with loss 1.70969557762146, accuracy 0.4305555522441864.\n",
      "Training epoch 6 batch 239 with loss 1.7939226627349854, accuracy 0.17222222685813904.\n",
      "Training epoch 6 batch 240 with loss 1.6656821966171265, accuracy 0.3861111104488373.\n",
      "Training epoch 6 batch 241 with loss 1.7794262170791626, accuracy 0.24444445967674255.\n",
      "Training epoch 6 batch 242 with loss 1.7227962017059326, accuracy 0.4126984179019928.\n",
      "Training epoch 6 batch 243 with loss 1.71480393409729, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 244 with loss 1.9070765972137451, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 245 with loss 1.7205593585968018, accuracy 0.23333334922790527.\n",
      "Training epoch 6 batch 246 with loss 1.8536665439605713, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 247 with loss 1.705672025680542, accuracy 0.2738095223903656.\n",
      "Training epoch 6 batch 248 with loss 1.7538751363754272, accuracy 0.25555554032325745.\n",
      "Training epoch 6 batch 249 with loss 1.799689531326294, accuracy 0.2460317462682724.\n",
      "Training epoch 6 batch 250 with loss 1.7775681018829346, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 251 with loss 1.7651493549346924, accuracy 0.29722222685813904.\n",
      "Training epoch 6 batch 252 with loss 1.7954018115997314, accuracy 0.23888888955116272.\n",
      "Training epoch 6 batch 253 with loss 1.8782150745391846, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 254 with loss 1.7449629306793213, accuracy 0.25.\n",
      "Training epoch 6 batch 255 with loss 1.9180517196655273, accuracy 0.03333333507180214.\n",
      "Training epoch 6 batch 256 with loss 1.6860921382904053, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 257 with loss 1.7495174407958984, accuracy 0.21111111342906952.\n",
      "Training epoch 6 batch 258 with loss 1.736096978187561, accuracy 0.3125.\n",
      "Training epoch 6 batch 259 with loss 1.7702150344848633, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 260 with loss 1.8035764694213867, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 261 with loss 1.7443615198135376, accuracy 0.25.\n",
      "Training epoch 6 batch 262 with loss 1.7160933017730713, accuracy 0.3444444537162781.\n",
      "Training epoch 6 batch 263 with loss 1.7531410455703735, accuracy 0.3432539701461792.\n",
      "Training epoch 6 batch 264 with loss 1.8763306140899658, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 265 with loss 1.8859522342681885, accuracy 0.16388890147209167.\n",
      "Training epoch 6 batch 266 with loss 1.7734653949737549, accuracy 0.12037037312984467.\n",
      "Training epoch 6 batch 267 with loss 1.8013675212860107, accuracy 0.1587301641702652.\n",
      "Training epoch 6 batch 268 with loss 1.747837781906128, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 269 with loss 1.8872261047363281, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 270 with loss 1.6747562885284424, accuracy 0.3115079402923584.\n",
      "Training epoch 6 batch 271 with loss 1.828334093093872, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 272 with loss 1.6835734844207764, accuracy 0.24166667461395264.\n",
      "Training epoch 6 batch 273 with loss 1.6462920904159546, accuracy 0.4555555582046509.\n",
      "Training epoch 6 batch 274 with loss 1.7681013345718384, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 275 with loss 1.7524160146713257, accuracy 0.2888889014720917.\n",
      "Training epoch 6 batch 276 with loss 1.737942099571228, accuracy 0.09583333134651184.\n",
      "Training epoch 6 batch 277 with loss 1.6867074966430664, accuracy 0.23888888955116272.\n",
      "Training epoch 6 batch 278 with loss 1.804105520248413, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 279 with loss 1.828352928161621, accuracy 0.236111119389534.\n",
      "Training epoch 6 batch 280 with loss 1.7676101922988892, accuracy 0.3166666626930237.\n",
      "Training epoch 6 batch 281 with loss 1.784550428390503, accuracy 0.3531745970249176.\n",
      "Training epoch 6 batch 282 with loss 1.7469838857650757, accuracy 0.125.\n",
      "Training epoch 6 batch 283 with loss 1.7972185611724854, accuracy 0.144841268658638.\n",
      "Training epoch 6 batch 284 with loss 1.7393226623535156, accuracy 0.130952388048172.\n",
      "Training epoch 6 batch 285 with loss 1.7672027349472046, accuracy 0.28928571939468384.\n",
      "Training epoch 6 batch 286 with loss 1.7557929754257202, accuracy 0.20000001788139343.\n",
      "Training epoch 6 batch 287 with loss 1.7623350620269775, accuracy 0.28333333134651184.\n",
      "Training epoch 6 batch 288 with loss 1.826770544052124, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 289 with loss 1.835858702659607, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 290 with loss 1.8121551275253296, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 291 with loss 1.7384071350097656, accuracy 0.2944444417953491.\n",
      "Training epoch 6 batch 292 with loss 1.7954301834106445, accuracy 0.1626984179019928.\n",
      "Training epoch 6 batch 293 with loss 1.769989252090454, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 294 with loss 1.8234186172485352, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 295 with loss 1.80287766456604, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 296 with loss 1.7090568542480469, accuracy 0.26944443583488464.\n",
      "Training epoch 6 batch 297 with loss 1.722378134727478, accuracy 0.18611112236976624.\n",
      "Training epoch 6 batch 298 with loss 1.7781810760498047, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 299 with loss 1.7704029083251953, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 300 with loss 1.7389370203018188, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 301 with loss 1.8429924249649048, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 302 with loss 1.881453275680542, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 303 with loss 1.6553226709365845, accuracy 0.375.\n",
      "Training epoch 6 batch 304 with loss 1.8129934072494507, accuracy 0.1587301641702652.\n",
      "Training epoch 6 batch 305 with loss 1.621636152267456, accuracy 0.5208333730697632.\n",
      "Training epoch 6 batch 306 with loss 1.6809289455413818, accuracy 0.40833333134651184.\n",
      "Training epoch 6 batch 307 with loss 1.7739578485488892, accuracy 0.21388889849185944.\n",
      "Training epoch 6 batch 308 with loss 1.7298109531402588, accuracy 0.17777778208255768.\n",
      "Training epoch 6 batch 309 with loss 1.8217235803604126, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 310 with loss 1.7798048257827759, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 311 with loss 1.868006706237793, accuracy 0.13055555522441864.\n",
      "Training epoch 6 batch 312 with loss 1.7840694189071655, accuracy 0.1349206417798996.\n",
      "Training epoch 6 batch 313 with loss 1.789576530456543, accuracy 0.25.\n",
      "Training epoch 6 batch 314 with loss 1.7513706684112549, accuracy 0.17777778208255768.\n",
      "Training epoch 6 batch 315 with loss 1.8539565801620483, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 316 with loss 1.8730976581573486, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 317 with loss 1.8383121490478516, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 318 with loss 1.827062964439392, accuracy 0.125.\n",
      "Training epoch 6 batch 319 with loss 1.885303258895874, accuracy 0.2361111044883728.\n",
      "Training epoch 6 batch 320 with loss 1.7840182781219482, accuracy 0.3214285671710968.\n",
      "Training epoch 6 batch 321 with loss 1.695870041847229, accuracy 0.2666666805744171.\n",
      "Training epoch 6 batch 322 with loss 1.863718032836914, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 323 with loss 1.7560218572616577, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 324 with loss 1.7880077362060547, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 325 with loss 1.7243684530258179, accuracy 0.25833332538604736.\n",
      "Training epoch 6 batch 326 with loss 1.7819464206695557, accuracy 0.40833336114883423.\n",
      "Training epoch 6 batch 327 with loss 1.669824242591858, accuracy 0.3777777850627899.\n",
      "Training epoch 6 batch 328 with loss 1.7721469402313232, accuracy 0.28703704476356506.\n",
      "Training epoch 6 batch 329 with loss 1.8453887701034546, accuracy 0.2750000059604645.\n",
      "Training epoch 6 batch 330 with loss 1.8053028583526611, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 331 with loss 1.7510658502578735, accuracy 0.4000000059604645.\n",
      "Training epoch 6 batch 332 with loss 1.8737024068832397, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 333 with loss 1.7866519689559937, accuracy 0.27000001072883606.\n",
      "Training epoch 6 batch 334 with loss 1.8453153371810913, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 335 with loss 1.7888091802597046, accuracy 0.22777777910232544.\n",
      "Training epoch 6 batch 336 with loss 1.6774955987930298, accuracy 0.3849206268787384.\n",
      "Training epoch 6 batch 337 with loss 1.715186357498169, accuracy 0.3166666626930237.\n",
      "Training epoch 6 batch 338 with loss 1.782907485961914, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 339 with loss 1.7195072174072266, accuracy 0.21111111342906952.\n",
      "Training epoch 6 batch 340 with loss 1.706986427307129, accuracy 0.3583333194255829.\n",
      "Training epoch 6 batch 341 with loss 1.7368742227554321, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 342 with loss 1.796972632408142, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 343 with loss 1.8764429092407227, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 344 with loss 1.786230444908142, accuracy 0.17222222685813904.\n",
      "Training epoch 6 batch 345 with loss 1.821030616760254, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 346 with loss 1.827429175376892, accuracy 0.1071428582072258.\n",
      "Training epoch 6 batch 347 with loss 1.6710033416748047, accuracy 0.3305555582046509.\n",
      "Training epoch 6 batch 348 with loss 1.7165429592132568, accuracy 0.0793650820851326.\n",
      "Training epoch 6 batch 349 with loss 1.823580026626587, accuracy 0.1587301641702652.\n",
      "Training epoch 6 batch 350 with loss 1.770324468612671, accuracy 0.16851851344108582.\n",
      "Training epoch 6 batch 351 with loss 1.7719247341156006, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 352 with loss 1.7054154872894287, accuracy 0.5.\n",
      "Training epoch 6 batch 353 with loss 1.7572400569915771, accuracy 0.21944445371627808.\n",
      "Training epoch 6 batch 354 with loss 1.8179528713226318, accuracy 0.3333333730697632.\n",
      "Training epoch 6 batch 355 with loss 1.729296326637268, accuracy 0.19722223281860352.\n",
      "Training epoch 6 batch 356 with loss 1.6925894021987915, accuracy 0.3777777850627899.\n",
      "Training epoch 6 batch 357 with loss 1.7494617700576782, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 358 with loss 1.8515514135360718, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 359 with loss 1.7282838821411133, accuracy 0.13333334028720856.\n",
      "Training epoch 6 batch 360 with loss 1.6808990240097046, accuracy 0.39444446563720703.\n",
      "Training epoch 6 batch 361 with loss 1.7991451025009155, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 362 with loss 1.8349355459213257, accuracy 0.125.\n",
      "Training epoch 6 batch 363 with loss 1.7777080535888672, accuracy 0.347222238779068.\n",
      "Training epoch 6 batch 364 with loss 1.853645920753479, accuracy 0.2380952537059784.\n",
      "Training epoch 6 batch 365 with loss 1.7024132013320923, accuracy 0.48055556416511536.\n",
      "Training epoch 6 batch 366 with loss 1.8778507709503174, accuracy 0.06666667014360428.\n",
      "Training epoch 6 batch 367 with loss 1.7887916564941406, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 368 with loss 1.8536226749420166, accuracy 0.2111111283302307.\n",
      "Training epoch 6 batch 369 with loss 1.7904083728790283, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 370 with loss 1.7936805486679077, accuracy 0.2142857164144516.\n",
      "Training epoch 6 batch 371 with loss 1.6889861822128296, accuracy 0.1924603283405304.\n",
      "Training epoch 6 batch 372 with loss 1.8375709056854248, accuracy 0.125.\n",
      "Training epoch 6 batch 373 with loss 1.7505676746368408, accuracy 0.3444444537162781.\n",
      "Training epoch 6 batch 374 with loss 1.685516595840454, accuracy 0.3194444477558136.\n",
      "Training epoch 6 batch 375 with loss 1.707261323928833, accuracy 0.2888889014720917.\n",
      "Training epoch 6 batch 376 with loss 1.806146264076233, accuracy 0.3222222328186035.\n",
      "Training epoch 6 batch 377 with loss 1.8088403940200806, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 378 with loss 1.8315585851669312, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 379 with loss 1.8786453008651733, accuracy 0.06666667014360428.\n",
      "Training epoch 6 batch 380 with loss 1.7511632442474365, accuracy 0.236111119389534.\n",
      "Training epoch 6 batch 381 with loss 1.795060396194458, accuracy 0.1944444626569748.\n",
      "Training epoch 6 batch 382 with loss 1.8362884521484375, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 383 with loss 1.7919946908950806, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 384 with loss 1.8978418111801147, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 385 with loss 1.822597861289978, accuracy 0.17222222685813904.\n",
      "Training epoch 6 batch 386 with loss 1.7780325412750244, accuracy 0.2916666865348816.\n",
      "Training epoch 6 batch 387 with loss 1.818129539489746, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 388 with loss 1.703204870223999, accuracy 0.3611111342906952.\n",
      "Training epoch 6 batch 389 with loss 1.8037264347076416, accuracy 0.2666666805744171.\n",
      "Training epoch 6 batch 390 with loss 1.8169517517089844, accuracy 0.1626984179019928.\n",
      "Training epoch 6 batch 391 with loss 1.8899171352386475, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 392 with loss 1.7183204889297485, accuracy 0.3083333373069763.\n",
      "Training epoch 6 batch 393 with loss 1.872166395187378, accuracy 0.1805555671453476.\n",
      "Training epoch 6 batch 394 with loss 1.629110336303711, accuracy 0.2925925850868225.\n",
      "Training epoch 6 batch 395 with loss 1.6859815120697021, accuracy 0.3333333432674408.\n",
      "Training epoch 6 batch 396 with loss 1.8990840911865234, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 397 with loss 1.7672555446624756, accuracy 0.2430555522441864.\n",
      "Training epoch 6 batch 398 with loss 1.740884780883789, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 399 with loss 1.729388952255249, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 400 with loss 1.7427809238433838, accuracy 0.3055555522441864.\n",
      "Training epoch 6 batch 401 with loss 1.8111164569854736, accuracy 0.23333334922790527.\n",
      "Training epoch 6 batch 402 with loss 1.7300466299057007, accuracy 0.25.\n",
      "Training epoch 6 batch 403 with loss 1.831920862197876, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 404 with loss 1.7793676853179932, accuracy 0.3263889253139496.\n",
      "Training epoch 6 batch 405 with loss 1.7498401403427124, accuracy 0.3333333432674408.\n",
      "Training epoch 6 batch 406 with loss 1.8501040935516357, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 407 with loss 1.7433468103408813, accuracy 0.35277777910232544.\n",
      "Training epoch 6 batch 408 with loss 1.8656737804412842, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 409 with loss 1.8442468643188477, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 410 with loss 1.8351364135742188, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 411 with loss 1.746633529663086, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 412 with loss 1.7226686477661133, accuracy 0.1865079402923584.\n",
      "Training epoch 6 batch 413 with loss 1.7500702142715454, accuracy 0.3194444477558136.\n",
      "Training epoch 6 batch 414 with loss 1.8299623727798462, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 415 with loss 1.8264821767807007, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 416 with loss 1.7395408153533936, accuracy 0.06666667014360428.\n",
      "Training epoch 6 batch 417 with loss 1.7614421844482422, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 418 with loss 1.781601905822754, accuracy 0.24166667461395264.\n",
      "Training epoch 6 batch 419 with loss 1.7383464574813843, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 420 with loss 1.712601900100708, accuracy 0.22777777910232544.\n",
      "Training epoch 6 batch 421 with loss 1.698838472366333, accuracy 0.5.\n",
      "Training epoch 6 batch 422 with loss 1.7198905944824219, accuracy 0.25.\n",
      "Training epoch 6 batch 423 with loss 1.7996680736541748, accuracy 0.20555555820465088.\n",
      "Training epoch 6 batch 424 with loss 1.7281179428100586, accuracy 0.5059523582458496.\n",
      "Training epoch 6 batch 425 with loss 1.751344084739685, accuracy 0.3333333432674408.\n",
      "Training epoch 6 batch 426 with loss 1.7587926387786865, accuracy 0.06666667014360428.\n",
      "Training epoch 6 batch 427 with loss 1.7515729665756226, accuracy 0.2361111044883728.\n",
      "Training epoch 6 batch 428 with loss 1.747794508934021, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 429 with loss 1.7464758157730103, accuracy 0.3055555522441864.\n",
      "Training epoch 6 batch 430 with loss 1.7564998865127563, accuracy 0.15833333134651184.\n",
      "Training epoch 6 batch 431 with loss 1.8336670398712158, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 432 with loss 1.817426323890686, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 433 with loss 1.8615005016326904, accuracy 0.10833333432674408.\n",
      "Training epoch 6 batch 434 with loss 1.7583287954330444, accuracy 0.35317462682724.\n",
      "Training epoch 6 batch 435 with loss 1.62704336643219, accuracy 0.2380952537059784.\n",
      "Training epoch 6 batch 436 with loss 1.8598295450210571, accuracy 0.24166667461395264.\n",
      "Training epoch 6 batch 437 with loss 1.8656638860702515, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 438 with loss 1.8016674518585205, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 439 with loss 1.795701026916504, accuracy 0.23333334922790527.\n",
      "Training epoch 6 batch 440 with loss 1.7281173467636108, accuracy 0.3888888955116272.\n",
      "Training epoch 6 batch 441 with loss 1.7240400314331055, accuracy 0.3055555820465088.\n",
      "Training epoch 6 batch 442 with loss 1.7626535892486572, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 443 with loss 1.7404718399047852, accuracy 0.3777777850627899.\n",
      "Training epoch 6 batch 444 with loss 1.7962102890014648, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 445 with loss 1.6906713247299194, accuracy 0.329365074634552.\n",
      "Training epoch 6 batch 446 with loss 1.731837511062622, accuracy 0.3115079402923584.\n",
      "Training epoch 6 batch 447 with loss 1.6930506229400635, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 448 with loss 1.749667763710022, accuracy 0.10000000894069672.\n",
      "Training epoch 6 batch 449 with loss 1.7090610265731812, accuracy 0.4404761791229248.\n",
      "Training epoch 6 batch 450 with loss 1.7839692831039429, accuracy 0.1319444477558136.\n",
      "Training epoch 6 batch 451 with loss 1.9173259735107422, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 452 with loss 1.7271287441253662, accuracy 0.25.\n",
      "Training epoch 6 batch 453 with loss 1.771851897239685, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 454 with loss 1.7195672988891602, accuracy 0.4027777910232544.\n",
      "Training epoch 6 batch 455 with loss 1.8742492198944092, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 456 with loss 1.7541189193725586, accuracy 0.24166667461395264.\n",
      "Training epoch 6 batch 457 with loss 1.7888450622558594, accuracy 0.16388888657093048.\n",
      "Training epoch 6 batch 458 with loss 1.7512394189834595, accuracy 0.29722222685813904.\n",
      "Training epoch 6 batch 459 with loss 1.7255808115005493, accuracy 0.29920634627342224.\n",
      "Training epoch 6 batch 460 with loss 1.7929967641830444, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 461 with loss 1.9230644702911377, accuracy 0.06111111491918564.\n",
      "Training epoch 6 batch 462 with loss 1.7390295267105103, accuracy 0.2916666567325592.\n",
      "Training epoch 6 batch 463 with loss 1.7741892337799072, accuracy 0.125.\n",
      "Training epoch 6 batch 464 with loss 1.7276277542114258, accuracy 0.31111112236976624.\n",
      "Training epoch 6 batch 465 with loss 1.8607044219970703, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 466 with loss 1.8321434259414673, accuracy 0.10833333432674408.\n",
      "Training epoch 6 batch 467 with loss 1.7371413707733154, accuracy 0.4027777910232544.\n",
      "Training epoch 6 batch 468 with loss 1.7830079793930054, accuracy 0.0892857164144516.\n",
      "Training epoch 6 batch 469 with loss 1.74970281124115, accuracy 0.36666667461395264.\n",
      "Training epoch 6 batch 470 with loss 1.7664934396743774, accuracy 0.3194444477558136.\n",
      "Training epoch 6 batch 471 with loss 1.7310459613800049, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 472 with loss 1.7520252466201782, accuracy 0.2611111104488373.\n",
      "Training epoch 6 batch 473 with loss 1.8979078531265259, accuracy 0.02380952425301075.\n",
      "Training epoch 6 batch 474 with loss 1.8215211629867554, accuracy 0.2599206268787384.\n",
      "Training epoch 6 batch 475 with loss 1.6929190158843994, accuracy 0.1825396865606308.\n",
      "Training epoch 6 batch 476 with loss 1.6736314296722412, accuracy 0.29722222685813904.\n",
      "Training epoch 6 batch 477 with loss 1.7516095638275146, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 478 with loss 1.7639669179916382, accuracy 0.14761905372142792.\n",
      "Training epoch 6 batch 479 with loss 1.8036998510360718, accuracy 0.15833333134651184.\n",
      "Training epoch 6 batch 480 with loss 1.6747586727142334, accuracy 0.36666667461395264.\n",
      "Training epoch 6 batch 481 with loss 1.7441976070404053, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 482 with loss 1.8476083278656006, accuracy 0.12222222983837128.\n",
      "Training epoch 6 batch 483 with loss 1.826850175857544, accuracy 0.1607142835855484.\n",
      "Training epoch 6 batch 484 with loss 1.7668812274932861, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 485 with loss 1.7420575618743896, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 486 with loss 1.851027250289917, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 487 with loss 1.826244592666626, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 488 with loss 1.7225492000579834, accuracy 0.06666667014360428.\n",
      "Training epoch 6 batch 489 with loss 1.7210420370101929, accuracy 0.375.\n",
      "Training epoch 6 batch 490 with loss 1.9067213535308838, accuracy 0.02777777798473835.\n",
      "Training epoch 6 batch 491 with loss 1.7671209573745728, accuracy 0.130952388048172.\n",
      "Training epoch 6 batch 492 with loss 1.814026117324829, accuracy 0.3333333432674408.\n",
      "Training epoch 6 batch 493 with loss 1.8191344738006592, accuracy 0.2003968358039856.\n",
      "Training epoch 6 batch 494 with loss 1.7012622356414795, accuracy 0.5.\n",
      "Training epoch 6 batch 495 with loss 1.7904808521270752, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 496 with loss 1.7142846584320068, accuracy 0.3055555522441864.\n",
      "Training epoch 6 batch 497 with loss 1.720131278038025, accuracy 0.2361111044883728.\n",
      "Training epoch 6 batch 498 with loss 1.876106858253479, accuracy 0.2361111044883728.\n",
      "Training epoch 6 batch 499 with loss 1.8100993633270264, accuracy 0.25.\n",
      "Training epoch 6 batch 500 with loss 1.988925576210022, accuracy 0.03333333507180214.\n",
      "Training epoch 6 batch 501 with loss 1.7250778675079346, accuracy 0.23888888955116272.\n",
      "Training epoch 6 batch 502 with loss 1.7150704860687256, accuracy 0.3611111044883728.\n",
      "Training epoch 6 batch 503 with loss 1.668601393699646, accuracy 0.44999998807907104.\n",
      "Training epoch 6 batch 504 with loss 1.79842209815979, accuracy 0.09444444626569748.\n",
      "Training epoch 6 batch 505 with loss 1.6869986057281494, accuracy 0.4722222089767456.\n",
      "Training epoch 6 batch 506 with loss 1.9124248027801514, accuracy 0.065476194024086.\n",
      "Training epoch 6 batch 507 with loss 1.6777769327163696, accuracy 0.324404776096344.\n",
      "Training epoch 6 batch 508 with loss 1.6976836919784546, accuracy 0.4849206209182739.\n",
      "Training epoch 6 batch 509 with loss 1.6728179454803467, accuracy 0.222222238779068.\n",
      "Training epoch 6 batch 510 with loss 1.8225113153457642, accuracy 0.14166666567325592.\n",
      "Training epoch 6 batch 511 with loss 1.7686240673065186, accuracy 0.25.\n",
      "Training epoch 6 batch 512 with loss 1.8986053466796875, accuracy 0.05416666716337204.\n",
      "Training epoch 6 batch 513 with loss 1.7866290807724, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 514 with loss 1.8531726598739624, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 515 with loss 1.8249706029891968, accuracy 0.1488095223903656.\n",
      "Training epoch 6 batch 516 with loss 1.6945221424102783, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 517 with loss 1.8299639225006104, accuracy 0.24722221493721008.\n",
      "Training epoch 6 batch 518 with loss 1.9035753011703491, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 519 with loss 1.749394178390503, accuracy 0.18518517911434174.\n",
      "Training epoch 6 batch 520 with loss 1.8281924724578857, accuracy 0.3142857253551483.\n",
      "Training epoch 6 batch 521 with loss 1.6387813091278076, accuracy 0.5126984119415283.\n",
      "Training epoch 6 batch 522 with loss 1.7750961780548096, accuracy 0.190476194024086.\n",
      "Training epoch 6 batch 523 with loss 1.7751410007476807, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 524 with loss 1.851772665977478, accuracy 0.13055555522441864.\n",
      "Training epoch 6 batch 525 with loss 1.7089178562164307, accuracy 0.2666666805744171.\n",
      "Training epoch 6 batch 526 with loss 1.7989814281463623, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 527 with loss 1.7927967309951782, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 528 with loss 1.8067543506622314, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 529 with loss 1.7253847122192383, accuracy 0.18333333730697632.\n",
      "Training epoch 6 batch 530 with loss 1.71843683719635, accuracy 0.2916666567325592.\n",
      "Training epoch 6 batch 531 with loss 1.7959696054458618, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 532 with loss 1.774462103843689, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 533 with loss 1.7292896509170532, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 534 with loss 1.750594139099121, accuracy 0.3055555522441864.\n",
      "Training epoch 6 batch 535 with loss 1.7309362888336182, accuracy 0.12222222983837128.\n",
      "Training epoch 6 batch 536 with loss 1.7501100301742554, accuracy 0.2142857313156128.\n",
      "Training epoch 6 batch 537 with loss 1.800244927406311, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 538 with loss 1.7854292392730713, accuracy 0.15833334624767303.\n",
      "Training epoch 6 batch 539 with loss 1.7889187335968018, accuracy 0.21388888359069824.\n",
      "Training epoch 6 batch 540 with loss 1.9611990451812744, accuracy 0.0.\n",
      "Training epoch 6 batch 541 with loss 1.5471625328063965, accuracy 0.3444444537162781.\n",
      "Training epoch 6 batch 542 with loss 1.743483543395996, accuracy 0.130952388048172.\n",
      "Training epoch 6 batch 543 with loss 1.7730375528335571, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 544 with loss 1.7944790124893188, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 545 with loss 1.7938915491104126, accuracy 0.3571428656578064.\n",
      "Training epoch 6 batch 546 with loss 1.7236320972442627, accuracy 0.3722222149372101.\n",
      "Training epoch 6 batch 547 with loss 1.7027816772460938, accuracy 0.4791666865348816.\n",
      "Training epoch 6 batch 548 with loss 1.7871557474136353, accuracy 0.125.\n",
      "Training epoch 6 batch 549 with loss 1.7635853290557861, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 550 with loss 1.8814233541488647, accuracy 0.05185185372829437.\n",
      "Training epoch 6 batch 551 with loss 1.7075996398925781, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 552 with loss 1.8506672382354736, accuracy 0.12222222983837128.\n",
      "Training epoch 6 batch 553 with loss 1.836704969406128, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 554 with loss 1.8148260116577148, accuracy 0.21944445371627808.\n",
      "Training epoch 6 batch 555 with loss 1.935961365699768, accuracy 0.236111119389534.\n",
      "Training epoch 6 batch 556 with loss 1.769628882408142, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 557 with loss 1.848454475402832, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 558 with loss 1.7799656391143799, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 559 with loss 1.8013070821762085, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 560 with loss 1.8490629196166992, accuracy 0.14166666567325592.\n",
      "Training epoch 6 batch 561 with loss 1.7026021480560303, accuracy 0.42222222685813904.\n",
      "Training epoch 6 batch 562 with loss 1.8516743183135986, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 563 with loss 1.8341299295425415, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 564 with loss 1.7258226871490479, accuracy 0.24722221493721008.\n",
      "Training epoch 6 batch 565 with loss 1.7507673501968384, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 566 with loss 1.6834077835083008, accuracy 0.2805555760860443.\n",
      "Training epoch 6 batch 567 with loss 1.856244683265686, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 568 with loss 1.8291778564453125, accuracy 0.111111119389534.\n",
      "Training epoch 6 batch 569 with loss 1.7365138530731201, accuracy 0.2013888955116272.\n",
      "Training epoch 6 batch 570 with loss 1.8492958545684814, accuracy 0.15555556118488312.\n",
      "Training epoch 6 batch 571 with loss 1.801740050315857, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 572 with loss 1.8350763320922852, accuracy 0.12222222983837128.\n",
      "Training epoch 6 batch 573 with loss 1.7207376956939697, accuracy 0.24166667461395264.\n",
      "Training epoch 6 batch 574 with loss 1.8074668645858765, accuracy 0.1349206417798996.\n",
      "Training epoch 6 batch 575 with loss 1.7243616580963135, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 576 with loss 1.7483465671539307, accuracy 0.15079365670681.\n",
      "Training epoch 6 batch 577 with loss 1.7402336597442627, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 578 with loss 1.6804136037826538, accuracy 0.2797619104385376.\n",
      "Training epoch 6 batch 579 with loss 1.9113423824310303, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 580 with loss 1.679195761680603, accuracy 0.3452380895614624.\n",
      "Training epoch 6 batch 581 with loss 1.791961431503296, accuracy 0.23888888955116272.\n",
      "Training epoch 6 batch 582 with loss 1.6989479064941406, accuracy 0.375.\n",
      "Training epoch 6 batch 583 with loss 1.8249174356460571, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 584 with loss 1.7866233587265015, accuracy 0.25.\n",
      "Training epoch 6 batch 585 with loss 1.7531578540802002, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 586 with loss 1.79421865940094, accuracy 0.10833333432674408.\n",
      "Training epoch 6 batch 587 with loss 1.6821346282958984, accuracy 0.3444444537162781.\n",
      "Training epoch 6 batch 588 with loss 1.818861722946167, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 589 with loss 1.8049894571304321, accuracy 0.236111119389534.\n",
      "Training epoch 6 batch 590 with loss 1.8123441934585571, accuracy 0.0694444477558136.\n",
      "Training epoch 6 batch 591 with loss 1.7968460321426392, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 592 with loss 1.7365598678588867, accuracy 0.18888889253139496.\n",
      "Training epoch 6 batch 593 with loss 1.759272813796997, accuracy 0.409722238779068.\n",
      "Training epoch 6 batch 594 with loss 1.812822937965393, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 595 with loss 1.8209364414215088, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 596 with loss 1.8354854583740234, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 597 with loss 1.749910593032837, accuracy 0.2611111104488373.\n",
      "Training epoch 6 batch 598 with loss 1.8316110372543335, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 599 with loss 1.7492501735687256, accuracy 0.3055555522441864.\n",
      "Training epoch 6 batch 600 with loss 1.722461462020874, accuracy 0.255952388048172.\n",
      "Training epoch 6 batch 601 with loss 1.7508920431137085, accuracy 0.190476194024086.\n",
      "Training epoch 6 batch 602 with loss 1.79690682888031, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 603 with loss 1.7502973079681396, accuracy 0.12857143580913544.\n",
      "Training epoch 6 batch 604 with loss 1.8912866115570068, accuracy 0.25.\n",
      "Training epoch 6 batch 605 with loss 1.7124178409576416, accuracy 0.2480158656835556.\n",
      "Training epoch 6 batch 606 with loss 1.7797868251800537, accuracy 0.25.\n",
      "Training epoch 6 batch 607 with loss 1.8725963830947876, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 608 with loss 1.8146374225616455, accuracy 0.222222238779068.\n",
      "Training epoch 6 batch 609 with loss 1.8601669073104858, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 610 with loss 1.7705990076065063, accuracy 0.18333333730697632.\n",
      "Training epoch 6 batch 611 with loss 1.7050597667694092, accuracy 0.4583333432674408.\n",
      "Training epoch 6 batch 612 with loss 1.7793073654174805, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 613 with loss 1.7557929754257202, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 614 with loss 1.7495800256729126, accuracy 0.3194444477558136.\n",
      "Training epoch 6 batch 615 with loss 1.647028923034668, accuracy 0.1865079402923584.\n",
      "Training epoch 6 batch 616 with loss 1.8158466815948486, accuracy 0.125.\n",
      "Training epoch 6 batch 617 with loss 1.726737380027771, accuracy 0.23333334922790527.\n",
      "Training epoch 6 batch 618 with loss 1.83501398563385, accuracy 0.21944445371627808.\n",
      "Training epoch 6 batch 619 with loss 1.8335716724395752, accuracy 0.03333333507180214.\n",
      "Training epoch 6 batch 620 with loss 1.6659133434295654, accuracy 0.2847222089767456.\n",
      "Training epoch 6 batch 621 with loss 1.7902946472167969, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 622 with loss 1.8296695947647095, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 623 with loss 1.908850908279419, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 624 with loss 1.688162088394165, accuracy 0.3777777850627899.\n",
      "Training epoch 6 batch 625 with loss 1.8337312936782837, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 626 with loss 1.72808039188385, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 627 with loss 1.7590882778167725, accuracy 0.3125.\n",
      "Training epoch 6 batch 628 with loss 1.726607322692871, accuracy 0.10000000894069672.\n",
      "Training epoch 6 batch 629 with loss 1.7715297937393188, accuracy 0.22777777910232544.\n",
      "Training epoch 6 batch 630 with loss 1.7589304447174072, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 631 with loss 1.8507407903671265, accuracy 0.17777778208255768.\n",
      "Training epoch 6 batch 632 with loss 1.7370684146881104, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 633 with loss 1.7653175592422485, accuracy 0.3444444537162781.\n",
      "Training epoch 6 batch 634 with loss 1.7988983392715454, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 635 with loss 1.7659862041473389, accuracy 0.28333333134651184.\n",
      "Training epoch 6 batch 636 with loss 1.8079122304916382, accuracy 0.17777778208255768.\n",
      "Training epoch 6 batch 637 with loss 1.7453705072402954, accuracy 0.1805555671453476.\n",
      "Training epoch 6 batch 638 with loss 1.8139371871948242, accuracy 0.06111111491918564.\n",
      "Training epoch 6 batch 639 with loss 1.798734426498413, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 640 with loss 1.7230708599090576, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 641 with loss 1.7977001667022705, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 642 with loss 1.8072550296783447, accuracy 0.236111119389534.\n",
      "Training epoch 6 batch 643 with loss 1.865387201309204, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 644 with loss 1.7554868459701538, accuracy 0.3055555820465088.\n",
      "Training epoch 6 batch 645 with loss 1.690972924232483, accuracy 0.2361111044883728.\n",
      "Training epoch 6 batch 646 with loss 1.8874133825302124, accuracy 0.2916666865348816.\n",
      "Training epoch 6 batch 647 with loss 1.7297245264053345, accuracy 0.4333333373069763.\n",
      "Training epoch 6 batch 648 with loss 1.8383591175079346, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 649 with loss 1.770444631576538, accuracy 0.19603174924850464.\n",
      "Training epoch 6 batch 650 with loss 1.8881868124008179, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 651 with loss 1.7583224773406982, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 652 with loss 1.7213523387908936, accuracy 0.21944445371627808.\n",
      "Training epoch 6 batch 653 with loss 1.8128665685653687, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 654 with loss 1.7703332901000977, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 655 with loss 1.8546905517578125, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 656 with loss 1.778586745262146, accuracy 0.24166667461395264.\n",
      "Training epoch 6 batch 657 with loss 1.8581607341766357, accuracy 0.02777777798473835.\n",
      "Training epoch 6 batch 658 with loss 1.818225622177124, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 659 with loss 1.780753493309021, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 660 with loss 1.751800775527954, accuracy 0.2750000059604645.\n",
      "Training epoch 6 batch 661 with loss 1.7134637832641602, accuracy 0.33888888359069824.\n",
      "Training epoch 6 batch 662 with loss 1.8136024475097656, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 663 with loss 1.765798568725586, accuracy 0.11666666716337204.\n",
      "Training epoch 6 batch 664 with loss 1.813681960105896, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 665 with loss 1.8164085149765015, accuracy 0.1071428582072258.\n",
      "Training epoch 6 batch 666 with loss 1.8177169561386108, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 667 with loss 1.7975438833236694, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 668 with loss 1.83524489402771, accuracy 0.17222222685813904.\n",
      "Training epoch 6 batch 669 with loss 1.578359842300415, accuracy 0.491666704416275.\n",
      "Training epoch 6 batch 670 with loss 1.7538331747055054, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 671 with loss 1.809704065322876, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 672 with loss 1.8141340017318726, accuracy 0.35277777910232544.\n",
      "Training epoch 6 batch 673 with loss 1.8371477127075195, accuracy 0.125.\n",
      "Training epoch 6 batch 674 with loss 1.7904914617538452, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 675 with loss 1.7120612859725952, accuracy 0.347222238779068.\n",
      "Training epoch 6 batch 676 with loss 1.799960732460022, accuracy 0.0694444477558136.\n",
      "Training epoch 6 batch 677 with loss 1.611732840538025, accuracy 0.18333333730697632.\n",
      "Training epoch 6 batch 678 with loss 1.7767055034637451, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 679 with loss 1.8593324422836304, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 680 with loss 1.7027231454849243, accuracy 0.26944443583488464.\n",
      "Training epoch 6 batch 681 with loss 1.9055538177490234, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 682 with loss 1.7966687679290771, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 683 with loss 1.8364862203598022, accuracy 0.3849206566810608.\n",
      "Training epoch 6 batch 684 with loss 1.6651630401611328, accuracy 0.38055557012557983.\n",
      "Training epoch 6 batch 685 with loss 1.7841689586639404, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 686 with loss 1.708242654800415, accuracy 0.17222222685813904.\n",
      "Training epoch 6 batch 687 with loss 1.7078994512557983, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 688 with loss 1.8330656290054321, accuracy 0.1071428582072258.\n",
      "Training epoch 6 batch 689 with loss 1.6894347667694092, accuracy 0.26944446563720703.\n",
      "Training epoch 6 batch 690 with loss 1.8465440273284912, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 691 with loss 1.832614541053772, accuracy 0.22777777910232544.\n",
      "Training epoch 6 batch 692 with loss 1.8075050115585327, accuracy 0.20555555820465088.\n",
      "Training epoch 6 batch 693 with loss 1.7590446472167969, accuracy 0.24722224473953247.\n",
      "Training epoch 6 batch 694 with loss 1.829573631286621, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 695 with loss 1.768255591392517, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 696 with loss 1.7818715572357178, accuracy 0.18611112236976624.\n",
      "Training epoch 6 batch 697 with loss 1.726419448852539, accuracy 0.18518519401550293.\n",
      "Training epoch 6 batch 698 with loss 1.830575704574585, accuracy 0.23333334922790527.\n",
      "Training epoch 6 batch 699 with loss 1.8195421695709229, accuracy 0.18611110746860504.\n",
      "Training epoch 6 batch 700 with loss 1.7096433639526367, accuracy 0.16547618806362152.\n",
      "Training epoch 6 batch 701 with loss 1.6543233394622803, accuracy 0.3888888955116272.\n",
      "Training epoch 6 batch 702 with loss 1.9018001556396484, accuracy 0.02380952425301075.\n",
      "Training epoch 6 batch 703 with loss 1.715074896812439, accuracy 0.3055555522441864.\n",
      "Training epoch 6 batch 704 with loss 1.8642375469207764, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 705 with loss 1.8079420328140259, accuracy 0.12222222983837128.\n",
      "Training epoch 6 batch 706 with loss 1.7576690912246704, accuracy 0.2527777850627899.\n",
      "Training epoch 6 batch 707 with loss 1.8130168914794922, accuracy 0.20555555820465088.\n",
      "Training epoch 6 batch 708 with loss 1.7491977214813232, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 709 with loss 1.6428779363632202, accuracy 0.13809524476528168.\n",
      "Training epoch 6 batch 710 with loss 1.7826368808746338, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 711 with loss 1.8565549850463867, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 712 with loss 1.8131647109985352, accuracy 0.30158731341362.\n",
      "Training epoch 6 batch 713 with loss 1.8256962299346924, accuracy 0.20555555820465088.\n",
      "Training epoch 6 batch 714 with loss 1.8736584186553955, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 715 with loss 1.8305155038833618, accuracy 0.3611111342906952.\n",
      "Training epoch 6 batch 716 with loss 1.8118994235992432, accuracy 0.10833333432674408.\n",
      "Training epoch 6 batch 717 with loss 1.8364225625991821, accuracy 0.2361111044883728.\n",
      "Training epoch 6 batch 718 with loss 1.7330238819122314, accuracy 0.4027777910232544.\n",
      "Training epoch 6 batch 719 with loss 1.8511775732040405, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 720 with loss 1.8053697347640991, accuracy 0.125.\n",
      "Training epoch 6 batch 721 with loss 1.7689025402069092, accuracy 0.21587303280830383.\n",
      "Training epoch 6 batch 722 with loss 1.6679738759994507, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 723 with loss 1.734412431716919, accuracy 0.2916666567325592.\n",
      "Training epoch 6 batch 724 with loss 1.7618052959442139, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 725 with loss 1.8083397150039673, accuracy 0.3333333432674408.\n",
      "Training epoch 6 batch 726 with loss 1.8975204229354858, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 727 with loss 1.889331579208374, accuracy 0.06666667014360428.\n",
      "Training epoch 6 batch 728 with loss 1.778517723083496, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 729 with loss 1.7881252765655518, accuracy 0.210317462682724.\n",
      "Training epoch 6 batch 730 with loss 1.7695707082748413, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 731 with loss 1.8014276027679443, accuracy 0.03333333507180214.\n",
      "Training epoch 6 batch 732 with loss 1.864324927330017, accuracy 0.02380952425301075.\n",
      "Training epoch 6 batch 733 with loss 1.782226324081421, accuracy 0.3333333432674408.\n",
      "Training epoch 6 batch 734 with loss 1.8582929372787476, accuracy 0.17222222685813904.\n",
      "Training epoch 6 batch 735 with loss 1.7491956949234009, accuracy 0.41111111640930176.\n",
      "Training epoch 6 batch 736 with loss 1.8404134511947632, accuracy 0.17500001192092896.\n",
      "Training epoch 6 batch 737 with loss 1.7299293279647827, accuracy 0.25.\n",
      "Training epoch 6 batch 738 with loss 1.7734267711639404, accuracy 0.1825396865606308.\n",
      "Training epoch 6 batch 739 with loss 1.6959317922592163, accuracy 0.2916666567325592.\n",
      "Training epoch 6 batch 740 with loss 1.8347835540771484, accuracy 0.15833333134651184.\n",
      "Training epoch 6 batch 741 with loss 1.7047399282455444, accuracy 0.4555555582046509.\n",
      "Training epoch 6 batch 742 with loss 1.8165594339370728, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 743 with loss 1.9070574045181274, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 744 with loss 1.823510766029358, accuracy 0.30000001192092896.\n",
      "Training epoch 6 batch 745 with loss 1.8990352153778076, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 746 with loss 1.813550353050232, accuracy 0.0.\n",
      "Training epoch 6 batch 747 with loss 1.7982804775238037, accuracy 0.14666667580604553.\n",
      "Training epoch 6 batch 748 with loss 1.9035638570785522, accuracy 0.06111111491918564.\n",
      "Training epoch 6 batch 749 with loss 1.854013204574585, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 750 with loss 1.684077262878418, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 751 with loss 1.8207390308380127, accuracy 0.3027777671813965.\n",
      "Training epoch 6 batch 752 with loss 1.803382158279419, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 753 with loss 1.8148733377456665, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 754 with loss 1.7387237548828125, accuracy 0.22777777910232544.\n",
      "Training epoch 6 batch 755 with loss 1.7876641750335693, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 756 with loss 1.768221139907837, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 757 with loss 1.769727110862732, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 758 with loss 1.6851978302001953, accuracy 0.2750000059604645.\n",
      "Training epoch 6 batch 759 with loss 1.9544261693954468, accuracy 0.0.\n",
      "Training epoch 6 batch 760 with loss 1.8002121448516846, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 761 with loss 1.8361918926239014, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 762 with loss 1.7174341678619385, accuracy 0.2916666567325592.\n",
      "Training epoch 6 batch 763 with loss 1.7707668542861938, accuracy 0.10833333432674408.\n",
      "Training epoch 6 batch 764 with loss 1.6915111541748047, accuracy 0.2916666865348816.\n",
      "Training epoch 6 batch 765 with loss 1.678036093711853, accuracy 0.4222222566604614.\n",
      "Training epoch 6 batch 766 with loss 1.6855738162994385, accuracy 0.3611111044883728.\n",
      "Training epoch 6 batch 767 with loss 1.9057188034057617, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 768 with loss 1.8421344757080078, accuracy 0.1597222238779068.\n",
      "Training epoch 6 batch 769 with loss 1.8117501735687256, accuracy 0.2750000059604645.\n",
      "Training epoch 6 batch 770 with loss 1.713097333908081, accuracy 0.13650794327259064.\n",
      "Training epoch 6 batch 771 with loss 1.6855109930038452, accuracy 0.31309524178504944.\n",
      "Training epoch 6 batch 772 with loss 1.7235053777694702, accuracy 0.38611114025115967.\n",
      "Training epoch 6 batch 773 with loss 1.718686819076538, accuracy 0.23333334922790527.\n",
      "Training epoch 6 batch 774 with loss 1.8093583583831787, accuracy 0.18333333730697632.\n",
      "Training epoch 6 batch 775 with loss 1.7058387994766235, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 776 with loss 1.797799825668335, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 777 with loss 1.7810392379760742, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 778 with loss 1.718510389328003, accuracy 0.2666666805744171.\n",
      "Training epoch 6 batch 779 with loss 1.7087476253509521, accuracy 0.3361110985279083.\n",
      "Training epoch 6 batch 780 with loss 1.7801433801651, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 781 with loss 1.8450924158096313, accuracy 0.13055555522441864.\n",
      "Training epoch 6 batch 782 with loss 1.7190430164337158, accuracy 0.28333333134651184.\n",
      "Training epoch 6 batch 783 with loss 1.7981857061386108, accuracy 0.347222238779068.\n",
      "Training epoch 6 batch 784 with loss 1.762375831604004, accuracy 0.2666666805744171.\n",
      "Training epoch 6 batch 785 with loss 1.8489662408828735, accuracy 0.1805555671453476.\n",
      "Training epoch 6 batch 786 with loss 1.8624956607818604, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 787 with loss 1.7439082860946655, accuracy 0.125.\n",
      "Training epoch 6 batch 788 with loss 1.786982536315918, accuracy 0.28333333134651184.\n",
      "Training epoch 6 batch 789 with loss 1.9137065410614014, accuracy 0.31111112236976624.\n",
      "Training epoch 6 batch 790 with loss 1.8668493032455444, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 791 with loss 1.794996976852417, accuracy 0.2361111044883728.\n",
      "Training epoch 6 batch 792 with loss 1.808123230934143, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 793 with loss 1.7738841772079468, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 794 with loss 1.8054090738296509, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 795 with loss 1.6965599060058594, accuracy 0.45000001788139343.\n",
      "Training epoch 6 batch 796 with loss 1.7511078119277954, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 797 with loss 1.8188072443008423, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 798 with loss 1.9097055196762085, accuracy 0.20000001788139343.\n",
      "Training epoch 6 batch 799 with loss 1.8354400396347046, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 800 with loss 1.8522106409072876, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 801 with loss 1.8551442623138428, accuracy 0.21388888359069824.\n",
      "Training epoch 6 batch 802 with loss 1.8065112829208374, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 803 with loss 1.7911790609359741, accuracy 0.2666666805744171.\n",
      "Training epoch 6 batch 804 with loss 1.7801382541656494, accuracy 0.2519841492176056.\n",
      "Training epoch 6 batch 805 with loss 1.8046839237213135, accuracy 0.32500001788139343.\n",
      "Training epoch 6 batch 806 with loss 1.9248580932617188, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 807 with loss 1.6203889846801758, accuracy 0.18333333730697632.\n",
      "Training epoch 6 batch 808 with loss 1.8556798696517944, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 809 with loss 1.7359596490859985, accuracy 0.17777778208255768.\n",
      "Training epoch 6 batch 810 with loss 1.82797110080719, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 811 with loss 1.905491828918457, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 812 with loss 1.7750135660171509, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 813 with loss 1.9023382663726807, accuracy 0.10000000894069672.\n",
      "Training epoch 6 batch 814 with loss 1.7423232793807983, accuracy 0.3055555522441864.\n",
      "Training epoch 6 batch 815 with loss 1.7174125909805298, accuracy 0.19166666269302368.\n",
      "Training epoch 6 batch 816 with loss 1.7498611211776733, accuracy 0.25.\n",
      "Training epoch 6 batch 817 with loss 1.862837553024292, accuracy 0.17777778208255768.\n",
      "Training epoch 6 batch 818 with loss 1.8618472814559937, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 819 with loss 1.682254433631897, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 820 with loss 1.799501657485962, accuracy 0.08095238357782364.\n",
      "Training epoch 6 batch 821 with loss 1.7554031610488892, accuracy 0.12222222983837128.\n",
      "Training epoch 6 batch 822 with loss 1.7683498859405518, accuracy 0.1597222238779068.\n",
      "Training epoch 6 batch 823 with loss 1.8141090869903564, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 824 with loss 1.8219457864761353, accuracy 0.2361111044883728.\n",
      "Training epoch 6 batch 825 with loss 1.7247765064239502, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 826 with loss 1.7547111511230469, accuracy 0.2876984179019928.\n",
      "Training epoch 6 batch 827 with loss 1.8154265880584717, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 828 with loss 1.9285545349121094, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 829 with loss 1.8108570575714111, accuracy 0.111111119389534.\n",
      "Training epoch 6 batch 830 with loss 1.7687654495239258, accuracy 0.3222222328186035.\n",
      "Training epoch 6 batch 831 with loss 1.7524669170379639, accuracy 0.30277779698371887.\n",
      "Training epoch 6 batch 832 with loss 1.6959196329116821, accuracy 0.30666667222976685.\n",
      "Training epoch 6 batch 833 with loss 1.6461303234100342, accuracy 0.19166666269302368.\n",
      "Training epoch 6 batch 834 with loss 1.899591088294983, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 835 with loss 1.8381836414337158, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 836 with loss 1.8021419048309326, accuracy 0.125.\n",
      "Training epoch 6 batch 837 with loss 1.8472263813018799, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 838 with loss 1.9432458877563477, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 839 with loss 1.7621791362762451, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 840 with loss 1.756791353225708, accuracy 0.10833333432674408.\n",
      "Training epoch 6 batch 841 with loss 1.815197229385376, accuracy 0.1626984179019928.\n",
      "Training epoch 6 batch 842 with loss 1.8357369899749756, accuracy 0.3194444477558136.\n",
      "Training epoch 6 batch 843 with loss 1.9869861602783203, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 844 with loss 1.7764495611190796, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 845 with loss 1.7649307250976562, accuracy 0.277777761220932.\n",
      "Training epoch 6 batch 846 with loss 1.6699985265731812, accuracy 0.2531746029853821.\n",
      "Training epoch 6 batch 847 with loss 1.7278045415878296, accuracy 0.33888888359069824.\n",
      "Training epoch 6 batch 848 with loss 1.751861333847046, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 849 with loss 1.810084342956543, accuracy 0.125.\n",
      "Training epoch 6 batch 850 with loss 1.7428334951400757, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 851 with loss 1.6374480724334717, accuracy 0.3916666805744171.\n",
      "Training epoch 6 batch 852 with loss 1.8141908645629883, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 853 with loss 1.9144160747528076, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 854 with loss 1.8397678136825562, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 855 with loss 1.8723723888397217, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 856 with loss 1.752099633216858, accuracy 0.21388889849185944.\n",
      "Training epoch 6 batch 857 with loss 1.7745195627212524, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 858 with loss 1.861380934715271, accuracy 0.0694444477558136.\n",
      "Training epoch 6 batch 859 with loss 1.7999427318572998, accuracy 0.28333333134651184.\n",
      "Training epoch 6 batch 860 with loss 1.723663091659546, accuracy 0.35277777910232544.\n",
      "Training epoch 6 batch 861 with loss 1.733093500137329, accuracy 0.24166667461395264.\n",
      "Training epoch 6 batch 862 with loss 1.8298848867416382, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 863 with loss 1.716454267501831, accuracy 0.3055555522441864.\n",
      "Training epoch 6 batch 864 with loss 1.7417665719985962, accuracy 0.44999998807907104.\n",
      "Training epoch 6 batch 865 with loss 1.7563199996948242, accuracy 0.20158730447292328.\n",
      "Training epoch 6 batch 866 with loss 1.7425123453140259, accuracy 0.24166667461395264.\n",
      "Training epoch 6 batch 867 with loss 1.7095270156860352, accuracy 0.2876984179019928.\n",
      "Training epoch 6 batch 868 with loss 1.8480949401855469, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 869 with loss 1.777118444442749, accuracy 0.3055555522441864.\n",
      "Training epoch 6 batch 870 with loss 1.7649004459381104, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 871 with loss 1.824310541152954, accuracy 0.20555555820465088.\n",
      "Training epoch 6 batch 872 with loss 1.7786827087402344, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 873 with loss 1.9282289743423462, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 874 with loss 1.848183035850525, accuracy 0.3333333432674408.\n",
      "Training epoch 6 batch 875 with loss 1.7893571853637695, accuracy 0.366666704416275.\n",
      "Training epoch 6 batch 876 with loss 1.8095614910125732, accuracy 0.284722238779068.\n",
      "Training epoch 6 batch 877 with loss 1.8608309030532837, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 878 with loss 1.8790422677993774, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 879 with loss 1.7704217433929443, accuracy 0.3194444477558136.\n",
      "Training epoch 6 batch 880 with loss 1.7788543701171875, accuracy 0.1587301641702652.\n",
      "Training epoch 6 batch 881 with loss 1.7156555652618408, accuracy 0.21388889849185944.\n",
      "Training epoch 6 batch 882 with loss 1.7759904861450195, accuracy 0.23888888955116272.\n",
      "Training epoch 6 batch 883 with loss 1.7771377563476562, accuracy 0.25.\n",
      "Training epoch 6 batch 884 with loss 1.8180103302001953, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 885 with loss 1.8477637767791748, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 886 with loss 1.9719892740249634, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 887 with loss 1.660036325454712, accuracy 0.4000000059604645.\n",
      "Training epoch 6 batch 888 with loss 1.7726709842681885, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 889 with loss 1.7860456705093384, accuracy 0.23333334922790527.\n",
      "Training epoch 6 batch 890 with loss 1.7416244745254517, accuracy 0.125.\n",
      "Training epoch 6 batch 891 with loss 1.7164719104766846, accuracy 0.4583333730697632.\n",
      "Training epoch 6 batch 892 with loss 1.7785629034042358, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 893 with loss 1.9045501947402954, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 894 with loss 1.8683754205703735, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 895 with loss 1.876235008239746, accuracy 0.0.\n",
      "Training epoch 6 batch 896 with loss 1.785933256149292, accuracy 0.3888888955116272.\n",
      "Training epoch 6 batch 897 with loss 1.8342735767364502, accuracy 0.1071428582072258.\n",
      "Training epoch 6 batch 898 with loss 1.8786404132843018, accuracy 0.17777778208255768.\n",
      "Training epoch 6 batch 899 with loss 1.7832494974136353, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 900 with loss 1.8312900066375732, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 901 with loss 1.8376010656356812, accuracy 0.1587301641702652.\n",
      "Training epoch 6 batch 902 with loss 1.6863152980804443, accuracy 0.23888888955116272.\n",
      "Training epoch 6 batch 903 with loss 1.8637220859527588, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 904 with loss 1.8154170513153076, accuracy 0.12222222983837128.\n",
      "Training epoch 6 batch 905 with loss 1.8837146759033203, accuracy 0.03333333507180214.\n",
      "Training epoch 6 batch 906 with loss 1.7684704065322876, accuracy 0.10833333432674408.\n",
      "Training epoch 6 batch 907 with loss 1.8747316598892212, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 908 with loss 1.6978113651275635, accuracy 0.21111111342906952.\n",
      "Training epoch 6 batch 909 with loss 1.7937195301055908, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 910 with loss 1.8579738140106201, accuracy 0.06111111491918564.\n",
      "Training epoch 6 batch 911 with loss 1.7835452556610107, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 912 with loss 1.7482131719589233, accuracy 0.21111111342906952.\n",
      "Training epoch 6 batch 913 with loss 1.7658323049545288, accuracy 0.1875.\n",
      "Training epoch 6 batch 914 with loss 1.8307793140411377, accuracy 0.25.\n",
      "Training epoch 6 batch 915 with loss 1.7572218179702759, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 916 with loss 1.7339074611663818, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 917 with loss 1.8011484146118164, accuracy 0.2666666805744171.\n",
      "Training epoch 6 batch 918 with loss 1.7526721954345703, accuracy 0.2750000059604645.\n",
      "Training epoch 6 batch 919 with loss 1.7504346370697021, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 920 with loss 1.7624238729476929, accuracy 0.23333333432674408.\n",
      "Training epoch 6 batch 921 with loss 1.9582971334457397, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 922 with loss 1.737514853477478, accuracy 0.2599206268787384.\n",
      "Training epoch 6 batch 923 with loss 1.7746522426605225, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 924 with loss 1.7734943628311157, accuracy 0.18611110746860504.\n",
      "Training epoch 6 batch 925 with loss 1.8623254299163818, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 926 with loss 1.8958508968353271, accuracy 0.06666667014360428.\n",
      "Training epoch 6 batch 927 with loss 1.8413423299789429, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 928 with loss 1.727224588394165, accuracy 0.23888888955116272.\n",
      "Training epoch 6 batch 929 with loss 1.8020803928375244, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 930 with loss 1.7870298624038696, accuracy 0.10000000894069672.\n",
      "Training epoch 6 batch 931 with loss 1.6859655380249023, accuracy 0.4226190447807312.\n",
      "Training epoch 6 batch 932 with loss 1.8569406270980835, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 933 with loss 1.8661177158355713, accuracy 0.21388889849185944.\n",
      "Training epoch 6 batch 934 with loss 1.816650629043579, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 935 with loss 1.8077195882797241, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 936 with loss 1.7992080450057983, accuracy 0.15833333134651184.\n",
      "Training epoch 6 batch 937 with loss 1.797088861465454, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 938 with loss 1.8481085300445557, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 939 with loss 1.8276042938232422, accuracy 0.10000000894069672.\n",
      "Training epoch 6 batch 940 with loss 1.9330778121948242, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 941 with loss 1.77802312374115, accuracy 0.15833333134651184.\n",
      "Training epoch 6 batch 942 with loss 1.8057677745819092, accuracy 0.17777778208255768.\n",
      "Training epoch 6 batch 943 with loss 1.958759069442749, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 944 with loss 1.8886854648590088, accuracy 0.17777778208255768.\n",
      "Training epoch 6 batch 945 with loss 1.7781600952148438, accuracy 0.4238095283508301.\n",
      "Training epoch 6 batch 946 with loss 1.7940257787704468, accuracy 0.29722222685813904.\n",
      "Training epoch 6 batch 947 with loss 1.8476543426513672, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 948 with loss 1.7110382318496704, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 949 with loss 1.8257887363433838, accuracy 0.10000000894069672.\n",
      "Training epoch 6 batch 950 with loss 1.7754846811294556, accuracy 0.22962963581085205.\n",
      "Training epoch 6 batch 951 with loss 1.707350730895996, accuracy 0.38611114025115967.\n",
      "Training epoch 6 batch 952 with loss 1.724364995956421, accuracy 0.2685185372829437.\n",
      "Training epoch 6 batch 953 with loss 1.7751836776733398, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 954 with loss 1.78204345703125, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 955 with loss 1.8729143142700195, accuracy 0.0.\n",
      "Training epoch 6 batch 956 with loss 1.8412920236587524, accuracy 0.125.\n",
      "Training epoch 6 batch 957 with loss 1.726122260093689, accuracy 0.1170634925365448.\n",
      "Training epoch 6 batch 958 with loss 1.7437978982925415, accuracy 0.4166666865348816.\n",
      "Training epoch 6 batch 959 with loss 1.872231125831604, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 960 with loss 1.859954833984375, accuracy 0.15555556118488312.\n",
      "Training epoch 6 batch 961 with loss 1.832990288734436, accuracy 0.222222238779068.\n",
      "Training epoch 6 batch 962 with loss 1.7753340005874634, accuracy 0.3263888955116272.\n",
      "Training epoch 6 batch 963 with loss 1.7762044668197632, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 964 with loss 1.8443902730941772, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 965 with loss 1.9572490453720093, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 966 with loss 1.7406189441680908, accuracy 0.17499999701976776.\n",
      "Training epoch 6 batch 967 with loss 1.7579320669174194, accuracy 0.3222222328186035.\n",
      "Training epoch 6 batch 968 with loss 1.8252465724945068, accuracy 0.14166668057441711.\n",
      "Training epoch 6 batch 969 with loss 1.784623384475708, accuracy 0.3638888895511627.\n",
      "Training epoch 6 batch 970 with loss 1.689176321029663, accuracy 0.3948412835597992.\n",
      "Training epoch 6 batch 971 with loss 1.7867543697357178, accuracy 0.375.\n",
      "Training epoch 6 batch 972 with loss 1.8755604028701782, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 973 with loss 1.8404686450958252, accuracy 0.3035714328289032.\n",
      "Training epoch 6 batch 974 with loss 1.7945970296859741, accuracy 0.18333333730697632.\n",
      "Training epoch 6 batch 975 with loss 1.7354021072387695, accuracy 0.15595239400863647.\n",
      "Training epoch 6 batch 976 with loss 1.833402395248413, accuracy 0.21111111342906952.\n",
      "Training epoch 6 batch 977 with loss 1.658268690109253, accuracy 0.3333333432674408.\n",
      "Training epoch 6 batch 978 with loss 1.7934865951538086, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 979 with loss 1.7594712972640991, accuracy 0.3194444477558136.\n",
      "Training epoch 6 batch 980 with loss 1.7454429864883423, accuracy 0.23333333432674408.\n",
      "Training epoch 6 batch 981 with loss 1.8982982635498047, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 982 with loss 1.821112036705017, accuracy 0.3611111044883728.\n",
      "Training epoch 6 batch 983 with loss 1.8034225702285767, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 984 with loss 1.8185943365097046, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 985 with loss 1.816974401473999, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 986 with loss 1.7717864513397217, accuracy 0.2361111044883728.\n",
      "Training epoch 6 batch 987 with loss 1.8211212158203125, accuracy 0.3194444477558136.\n",
      "Training epoch 6 batch 988 with loss 1.8191587924957275, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 989 with loss 1.7050727605819702, accuracy 0.3222222328186035.\n",
      "Training epoch 6 batch 990 with loss 1.8447965383529663, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 991 with loss 1.849745512008667, accuracy 0.0625.\n",
      "Training epoch 6 batch 992 with loss 1.7749483585357666, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 993 with loss 1.7540109157562256, accuracy 0.1805555671453476.\n",
      "Training epoch 6 batch 994 with loss 1.8442929983139038, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 995 with loss 1.749474287033081, accuracy 0.35555559396743774.\n",
      "Training epoch 6 batch 996 with loss 1.8521664142608643, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 997 with loss 1.779625654220581, accuracy 0.2904762029647827.\n",
      "Training epoch 6 batch 998 with loss 1.775102972984314, accuracy 0.25.\n",
      "Training epoch 6 batch 999 with loss 1.8419158458709717, accuracy 0.06666667014360428.\n",
      "Training epoch 6 batch 1000 with loss 1.7060449123382568, accuracy 0.2291666865348816.\n",
      "Training epoch 6 batch 1001 with loss 1.821435570716858, accuracy 0.17222222685813904.\n",
      "Training epoch 6 batch 1002 with loss 1.794687032699585, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 1003 with loss 1.7952783107757568, accuracy 0.14166668057441711.\n",
      "Training epoch 6 batch 1004 with loss 1.7543954849243164, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 1005 with loss 1.8468084335327148, accuracy 0.125.\n",
      "Training epoch 6 batch 1006 with loss 1.8670880794525146, accuracy 0.1210317462682724.\n",
      "Training epoch 6 batch 1007 with loss 1.6778905391693115, accuracy 0.3055555522441864.\n",
      "Training epoch 6 batch 1008 with loss 1.729669213294983, accuracy 0.2986111044883728.\n",
      "Training epoch 6 batch 1009 with loss 1.8900028467178345, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1010 with loss 1.6872543096542358, accuracy 0.2916666567325592.\n",
      "Training epoch 6 batch 1011 with loss 1.839267373085022, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 1012 with loss 1.7286676168441772, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 1013 with loss 1.7628660202026367, accuracy 0.2944444417953491.\n",
      "Training epoch 6 batch 1014 with loss 1.7534023523330688, accuracy 0.17222222685813904.\n",
      "Training epoch 6 batch 1015 with loss 1.8673555850982666, accuracy 0.03333333507180214.\n",
      "Training epoch 6 batch 1016 with loss 1.8797403573989868, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 1017 with loss 1.898785948753357, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 1018 with loss 1.8160717487335205, accuracy 0.1210317462682724.\n",
      "Training epoch 6 batch 1019 with loss 1.8020744323730469, accuracy 0.13055555522441864.\n",
      "Training epoch 6 batch 1020 with loss 1.833087682723999, accuracy 0.21388888359069824.\n",
      "Training epoch 6 batch 1021 with loss 1.7725179195404053, accuracy 0.3888888955116272.\n",
      "Training epoch 6 batch 1022 with loss 1.5998526811599731, accuracy 0.4555555582046509.\n",
      "Training epoch 6 batch 1023 with loss 1.6711219549179077, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 1024 with loss 1.7486366033554077, accuracy 0.46388891339302063.\n",
      "Training epoch 6 batch 1025 with loss 1.806361198425293, accuracy 0.125.\n",
      "Training epoch 6 batch 1026 with loss 1.8677780628204346, accuracy 0.3055555522441864.\n",
      "Training epoch 6 batch 1027 with loss 1.8017734289169312, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1028 with loss 1.768326759338379, accuracy 0.1349206417798996.\n",
      "Training epoch 6 batch 1029 with loss 1.8620796203613281, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 1030 with loss 1.6833000183105469, accuracy 0.3333333432674408.\n",
      "Training epoch 6 batch 1031 with loss 1.8286888599395752, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1032 with loss 1.7987865209579468, accuracy 0.2341269850730896.\n",
      "Training epoch 6 batch 1033 with loss 1.8920589685440063, accuracy 0.05714286118745804.\n",
      "Training epoch 6 batch 1034 with loss 1.6676441431045532, accuracy 0.23333334922790527.\n",
      "Training epoch 6 batch 1035 with loss 1.7391040325164795, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 1036 with loss 1.7729406356811523, accuracy 0.12222222983837128.\n",
      "Training epoch 6 batch 1037 with loss 1.8431203365325928, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 1038 with loss 1.7998298406600952, accuracy 0.319444477558136.\n",
      "Training epoch 6 batch 1039 with loss 1.7876039743423462, accuracy 0.18333333730697632.\n",
      "Training epoch 6 batch 1040 with loss 2.0135931968688965, accuracy 0.0.\n",
      "Training epoch 6 batch 1041 with loss 1.7625410556793213, accuracy 0.130952388048172.\n",
      "Training epoch 6 batch 1042 with loss 1.8102052211761475, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 1043 with loss 1.8711763620376587, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 1044 with loss 1.8261489868164062, accuracy 0.1180555522441864.\n",
      "Training epoch 6 batch 1045 with loss 1.725811243057251, accuracy 0.18611110746860504.\n",
      "Training epoch 6 batch 1046 with loss 1.8449413776397705, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1047 with loss 1.685302972793579, accuracy 0.3444444537162781.\n",
      "Training epoch 6 batch 1048 with loss 1.865457534790039, accuracy 0.15833334624767303.\n",
      "Training epoch 6 batch 1049 with loss 1.802599310874939, accuracy 0.2888889014720917.\n",
      "Training epoch 6 batch 1050 with loss 1.8519378900527954, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 1051 with loss 1.8036301136016846, accuracy 0.13055555522441864.\n",
      "Training epoch 6 batch 1052 with loss 1.7629129886627197, accuracy 0.22499999403953552.\n",
      "Training epoch 6 batch 1053 with loss 1.790480375289917, accuracy 0.13055555522441864.\n",
      "Training epoch 6 batch 1054 with loss 1.7439638376235962, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 1055 with loss 1.760713815689087, accuracy 0.27539682388305664.\n",
      "Training epoch 6 batch 1056 with loss 1.7381551265716553, accuracy 0.2519841194152832.\n",
      "Training epoch 6 batch 1057 with loss 1.714198112487793, accuracy 0.32500001788139343.\n",
      "Training epoch 6 batch 1058 with loss 1.8273159265518188, accuracy 0.06111111491918564.\n",
      "Training epoch 6 batch 1059 with loss 1.8250868320465088, accuracy 0.17222222685813904.\n",
      "Training epoch 6 batch 1060 with loss 1.814963936805725, accuracy 0.02777777798473835.\n",
      "Training epoch 6 batch 1061 with loss 1.856499433517456, accuracy 0.0.\n",
      "Training epoch 6 batch 1062 with loss 1.8377975225448608, accuracy 0.25.\n",
      "Training epoch 6 batch 1063 with loss 1.733191728591919, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 1064 with loss 1.7211107015609741, accuracy 0.19761905074119568.\n",
      "Training epoch 6 batch 1065 with loss 1.7835725545883179, accuracy 0.17777778208255768.\n",
      "Training epoch 6 batch 1066 with loss 1.7972809076309204, accuracy 0.4694444239139557.\n",
      "Training epoch 6 batch 1067 with loss 1.8012498617172241, accuracy 0.18611110746860504.\n",
      "Training epoch 6 batch 1068 with loss 1.7924473285675049, accuracy 0.21944445371627808.\n",
      "Training epoch 6 batch 1069 with loss 1.8067106008529663, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 1070 with loss 1.8164068460464478, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 1071 with loss 1.8568089008331299, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 1072 with loss 1.709108591079712, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 1073 with loss 1.8976051807403564, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1074 with loss 1.8584585189819336, accuracy 0.1805555671453476.\n",
      "Training epoch 6 batch 1075 with loss 1.7280445098876953, accuracy 0.30000001192092896.\n",
      "Training epoch 6 batch 1076 with loss 1.8550914525985718, accuracy 0.12222222983837128.\n",
      "Training epoch 6 batch 1077 with loss 1.8412061929702759, accuracy 0.17777778208255768.\n",
      "Training epoch 6 batch 1078 with loss 1.8085851669311523, accuracy 0.2847222089767456.\n",
      "Training epoch 6 batch 1079 with loss 1.8305387496948242, accuracy 0.130952388048172.\n",
      "Training epoch 6 batch 1080 with loss 1.8686708211898804, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 1081 with loss 1.8768198490142822, accuracy 0.13055555522441864.\n",
      "Training epoch 6 batch 1082 with loss 1.8237555027008057, accuracy 0.21944445371627808.\n",
      "Training epoch 6 batch 1083 with loss 1.864942193031311, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 1084 with loss 1.7091548442840576, accuracy 0.21111111342906952.\n",
      "Training epoch 6 batch 1085 with loss 1.8260663747787476, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1086 with loss 1.735289216041565, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 1087 with loss 1.8163789510726929, accuracy 0.2291666716337204.\n",
      "Training epoch 6 batch 1088 with loss 1.7606332302093506, accuracy 0.15555556118488312.\n",
      "Training epoch 6 batch 1089 with loss 1.7780271768569946, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1090 with loss 1.8208503723144531, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 1091 with loss 1.8418207168579102, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 1092 with loss 1.8236547708511353, accuracy 0.15833333134651184.\n",
      "Training epoch 6 batch 1093 with loss 1.7459886074066162, accuracy 0.21388888359069824.\n",
      "Training epoch 6 batch 1094 with loss 1.8092691898345947, accuracy 0.2708333432674408.\n",
      "Training epoch 6 batch 1095 with loss 1.8135616779327393, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 1096 with loss 1.7687877416610718, accuracy 0.16190476715564728.\n",
      "Training epoch 6 batch 1097 with loss 1.7267624139785767, accuracy 0.19166666269302368.\n",
      "Training epoch 6 batch 1098 with loss 1.7054634094238281, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 1099 with loss 1.775511384010315, accuracy 0.27222222089767456.\n",
      "Training epoch 6 batch 1100 with loss 1.83258855342865, accuracy 0.13518518209457397.\n",
      "Training epoch 6 batch 1101 with loss 1.8517364263534546, accuracy 0.130952388048172.\n",
      "Training epoch 6 batch 1102 with loss 1.8516552448272705, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 1103 with loss 1.8302770853042603, accuracy 0.33888888359069824.\n",
      "Training epoch 6 batch 1104 with loss 1.7241084575653076, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 1105 with loss 1.772193193435669, accuracy 0.22777777910232544.\n",
      "Training epoch 6 batch 1106 with loss 2.0504655838012695, accuracy 0.0.\n",
      "Training epoch 6 batch 1107 with loss 1.661970853805542, accuracy 0.25.\n",
      "Training epoch 6 batch 1108 with loss 1.889704704284668, accuracy 0.02380952425301075.\n",
      "Training epoch 6 batch 1109 with loss 1.6542778015136719, accuracy 0.347222238779068.\n",
      "Training epoch 6 batch 1110 with loss 1.7626514434814453, accuracy 0.3055555820465088.\n",
      "Training epoch 6 batch 1111 with loss 1.6712329387664795, accuracy 0.4055555462837219.\n",
      "Training epoch 6 batch 1112 with loss 1.786706566810608, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 1113 with loss 1.8377714157104492, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 1114 with loss 1.6442943811416626, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 1115 with loss 1.9415363073349, accuracy 0.05416666716337204.\n",
      "Training epoch 6 batch 1116 with loss 1.8522617816925049, accuracy 0.222222238779068.\n",
      "Training epoch 6 batch 1117 with loss 1.8673995733261108, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 1118 with loss 1.7244904041290283, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 1119 with loss 1.7832787036895752, accuracy 0.19722223281860352.\n",
      "Training epoch 6 batch 1120 with loss 1.6979472637176514, accuracy 0.25.\n",
      "Training epoch 6 batch 1121 with loss 1.7782281637191772, accuracy 0.06666667014360428.\n",
      "Training epoch 6 batch 1122 with loss 1.8378928899765015, accuracy 0.2460317462682724.\n",
      "Training epoch 6 batch 1123 with loss 1.8809645175933838, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 1124 with loss 1.8331773281097412, accuracy 0.26944443583488464.\n",
      "Training epoch 6 batch 1125 with loss 1.8076088428497314, accuracy 0.15555556118488312.\n",
      "Training epoch 6 batch 1126 with loss 1.9018129110336304, accuracy 0.02083333395421505.\n",
      "Training epoch 6 batch 1127 with loss 1.9231281280517578, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1128 with loss 1.8897044658660889, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 1129 with loss 1.7602155208587646, accuracy 0.1587301641702652.\n",
      "Training epoch 6 batch 1130 with loss 1.791372299194336, accuracy 0.33492064476013184.\n",
      "Training epoch 6 batch 1131 with loss 1.747227430343628, accuracy 0.2182539701461792.\n",
      "Training epoch 6 batch 1132 with loss 1.8046001195907593, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 1133 with loss 1.7534520626068115, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1134 with loss 1.8456894159317017, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 1135 with loss 1.8516708612442017, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1136 with loss 1.7851454019546509, accuracy 0.25.\n",
      "Training epoch 6 batch 1137 with loss 1.8229103088378906, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 1138 with loss 1.7831382751464844, accuracy 0.17222222685813904.\n",
      "Training epoch 6 batch 1139 with loss 1.751607894897461, accuracy 0.20000001788139343.\n",
      "Training epoch 6 batch 1140 with loss 1.6940460205078125, accuracy 0.16428571939468384.\n",
      "Training epoch 6 batch 1141 with loss 1.763159990310669, accuracy 0.24444445967674255.\n",
      "Training epoch 6 batch 1142 with loss 1.7800633907318115, accuracy 0.3055555820465088.\n",
      "Training epoch 6 batch 1143 with loss 1.7357410192489624, accuracy 0.17222222685813904.\n",
      "Training epoch 6 batch 1144 with loss 1.8146705627441406, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 1145 with loss 1.7990837097167969, accuracy 0.4027777910232544.\n",
      "Training epoch 6 batch 1146 with loss 1.796814203262329, accuracy 0.5138888955116272.\n",
      "Training epoch 6 batch 1147 with loss 1.7662603855133057, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 1148 with loss 1.7124494314193726, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 1149 with loss 1.7988840341567993, accuracy 0.12222222983837128.\n",
      "Training epoch 6 batch 1150 with loss 1.7847950458526611, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 1151 with loss 1.7189441919326782, accuracy 0.33888888359069824.\n",
      "Training epoch 6 batch 1152 with loss 1.8024743795394897, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 1153 with loss 1.7686468362808228, accuracy 0.21388889849185944.\n",
      "Training epoch 6 batch 1154 with loss 1.7094125747680664, accuracy 0.3027777671813965.\n",
      "Training epoch 6 batch 1155 with loss 1.7158746719360352, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 1156 with loss 1.806243658065796, accuracy 0.0694444477558136.\n",
      "Training epoch 6 batch 1157 with loss 1.6077172756195068, accuracy 0.37222224473953247.\n",
      "Training epoch 6 batch 1158 with loss 1.8972232341766357, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 1159 with loss 1.8223813772201538, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 1160 with loss 1.8135982751846313, accuracy 0.13055555522441864.\n",
      "Training epoch 6 batch 1161 with loss 1.7792928218841553, accuracy 0.10277777910232544.\n",
      "Training epoch 6 batch 1162 with loss 1.7796608209609985, accuracy 0.375.\n",
      "Training epoch 6 batch 1163 with loss 1.9538215398788452, accuracy 0.03333333507180214.\n",
      "Training epoch 6 batch 1164 with loss 1.8254728317260742, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 1165 with loss 1.7702043056488037, accuracy 0.1805555671453476.\n",
      "Training epoch 6 batch 1166 with loss 1.703015923500061, accuracy 0.0694444477558136.\n",
      "Training epoch 6 batch 1167 with loss 1.8336784839630127, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1168 with loss 1.854077935218811, accuracy 0.03333333507180214.\n",
      "Training epoch 6 batch 1169 with loss 1.841484785079956, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1170 with loss 1.786072015762329, accuracy 0.144841268658638.\n",
      "Training epoch 6 batch 1171 with loss 1.7528560161590576, accuracy 0.18769842386245728.\n",
      "Training epoch 6 batch 1172 with loss 1.908766746520996, accuracy 0.0.\n",
      "Training epoch 6 batch 1173 with loss 1.7620080709457397, accuracy 0.25.\n",
      "Training epoch 6 batch 1174 with loss 1.7387993335723877, accuracy 0.36666667461395264.\n",
      "Training epoch 6 batch 1175 with loss 1.7130298614501953, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 1176 with loss 1.6865618228912354, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 1177 with loss 1.7847474813461304, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 1178 with loss 1.8434534072875977, accuracy 0.19722223281860352.\n",
      "Training epoch 6 batch 1179 with loss 1.8662277460098267, accuracy 0.2182539701461792.\n",
      "Training epoch 6 batch 1180 with loss 1.8228108882904053, accuracy 0.10833333432674408.\n",
      "Training epoch 6 batch 1181 with loss 1.8104394674301147, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1182 with loss 1.828552007675171, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 1183 with loss 1.7772808074951172, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 1184 with loss 1.721623182296753, accuracy 0.375.\n",
      "Training epoch 6 batch 1185 with loss 1.8888695240020752, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1186 with loss 1.7633203268051147, accuracy 0.14166666567325592.\n",
      "Training epoch 6 batch 1187 with loss 1.8425514698028564, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1188 with loss 1.7849363088607788, accuracy 0.2750000059604645.\n",
      "Training epoch 6 batch 1189 with loss 1.802262306213379, accuracy 0.15833333134651184.\n",
      "Training epoch 6 batch 1190 with loss 1.8460149765014648, accuracy 0.4500000476837158.\n",
      "Training epoch 6 batch 1191 with loss 1.9185981750488281, accuracy 0.0892857164144516.\n",
      "Training epoch 6 batch 1192 with loss 1.8079307079315186, accuracy 0.2916666865348816.\n",
      "Training epoch 6 batch 1193 with loss 1.722792625427246, accuracy 0.23333333432674408.\n",
      "Training epoch 6 batch 1194 with loss 1.887202501296997, accuracy 0.17222222685813904.\n",
      "Training epoch 6 batch 1195 with loss 1.8505117893218994, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 1196 with loss 1.6893001794815063, accuracy 0.25.\n",
      "Training epoch 6 batch 1197 with loss 1.8160336017608643, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 1198 with loss 1.8445501327514648, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 1199 with loss 1.8368362188339233, accuracy 0.3055555522441864.\n",
      "Training epoch 6 batch 1200 with loss 1.7858092784881592, accuracy 0.20555555820465088.\n",
      "Training epoch 6 batch 1201 with loss 1.831190824508667, accuracy 0.0694444477558136.\n",
      "Training epoch 6 batch 1202 with loss 1.852545976638794, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 1203 with loss 1.8387775421142578, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1204 with loss 1.7751470804214478, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 1205 with loss 1.7931709289550781, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 1206 with loss 1.8961536884307861, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 1207 with loss 1.692617654800415, accuracy 0.2666666507720947.\n",
      "Training epoch 6 batch 1208 with loss 1.81368088722229, accuracy 0.125.\n",
      "Training epoch 6 batch 1209 with loss 1.922287940979004, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 1210 with loss 1.8759769201278687, accuracy 0.12380952388048172.\n",
      "Training epoch 6 batch 1211 with loss 1.754333734512329, accuracy 0.444444477558136.\n",
      "Training epoch 6 batch 1212 with loss 1.7998549938201904, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 1213 with loss 1.861260175704956, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1214 with loss 1.8387908935546875, accuracy 0.3611111044883728.\n",
      "Training epoch 6 batch 1215 with loss 1.7117130756378174, accuracy 0.2083333283662796.\n",
      "Training epoch 6 batch 1216 with loss 1.9105558395385742, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 1217 with loss 1.785955786705017, accuracy 0.3055555522441864.\n",
      "Training epoch 6 batch 1218 with loss 1.8147449493408203, accuracy 0.1458333432674408.\n",
      "Training epoch 6 batch 1219 with loss 1.8551933765411377, accuracy 0.03333333507180214.\n",
      "Training epoch 6 batch 1220 with loss 1.9297596216201782, accuracy 0.07407407462596893.\n",
      "Training epoch 6 batch 1221 with loss 1.743737816810608, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 1222 with loss 1.7289085388183594, accuracy 0.2527777850627899.\n",
      "Training epoch 6 batch 1223 with loss 1.8731415271759033, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 1224 with loss 1.843599557876587, accuracy 0.0793650820851326.\n",
      "Training epoch 6 batch 1225 with loss 1.8499698638916016, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 1226 with loss 1.7186371088027954, accuracy 0.4194444417953491.\n",
      "Training epoch 6 batch 1227 with loss 1.7954879999160767, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 1228 with loss 1.7626405954360962, accuracy 0.21944445371627808.\n",
      "Training epoch 6 batch 1229 with loss 1.8891267776489258, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 1230 with loss 1.8183958530426025, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 1231 with loss 1.8906919956207275, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 1232 with loss 1.8413517475128174, accuracy 0.3194444477558136.\n",
      "Training epoch 6 batch 1233 with loss 1.8883758783340454, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 1234 with loss 1.8473659753799438, accuracy 0.125.\n",
      "Training epoch 6 batch 1235 with loss 1.7828638553619385, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 1236 with loss 1.8238283395767212, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 1237 with loss 1.7635198831558228, accuracy 0.3472222089767456.\n",
      "Training epoch 6 batch 1238 with loss 1.7823903560638428, accuracy 0.3787878751754761.\n",
      "Training epoch 6 batch 1239 with loss 1.810004472732544, accuracy 0.36428573727607727.\n",
      "Training epoch 6 batch 1240 with loss 1.7923942804336548, accuracy 0.09444444626569748.\n",
      "Training epoch 6 batch 1241 with loss 1.7290980815887451, accuracy 0.2698412835597992.\n",
      "Training epoch 6 batch 1242 with loss 1.7418301105499268, accuracy 0.36666667461395264.\n",
      "Training epoch 6 batch 1243 with loss 1.9717257022857666, accuracy 0.0.\n",
      "Training epoch 6 batch 1244 with loss 1.849226951599121, accuracy 0.12037037312984467.\n",
      "Training epoch 6 batch 1245 with loss 1.947882890701294, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 1246 with loss 1.7554165124893188, accuracy 0.222222238779068.\n",
      "Training epoch 6 batch 1247 with loss 1.7718302011489868, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 1248 with loss 1.8425140380859375, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 1249 with loss 1.828425407409668, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 1250 with loss 1.8837705850601196, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 1251 with loss 1.7813221216201782, accuracy 0.3055555522441864.\n",
      "Training epoch 6 batch 1252 with loss 1.704903244972229, accuracy 0.3154761791229248.\n",
      "Training epoch 6 batch 1253 with loss 1.7877006530761719, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 1254 with loss 1.8309119939804077, accuracy 0.05185185372829437.\n",
      "Training epoch 6 batch 1255 with loss 1.7398102283477783, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 1256 with loss 1.8712308406829834, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1257 with loss 1.6632474660873413, accuracy 0.17817460000514984.\n",
      "Training epoch 6 batch 1258 with loss 1.7793283462524414, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1259 with loss 1.6778818368911743, accuracy 0.23888888955116272.\n",
      "Training epoch 6 batch 1260 with loss 1.8078663349151611, accuracy 0.18611112236976624.\n",
      "Training epoch 6 batch 1261 with loss 1.7979600429534912, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 1262 with loss 1.8213539123535156, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 1263 with loss 1.844252586364746, accuracy 0.03333333507180214.\n",
      "Training epoch 6 batch 1264 with loss 1.9649336338043213, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 1265 with loss 1.7780532836914062, accuracy 0.12261904776096344.\n",
      "Training epoch 6 batch 1266 with loss 1.7479196786880493, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 1267 with loss 1.8202320337295532, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 1268 with loss 1.8448442220687866, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1269 with loss 1.8177125453948975, accuracy 0.2182539701461792.\n",
      "Training epoch 6 batch 1270 with loss 1.6903114318847656, accuracy 0.3920634984970093.\n",
      "Training epoch 6 batch 1271 with loss 1.7401390075683594, accuracy 0.3194444477558136.\n",
      "Training epoch 6 batch 1272 with loss 1.888903021812439, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 1273 with loss 1.7646373510360718, accuracy 0.2916666865348816.\n",
      "Training epoch 6 batch 1274 with loss 1.6672223806381226, accuracy 0.3680555820465088.\n",
      "Training epoch 6 batch 1275 with loss 1.9052715301513672, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1276 with loss 1.8071506023406982, accuracy 0.1875.\n",
      "Training epoch 6 batch 1277 with loss 1.8271148204803467, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1278 with loss 1.7412751913070679, accuracy 0.18888890743255615.\n",
      "Training epoch 6 batch 1279 with loss 1.775001883506775, accuracy 0.222222238779068.\n",
      "Training epoch 6 batch 1280 with loss 1.7496545314788818, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 1281 with loss 1.8487815856933594, accuracy 0.125.\n",
      "Training epoch 6 batch 1282 with loss 1.8944705724716187, accuracy 0.2916666567325592.\n",
      "Training epoch 6 batch 1283 with loss 1.8136850595474243, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 1284 with loss 1.8414798974990845, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 1285 with loss 1.8064721822738647, accuracy 0.22777777910232544.\n",
      "Training epoch 6 batch 1286 with loss 1.788809061050415, accuracy 0.3472222089767456.\n",
      "Training epoch 6 batch 1287 with loss 1.8016847372055054, accuracy 0.1071428582072258.\n",
      "Training epoch 6 batch 1288 with loss 1.9100347757339478, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 1289 with loss 1.775238275527954, accuracy 0.17222222685813904.\n",
      "Training epoch 6 batch 1290 with loss 1.7270185947418213, accuracy 0.13055555522441864.\n",
      "Training epoch 6 batch 1291 with loss 1.8196979761123657, accuracy 0.24166667461395264.\n",
      "Training epoch 6 batch 1292 with loss 1.7070516347885132, accuracy 0.2797619104385376.\n",
      "Training epoch 6 batch 1293 with loss 1.8102235794067383, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 1294 with loss 1.7627928256988525, accuracy 0.2888889014720917.\n",
      "Training epoch 6 batch 1295 with loss 1.7946898937225342, accuracy 0.1547619104385376.\n",
      "Training epoch 6 batch 1296 with loss 1.8162095546722412, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1297 with loss 1.8285411596298218, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1298 with loss 1.6762714385986328, accuracy 0.25.\n",
      "Training epoch 6 batch 1299 with loss 1.8178513050079346, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 1300 with loss 1.796557068824768, accuracy 0.222222238779068.\n",
      "Training epoch 6 batch 1301 with loss 1.8097995519638062, accuracy 0.3432539701461792.\n",
      "Training epoch 6 batch 1302 with loss 1.641980767250061, accuracy 0.3611111342906952.\n",
      "Training epoch 6 batch 1303 with loss 1.826257348060608, accuracy 0.28333333134651184.\n",
      "Training epoch 6 batch 1304 with loss 1.7359838485717773, accuracy 0.210317462682724.\n",
      "Training epoch 6 batch 1305 with loss 1.8947229385375977, accuracy 0.125.\n",
      "Training epoch 6 batch 1306 with loss 1.787493109703064, accuracy 0.125.\n",
      "Training epoch 6 batch 1307 with loss 1.7111724615097046, accuracy 0.17499999701976776.\n",
      "Training epoch 6 batch 1308 with loss 1.8539386987686157, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1309 with loss 1.8523509502410889, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 1310 with loss 1.7989753484725952, accuracy 0.27936509251594543.\n",
      "Training epoch 6 batch 1311 with loss 1.709144949913025, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 1312 with loss 1.720416784286499, accuracy 0.13611111044883728.\n",
      "Training epoch 6 batch 1313 with loss 1.745867133140564, accuracy 0.2666666805744171.\n",
      "Training epoch 6 batch 1314 with loss 1.9089374542236328, accuracy 0.0.\n",
      "Training epoch 6 batch 1315 with loss 1.752423644065857, accuracy 0.19603174924850464.\n",
      "Training epoch 6 batch 1316 with loss 1.8398749828338623, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 1317 with loss 1.789658546447754, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 1318 with loss 1.7139497995376587, accuracy 0.2888889014720917.\n",
      "Training epoch 6 batch 1319 with loss 1.7588260173797607, accuracy 0.27222222089767456.\n",
      "Training epoch 6 batch 1320 with loss 1.8596919775009155, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 1321 with loss 1.8240501880645752, accuracy 0.1071428582072258.\n",
      "Training epoch 6 batch 1322 with loss 1.7801364660263062, accuracy 0.2658730149269104.\n",
      "Training epoch 6 batch 1323 with loss 1.7859712839126587, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 1324 with loss 1.793328046798706, accuracy 0.17222222685813904.\n",
      "Training epoch 6 batch 1325 with loss 1.7967936992645264, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 1326 with loss 1.8410208225250244, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 1327 with loss 1.796748161315918, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1328 with loss 1.819166898727417, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1329 with loss 1.7840750217437744, accuracy 0.25.\n",
      "Training epoch 6 batch 1330 with loss 1.7308546304702759, accuracy 0.3988095223903656.\n",
      "Training epoch 6 batch 1331 with loss 1.7711689472198486, accuracy 0.2916666865348816.\n",
      "Training epoch 6 batch 1332 with loss 1.8901666402816772, accuracy 0.03333333507180214.\n",
      "Training epoch 6 batch 1333 with loss 1.7841771841049194, accuracy 0.222222238779068.\n",
      "Training epoch 6 batch 1334 with loss 1.792778730392456, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 1335 with loss 1.8257347345352173, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 1336 with loss 1.7923113107681274, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 1337 with loss 1.712545394897461, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 1338 with loss 1.8147718906402588, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 1339 with loss 1.671661615371704, accuracy 0.31388887763023376.\n",
      "Training epoch 6 batch 1340 with loss 1.7164630889892578, accuracy 0.5111111402511597.\n",
      "Training epoch 6 batch 1341 with loss 1.8221638202667236, accuracy 0.17777778208255768.\n",
      "Training epoch 6 batch 1342 with loss 1.87564218044281, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 1343 with loss 1.7027877569198608, accuracy 0.4222222566604614.\n",
      "Training epoch 6 batch 1344 with loss 1.895245909690857, accuracy 0.1587301641702652.\n",
      "Training epoch 6 batch 1345 with loss 1.738538384437561, accuracy 0.33888888359069824.\n",
      "Training epoch 6 batch 1346 with loss 1.799626350402832, accuracy 0.22777777910232544.\n",
      "Training epoch 6 batch 1347 with loss 1.8469486236572266, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 1348 with loss 1.8320808410644531, accuracy 0.1547619104385376.\n",
      "Training epoch 6 batch 1349 with loss 1.8573001623153687, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 1350 with loss 1.7360395193099976, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 1351 with loss 1.8331899642944336, accuracy 0.12261904776096344.\n",
      "Training epoch 6 batch 1352 with loss 1.8230040073394775, accuracy 0.33888888359069824.\n",
      "Training epoch 6 batch 1353 with loss 1.7978713512420654, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 1354 with loss 1.9030940532684326, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 1355 with loss 1.7900011539459229, accuracy 0.22777777910232544.\n",
      "Training epoch 6 batch 1356 with loss 1.7065156698226929, accuracy 0.347222238779068.\n",
      "Training epoch 6 batch 1357 with loss 1.9114959239959717, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 1358 with loss 1.9233217239379883, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1359 with loss 1.8519237041473389, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1360 with loss 1.8271052837371826, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 1361 with loss 1.7979848384857178, accuracy 0.3115079402923584.\n",
      "Training epoch 6 batch 1362 with loss 1.7355924844741821, accuracy 0.3055555820465088.\n",
      "Training epoch 6 batch 1363 with loss 1.8013423681259155, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 1364 with loss 1.7423127889633179, accuracy 0.3055555522441864.\n",
      "Training epoch 6 batch 1365 with loss 1.7782615423202515, accuracy 0.2750000059604645.\n",
      "Training epoch 6 batch 1366 with loss 1.9256861209869385, accuracy 0.0.\n",
      "Training epoch 6 batch 1367 with loss 1.6955276727676392, accuracy 0.3472222089767456.\n",
      "Training epoch 6 batch 1368 with loss 1.8727022409439087, accuracy 0.03333333507180214.\n",
      "Training epoch 6 batch 1369 with loss 1.8854278326034546, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1370 with loss 1.818942666053772, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1371 with loss 1.7771276235580444, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 1372 with loss 1.8096554279327393, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 1373 with loss 1.7508726119995117, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 1374 with loss 1.910362958908081, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 1375 with loss 1.7650518417358398, accuracy 0.4055555462837219.\n",
      "Training epoch 6 batch 1376 with loss 1.867466926574707, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1377 with loss 1.859566330909729, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 1378 with loss 1.7862446308135986, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 1379 with loss 1.7808701992034912, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1380 with loss 1.6918882131576538, accuracy 0.32500001788139343.\n",
      "Training epoch 6 batch 1381 with loss 1.8613698482513428, accuracy 0.095238097012043.\n",
      "Training epoch 6 batch 1382 with loss 1.7752994298934937, accuracy 0.15555556118488312.\n",
      "Training epoch 6 batch 1383 with loss 1.7246698141098022, accuracy 0.3194444477558136.\n",
      "Training epoch 6 batch 1384 with loss 1.8423904180526733, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 1385 with loss 1.7978496551513672, accuracy 0.2500000298023224.\n",
      "Training epoch 6 batch 1386 with loss 1.8380095958709717, accuracy 0.1071428582072258.\n",
      "Training epoch 6 batch 1387 with loss 1.7967431545257568, accuracy 0.13333334028720856.\n",
      "Training epoch 6 batch 1388 with loss 1.8494036197662354, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 1389 with loss 1.7420198917388916, accuracy 0.49761906266212463.\n",
      "Training epoch 6 batch 1390 with loss 1.8156143426895142, accuracy 0.222222238779068.\n",
      "Training epoch 6 batch 1391 with loss 1.7959915399551392, accuracy 0.2222222238779068.\n",
      "Training epoch 6 batch 1392 with loss 1.8533817529678345, accuracy 0.065476194024086.\n",
      "Training epoch 6 batch 1393 with loss 1.8632771968841553, accuracy 0.2738095223903656.\n",
      "Training epoch 6 batch 1394 with loss 1.837644338607788, accuracy 0.204365074634552.\n",
      "Training epoch 6 batch 1395 with loss 1.6844379901885986, accuracy 0.28333333134651184.\n",
      "Training epoch 6 batch 1396 with loss 1.817437767982483, accuracy 0.1865079402923584.\n",
      "Training epoch 6 batch 1397 with loss 1.7727365493774414, accuracy 0.21944445371627808.\n",
      "Training epoch 6 batch 1398 with loss 1.8706954717636108, accuracy 0.0694444477558136.\n",
      "Training epoch 6 batch 1399 with loss 1.806382417678833, accuracy 0.31111112236976624.\n",
      "Training epoch 6 batch 1400 with loss 1.8603026866912842, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 1401 with loss 1.761125922203064, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 1402 with loss 1.7814346551895142, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 1403 with loss 1.8328964710235596, accuracy 0.05833333730697632.\n",
      "Training epoch 6 batch 1404 with loss 1.7503916025161743, accuracy 0.46388888359069824.\n",
      "Training epoch 6 batch 1405 with loss 1.6739089488983154, accuracy 0.3055555522441864.\n",
      "Training epoch 6 batch 1406 with loss 1.7964423894882202, accuracy 0.25555557012557983.\n",
      "Training epoch 6 batch 1407 with loss 1.863019347190857, accuracy 0.0555555559694767.\n",
      "Training epoch 6 batch 1408 with loss 1.7013530731201172, accuracy 0.1865079402923584.\n",
      "Training epoch 6 batch 1409 with loss 1.8849420547485352, accuracy 0.2876984179019928.\n",
      "Training epoch 6 batch 1410 with loss 1.8006902933120728, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1411 with loss 1.9008886814117432, accuracy 0.18333333730697632.\n",
      "Training epoch 6 batch 1412 with loss 1.744314193725586, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1413 with loss 1.8141695261001587, accuracy 0.432539701461792.\n",
      "Training epoch 6 batch 1414 with loss 1.7008216381072998, accuracy 0.3226190507411957.\n",
      "Training epoch 6 batch 1415 with loss 1.8764034509658813, accuracy 0.06666667014360428.\n",
      "Training epoch 6 batch 1416 with loss 1.7026703357696533, accuracy 0.4027778208255768.\n",
      "Training epoch 6 batch 1417 with loss 1.705257773399353, accuracy 0.11666667461395264.\n",
      "Training epoch 6 batch 1418 with loss 1.8941558599472046, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 1419 with loss 1.7369234561920166, accuracy 0.3194444477558136.\n",
      "Training epoch 6 batch 1420 with loss 1.804595708847046, accuracy 0.1527777761220932.\n",
      "Training epoch 6 batch 1421 with loss 1.777752161026001, accuracy 0.3500000238418579.\n",
      "Training epoch 6 batch 1422 with loss 1.7475507259368896, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 1423 with loss 1.91824471950531, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 1424 with loss 1.8042997121810913, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 1425 with loss 1.8193773031234741, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 1426 with loss 1.8217127323150635, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1427 with loss 1.7774146795272827, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 1428 with loss 1.8207578659057617, accuracy 0.222222238779068.\n",
      "Training epoch 6 batch 1429 with loss 1.7950623035430908, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 1430 with loss 1.8517040014266968, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 1431 with loss 1.8194620609283447, accuracy 0.1111111119389534.\n",
      "Training epoch 6 batch 1432 with loss 1.8775451183319092, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 1433 with loss 1.7391010522842407, accuracy 0.25.\n",
      "Training epoch 6 batch 1434 with loss 1.6551662683486938, accuracy 0.29722222685813904.\n",
      "Training epoch 6 batch 1435 with loss 1.8364474773406982, accuracy 0.065476194024086.\n",
      "Training epoch 6 batch 1436 with loss 1.7806657552719116, accuracy 0.065476194024086.\n",
      "Training epoch 6 batch 1437 with loss 1.8668544292449951, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 1438 with loss 1.7866830825805664, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 1439 with loss 1.8425754308700562, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 1440 with loss 1.8535568714141846, accuracy 0.1071428582072258.\n",
      "Training epoch 6 batch 1441 with loss 1.7715632915496826, accuracy 0.21666666865348816.\n",
      "Training epoch 6 batch 1442 with loss 1.8237155675888062, accuracy 0.125.\n",
      "Training epoch 6 batch 1443 with loss 1.7474234104156494, accuracy 0.236111119389534.\n",
      "Training epoch 6 batch 1444 with loss 1.7702443599700928, accuracy 0.236111119389534.\n",
      "Training epoch 6 batch 1445 with loss 1.7569233179092407, accuracy 0.22777777910232544.\n",
      "Training epoch 6 batch 1446 with loss 1.8000829219818115, accuracy 0.2638888955116272.\n",
      "Training epoch 6 batch 1447 with loss 1.9044215679168701, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1448 with loss 1.7922039031982422, accuracy 0.2916666865348816.\n",
      "Training epoch 6 batch 1449 with loss 1.665601134300232, accuracy 0.5158730149269104.\n",
      "Training epoch 6 batch 1450 with loss 1.7877576351165771, accuracy 0.31666669249534607.\n",
      "Training epoch 6 batch 1451 with loss 1.8322557210922241, accuracy 0.15333333611488342.\n",
      "Training epoch 6 batch 1452 with loss 1.7093712091445923, accuracy 0.3027777671813965.\n",
      "Training epoch 6 batch 1453 with loss 1.7744745016098022, accuracy 0.33888888359069824.\n",
      "Training epoch 6 batch 1454 with loss 1.8467817306518555, accuracy 0.1805555522441864.\n",
      "Training epoch 6 batch 1455 with loss 1.728967308998108, accuracy 0.25.\n",
      "Training epoch 6 batch 1456 with loss 1.771749496459961, accuracy 0.23333334922790527.\n",
      "Training epoch 6 batch 1457 with loss 1.741586446762085, accuracy 0.2698412835597992.\n",
      "Training epoch 6 batch 1458 with loss 1.8815596103668213, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1459 with loss 1.7610126733779907, accuracy 0.2083333432674408.\n",
      "Training epoch 6 batch 1460 with loss 1.7901862859725952, accuracy 0.13055555522441864.\n",
      "Training epoch 6 batch 1461 with loss 1.7888908386230469, accuracy 0.1180555522441864.\n",
      "Training epoch 6 batch 1462 with loss 1.699845552444458, accuracy 0.2718254029750824.\n",
      "Training epoch 6 batch 1463 with loss 1.7267391681671143, accuracy 0.15555556118488312.\n",
      "Training epoch 6 batch 1464 with loss 1.8235441446304321, accuracy 0.03333333507180214.\n",
      "Training epoch 6 batch 1465 with loss 1.7671337127685547, accuracy 0.4166666865348816.\n",
      "Training epoch 6 batch 1466 with loss 1.7437585592269897, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1467 with loss 1.8079532384872437, accuracy 0.17777778208255768.\n",
      "Training epoch 6 batch 1468 with loss 1.7797269821166992, accuracy 0.2152777910232544.\n",
      "Training epoch 6 batch 1469 with loss 1.7912555932998657, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 1470 with loss 1.7241560220718384, accuracy 0.1666666716337204.\n",
      "Training epoch 6 batch 1471 with loss 1.86700439453125, accuracy 0.07500000298023224.\n",
      "Training epoch 6 batch 1472 with loss 1.7800251245498657, accuracy 0.25.\n",
      "Training epoch 6 batch 1473 with loss 1.7648098468780518, accuracy 0.10833333432674408.\n",
      "Training epoch 6 batch 1474 with loss 1.7121622562408447, accuracy 0.2750000059604645.\n",
      "Training epoch 6 batch 1475 with loss 1.7486950159072876, accuracy 0.2361111044883728.\n",
      "Training epoch 6 batch 1476 with loss 1.8535499572753906, accuracy 0.08888889104127884.\n",
      "Training epoch 6 batch 1477 with loss 1.8500388860702515, accuracy 0.06111111491918564.\n",
      "Training epoch 6 batch 1478 with loss 1.81179678440094, accuracy 0.2750000059604645.\n",
      "Training epoch 6 batch 1479 with loss 1.8943917751312256, accuracy 0.0972222238779068.\n",
      "Training epoch 6 batch 1480 with loss 1.8406217098236084, accuracy 0.20000000298023224.\n",
      "Training epoch 6 batch 1481 with loss 1.7470667362213135, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1482 with loss 1.767059326171875, accuracy 0.18611112236976624.\n",
      "Training epoch 6 batch 1483 with loss 1.7680177688598633, accuracy 0.05714286118745804.\n",
      "Training epoch 6 batch 1484 with loss 1.8189748525619507, accuracy 0.1349206417798996.\n",
      "Training epoch 6 batch 1485 with loss 1.7681795358657837, accuracy 0.2777777910232544.\n",
      "Training epoch 6 batch 1486 with loss 1.7879583835601807, accuracy 0.1944444477558136.\n",
      "Training epoch 6 batch 1487 with loss 1.7938916683197021, accuracy 0.1458333283662796.\n",
      "Training epoch 6 batch 1488 with loss 1.9543583393096924, accuracy 0.1388888955116272.\n",
      "Training epoch 6 batch 1489 with loss 1.7622514963150024, accuracy 0.3392857313156128.\n",
      "Training epoch 6 batch 1490 with loss 1.8203527927398682, accuracy 0.02777777798473835.\n",
      "Training epoch 6 batch 1491 with loss 1.8994064331054688, accuracy 0.0416666679084301.\n",
      "Training epoch 6 batch 1492 with loss 1.7655588388442993, accuracy 0.1527777910232544.\n",
      "Training epoch 6 batch 1493 with loss 1.8900206089019775, accuracy 0.0.\n",
      "Training epoch 6 batch 1494 with loss 1.7676210403442383, accuracy 0.14444445073604584.\n",
      "Training epoch 6 batch 1495 with loss 1.7681806087493896, accuracy 0.15000000596046448.\n",
      "Training epoch 6 batch 1496 with loss 1.7995134592056274, accuracy 0.0.\n",
      "Training epoch 6 batch 1497 with loss 1.7730060815811157, accuracy 0.3055555820465088.\n",
      "Training epoch 6 batch 1498 with loss 1.8681522607803345, accuracy 0.0833333358168602.\n",
      "Training epoch 6 batch 1499 with loss 1.7323176860809326, accuracy 0.1666666716337204.\n",
      "Test batch 0 with loss 1.8037807941436768 and accuracy 0.19722223281860352.\n",
      "Test batch 1 with loss 1.9363377094268799 and accuracy 0.0416666679084301.\n",
      "Test batch 2 with loss 1.8483521938323975 and accuracy 0.0555555559694767.\n",
      "Test batch 3 with loss 1.8041130304336548 and accuracy 0.2083333432674408.\n",
      "Test batch 4 with loss 1.9174715280532837 and accuracy 0.13055555522441864.\n",
      "Test batch 5 with loss 1.7977850437164307 and accuracy 0.24166667461395264.\n",
      "Test batch 6 with loss 1.7800796031951904 and accuracy 0.2916666567325592.\n",
      "Test batch 7 with loss 1.7359960079193115 and accuracy 0.3777777850627899.\n",
      "Test batch 8 with loss 1.8121181726455688 and accuracy 0.15833333134651184.\n",
      "Test batch 9 with loss 1.8165967464447021 and accuracy 0.2777777910232544.\n",
      "Test batch 10 with loss 1.8234922885894775 and accuracy 0.20000000298023224.\n",
      "Test batch 11 with loss 1.6892039775848389 and accuracy 0.33888888359069824.\n",
      "Test batch 12 with loss 1.8680559396743774 and accuracy 0.1111111119389534.\n",
      "Test batch 13 with loss 1.7846715450286865 and accuracy 0.1944444477558136.\n",
      "Test batch 14 with loss 1.8433096408843994 and accuracy 0.204365074634552.\n",
      "Test batch 15 with loss 1.8068950176239014 and accuracy 0.13333334028720856.\n",
      "Test batch 16 with loss 1.8248344659805298 and accuracy 0.0.\n",
      "Test batch 17 with loss 1.7679870128631592 and accuracy 0.10000000894069672.\n",
      "Test batch 18 with loss 1.8162460327148438 and accuracy 0.1944444477558136.\n",
      "Test batch 19 with loss 1.8525683879852295 and accuracy 0.0555555559694767.\n",
      "Test batch 20 with loss 1.7935060262680054 and accuracy 0.0972222238779068.\n",
      "Test batch 21 with loss 1.788731575012207 and accuracy 0.3194444477558136.\n",
      "Test batch 22 with loss 1.761649489402771 and accuracy 0.125.\n",
      "Test batch 23 with loss 1.8048467636108398 and accuracy 0.16333332657814026.\n",
      "Test batch 24 with loss 1.8507064580917358 and accuracy 0.29722222685813904.\n",
      "Test batch 25 with loss 1.770825982093811 and accuracy 0.25.\n",
      "Test batch 26 with loss 1.8540706634521484 and accuracy 0.02380952425301075.\n",
      "Test batch 27 with loss 1.8839292526245117 and accuracy 0.095238097012043.\n",
      "Test batch 28 with loss 1.8243522644042969 and accuracy 0.30000001192092896.\n",
      "Test batch 29 with loss 1.9182710647583008 and accuracy 0.1388888955116272.\n",
      "Test batch 30 with loss 1.8286679983139038 and accuracy 0.07500000298023224.\n",
      "Test batch 31 with loss 1.8840084075927734 and accuracy 0.1349206417798996.\n",
      "Test batch 32 with loss 1.8691707849502563 and accuracy 0.0892857164144516.\n",
      "Test batch 33 with loss 1.810691237449646 and accuracy 0.1111111119389534.\n",
      "Test batch 34 with loss 1.8634097576141357 and accuracy 0.12777778506278992.\n",
      "Test batch 35 with loss 1.7877864837646484 and accuracy 0.10000000149011612.\n",
      "Test batch 36 with loss 1.8662211894989014 and accuracy 0.0694444477558136.\n",
      "Test batch 37 with loss 1.818892478942871 and accuracy 0.0833333358168602.\n",
      "Test batch 38 with loss 1.850515604019165 and accuracy 0.02777777798473835.\n",
      "Test batch 39 with loss 1.9491592645645142 and accuracy 0.0.\n",
      "Test batch 40 with loss 1.9175443649291992 and accuracy 0.0.\n",
      "Test batch 41 with loss 1.7455580234527588 and accuracy 0.1388888955116272.\n",
      "Test batch 42 with loss 1.7858432531356812 and accuracy 0.2738095223903656.\n",
      "Test batch 43 with loss 1.8053009510040283 and accuracy 0.12222222983837128.\n",
      "Test batch 44 with loss 1.7537319660186768 and accuracy 0.25.\n",
      "Test batch 45 with loss 1.8505198955535889 and accuracy 0.0694444477558136.\n",
      "Test batch 46 with loss 1.7897955179214478 and accuracy 0.17777778208255768.\n",
      "Test batch 47 with loss 1.757257103919983 and accuracy 0.22777777910232544.\n",
      "Test batch 48 with loss 1.7357937097549438 and accuracy 0.18333333730697632.\n",
      "Test batch 49 with loss 1.880422830581665 and accuracy 0.1805555671453476.\n",
      "Test batch 50 with loss 1.8967301845550537 and accuracy 0.03333333507180214.\n",
      "Test batch 51 with loss 1.8882402181625366 and accuracy 0.3166666626930237.\n",
      "Test batch 52 with loss 1.8434511423110962 and accuracy 0.02380952425301075.\n",
      "Test batch 53 with loss 1.8058973550796509 and accuracy 0.1111111119389534.\n",
      "Test batch 54 with loss 1.8735183477401733 and accuracy 0.1666666716337204.\n",
      "Test batch 55 with loss 1.8797553777694702 and accuracy 0.14166666567325592.\n",
      "Test batch 56 with loss 1.997926115989685 and accuracy 0.0.\n",
      "Test batch 57 with loss 1.8434817790985107 and accuracy 0.125.\n",
      "Test batch 58 with loss 1.8623425960540771 and accuracy 0.1388888955116272.\n",
      "Test batch 59 with loss 1.8978437185287476 and accuracy 0.0833333358168602.\n",
      "Test batch 60 with loss 1.7911443710327148 and accuracy 0.2944444715976715.\n",
      "Test batch 61 with loss 1.8259989023208618 and accuracy 0.1031746044754982.\n",
      "Test batch 62 with loss 1.8366749286651611 and accuracy 0.1626984179019928.\n",
      "Test batch 63 with loss 1.8292675018310547 and accuracy 0.1388888955116272.\n",
      "Test batch 64 with loss 1.8738222122192383 and accuracy 0.07500000298023224.\n",
      "Test batch 65 with loss 1.7990291118621826 and accuracy 0.21388888359069824.\n",
      "Test batch 66 with loss 1.8738120794296265 and accuracy 0.08888889104127884.\n",
      "Test batch 67 with loss 1.8919928073883057 and accuracy 0.08888889104127884.\n",
      "Test batch 68 with loss 1.8302738666534424 and accuracy 0.15833333134651184.\n",
      "Test batch 69 with loss 1.832775354385376 and accuracy 0.09444444626569748.\n",
      "Test batch 70 with loss 1.7842795848846436 and accuracy 0.0555555559694767.\n",
      "Test batch 71 with loss 1.8252894878387451 and accuracy 0.02380952425301075.\n",
      "Test batch 72 with loss 1.835741639137268 and accuracy 0.18888889253139496.\n",
      "Test batch 73 with loss 1.852738618850708 and accuracy 0.03333333507180214.\n",
      "Test batch 74 with loss 1.897582769393921 and accuracy 0.0833333358168602.\n",
      "Test batch 75 with loss 1.9026991128921509 and accuracy 0.1666666716337204.\n",
      "Test batch 76 with loss 1.8323787450790405 and accuracy 0.1944444477558136.\n",
      "Test batch 77 with loss 1.8325035572052002 and accuracy 0.02380952425301075.\n",
      "Test batch 78 with loss 1.760193109512329 and accuracy 0.5000000596046448.\n",
      "Test batch 79 with loss 1.7631734609603882 and accuracy 0.125.\n",
      "Test batch 80 with loss 1.856008768081665 and accuracy 0.22685185074806213.\n",
      "Test batch 81 with loss 1.8551838397979736 and accuracy 0.20000000298023224.\n",
      "Test batch 82 with loss 1.850939154624939 and accuracy 0.0416666679084301.\n",
      "Test batch 83 with loss 1.7924776077270508 and accuracy 0.06666667014360428.\n",
      "Test batch 84 with loss 1.8187525272369385 and accuracy 0.14166668057441711.\n",
      "Test batch 85 with loss 1.8634655475616455 and accuracy 0.4861111044883728.\n",
      "Test batch 86 with loss 1.8204275369644165 and accuracy 0.2222222238779068.\n",
      "Test batch 87 with loss 1.8862552642822266 and accuracy 0.0416666679084301.\n",
      "Test batch 88 with loss 1.9135630130767822 and accuracy 0.0416666679084301.\n",
      "Test batch 89 with loss 1.755358099937439 and accuracy 0.1666666716337204.\n",
      "Test batch 90 with loss 1.955646276473999 and accuracy 0.0.\n",
      "Test batch 91 with loss 1.8230173587799072 and accuracy 0.1666666716337204.\n",
      "Test batch 92 with loss 1.8833659887313843 and accuracy 0.2916666865348816.\n",
      "Test batch 93 with loss 1.8358227014541626 and accuracy 0.125.\n",
      "Test batch 94 with loss 1.8284385204315186 and accuracy 0.0972222238779068.\n",
      "Test batch 95 with loss 1.8208869695663452 and accuracy 0.1111111119389534.\n",
      "Test batch 96 with loss 1.8398853540420532 and accuracy 0.21388888359069824.\n",
      "Test batch 97 with loss 1.712752103805542 and accuracy 0.4861111044883728.\n",
      "Test batch 98 with loss 1.8463550806045532 and accuracy 0.14444445073604584.\n",
      "Test batch 99 with loss 1.8429063558578491 and accuracy 0.06666667014360428.\n",
      "Test batch 100 with loss 1.7924516201019287 and accuracy 0.0694444477558136.\n",
      "Test batch 101 with loss 1.7939544916152954 and accuracy 0.0476190485060215.\n",
      "Test batch 102 with loss 1.8075815439224243 and accuracy 0.0.\n",
      "Test batch 103 with loss 1.766296148300171 and accuracy 0.1319444477558136.\n",
      "Test batch 104 with loss 1.8846343755722046 and accuracy 0.190476194024086.\n",
      "Test batch 105 with loss 1.8638298511505127 and accuracy 0.222222238779068.\n",
      "Test batch 106 with loss 1.8021942377090454 and accuracy 0.222222238779068.\n",
      "Test batch 107 with loss 1.7789169549942017 and accuracy 0.0555555559694767.\n",
      "Test batch 108 with loss 1.735873818397522 and accuracy 0.14444445073604584.\n",
      "Test batch 109 with loss 1.8819297552108765 and accuracy 0.08888889104127884.\n",
      "Test batch 110 with loss 1.7075378894805908 and accuracy 0.4027777910232544.\n",
      "Test batch 111 with loss 1.8592609167099 and accuracy 0.2083333432674408.\n",
      "Test batch 112 with loss 1.8070220947265625 and accuracy 0.1111111119389534.\n",
      "Test batch 113 with loss 1.8018133640289307 and accuracy 0.14444445073604584.\n",
      "Test batch 114 with loss 1.856433629989624 and accuracy 0.08888889104127884.\n",
      "Test batch 115 with loss 1.8938707113265991 and accuracy 0.23333334922790527.\n",
      "Test batch 116 with loss 1.6978496313095093 and accuracy 0.2944444417953491.\n",
      "Test batch 117 with loss 1.8035398721694946 and accuracy 0.2698412835597992.\n",
      "Test batch 118 with loss 1.7443749904632568 and accuracy 0.22777777910232544.\n",
      "Test batch 119 with loss 1.9222028255462646 and accuracy 0.17777778208255768.\n",
      "Test batch 120 with loss 1.8458681106567383 and accuracy 0.1388888955116272.\n",
      "Test batch 121 with loss 1.8570137023925781 and accuracy 0.10000000894069672.\n",
      "Test batch 122 with loss 1.8143062591552734 and accuracy 0.125.\n",
      "Test batch 123 with loss 1.8071972131729126 and accuracy 0.31111112236976624.\n",
      "Test batch 124 with loss 1.7434885501861572 and accuracy 0.11666667461395264.\n",
      "Test batch 125 with loss 1.8249728679656982 and accuracy 0.17777778208255768.\n",
      "Test batch 126 with loss 1.925843596458435 and accuracy 0.1488095223903656.\n",
      "Test batch 127 with loss 1.9105112552642822 and accuracy 0.1071428582072258.\n",
      "Test batch 128 with loss 1.8727436065673828 and accuracy 0.1388888955116272.\n",
      "Test batch 129 with loss 1.8508703708648682 and accuracy 0.1666666716337204.\n",
      "Test batch 130 with loss 1.8173682689666748 and accuracy 0.17222222685813904.\n",
      "Test batch 131 with loss 2.0288610458374023 and accuracy 0.0833333358168602.\n",
      "Test batch 132 with loss 1.7809911966323853 and accuracy 0.24722224473953247.\n",
      "Test batch 133 with loss 1.8171886205673218 and accuracy 0.2083333432674408.\n",
      "Test batch 134 with loss 1.8360402584075928 and accuracy 0.02380952425301075.\n",
      "Test batch 135 with loss 1.9148380756378174 and accuracy 0.0416666679084301.\n",
      "Test batch 136 with loss 1.9027900695800781 and accuracy 0.1666666716337204.\n",
      "Test batch 137 with loss 1.8408820629119873 and accuracy 0.32261908054351807.\n",
      "Test batch 138 with loss 1.9029178619384766 and accuracy 0.14444445073604584.\n",
      "Test batch 139 with loss 1.8919719457626343 and accuracy 0.1666666716337204.\n",
      "Test batch 140 with loss 1.8559958934783936 and accuracy 0.2611111104488373.\n",
      "Test batch 141 with loss 1.8127597570419312 and accuracy 0.13333334028720856.\n",
      "Test batch 142 with loss 1.816504716873169 and accuracy 0.0972222238779068.\n",
      "Test batch 143 with loss 1.8694947957992554 and accuracy 0.1071428582072258.\n",
      "Test batch 144 with loss 1.7687513828277588 and accuracy 0.16388890147209167.\n",
      "Test batch 145 with loss 1.9457134008407593 and accuracy 0.2380952537059784.\n",
      "Test batch 146 with loss 1.7029287815093994 and accuracy 0.2888889014720917.\n",
      "Test batch 147 with loss 1.7455835342407227 and accuracy 0.1805555522441864.\n",
      "Test batch 148 with loss 1.7887684106826782 and accuracy 0.0972222238779068.\n",
      "Test batch 149 with loss 1.7247498035430908 and accuracy 0.3444444537162781.\n",
      "Test batch 150 with loss 1.8695656061172485 and accuracy 0.1388888955116272.\n",
      "Test batch 151 with loss 1.7684543132781982 and accuracy 0.17222222685813904.\n",
      "Test batch 152 with loss 1.8126474618911743 and accuracy 0.2083333432674408.\n",
      "Test batch 153 with loss 1.8613545894622803 and accuracy 0.1805555522441864.\n",
      "Test batch 154 with loss 1.906587839126587 and accuracy 0.03333333507180214.\n",
      "Test batch 155 with loss 1.7619823217391968 and accuracy 0.3083333373069763.\n",
      "Test batch 156 with loss 1.801796317100525 and accuracy 0.30158731341362.\n",
      "Test batch 157 with loss 1.8233556747436523 and accuracy 0.03333333507180214.\n",
      "Test batch 158 with loss 1.7986526489257812 and accuracy 0.1666666716337204.\n",
      "Test batch 159 with loss 1.6995627880096436 and accuracy 0.2777777910232544.\n",
      "Test batch 160 with loss 1.8162736892700195 and accuracy 0.12222222983837128.\n",
      "Test batch 161 with loss 1.8649570941925049 and accuracy 0.018518518656492233.\n",
      "Test batch 162 with loss 1.8400859832763672 and accuracy 0.2611111104488373.\n",
      "Test batch 163 with loss 1.9275588989257812 and accuracy 0.0833333358168602.\n",
      "Test batch 164 with loss 1.7723815441131592 and accuracy 0.15000000596046448.\n",
      "Test batch 165 with loss 1.8103077411651611 and accuracy 0.12222222983837128.\n",
      "Test batch 166 with loss 1.9058573246002197 and accuracy 0.0833333358168602.\n",
      "Test batch 167 with loss 1.8177999258041382 and accuracy 0.1111111119389534.\n",
      "Test batch 168 with loss 1.9089686870574951 and accuracy 0.14444445073604584.\n",
      "Test batch 169 with loss 1.8145456314086914 and accuracy 0.0972222238779068.\n",
      "Test batch 170 with loss 1.8458518981933594 and accuracy 0.2611111104488373.\n",
      "Test batch 171 with loss 1.8490550518035889 and accuracy 0.1527777761220932.\n",
      "Test batch 172 with loss 1.9004989862442017 and accuracy 0.0.\n",
      "Test batch 173 with loss 1.8426004648208618 and accuracy 0.11666667461395264.\n",
      "Test batch 174 with loss 1.7611496448516846 and accuracy 0.3976190686225891.\n",
      "Test batch 175 with loss 1.7541782855987549 and accuracy 0.21944445371627808.\n",
      "Test batch 176 with loss 1.7787010669708252 and accuracy 0.33095240592956543.\n",
      "Test batch 177 with loss 1.852052092552185 and accuracy 0.06666667014360428.\n",
      "Test batch 178 with loss 1.7444336414337158 and accuracy 0.0892857164144516.\n",
      "Test batch 179 with loss 1.836605429649353 and accuracy 0.0555555559694767.\n",
      "Test batch 180 with loss 1.9109033346176147 and accuracy 0.2638888955116272.\n",
      "Test batch 181 with loss 1.8767480850219727 and accuracy 0.02777777798473835.\n",
      "Test batch 182 with loss 1.888420820236206 and accuracy 0.2250000238418579.\n",
      "Test batch 183 with loss 1.7631597518920898 and accuracy 0.255952388048172.\n",
      "Test batch 184 with loss 1.7723300457000732 and accuracy 0.17222222685813904.\n",
      "Test batch 185 with loss 1.870842695236206 and accuracy 0.08888889104127884.\n",
      "Test batch 186 with loss 1.7977244853973389 and accuracy 0.3531745970249176.\n",
      "Test batch 187 with loss 1.848406195640564 and accuracy 0.14444445073604584.\n",
      "Test batch 188 with loss 1.9774589538574219 and accuracy 0.11666667461395264.\n",
      "Test batch 189 with loss 1.8030697107315063 and accuracy 0.21944445371627808.\n",
      "Test batch 190 with loss 1.7572708129882812 and accuracy 0.3333333432674408.\n",
      "Test batch 191 with loss 1.8952884674072266 and accuracy 0.0763888880610466.\n",
      "Test batch 192 with loss 1.7690223455429077 and accuracy 0.3166666626930237.\n",
      "Test batch 193 with loss 1.7664735317230225 and accuracy 0.1944444477558136.\n",
      "Test batch 194 with loss 1.8380296230316162 and accuracy 0.3055555820465088.\n",
      "Test batch 195 with loss 1.8294662237167358 and accuracy 0.16388890147209167.\n",
      "Test batch 196 with loss 1.8441197872161865 and accuracy 0.0833333358168602.\n",
      "Test batch 197 with loss 1.8097450733184814 and accuracy 0.15833333134651184.\n",
      "Test batch 198 with loss 1.7878448963165283 and accuracy 0.15000000596046448.\n",
      "Test batch 199 with loss 1.790732979774475 and accuracy 0.0833333358168602.\n",
      "Test batch 200 with loss 1.8055684566497803 and accuracy 0.24166667461395264.\n",
      "Test batch 201 with loss 1.7791239023208618 and accuracy 0.3722222149372101.\n",
      "Test batch 202 with loss 1.8354456424713135 and accuracy 0.11428572237491608.\n",
      "Test batch 203 with loss 1.865248441696167 and accuracy 0.125.\n",
      "Test batch 204 with loss 1.8721897602081299 and accuracy 0.15000000596046448.\n",
      "Test batch 205 with loss 1.791776418685913 and accuracy 0.1527777761220932.\n",
      "Test batch 206 with loss 1.69003164768219 and accuracy 0.39444443583488464.\n",
      "Test batch 207 with loss 1.792456030845642 and accuracy 0.1210317462682724.\n",
      "Test batch 208 with loss 1.8898998498916626 and accuracy 0.08888889104127884.\n",
      "Test batch 209 with loss 1.8435580730438232 and accuracy 0.17222222685813904.\n",
      "Test batch 210 with loss 1.7962391376495361 and accuracy 0.06666667014360428.\n",
      "Test batch 211 with loss 1.939719557762146 and accuracy 0.0.\n",
      "Test batch 212 with loss 1.825528860092163 and accuracy 0.17222222685813904.\n",
      "Test batch 213 with loss 1.8480570316314697 and accuracy 0.1111111119389534.\n",
      "Test batch 214 with loss 1.8320424556732178 and accuracy 0.10833333432674408.\n",
      "Test batch 215 with loss 1.824288010597229 and accuracy 0.17222222685813904.\n",
      "Test batch 216 with loss 1.775198221206665 and accuracy 0.07500000298023224.\n",
      "Test batch 217 with loss 1.8449703454971313 and accuracy 0.13055555522441864.\n",
      "Test batch 218 with loss 1.8028128147125244 and accuracy 0.0972222238779068.\n",
      "Test batch 219 with loss 1.8170948028564453 and accuracy 0.0972222238779068.\n",
      "Test batch 220 with loss 1.717816948890686 and accuracy 0.2063492238521576.\n",
      "Test batch 221 with loss 1.8297971487045288 and accuracy 0.1547619104385376.\n",
      "Test batch 222 with loss 1.829960584640503 and accuracy 0.222222238779068.\n",
      "Test batch 223 with loss 1.8503910303115845 and accuracy 0.0833333358168602.\n",
      "Test batch 224 with loss 1.7913007736206055 and accuracy 0.25555557012557983.\n",
      "Test batch 225 with loss 1.8063262701034546 and accuracy 0.03333333507180214.\n",
      "Test batch 226 with loss 1.8350553512573242 and accuracy 0.095238097012043.\n",
      "Test batch 227 with loss 1.7913782596588135 and accuracy 0.1944444477558136.\n",
      "Test batch 228 with loss 1.7910511493682861 and accuracy 0.0694444477558136.\n",
      "Test batch 229 with loss 1.8328453302383423 and accuracy 0.11666667461395264.\n",
      "Test batch 230 with loss 1.7398884296417236 and accuracy 0.42500001192092896.\n",
      "Test batch 231 with loss 1.9084656238555908 and accuracy 0.14444445073604584.\n",
      "Test batch 232 with loss 1.808934211730957 and accuracy 0.20000001788139343.\n",
      "Test batch 233 with loss 1.8158305883407593 and accuracy 0.3333333432674408.\n",
      "Test batch 234 with loss 1.815040946006775 and accuracy 0.19166666269302368.\n",
      "Test batch 235 with loss 1.7434431314468384 and accuracy 0.3638888895511627.\n",
      "Test batch 236 with loss 1.851528525352478 and accuracy 0.1388888955116272.\n",
      "Test batch 237 with loss 1.802825689315796 and accuracy 0.1805555671453476.\n",
      "Test batch 238 with loss 1.7733110189437866 and accuracy 0.2888889014720917.\n",
      "Test batch 239 with loss 1.7490489482879639 and accuracy 0.144841268658638.\n",
      "Test batch 240 with loss 1.765228509902954 and accuracy 0.2777777910232544.\n",
      "Test batch 241 with loss 1.824314832687378 and accuracy 0.13055555522441864.\n",
      "Test batch 242 with loss 1.761980652809143 and accuracy 0.2083333283662796.\n",
      "Test batch 243 with loss 1.8825368881225586 and accuracy 0.2083333432674408.\n",
      "Test batch 244 with loss 1.9407227039337158 and accuracy 0.06666667014360428.\n",
      "Test batch 245 with loss 1.863477349281311 and accuracy 0.08095238357782364.\n",
      "Test batch 246 with loss 1.9824355840682983 and accuracy 0.1111111119389534.\n",
      "Test batch 247 with loss 1.840989112854004 and accuracy 0.0833333358168602.\n",
      "Test batch 248 with loss 1.7675535678863525 and accuracy 0.0555555559694767.\n",
      "Test batch 249 with loss 1.8106610774993896 and accuracy 0.29722222685813904.\n",
      "Test batch 250 with loss 1.9136121273040771 and accuracy 0.03333333507180214.\n",
      "Test batch 251 with loss 1.7731742858886719 and accuracy 0.222222238779068.\n",
      "Test batch 252 with loss 1.7724918127059937 and accuracy 0.08888889104127884.\n",
      "Test batch 253 with loss 1.7872626781463623 and accuracy 0.347222238779068.\n",
      "Test batch 254 with loss 1.8269455432891846 and accuracy 0.1805555522441864.\n",
      "Test batch 255 with loss 1.8381325006484985 and accuracy 0.07500000298023224.\n",
      "Test batch 256 with loss 1.7817847728729248 and accuracy 0.144841268658638.\n",
      "Test batch 257 with loss 1.7833513021469116 and accuracy 0.472222238779068.\n",
      "Test batch 258 with loss 1.8017534017562866 and accuracy 0.24166667461395264.\n",
      "Test batch 259 with loss 1.8046468496322632 and accuracy 0.10833333432674408.\n",
      "Test batch 260 with loss 1.9062563180923462 and accuracy 0.0972222238779068.\n",
      "Test batch 261 with loss 1.7885364294052124 and accuracy 0.0833333358168602.\n",
      "Test batch 262 with loss 1.7549870014190674 and accuracy 0.1527777761220932.\n",
      "Test batch 263 with loss 1.9855047464370728 and accuracy 0.07500000298023224.\n",
      "Test batch 264 with loss 1.9061391353607178 and accuracy 0.0833333358168602.\n",
      "Test batch 265 with loss 1.8707141876220703 and accuracy 0.0416666679084301.\n",
      "Test batch 266 with loss 1.7737581729888916 and accuracy 0.0972222238779068.\n",
      "Test batch 267 with loss 1.829170823097229 and accuracy 0.10000000149011612.\n",
      "Test batch 268 with loss 1.8382854461669922 and accuracy 0.0416666679084301.\n",
      "Test batch 269 with loss 1.7247552871704102 and accuracy 0.1527777761220932.\n",
      "Test batch 270 with loss 1.8530826568603516 and accuracy 0.2638888955116272.\n",
      "Test batch 271 with loss 1.7467588186264038 and accuracy 0.2916666865348816.\n",
      "Test batch 272 with loss 1.7977222204208374 and accuracy 0.1944444477558136.\n",
      "Test batch 273 with loss 1.9493976831436157 and accuracy 0.08888889104127884.\n",
      "Test batch 274 with loss 1.7800500392913818 and accuracy 0.1388888955116272.\n",
      "Test batch 275 with loss 1.7476825714111328 and accuracy 0.2321428656578064.\n",
      "Test batch 276 with loss 1.7504152059555054 and accuracy 0.2222222238779068.\n",
      "Test batch 277 with loss 1.8974287509918213 and accuracy 0.13055555522441864.\n",
      "Test batch 278 with loss 1.8810068368911743 and accuracy 0.2083333432674408.\n",
      "Test batch 279 with loss 1.8628740310668945 and accuracy 0.14444445073604584.\n",
      "Test batch 280 with loss 1.7740192413330078 and accuracy 0.409722238779068.\n",
      "Test batch 281 with loss 1.9102891683578491 and accuracy 0.0833333358168602.\n",
      "Test batch 282 with loss 1.8493859767913818 and accuracy 0.08888889104127884.\n",
      "Test batch 283 with loss 1.9004456996917725 and accuracy 0.0833333358168602.\n",
      "Test batch 284 with loss 1.8503633737564087 and accuracy 0.1388888955116272.\n",
      "Test batch 285 with loss 1.8393052816390991 and accuracy 0.0694444477558136.\n",
      "Test batch 286 with loss 1.831165075302124 and accuracy 0.0833333358168602.\n",
      "Test batch 287 with loss 1.7778774499893188 and accuracy 0.38055557012557983.\n",
      "Test batch 288 with loss 1.8987230062484741 and accuracy 0.05714286118745804.\n",
      "Test batch 289 with loss 1.8494598865509033 and accuracy 0.1666666716337204.\n",
      "Test batch 290 with loss 1.721353530883789 and accuracy 0.17222222685813904.\n",
      "Test batch 291 with loss 1.9386730194091797 and accuracy 0.13055555522441864.\n",
      "Test batch 292 with loss 1.9355249404907227 and accuracy 0.0763888880610466.\n",
      "Test batch 293 with loss 1.8157964944839478 and accuracy 0.1388888955116272.\n",
      "Test batch 294 with loss 1.9316463470458984 and accuracy 0.21944445371627808.\n",
      "Test batch 295 with loss 1.8691952228546143 and accuracy 0.125.\n",
      "Test batch 296 with loss 1.7384874820709229 and accuracy 0.3531745970249176.\n",
      "Test batch 297 with loss 1.8856747150421143 and accuracy 0.0833333358168602.\n",
      "Test batch 298 with loss 1.778956651687622 and accuracy 0.2916666865348816.\n",
      "Test batch 299 with loss 1.8028786182403564 and accuracy 0.1388888955116272.\n",
      "Test batch 300 with loss 1.8754193782806396 and accuracy 0.14444445073604584.\n",
      "Test batch 301 with loss 1.7831062078475952 and accuracy 0.24722224473953247.\n",
      "Test batch 302 with loss 1.8605594635009766 and accuracy 0.0416666679084301.\n",
      "Test batch 303 with loss 1.6778656244277954 and accuracy 0.30158731341362.\n",
      "Test batch 304 with loss 1.8805294036865234 and accuracy 0.07500000298023224.\n",
      "Test batch 305 with loss 1.740643858909607 and accuracy 0.20000000298023224.\n",
      "Test batch 306 with loss 1.957416296005249 and accuracy 0.0416666679084301.\n",
      "Test batch 307 with loss 1.7964366674423218 and accuracy 0.236111119389534.\n",
      "Test batch 308 with loss 1.8419862985610962 and accuracy 0.0793650820851326.\n",
      "Test batch 309 with loss 1.8284788131713867 and accuracy 0.1666666716337204.\n",
      "Test batch 310 with loss 1.745689034461975 and accuracy 0.261904776096344.\n",
      "Test batch 311 with loss 1.8949575424194336 and accuracy 0.17222222685813904.\n",
      "Test batch 312 with loss 1.770743727684021 and accuracy 0.2638888955116272.\n",
      "Test batch 313 with loss 1.8364019393920898 and accuracy 0.25.\n",
      "Test batch 314 with loss 1.7274420261383057 and accuracy 0.3055555522441864.\n",
      "Test batch 315 with loss 1.845353364944458 and accuracy 0.25.\n",
      "Test batch 316 with loss 1.7631454467773438 and accuracy 0.49166664481163025.\n",
      "Test batch 317 with loss 1.79622483253479 and accuracy 0.1666666716337204.\n",
      "Test batch 318 with loss 1.868777871131897 and accuracy 0.0972222238779068.\n",
      "Test batch 319 with loss 1.840619444847107 and accuracy 0.12222222983837128.\n",
      "Test batch 320 with loss 1.8279914855957031 and accuracy 0.06666667014360428.\n",
      "Test batch 321 with loss 1.849429726600647 and accuracy 0.0833333358168602.\n",
      "Test batch 322 with loss 1.931372046470642 and accuracy 0.1111111119389534.\n",
      "Test batch 323 with loss 1.8628623485565186 and accuracy 0.19166667759418488.\n",
      "Test batch 324 with loss 1.8460595607757568 and accuracy 0.24166667461395264.\n",
      "Test batch 325 with loss 1.7868459224700928 and accuracy 0.2611111104488373.\n",
      "Test batch 326 with loss 1.7818702459335327 and accuracy 0.10000000149011612.\n",
      "Test batch 327 with loss 1.8531618118286133 and accuracy 0.05833333730697632.\n",
      "Test batch 328 with loss 1.8329212665557861 and accuracy 0.11666667461395264.\n",
      "Test batch 329 with loss 1.7589733600616455 and accuracy 0.14444445073604584.\n",
      "Test batch 330 with loss 1.7556982040405273 and accuracy 0.1071428582072258.\n",
      "Test batch 331 with loss 1.8295402526855469 and accuracy 0.09259259700775146.\n",
      "Test batch 332 with loss 1.870507001876831 and accuracy 0.1349206417798996.\n",
      "Test batch 333 with loss 1.748653769493103 and accuracy 0.1666666716337204.\n",
      "Test batch 334 with loss 1.773403525352478 and accuracy 0.1944444477558136.\n",
      "Test batch 335 with loss 1.76595938205719 and accuracy 0.3888889253139496.\n",
      "Test batch 336 with loss 1.8561012744903564 and accuracy 0.0555555559694767.\n",
      "Test batch 337 with loss 1.9620120525360107 and accuracy 0.0555555559694767.\n",
      "Test batch 338 with loss 1.80498468875885 and accuracy 0.12222222983837128.\n",
      "Test batch 339 with loss 1.8532905578613281 and accuracy 0.1319444477558136.\n",
      "Test batch 340 with loss 1.8076269626617432 and accuracy 0.0972222238779068.\n",
      "Test batch 341 with loss 1.7805888652801514 and accuracy 0.236111119389534.\n",
      "Test batch 342 with loss 1.7348296642303467 and accuracy 0.3194444477558136.\n",
      "Test batch 343 with loss 1.8705775737762451 and accuracy 0.0972222238779068.\n",
      "Test batch 344 with loss 1.8800474405288696 and accuracy 0.065476194024086.\n",
      "Test batch 345 with loss 1.8330497741699219 and accuracy 0.02380952425301075.\n",
      "Test batch 346 with loss 1.8953030109405518 and accuracy 0.0694444477558136.\n",
      "Test batch 347 with loss 1.9332787990570068 and accuracy 0.0972222238779068.\n",
      "Test batch 348 with loss 1.7226911783218384 and accuracy 0.284722238779068.\n",
      "Test batch 349 with loss 1.9034637212753296 and accuracy 0.0763888880610466.\n",
      "Test batch 350 with loss 1.7702754735946655 and accuracy 0.1805555522441864.\n",
      "Test batch 351 with loss 1.7624638080596924 and accuracy 0.1527777761220932.\n",
      "Test batch 352 with loss 1.8303430080413818 and accuracy 0.25.\n",
      "Test batch 353 with loss 1.7711598873138428 and accuracy 0.2750000059604645.\n",
      "Test batch 354 with loss 1.7961629629135132 and accuracy 0.2666666805744171.\n",
      "Test batch 355 with loss 1.8554229736328125 and accuracy 0.1388888955116272.\n",
      "Test batch 356 with loss 1.8630845546722412 and accuracy 0.2083333432674408.\n",
      "Test batch 357 with loss 1.8594799041748047 and accuracy 0.065476194024086.\n",
      "Test batch 358 with loss 1.8549960851669312 and accuracy 0.21388888359069824.\n",
      "Test batch 359 with loss 1.8281211853027344 and accuracy 0.06666667014360428.\n",
      "Test batch 360 with loss 1.8830515146255493 and accuracy 0.0833333358168602.\n",
      "Test batch 361 with loss 1.8523457050323486 and accuracy 0.0833333358168602.\n",
      "Test batch 362 with loss 1.9174737930297852 and accuracy 0.10000000894069672.\n",
      "Test batch 363 with loss 1.944525122642517 and accuracy 0.0833333358168602.\n",
      "Test batch 364 with loss 1.8437601327896118 and accuracy 0.0.\n",
      "Test batch 365 with loss 1.8968324661254883 and accuracy 0.0625.\n",
      "Test batch 366 with loss 1.79287850856781 and accuracy 0.0694444477558136.\n",
      "Test batch 367 with loss 1.7768630981445312 and accuracy 0.1666666716337204.\n",
      "Test batch 368 with loss 1.7182197570800781 and accuracy 0.17222222685813904.\n",
      "Test batch 369 with loss 1.8637701272964478 and accuracy 0.03333333507180214.\n",
      "Test batch 370 with loss 1.8344085216522217 and accuracy 0.0416666679084301.\n",
      "Test batch 371 with loss 1.759883165359497 and accuracy 0.375.\n",
      "Test batch 372 with loss 1.884508490562439 and accuracy 0.2916666865348816.\n",
      "Test batch 373 with loss 1.7649948596954346 and accuracy 0.4027777910232544.\n",
      "Test batch 374 with loss 1.8264095783233643 and accuracy 0.15833333134651184.\n",
      "Test batch 375 with loss 1.8039586544036865 and accuracy 0.06666667014360428.\n",
      "Test batch 376 with loss 1.8505359888076782 and accuracy 0.30158731341362.\n",
      "Test batch 377 with loss 1.8275638818740845 and accuracy 0.2888889014720917.\n",
      "Test batch 378 with loss 1.7852022647857666 and accuracy 0.125.\n",
      "Test batch 379 with loss 1.7857716083526611 and accuracy 0.14166666567325592.\n",
      "Test batch 380 with loss 1.8521772623062134 and accuracy 0.0972222238779068.\n",
      "Test batch 381 with loss 1.7337232828140259 and accuracy 0.1626984179019928.\n",
      "Test batch 382 with loss 1.828332543373108 and accuracy 0.0972222238779068.\n",
      "Test batch 383 with loss 1.8681402206420898 and accuracy 0.1071428582072258.\n",
      "Test batch 384 with loss 1.7970802783966064 and accuracy 0.1666666716337204.\n",
      "Test batch 385 with loss 1.8072717189788818 and accuracy 0.0833333358168602.\n",
      "Test batch 386 with loss 1.8809007406234741 and accuracy 0.11666667461395264.\n",
      "Test batch 387 with loss 1.7829068899154663 and accuracy 0.1944444477558136.\n",
      "Test batch 388 with loss 1.768684983253479 and accuracy 0.2638888955116272.\n",
      "Test batch 389 with loss 1.7343963384628296 and accuracy 0.4750000238418579.\n",
      "Test batch 390 with loss 1.798079490661621 and accuracy 0.11666667461395264.\n",
      "Test batch 391 with loss 1.8790252208709717 and accuracy 0.0555555559694767.\n",
      "Test batch 392 with loss 1.810852289199829 and accuracy 0.2666666805744171.\n",
      "Test batch 393 with loss 1.7997688055038452 and accuracy 0.2986111342906952.\n",
      "Test batch 394 with loss 1.7934528589248657 and accuracy 0.2916666865348816.\n",
      "Test batch 395 with loss 1.8392035961151123 and accuracy 0.20555555820465088.\n",
      "Test batch 396 with loss 1.7940877676010132 and accuracy 0.130952388048172.\n",
      "Test batch 397 with loss 1.8000577688217163 and accuracy 0.18888889253139496.\n",
      "Test batch 398 with loss 1.8920085430145264 and accuracy 0.0555555559694767.\n",
      "Test batch 399 with loss 1.9290094375610352 and accuracy 0.1944444477558136.\n",
      "Test batch 400 with loss 1.8193044662475586 and accuracy 0.0972222238779068.\n",
      "Test batch 401 with loss 1.7222623825073242 and accuracy 0.24166665971279144.\n",
      "Test batch 402 with loss 1.8529632091522217 and accuracy 0.11666667461395264.\n",
      "Test batch 403 with loss 1.7621997594833374 and accuracy 0.15555556118488312.\n",
      "Test batch 404 with loss 1.8169406652450562 and accuracy 0.33888891339302063.\n",
      "Test batch 405 with loss 1.873806357383728 and accuracy 0.24166667461395264.\n",
      "Test batch 406 with loss 1.804452896118164 and accuracy 0.1388888955116272.\n",
      "Test batch 407 with loss 1.911352515220642 and accuracy 0.0555555559694767.\n",
      "Test batch 408 with loss 1.77389657497406 and accuracy 0.3194444477558136.\n",
      "Test batch 409 with loss 1.7451934814453125 and accuracy 0.366666704416275.\n",
      "Test batch 410 with loss 1.8392006158828735 and accuracy 0.23333333432674408.\n",
      "Test batch 411 with loss 1.873492956161499 and accuracy 0.14444445073604584.\n",
      "Test batch 412 with loss 1.773705244064331 and accuracy 0.0833333358168602.\n",
      "Test batch 413 with loss 1.8683280944824219 and accuracy 0.0.\n",
      "Test batch 414 with loss 1.7687410116195679 and accuracy 0.16428571939468384.\n",
      "Test batch 415 with loss 1.8550150394439697 and accuracy 0.236111119389534.\n",
      "Test batch 416 with loss 1.8180557489395142 and accuracy 0.319444477558136.\n",
      "Test batch 417 with loss 1.8462358713150024 and accuracy 0.1111111119389534.\n",
      "Test batch 418 with loss 1.828566551208496 and accuracy 0.08888889104127884.\n",
      "Test batch 419 with loss 1.7904512882232666 and accuracy 0.14444445073604584.\n",
      "Test batch 420 with loss 1.8743619918823242 and accuracy 0.0416666679084301.\n",
      "Test batch 421 with loss 1.863289475440979 and accuracy 0.08888889104127884.\n",
      "Test batch 422 with loss 1.7958217859268188 and accuracy 0.38055557012557983.\n",
      "Test batch 423 with loss 1.8555492162704468 and accuracy 0.15000000596046448.\n",
      "Test batch 424 with loss 1.781439185142517 and accuracy 0.2083333432674408.\n",
      "Test batch 425 with loss 1.761614441871643 and accuracy 0.125.\n",
      "Test batch 426 with loss 1.819064736366272 and accuracy 0.1666666716337204.\n",
      "Test batch 427 with loss 1.7384754419326782 and accuracy 0.1666666716337204.\n",
      "Test batch 428 with loss 1.7810348272323608 and accuracy 0.2698412835597992.\n",
      "Test batch 429 with loss 1.8749593496322632 and accuracy 0.09047619253396988.\n",
      "Test batch 430 with loss 1.7834274768829346 and accuracy 0.1210317462682724.\n",
      "Test batch 431 with loss 1.7305015325546265 and accuracy 0.3583333492279053.\n",
      "Test batch 432 with loss 1.8642005920410156 and accuracy 0.1944444477558136.\n",
      "Test batch 433 with loss 1.757367491722107 and accuracy 0.1944444477558136.\n",
      "Test batch 434 with loss 1.7670764923095703 and accuracy 0.3527778089046478.\n",
      "Test batch 435 with loss 1.8228645324707031 and accuracy 0.3055555522441864.\n",
      "Test batch 436 with loss 1.8593511581420898 and accuracy 0.0555555559694767.\n",
      "Test batch 437 with loss 1.8701921701431274 and accuracy 0.0972222238779068.\n",
      "Test batch 438 with loss 1.8443377017974854 and accuracy 0.125.\n",
      "Test batch 439 with loss 1.8418805599212646 and accuracy 0.2666666805744171.\n",
      "Test batch 440 with loss 1.90557861328125 and accuracy 0.02777777798473835.\n",
      "Test batch 441 with loss 1.8892396688461304 and accuracy 0.05714286118745804.\n",
      "Test batch 442 with loss 1.7695919275283813 and accuracy 0.25.\n",
      "Test batch 443 with loss 1.8014882802963257 and accuracy 0.2638888955116272.\n",
      "Test batch 444 with loss 1.817165732383728 and accuracy 0.14444445073604584.\n",
      "Test batch 445 with loss 1.855324149131775 and accuracy 0.236111119389534.\n",
      "Test batch 446 with loss 1.7497581243515015 and accuracy 0.21944445371627808.\n",
      "Test batch 447 with loss 1.8106749057769775 and accuracy 0.1388888955116272.\n",
      "Test batch 448 with loss 1.8152555227279663 and accuracy 0.12222222983837128.\n",
      "Test batch 449 with loss 1.7779409885406494 and accuracy 0.26944443583488464.\n",
      "Test batch 450 with loss 1.7848154306411743 and accuracy 0.2611111104488373.\n",
      "Test batch 451 with loss 1.7241389751434326 and accuracy 0.1527777761220932.\n",
      "Test batch 452 with loss 1.9158070087432861 and accuracy 0.02380952425301075.\n",
      "Test batch 453 with loss 1.7733062505722046 and accuracy 0.1041666716337204.\n",
      "Test batch 454 with loss 1.9063270092010498 and accuracy 0.1388888955116272.\n",
      "Test batch 455 with loss 1.935161828994751 and accuracy 0.03333333507180214.\n",
      "Test batch 456 with loss 1.8142378330230713 and accuracy 0.0833333358168602.\n",
      "Test batch 457 with loss 1.8284507989883423 and accuracy 0.02083333395421505.\n",
      "Test batch 458 with loss 1.8666374683380127 and accuracy 0.13055555522441864.\n",
      "Test batch 459 with loss 1.8085514307022095 and accuracy 0.1666666716337204.\n",
      "Test batch 460 with loss 1.8141998052597046 and accuracy 0.26333335041999817.\n",
      "Test batch 461 with loss 1.8359133005142212 and accuracy 0.125.\n",
      "Test batch 462 with loss 1.9015605449676514 and accuracy 0.06666667014360428.\n",
      "Test batch 463 with loss 1.7394845485687256 and accuracy 0.2944444417953491.\n",
      "Test batch 464 with loss 1.8560253381729126 and accuracy 0.21944445371627808.\n",
      "Test batch 465 with loss 1.782252550125122 and accuracy 0.2083333283662796.\n",
      "Test batch 466 with loss 1.8713306188583374 and accuracy 0.2222222238779068.\n",
      "Test batch 467 with loss 1.8462673425674438 and accuracy 0.0972222238779068.\n",
      "Test batch 468 with loss 1.9081239700317383 and accuracy 0.1111111119389534.\n",
      "Test batch 469 with loss 1.813950777053833 and accuracy 0.1527777761220932.\n",
      "Test batch 470 with loss 1.9500606060028076 and accuracy 0.2222222238779068.\n",
      "Test batch 471 with loss 1.8086092472076416 and accuracy 0.2083333432674408.\n",
      "Test batch 472 with loss 1.910391092300415 and accuracy 0.125.\n",
      "Test batch 473 with loss 1.8183317184448242 and accuracy 0.2638888955116272.\n",
      "Test batch 474 with loss 1.8136351108551025 and accuracy 0.0972222238779068.\n",
      "Test batch 475 with loss 1.832315444946289 and accuracy 0.15833334624767303.\n",
      "Test batch 476 with loss 1.871683120727539 and accuracy 0.02777777798473835.\n",
      "Test batch 477 with loss 1.7793033123016357 and accuracy 0.11666667461395264.\n",
      "Test batch 478 with loss 1.7479374408721924 and accuracy 0.23888888955116272.\n",
      "Test batch 479 with loss 1.8362360000610352 and accuracy 0.1349206417798996.\n",
      "Test batch 480 with loss 1.8871057033538818 and accuracy 0.125.\n",
      "Test batch 481 with loss 1.826493501663208 and accuracy 0.1388888955116272.\n",
      "Test batch 482 with loss 1.8250157833099365 and accuracy 0.25.\n",
      "Test batch 483 with loss 1.8554840087890625 and accuracy 0.2083333432674408.\n",
      "Test batch 484 with loss 1.852203369140625 and accuracy 0.2638888955116272.\n",
      "Test batch 485 with loss 1.839495301246643 and accuracy 0.1111111119389534.\n",
      "Test batch 486 with loss 1.89772629737854 and accuracy 0.0694444477558136.\n",
      "Test batch 487 with loss 1.8485949039459229 and accuracy 0.08888889104127884.\n",
      "Test batch 488 with loss 1.7994537353515625 and accuracy 0.2182539701461792.\n",
      "Test batch 489 with loss 1.8694413900375366 and accuracy 0.1388888955116272.\n",
      "Test batch 490 with loss 1.795924425125122 and accuracy 0.17222222685813904.\n",
      "Test batch 491 with loss 1.8086717128753662 and accuracy 0.236111119389534.\n",
      "Test batch 492 with loss 1.8628900051116943 and accuracy 0.0833333358168602.\n",
      "Test batch 493 with loss 1.9244626760482788 and accuracy 0.1388888955116272.\n",
      "Test batch 494 with loss 1.7252219915390015 and accuracy 0.1825396865606308.\n",
      "Test batch 495 with loss 1.878340482711792 and accuracy 0.125.\n",
      "Test batch 496 with loss 1.815359115600586 and accuracy 0.1111111119389534.\n",
      "Test batch 497 with loss 1.6840957403182983 and accuracy 0.3194444477558136.\n",
      "Test batch 498 with loss 1.8850290775299072 and accuracy 0.17222222685813904.\n",
      "Test batch 499 with loss 1.7280842065811157 and accuracy 0.3333333134651184.\n",
      "Training epoch 7 batch 0 with loss 1.773432731628418, accuracy 0.1041666716337204.\n",
      "Training epoch 7 batch 1 with loss 1.8671938180923462, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 2 with loss 1.7563917636871338, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 3 with loss 1.7689015865325928, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 4 with loss 1.6820595264434814, accuracy 0.3333333134651184.\n",
      "Training epoch 7 batch 5 with loss 1.7927643060684204, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 6 with loss 1.7836376428604126, accuracy 0.1686508059501648.\n",
      "Training epoch 7 batch 7 with loss 1.745917558670044, accuracy 0.4365079402923584.\n",
      "Training epoch 7 batch 8 with loss 1.732845664024353, accuracy 0.3710317611694336.\n",
      "Training epoch 7 batch 9 with loss 1.7995080947875977, accuracy 0.0.\n",
      "Training epoch 7 batch 10 with loss 1.757699728012085, accuracy 0.28333333134651184.\n",
      "Training epoch 7 batch 11 with loss 1.7741482257843018, accuracy 0.33888888359069824.\n",
      "Training epoch 7 batch 12 with loss 1.8703655004501343, accuracy 0.125.\n",
      "Training epoch 7 batch 13 with loss 1.7451074123382568, accuracy 0.13333334028720856.\n",
      "Training epoch 7 batch 14 with loss 1.86611008644104, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 15 with loss 1.6894937753677368, accuracy 0.3194444477558136.\n",
      "Training epoch 7 batch 16 with loss 1.9290382862091064, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 17 with loss 1.6981704235076904, accuracy 0.222222238779068.\n",
      "Training epoch 7 batch 18 with loss 1.7332813739776611, accuracy 0.1071428656578064.\n",
      "Training epoch 7 batch 19 with loss 1.7497587203979492, accuracy 0.2666666805744171.\n",
      "Training epoch 7 batch 20 with loss 1.7960981130599976, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 21 with loss 1.7532402276992798, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 22 with loss 1.9157238006591797, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 23 with loss 1.6856744289398193, accuracy 0.2750000059604645.\n",
      "Training epoch 7 batch 24 with loss 1.7768285274505615, accuracy 0.3055555522441864.\n",
      "Training epoch 7 batch 25 with loss 1.735784888267517, accuracy 0.18888890743255615.\n",
      "Training epoch 7 batch 26 with loss 1.8078858852386475, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 27 with loss 1.7450860738754272, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 28 with loss 1.686161756515503, accuracy 0.2611111104488373.\n",
      "Training epoch 7 batch 29 with loss 1.6829817295074463, accuracy 0.2916666567325592.\n",
      "Training epoch 7 batch 30 with loss 1.7102937698364258, accuracy 0.4138888716697693.\n",
      "Training epoch 7 batch 31 with loss 1.852500319480896, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 32 with loss 1.7280601263046265, accuracy 0.3492063581943512.\n",
      "Training epoch 7 batch 33 with loss 1.7705488204956055, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 34 with loss 1.671933889389038, accuracy 0.3285714387893677.\n",
      "Training epoch 7 batch 35 with loss 1.8206501007080078, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 36 with loss 1.798826813697815, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 37 with loss 1.7949912548065186, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 38 with loss 1.6989351511001587, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 39 with loss 1.7421672344207764, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 40 with loss 1.7767012119293213, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 41 with loss 1.7095119953155518, accuracy 0.37857145071029663.\n",
      "Training epoch 7 batch 42 with loss 1.7749335765838623, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 43 with loss 1.6971629858016968, accuracy 0.36666667461395264.\n",
      "Training epoch 7 batch 44 with loss 1.735512375831604, accuracy 0.19166666269302368.\n",
      "Training epoch 7 batch 45 with loss 1.6857486963272095, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 46 with loss 1.8117740154266357, accuracy 0.2916666567325592.\n",
      "Training epoch 7 batch 47 with loss 1.7721656560897827, accuracy 0.3055555522441864.\n",
      "Training epoch 7 batch 48 with loss 1.7387672662734985, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 49 with loss 1.632063627243042, accuracy 0.28703704476356506.\n",
      "Training epoch 7 batch 50 with loss 1.78213369846344, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 51 with loss 1.7590097188949585, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 52 with loss 1.841577172279358, accuracy 0.0.\n",
      "Training epoch 7 batch 53 with loss 1.7527763843536377, accuracy 0.25.\n",
      "Training epoch 7 batch 54 with loss 1.7867851257324219, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 55 with loss 1.7385978698730469, accuracy 0.284722238779068.\n",
      "Training epoch 7 batch 56 with loss 1.805882215499878, accuracy 0.0.\n",
      "Training epoch 7 batch 57 with loss 1.6847772598266602, accuracy 0.2916666865348816.\n",
      "Training epoch 7 batch 58 with loss 1.7919366359710693, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 59 with loss 1.7642329931259155, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 60 with loss 1.709737777709961, accuracy 0.1210317462682724.\n",
      "Training epoch 7 batch 61 with loss 1.7925469875335693, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 62 with loss 1.900726079940796, accuracy 0.30714285373687744.\n",
      "Training epoch 7 batch 63 with loss 1.8698208332061768, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 64 with loss 1.8141469955444336, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 65 with loss 1.7280323505401611, accuracy 0.3472222089767456.\n",
      "Training epoch 7 batch 66 with loss 1.7597458362579346, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 67 with loss 1.7968997955322266, accuracy 0.25.\n",
      "Training epoch 7 batch 68 with loss 1.7564252614974976, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 69 with loss 1.8129457235336304, accuracy 0.2182539701461792.\n",
      "Training epoch 7 batch 70 with loss 1.731726050376892, accuracy 0.347222238779068.\n",
      "Training epoch 7 batch 71 with loss 1.7761017084121704, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 72 with loss 1.701249361038208, accuracy 0.3611111044883728.\n",
      "Training epoch 7 batch 73 with loss 1.9062776565551758, accuracy 0.1626984179019928.\n",
      "Training epoch 7 batch 74 with loss 1.8624417781829834, accuracy 0.0.\n",
      "Training epoch 7 batch 75 with loss 1.7973616123199463, accuracy 0.16388890147209167.\n",
      "Training epoch 7 batch 76 with loss 1.8342756032943726, accuracy 0.2420634925365448.\n",
      "Training epoch 7 batch 77 with loss 1.7821848392486572, accuracy 0.1488095223903656.\n",
      "Training epoch 7 batch 78 with loss 1.726681113243103, accuracy 0.2666666805744171.\n",
      "Training epoch 7 batch 79 with loss 1.8900082111358643, accuracy 0.06666667014360428.\n",
      "Training epoch 7 batch 80 with loss 1.7661679983139038, accuracy 0.1785714328289032.\n",
      "Training epoch 7 batch 81 with loss 1.731419324874878, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 82 with loss 1.7889814376831055, accuracy 0.02380952425301075.\n",
      "Training epoch 7 batch 83 with loss 1.7113498449325562, accuracy 0.222222238779068.\n",
      "Training epoch 7 batch 84 with loss 1.7102572917938232, accuracy 0.08095238357782364.\n",
      "Training epoch 7 batch 85 with loss 1.7175897359848022, accuracy 0.23333334922790527.\n",
      "Training epoch 7 batch 86 with loss 1.8269163370132446, accuracy 0.24722221493721008.\n",
      "Training epoch 7 batch 87 with loss 1.7781829833984375, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 88 with loss 1.7700939178466797, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 89 with loss 1.7641435861587524, accuracy 0.277777761220932.\n",
      "Training epoch 7 batch 90 with loss 1.698895812034607, accuracy 0.3777777850627899.\n",
      "Training epoch 7 batch 91 with loss 1.8190410137176514, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 92 with loss 1.8195937871932983, accuracy 0.10833333432674408.\n",
      "Training epoch 7 batch 93 with loss 1.7420024871826172, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 94 with loss 1.9325631856918335, accuracy 0.0476190485060215.\n",
      "Training epoch 7 batch 95 with loss 1.8387501239776611, accuracy 0.1041666716337204.\n",
      "Training epoch 7 batch 96 with loss 1.8169691562652588, accuracy 0.1805555671453476.\n",
      "Training epoch 7 batch 97 with loss 1.701196312904358, accuracy 0.3444444537162781.\n",
      "Training epoch 7 batch 98 with loss 1.6803491115570068, accuracy 0.3222222328186035.\n",
      "Training epoch 7 batch 99 with loss 1.7904895544052124, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 100 with loss 1.7750431299209595, accuracy 0.22777777910232544.\n",
      "Training epoch 7 batch 101 with loss 1.7943795919418335, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 102 with loss 1.7514028549194336, accuracy 0.25555557012557983.\n",
      "Training epoch 7 batch 103 with loss 1.7030845880508423, accuracy 0.49537035822868347.\n",
      "Training epoch 7 batch 104 with loss 1.760583519935608, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 105 with loss 1.7749160528182983, accuracy 0.29722222685813904.\n",
      "Training epoch 7 batch 106 with loss 1.781437635421753, accuracy 0.30317461490631104.\n",
      "Training epoch 7 batch 107 with loss 1.7395099401474, accuracy 0.36666667461395264.\n",
      "Training epoch 7 batch 108 with loss 1.7892844676971436, accuracy 0.10277777910232544.\n",
      "Training epoch 7 batch 109 with loss 1.7763769626617432, accuracy 0.2083333283662796.\n",
      "Training epoch 7 batch 110 with loss 1.7622102499008179, accuracy 0.25.\n",
      "Training epoch 7 batch 111 with loss 1.7346084117889404, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 112 with loss 1.7655044794082642, accuracy 0.18611112236976624.\n",
      "Training epoch 7 batch 113 with loss 1.815972089767456, accuracy 0.2599206566810608.\n",
      "Training epoch 7 batch 114 with loss 1.7567503452301025, accuracy 0.21388889849185944.\n",
      "Training epoch 7 batch 115 with loss 1.7126352787017822, accuracy 0.35277777910232544.\n",
      "Training epoch 7 batch 116 with loss 1.8278911113739014, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 117 with loss 1.7324138879776, accuracy 0.3333333432674408.\n",
      "Training epoch 7 batch 118 with loss 1.815246820449829, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 119 with loss 1.715645432472229, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 120 with loss 1.7050199508666992, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 121 with loss 1.8270139694213867, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 122 with loss 1.6522563695907593, accuracy 0.3611111342906952.\n",
      "Training epoch 7 batch 123 with loss 1.6677331924438477, accuracy 0.3305555582046509.\n",
      "Training epoch 7 batch 124 with loss 1.704681634902954, accuracy 0.2750000059604645.\n",
      "Training epoch 7 batch 125 with loss 1.704271912574768, accuracy 0.35277777910232544.\n",
      "Training epoch 7 batch 126 with loss 1.7623790502548218, accuracy 0.125.\n",
      "Training epoch 7 batch 127 with loss 1.6651684045791626, accuracy 0.33888891339302063.\n",
      "Training epoch 7 batch 128 with loss 1.9098632335662842, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 129 with loss 1.7141786813735962, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 130 with loss 1.737216591835022, accuracy 0.4000000059604645.\n",
      "Training epoch 7 batch 131 with loss 1.7351882457733154, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 132 with loss 1.7721290588378906, accuracy 0.190476194024086.\n",
      "Training epoch 7 batch 133 with loss 1.7870254516601562, accuracy 0.06111111491918564.\n",
      "Training epoch 7 batch 134 with loss 1.7756147384643555, accuracy 0.1071428582072258.\n",
      "Training epoch 7 batch 135 with loss 1.7767099142074585, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 136 with loss 1.735433578491211, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 137 with loss 1.7016750574111938, accuracy 0.3916666805744171.\n",
      "Training epoch 7 batch 138 with loss 1.7282123565673828, accuracy 0.18611112236976624.\n",
      "Training epoch 7 batch 139 with loss 1.7610199451446533, accuracy 0.2916666567325592.\n",
      "Training epoch 7 batch 140 with loss 1.6815407276153564, accuracy 0.2797619104385376.\n",
      "Training epoch 7 batch 141 with loss 1.6559438705444336, accuracy 0.27222222089767456.\n",
      "Training epoch 7 batch 142 with loss 1.7543398141860962, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 143 with loss 1.846229910850525, accuracy 0.25555557012557983.\n",
      "Training epoch 7 batch 144 with loss 1.7182782888412476, accuracy 0.3916666805744171.\n",
      "Training epoch 7 batch 145 with loss 1.7688801288604736, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 146 with loss 1.7300891876220703, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 147 with loss 1.8606250286102295, accuracy 0.15833334624767303.\n",
      "Training epoch 7 batch 148 with loss 1.7118480205535889, accuracy 0.3888888955116272.\n",
      "Training epoch 7 batch 149 with loss 1.712738037109375, accuracy 0.24444445967674255.\n",
      "Training epoch 7 batch 150 with loss 1.7648159265518188, accuracy 0.4027777910232544.\n",
      "Training epoch 7 batch 151 with loss 1.8376657962799072, accuracy 0.21666666865348816.\n",
      "Training epoch 7 batch 152 with loss 1.8352845907211304, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 153 with loss 1.7805989980697632, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 154 with loss 1.7001047134399414, accuracy 0.21944445371627808.\n",
      "Training epoch 7 batch 155 with loss 1.708168387413025, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 156 with loss 1.7803165912628174, accuracy 0.13333334028720856.\n",
      "Training epoch 7 batch 157 with loss 1.8630378246307373, accuracy 0.16111111640930176.\n",
      "Training epoch 7 batch 158 with loss 1.7116310596466064, accuracy 0.1805555671453476.\n",
      "Training epoch 7 batch 159 with loss 1.7640306949615479, accuracy 0.25.\n",
      "Training epoch 7 batch 160 with loss 1.9671554565429688, accuracy 0.0.\n",
      "Training epoch 7 batch 161 with loss 1.8258914947509766, accuracy 0.21111111342906952.\n",
      "Training epoch 7 batch 162 with loss 1.8856515884399414, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 163 with loss 1.6457792520523071, accuracy 0.3777777850627899.\n",
      "Training epoch 7 batch 164 with loss 1.7869688272476196, accuracy 0.2916666567325592.\n",
      "Training epoch 7 batch 165 with loss 1.7586164474487305, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 166 with loss 1.749079942703247, accuracy 0.18333333730697632.\n",
      "Training epoch 7 batch 167 with loss 1.7557621002197266, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 168 with loss 1.7573585510253906, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 169 with loss 1.7274725437164307, accuracy 0.1597222238779068.\n",
      "Training epoch 7 batch 170 with loss 1.7534446716308594, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 171 with loss 1.8009666204452515, accuracy 0.125.\n",
      "Training epoch 7 batch 172 with loss 1.6787344217300415, accuracy 0.3055555522441864.\n",
      "Training epoch 7 batch 173 with loss 1.756079912185669, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 174 with loss 1.8196220397949219, accuracy 0.18333333730697632.\n",
      "Training epoch 7 batch 175 with loss 1.6531692743301392, accuracy 0.10833333432674408.\n",
      "Training epoch 7 batch 176 with loss 1.651888132095337, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 177 with loss 1.777374267578125, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 178 with loss 1.8346697092056274, accuracy 0.06666667014360428.\n",
      "Training epoch 7 batch 179 with loss 1.9228742122650146, accuracy 0.06666667014360428.\n",
      "Training epoch 7 batch 180 with loss 1.7908939123153687, accuracy 0.3055555820465088.\n",
      "Training epoch 7 batch 181 with loss 1.6572173833847046, accuracy 0.32499998807907104.\n",
      "Training epoch 7 batch 182 with loss 1.7619526386260986, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 183 with loss 1.7722785472869873, accuracy 0.25.\n",
      "Training epoch 7 batch 184 with loss 1.7307627201080322, accuracy 0.21666666865348816.\n",
      "Training epoch 7 batch 185 with loss 1.8281103372573853, accuracy 0.2083333283662796.\n",
      "Training epoch 7 batch 186 with loss 1.6569812297821045, accuracy 0.4638095200061798.\n",
      "Training epoch 7 batch 187 with loss 1.7761237621307373, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 188 with loss 1.7923463582992554, accuracy 0.4000000059604645.\n",
      "Training epoch 7 batch 189 with loss 1.739160180091858, accuracy 0.25555557012557983.\n",
      "Training epoch 7 batch 190 with loss 1.855466604232788, accuracy 0.125.\n",
      "Training epoch 7 batch 191 with loss 1.7609525918960571, accuracy 0.2916666865348816.\n",
      "Training epoch 7 batch 192 with loss 1.6556482315063477, accuracy 0.4000000059604645.\n",
      "Training epoch 7 batch 193 with loss 1.8267589807510376, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 194 with loss 1.677314043045044, accuracy 0.319444477558136.\n",
      "Training epoch 7 batch 195 with loss 1.8001760244369507, accuracy 0.18333333730697632.\n",
      "Training epoch 7 batch 196 with loss 1.774015188217163, accuracy 0.25.\n",
      "Training epoch 7 batch 197 with loss 1.7630360126495361, accuracy 0.31111112236976624.\n",
      "Training epoch 7 batch 198 with loss 1.747768759727478, accuracy 0.3492063581943512.\n",
      "Training epoch 7 batch 199 with loss 1.806694746017456, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 200 with loss 1.689790964126587, accuracy 0.2083333283662796.\n",
      "Training epoch 7 batch 201 with loss 1.804840326309204, accuracy 0.15833334624767303.\n",
      "Training epoch 7 batch 202 with loss 1.7732880115509033, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 203 with loss 1.7155523300170898, accuracy 0.2569444477558136.\n",
      "Training epoch 7 batch 204 with loss 1.7915817499160767, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 205 with loss 1.81955087184906, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 206 with loss 1.7795318365097046, accuracy 0.125.\n",
      "Training epoch 7 batch 207 with loss 1.6606371402740479, accuracy 0.4277777671813965.\n",
      "Training epoch 7 batch 208 with loss 1.9845383167266846, accuracy 0.3055555522441864.\n",
      "Training epoch 7 batch 209 with loss 1.6867210865020752, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 210 with loss 1.9261661767959595, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 211 with loss 1.7422689199447632, accuracy 0.19166667759418488.\n",
      "Training epoch 7 batch 212 with loss 1.758413553237915, accuracy 0.23333333432674408.\n",
      "Training epoch 7 batch 213 with loss 1.8513635396957397, accuracy 0.0.\n",
      "Training epoch 7 batch 214 with loss 1.7731813192367554, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 215 with loss 1.7319107055664062, accuracy 0.25.\n",
      "Training epoch 7 batch 216 with loss 1.8489751815795898, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 217 with loss 1.7250335216522217, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 218 with loss 1.871429443359375, accuracy 0.17222222685813904.\n",
      "Training epoch 7 batch 219 with loss 1.8000679016113281, accuracy 0.3125.\n",
      "Training epoch 7 batch 220 with loss 1.7351011037826538, accuracy 0.2083333283662796.\n",
      "Training epoch 7 batch 221 with loss 1.7749751806259155, accuracy 0.065476194024086.\n",
      "Training epoch 7 batch 222 with loss 1.8790314197540283, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 223 with loss 1.6934421062469482, accuracy 0.3888888955116272.\n",
      "Training epoch 7 batch 224 with loss 1.7695677280426025, accuracy 0.2916666865348816.\n",
      "Training epoch 7 batch 225 with loss 1.6802911758422852, accuracy 0.33888888359069824.\n",
      "Training epoch 7 batch 226 with loss 1.7951500415802002, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 227 with loss 1.8225526809692383, accuracy 0.1805555671453476.\n",
      "Training epoch 7 batch 228 with loss 1.6910966634750366, accuracy 0.38333332538604736.\n",
      "Training epoch 7 batch 229 with loss 1.854283094406128, accuracy 0.28333333134651184.\n",
      "Training epoch 7 batch 230 with loss 1.761143684387207, accuracy 0.3404761850833893.\n",
      "Training epoch 7 batch 231 with loss 1.793506383895874, accuracy 0.2666666805744171.\n",
      "Training epoch 7 batch 232 with loss 1.794811487197876, accuracy 0.29722222685813904.\n",
      "Training epoch 7 batch 233 with loss 1.6566333770751953, accuracy 0.46388891339302063.\n",
      "Training epoch 7 batch 234 with loss 1.7877464294433594, accuracy 0.10833333432674408.\n",
      "Training epoch 7 batch 235 with loss 1.844322919845581, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 236 with loss 1.7775217294692993, accuracy 0.236111119389534.\n",
      "Training epoch 7 batch 237 with loss 1.7895689010620117, accuracy 0.25555557012557983.\n",
      "Training epoch 7 batch 238 with loss 1.628348708152771, accuracy 0.3888888955116272.\n",
      "Training epoch 7 batch 239 with loss 1.7673890590667725, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 240 with loss 1.8947454690933228, accuracy 0.02380952425301075.\n",
      "Training epoch 7 batch 241 with loss 1.8400901556015015, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 242 with loss 1.7595775127410889, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 243 with loss 1.8270095586776733, accuracy 0.18333333730697632.\n",
      "Training epoch 7 batch 244 with loss 1.783069372177124, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 245 with loss 1.7768580913543701, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 246 with loss 1.6979213953018188, accuracy 0.3055555820465088.\n",
      "Training epoch 7 batch 247 with loss 1.7225606441497803, accuracy 0.3888889253139496.\n",
      "Training epoch 7 batch 248 with loss 1.7344173192977905, accuracy 0.21111111342906952.\n",
      "Training epoch 7 batch 249 with loss 1.7171661853790283, accuracy 0.2361111044883728.\n",
      "Training epoch 7 batch 250 with loss 1.7218841314315796, accuracy 0.24444445967674255.\n",
      "Training epoch 7 batch 251 with loss 1.8291782140731812, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 252 with loss 1.7055864334106445, accuracy 0.3305555582046509.\n",
      "Training epoch 7 batch 253 with loss 1.8336213827133179, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 254 with loss 1.7164033651351929, accuracy 0.130952388048172.\n",
      "Training epoch 7 batch 255 with loss 1.7002403736114502, accuracy 0.3611111342906952.\n",
      "Training epoch 7 batch 256 with loss 1.7673813104629517, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 257 with loss 1.6813452243804932, accuracy 0.432539701461792.\n",
      "Training epoch 7 batch 258 with loss 1.8043098449707031, accuracy 0.111111119389534.\n",
      "Training epoch 7 batch 259 with loss 1.6159613132476807, accuracy 0.4000000059604645.\n",
      "Training epoch 7 batch 260 with loss 1.819143295288086, accuracy 0.1130952388048172.\n",
      "Training epoch 7 batch 261 with loss 1.690659761428833, accuracy 0.3222222328186035.\n",
      "Training epoch 7 batch 262 with loss 1.740047812461853, accuracy 0.1944444626569748.\n",
      "Training epoch 7 batch 263 with loss 1.7209059000015259, accuracy 0.41111111640930176.\n",
      "Training epoch 7 batch 264 with loss 1.6747276782989502, accuracy 0.2658730149269104.\n",
      "Training epoch 7 batch 265 with loss 1.7452380657196045, accuracy 0.14166668057441711.\n",
      "Training epoch 7 batch 266 with loss 1.7842113971710205, accuracy 0.130952388048172.\n",
      "Training epoch 7 batch 267 with loss 1.705413579940796, accuracy 0.3472222089767456.\n",
      "Training epoch 7 batch 268 with loss 1.7780529260635376, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 269 with loss 1.7123721837997437, accuracy 0.3611111044883728.\n",
      "Training epoch 7 batch 270 with loss 1.7665414810180664, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 271 with loss 1.7271257638931274, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 272 with loss 1.7361564636230469, accuracy 0.5277777910232544.\n",
      "Training epoch 7 batch 273 with loss 1.690442442893982, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 274 with loss 1.7083747386932373, accuracy 0.4166666865348816.\n",
      "Training epoch 7 batch 275 with loss 1.7403390407562256, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 276 with loss 1.6956145763397217, accuracy 0.4333333373069763.\n",
      "Training epoch 7 batch 277 with loss 1.8351027965545654, accuracy 0.1924603283405304.\n",
      "Training epoch 7 batch 278 with loss 1.6998674869537354, accuracy 0.2750000059604645.\n",
      "Training epoch 7 batch 279 with loss 1.8411916494369507, accuracy 0.1865079402923584.\n",
      "Training epoch 7 batch 280 with loss 1.8371307849884033, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 281 with loss 1.7987282276153564, accuracy 0.0.\n",
      "Training epoch 7 batch 282 with loss 1.7724673748016357, accuracy 0.2527777850627899.\n",
      "Training epoch 7 batch 283 with loss 1.7051252126693726, accuracy 0.3722222149372101.\n",
      "Training epoch 7 batch 284 with loss 1.707532525062561, accuracy 0.3083333373069763.\n",
      "Training epoch 7 batch 285 with loss 1.8192071914672852, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 286 with loss 1.8715206384658813, accuracy 0.2916666865348816.\n",
      "Training epoch 7 batch 287 with loss 1.7858282327651978, accuracy 0.1626984179019928.\n",
      "Training epoch 7 batch 288 with loss 1.8312466144561768, accuracy 0.38055554032325745.\n",
      "Training epoch 7 batch 289 with loss 1.7977794408798218, accuracy 0.25.\n",
      "Training epoch 7 batch 290 with loss 1.7503187656402588, accuracy 0.18809524178504944.\n",
      "Training epoch 7 batch 291 with loss 1.6965992450714111, accuracy 0.3611111342906952.\n",
      "Training epoch 7 batch 292 with loss 1.8325779438018799, accuracy 0.06111111491918564.\n",
      "Training epoch 7 batch 293 with loss 1.692468285560608, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 294 with loss 1.8063513040542603, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 295 with loss 1.7407400608062744, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 296 with loss 1.7484986782073975, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 297 with loss 1.7182890176773071, accuracy 0.204365074634552.\n",
      "Training epoch 7 batch 298 with loss 1.739423394203186, accuracy 0.125.\n",
      "Training epoch 7 batch 299 with loss 1.7358806133270264, accuracy 0.3361110985279083.\n",
      "Training epoch 7 batch 300 with loss 1.689650297164917, accuracy 0.3500000238418579.\n",
      "Training epoch 7 batch 301 with loss 1.8019415140151978, accuracy 0.17777778208255768.\n",
      "Training epoch 7 batch 302 with loss 1.8134043216705322, accuracy 0.36666667461395264.\n",
      "Training epoch 7 batch 303 with loss 1.6934030055999756, accuracy 0.26944443583488464.\n",
      "Training epoch 7 batch 304 with loss 1.8460133075714111, accuracy 0.111111119389534.\n",
      "Training epoch 7 batch 305 with loss 1.776703119277954, accuracy 0.15555556118488312.\n",
      "Training epoch 7 batch 306 with loss 1.7762393951416016, accuracy 0.29722222685813904.\n",
      "Training epoch 7 batch 307 with loss 1.7939974069595337, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 308 with loss 1.7551958560943604, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 309 with loss 1.7034938335418701, accuracy 0.21388889849185944.\n",
      "Training epoch 7 batch 310 with loss 1.7958877086639404, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 311 with loss 1.8264760971069336, accuracy 0.2083333283662796.\n",
      "Training epoch 7 batch 312 with loss 1.7045313119888306, accuracy 0.1686507910490036.\n",
      "Training epoch 7 batch 313 with loss 1.8031669855117798, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 314 with loss 1.8103853464126587, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 315 with loss 1.8213392496109009, accuracy 0.17658731341362.\n",
      "Training epoch 7 batch 316 with loss 1.6871850490570068, accuracy 0.2916666567325592.\n",
      "Training epoch 7 batch 317 with loss 1.8332370519638062, accuracy 0.2611111104488373.\n",
      "Training epoch 7 batch 318 with loss 1.6239923238754272, accuracy 0.32499998807907104.\n",
      "Training epoch 7 batch 319 with loss 1.707579255104065, accuracy 0.190476194024086.\n",
      "Training epoch 7 batch 320 with loss 1.7463582754135132, accuracy 0.2916666567325592.\n",
      "Training epoch 7 batch 321 with loss 1.766057014465332, accuracy 0.284722238779068.\n",
      "Training epoch 7 batch 322 with loss 1.8117135763168335, accuracy 0.23888888955116272.\n",
      "Training epoch 7 batch 323 with loss 1.732277274131775, accuracy 0.3333333432674408.\n",
      "Training epoch 7 batch 324 with loss 1.761806845664978, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 325 with loss 1.7444286346435547, accuracy 0.210317462682724.\n",
      "Training epoch 7 batch 326 with loss 1.8946025371551514, accuracy 0.0.\n",
      "Training epoch 7 batch 327 with loss 1.8948867321014404, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 328 with loss 1.7934598922729492, accuracy 0.0694444477558136.\n",
      "Training epoch 7 batch 329 with loss 1.725746750831604, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 330 with loss 1.7137832641601562, accuracy 0.15833333134651184.\n",
      "Training epoch 7 batch 331 with loss 1.7449696063995361, accuracy 0.30000001192092896.\n",
      "Training epoch 7 batch 332 with loss 1.7176614999771118, accuracy 0.3055555522441864.\n",
      "Training epoch 7 batch 333 with loss 1.7431195974349976, accuracy 0.1964285671710968.\n",
      "Training epoch 7 batch 334 with loss 1.7819713354110718, accuracy 0.3035714328289032.\n",
      "Training epoch 7 batch 335 with loss 1.787657380104065, accuracy 0.1349206417798996.\n",
      "Training epoch 7 batch 336 with loss 1.866270661354065, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 337 with loss 1.7441647052764893, accuracy 0.319444477558136.\n",
      "Training epoch 7 batch 338 with loss 1.7880504131317139, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 339 with loss 1.7891483306884766, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 340 with loss 1.604467749595642, accuracy 0.4027777910232544.\n",
      "Training epoch 7 batch 341 with loss 1.6844581365585327, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 342 with loss 1.6893612146377563, accuracy 0.2888889014720917.\n",
      "Training epoch 7 batch 343 with loss 1.753965973854065, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 344 with loss 1.8353465795516968, accuracy 0.125.\n",
      "Training epoch 7 batch 345 with loss 1.8498718738555908, accuracy 0.09880952537059784.\n",
      "Training epoch 7 batch 346 with loss 1.6219183206558228, accuracy 0.3757142722606659.\n",
      "Training epoch 7 batch 347 with loss 1.7695125341415405, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 348 with loss 1.8743016719818115, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 349 with loss 1.651688575744629, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 350 with loss 1.785996437072754, accuracy 0.15833334624767303.\n",
      "Training epoch 7 batch 351 with loss 1.7052650451660156, accuracy 0.4166666865348816.\n",
      "Training epoch 7 batch 352 with loss 1.8487451076507568, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 353 with loss 1.766434907913208, accuracy 0.1587301641702652.\n",
      "Training epoch 7 batch 354 with loss 1.8527940511703491, accuracy 0.125.\n",
      "Training epoch 7 batch 355 with loss 1.7908823490142822, accuracy 0.2666666805744171.\n",
      "Training epoch 7 batch 356 with loss 1.6807609796524048, accuracy 0.2944444715976715.\n",
      "Training epoch 7 batch 357 with loss 1.7620084285736084, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 358 with loss 1.747272253036499, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 359 with loss 1.7393567562103271, accuracy 0.065476194024086.\n",
      "Training epoch 7 batch 360 with loss 1.7014878988265991, accuracy 0.40833336114883423.\n",
      "Training epoch 7 batch 361 with loss 1.8645118474960327, accuracy 0.2361111044883728.\n",
      "Training epoch 7 batch 362 with loss 1.7795302867889404, accuracy 0.21944445371627808.\n",
      "Training epoch 7 batch 363 with loss 2.0514941215515137, accuracy 0.15833333134651184.\n",
      "Training epoch 7 batch 364 with loss 1.7846075296401978, accuracy 0.3194444477558136.\n",
      "Training epoch 7 batch 365 with loss 1.8008346557617188, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 366 with loss 1.7403452396392822, accuracy 0.25.\n",
      "Training epoch 7 batch 367 with loss 1.854526162147522, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 368 with loss 1.80055832862854, accuracy 0.02777777798473835.\n",
      "Training epoch 7 batch 369 with loss 1.7591426372528076, accuracy 0.125.\n",
      "Training epoch 7 batch 370 with loss 1.8128442764282227, accuracy 0.125.\n",
      "Training epoch 7 batch 371 with loss 1.6822032928466797, accuracy 0.3305555582046509.\n",
      "Training epoch 7 batch 372 with loss 1.7497421503067017, accuracy 0.3305555582046509.\n",
      "Training epoch 7 batch 373 with loss 1.796947717666626, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 374 with loss 1.6548068523406982, accuracy 0.35476192831993103.\n",
      "Training epoch 7 batch 375 with loss 1.8584821224212646, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 376 with loss 1.7952944040298462, accuracy 0.2750000059604645.\n",
      "Training epoch 7 batch 377 with loss 1.7306674718856812, accuracy 0.36666667461395264.\n",
      "Training epoch 7 batch 378 with loss 1.729814887046814, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 379 with loss 1.796269416809082, accuracy 0.06111111491918564.\n",
      "Training epoch 7 batch 380 with loss 1.7526099681854248, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 381 with loss 1.7721402645111084, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 382 with loss 1.7362209558486938, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 383 with loss 1.779771089553833, accuracy 0.0694444477558136.\n",
      "Training epoch 7 batch 384 with loss 1.730670690536499, accuracy 0.19722223281860352.\n",
      "Training epoch 7 batch 385 with loss 1.76242196559906, accuracy 0.30317461490631104.\n",
      "Training epoch 7 batch 386 with loss 1.785093069076538, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 387 with loss 1.647204041481018, accuracy 0.3083333373069763.\n",
      "Training epoch 7 batch 388 with loss 1.711252212524414, accuracy 0.31111112236976624.\n",
      "Training epoch 7 batch 389 with loss 1.9084529876708984, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 390 with loss 1.6730539798736572, accuracy 0.43888887763023376.\n",
      "Training epoch 7 batch 391 with loss 1.7796186208724976, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 392 with loss 1.7368075847625732, accuracy 0.26944443583488464.\n",
      "Training epoch 7 batch 393 with loss 1.8370176553726196, accuracy 0.16388888657093048.\n",
      "Training epoch 7 batch 394 with loss 1.9238479137420654, accuracy 0.17222222685813904.\n",
      "Training epoch 7 batch 395 with loss 1.8704655170440674, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 396 with loss 1.6759233474731445, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 397 with loss 1.7017383575439453, accuracy 0.4194444417953491.\n",
      "Training epoch 7 batch 398 with loss 1.8770583868026733, accuracy 0.2611111104488373.\n",
      "Training epoch 7 batch 399 with loss 1.7393219470977783, accuracy 0.33888888359069824.\n",
      "Training epoch 7 batch 400 with loss 1.7415494918823242, accuracy 0.15833334624767303.\n",
      "Training epoch 7 batch 401 with loss 1.7149150371551514, accuracy 0.16984127461910248.\n",
      "Training epoch 7 batch 402 with loss 1.7813314199447632, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 403 with loss 1.7871776819229126, accuracy 0.3571428656578064.\n",
      "Training epoch 7 batch 404 with loss 1.7963298559188843, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 405 with loss 1.6993858814239502, accuracy 0.25555557012557983.\n",
      "Training epoch 7 batch 406 with loss 1.7330482006072998, accuracy 0.26944443583488464.\n",
      "Training epoch 7 batch 407 with loss 1.7454869747161865, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 408 with loss 1.770272970199585, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 409 with loss 1.6226638555526733, accuracy 0.3968254029750824.\n",
      "Training epoch 7 batch 410 with loss 1.7492307424545288, accuracy 0.261904776096344.\n",
      "Training epoch 7 batch 411 with loss 1.914648413658142, accuracy 0.26944443583488464.\n",
      "Training epoch 7 batch 412 with loss 1.819393515586853, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 413 with loss 1.789292335510254, accuracy 0.4027777910232544.\n",
      "Training epoch 7 batch 414 with loss 1.7768747806549072, accuracy 0.125.\n",
      "Training epoch 7 batch 415 with loss 1.8743022680282593, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 416 with loss 1.71991765499115, accuracy 0.42500001192092896.\n",
      "Training epoch 7 batch 417 with loss 1.9198477268218994, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 418 with loss 1.7951253652572632, accuracy 0.0694444477558136.\n",
      "Training epoch 7 batch 419 with loss 1.7089999914169312, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 420 with loss 1.8950027227401733, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 421 with loss 1.7847929000854492, accuracy 0.32777780294418335.\n",
      "Training epoch 7 batch 422 with loss 1.8175758123397827, accuracy 0.17777778208255768.\n",
      "Training epoch 7 batch 423 with loss 1.83767569065094, accuracy 0.31111112236976624.\n",
      "Training epoch 7 batch 424 with loss 1.78424072265625, accuracy 0.3499999940395355.\n",
      "Training epoch 7 batch 425 with loss 1.8792788982391357, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 426 with loss 1.8890177011489868, accuracy 0.0625.\n",
      "Training epoch 7 batch 427 with loss 1.7674354314804077, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 428 with loss 1.8112821578979492, accuracy 0.0793650820851326.\n",
      "Training epoch 7 batch 429 with loss 1.7926660776138306, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 430 with loss 1.7633882761001587, accuracy 0.2182539701461792.\n",
      "Training epoch 7 batch 431 with loss 1.8354127407073975, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 432 with loss 1.810215711593628, accuracy 0.2182539701461792.\n",
      "Training epoch 7 batch 433 with loss 1.7786086797714233, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 434 with loss 1.7567989826202393, accuracy 0.1071428582072258.\n",
      "Training epoch 7 batch 435 with loss 1.851013422012329, accuracy 0.13650794327259064.\n",
      "Training epoch 7 batch 436 with loss 1.6951076984405518, accuracy 0.472222238779068.\n",
      "Training epoch 7 batch 437 with loss 1.7808252573013306, accuracy 0.3194444477558136.\n",
      "Training epoch 7 batch 438 with loss 1.7028108835220337, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 439 with loss 1.6295257806777954, accuracy 0.40833333134651184.\n",
      "Training epoch 7 batch 440 with loss 1.741058588027954, accuracy 0.3492063581943512.\n",
      "Training epoch 7 batch 441 with loss 1.6653106212615967, accuracy 0.230158731341362.\n",
      "Training epoch 7 batch 442 with loss 1.789491891860962, accuracy 0.02380952425301075.\n",
      "Training epoch 7 batch 443 with loss 1.7043616771697998, accuracy 0.3055555522441864.\n",
      "Training epoch 7 batch 444 with loss 1.8030650615692139, accuracy 0.1130952388048172.\n",
      "Training epoch 7 batch 445 with loss 1.7212591171264648, accuracy 0.41111114621162415.\n",
      "Training epoch 7 batch 446 with loss 1.7772741317749023, accuracy 0.255952388048172.\n",
      "Training epoch 7 batch 447 with loss 1.7351830005645752, accuracy 0.3222222328186035.\n",
      "Training epoch 7 batch 448 with loss 1.7550487518310547, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 449 with loss 1.9269291162490845, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 450 with loss 1.7967761754989624, accuracy 0.28333333134651184.\n",
      "Training epoch 7 batch 451 with loss 1.775975227355957, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 452 with loss 1.8465745449066162, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 453 with loss 1.876987099647522, accuracy 0.02083333395421505.\n",
      "Training epoch 7 batch 454 with loss 1.7237495183944702, accuracy 0.1349206417798996.\n",
      "Training epoch 7 batch 455 with loss 1.900037407875061, accuracy 0.125.\n",
      "Training epoch 7 batch 456 with loss 1.8585987091064453, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 457 with loss 1.7668769359588623, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 458 with loss 1.8043887615203857, accuracy 0.11666666716337204.\n",
      "Training epoch 7 batch 459 with loss 1.9174617528915405, accuracy 0.1626984179019928.\n",
      "Training epoch 7 batch 460 with loss 1.7052650451660156, accuracy 0.3333333134651184.\n",
      "Training epoch 7 batch 461 with loss 1.7935587167739868, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 462 with loss 1.8121757507324219, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 463 with loss 1.7177612781524658, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 464 with loss 1.8360061645507812, accuracy 0.25.\n",
      "Training epoch 7 batch 465 with loss 1.807795524597168, accuracy 0.10833333432674408.\n",
      "Training epoch 7 batch 466 with loss 1.7011241912841797, accuracy 0.16031746566295624.\n",
      "Training epoch 7 batch 467 with loss 1.7477394342422485, accuracy 0.18333333730697632.\n",
      "Training epoch 7 batch 468 with loss 1.7188832759857178, accuracy 0.4583333432674408.\n",
      "Training epoch 7 batch 469 with loss 1.7715733051300049, accuracy 0.24761906266212463.\n",
      "Training epoch 7 batch 470 with loss 1.7926216125488281, accuracy 0.2916666865348816.\n",
      "Training epoch 7 batch 471 with loss 1.788651704788208, accuracy 0.1865079402923584.\n",
      "Training epoch 7 batch 472 with loss 1.7004165649414062, accuracy 0.3611111342906952.\n",
      "Training epoch 7 batch 473 with loss 1.7585664987564087, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 474 with loss 1.8183656930923462, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 475 with loss 1.7583643198013306, accuracy 0.230158731341362.\n",
      "Training epoch 7 batch 476 with loss 1.7617477178573608, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 477 with loss 1.7650537490844727, accuracy 0.2916666567325592.\n",
      "Training epoch 7 batch 478 with loss 1.867964506149292, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 479 with loss 1.7351391315460205, accuracy 0.17777778208255768.\n",
      "Training epoch 7 batch 480 with loss 1.8113734722137451, accuracy 0.347222238779068.\n",
      "Training epoch 7 batch 481 with loss 1.7543907165527344, accuracy 0.38333332538604736.\n",
      "Training epoch 7 batch 482 with loss 1.7278848886489868, accuracy 0.3305555582046509.\n",
      "Training epoch 7 batch 483 with loss 1.5952941179275513, accuracy 0.5730158686637878.\n",
      "Training epoch 7 batch 484 with loss 1.7499173879623413, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 485 with loss 1.8314123153686523, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 486 with loss 1.8525123596191406, accuracy 0.18611110746860504.\n",
      "Training epoch 7 batch 487 with loss 1.6130220890045166, accuracy 0.40833333134651184.\n",
      "Training epoch 7 batch 488 with loss 1.807094931602478, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 489 with loss 1.7737531661987305, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 490 with loss 1.8513591289520264, accuracy 0.3333333432674408.\n",
      "Training epoch 7 batch 491 with loss 1.8313201665878296, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 492 with loss 1.6475359201431274, accuracy 0.4166666567325592.\n",
      "Training epoch 7 batch 493 with loss 1.9289729595184326, accuracy 0.06666667014360428.\n",
      "Training epoch 7 batch 494 with loss 1.8070695400238037, accuracy 0.26944446563720703.\n",
      "Training epoch 7 batch 495 with loss 1.7955678701400757, accuracy 0.25.\n",
      "Training epoch 7 batch 496 with loss 1.7060143947601318, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 497 with loss 1.8346951007843018, accuracy 0.0793650820851326.\n",
      "Training epoch 7 batch 498 with loss 1.7906501293182373, accuracy 0.07407407462596893.\n",
      "Training epoch 7 batch 499 with loss 1.7228410243988037, accuracy 0.22499999403953552.\n",
      "Training epoch 7 batch 500 with loss 1.7740638256072998, accuracy 0.2361111044883728.\n",
      "Training epoch 7 batch 501 with loss 1.9000715017318726, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 502 with loss 1.7236554622650146, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 503 with loss 1.8293148279190063, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 504 with loss 1.7203861474990845, accuracy 0.15833333134651184.\n",
      "Training epoch 7 batch 505 with loss 1.7909139394760132, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 506 with loss 1.8176084756851196, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 507 with loss 1.7986218929290771, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 508 with loss 1.8010454177856445, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 509 with loss 1.8324520587921143, accuracy 0.27222222089767456.\n",
      "Training epoch 7 batch 510 with loss 1.892938256263733, accuracy 0.0694444477558136.\n",
      "Training epoch 7 batch 511 with loss 1.771781325340271, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 512 with loss 1.814910888671875, accuracy 0.0793650820851326.\n",
      "Training epoch 7 batch 513 with loss 1.8541450500488281, accuracy 0.1527777910232544.\n",
      "Training epoch 7 batch 514 with loss 1.7991230487823486, accuracy 0.1547619104385376.\n",
      "Training epoch 7 batch 515 with loss 1.660159707069397, accuracy 0.2888889014720917.\n",
      "Training epoch 7 batch 516 with loss 1.8019224405288696, accuracy 0.1071428582072258.\n",
      "Training epoch 7 batch 517 with loss 1.7671018838882446, accuracy 0.06111111491918564.\n",
      "Training epoch 7 batch 518 with loss 1.6263643503189087, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 519 with loss 1.760931372642517, accuracy 0.31111112236976624.\n",
      "Training epoch 7 batch 520 with loss 1.738289475440979, accuracy 0.31111112236976624.\n",
      "Training epoch 7 batch 521 with loss 1.7850710153579712, accuracy 0.17777779698371887.\n",
      "Training epoch 7 batch 522 with loss 1.7362420558929443, accuracy 0.2916666567325592.\n",
      "Training epoch 7 batch 523 with loss 1.6662299633026123, accuracy 0.3500000238418579.\n",
      "Training epoch 7 batch 524 with loss 1.8590635061264038, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 525 with loss 1.909769058227539, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 526 with loss 1.7536327838897705, accuracy 0.3253968358039856.\n",
      "Training epoch 7 batch 527 with loss 1.8745914697647095, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 528 with loss 1.740357756614685, accuracy 0.10833333432674408.\n",
      "Training epoch 7 batch 529 with loss 1.8220195770263672, accuracy 0.15833333134651184.\n",
      "Training epoch 7 batch 530 with loss 1.7610244750976562, accuracy 0.27222222089767456.\n",
      "Training epoch 7 batch 531 with loss 1.8235137462615967, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 532 with loss 1.7000072002410889, accuracy 0.09880952537059784.\n",
      "Training epoch 7 batch 533 with loss 1.78635573387146, accuracy 0.21111111342906952.\n",
      "Training epoch 7 batch 534 with loss 1.628026008605957, accuracy 0.4333333373069763.\n",
      "Training epoch 7 batch 535 with loss 1.8141286373138428, accuracy 0.02380952425301075.\n",
      "Training epoch 7 batch 536 with loss 1.7896884679794312, accuracy 0.24722221493721008.\n",
      "Training epoch 7 batch 537 with loss 1.7922664880752563, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 538 with loss 1.7724488973617554, accuracy 0.2916666567325592.\n",
      "Training epoch 7 batch 539 with loss 1.9080835580825806, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 540 with loss 1.7189531326293945, accuracy 0.4166666567325592.\n",
      "Training epoch 7 batch 541 with loss 1.801959753036499, accuracy 0.0892857164144516.\n",
      "Training epoch 7 batch 542 with loss 1.7570396661758423, accuracy 0.3194444477558136.\n",
      "Training epoch 7 batch 543 with loss 1.7384936809539795, accuracy 0.1041666716337204.\n",
      "Training epoch 7 batch 544 with loss 1.840004563331604, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 545 with loss 1.7893661260604858, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 546 with loss 1.781232476234436, accuracy 0.3055555820465088.\n",
      "Training epoch 7 batch 547 with loss 1.7612463235855103, accuracy 0.24074074625968933.\n",
      "Training epoch 7 batch 548 with loss 1.7626469135284424, accuracy 0.3055555522441864.\n",
      "Training epoch 7 batch 549 with loss 1.8390569686889648, accuracy 0.0.\n",
      "Training epoch 7 batch 550 with loss 1.8133599758148193, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 551 with loss 1.7264716625213623, accuracy 0.25.\n",
      "Training epoch 7 batch 552 with loss 1.8885036706924438, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 553 with loss 1.7894624471664429, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 554 with loss 1.883887529373169, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 555 with loss 1.6738615036010742, accuracy 0.3214285969734192.\n",
      "Training epoch 7 batch 556 with loss 1.7707526683807373, accuracy 0.1547619104385376.\n",
      "Training epoch 7 batch 557 with loss 1.7689969539642334, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 558 with loss 1.9164365530014038, accuracy 0.31111112236976624.\n",
      "Training epoch 7 batch 559 with loss 1.792693853378296, accuracy 0.21388888359069824.\n",
      "Training epoch 7 batch 560 with loss 1.8539520502090454, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 561 with loss 1.725804328918457, accuracy 0.21666666865348816.\n",
      "Training epoch 7 batch 562 with loss 1.8273861408233643, accuracy 0.1547619104385376.\n",
      "Training epoch 7 batch 563 with loss 1.7290769815444946, accuracy 0.3194444477558136.\n",
      "Training epoch 7 batch 564 with loss 1.8040828704833984, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 565 with loss 1.7680037021636963, accuracy 0.347222238779068.\n",
      "Training epoch 7 batch 566 with loss 1.7535812854766846, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 567 with loss 1.8064038753509521, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 568 with loss 1.796688437461853, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 569 with loss 1.6880096197128296, accuracy 0.17222222685813904.\n",
      "Training epoch 7 batch 570 with loss 1.817447304725647, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 571 with loss 1.772006630897522, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 572 with loss 1.7679246664047241, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 573 with loss 1.7009294033050537, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 574 with loss 1.746215581893921, accuracy 0.236111119389534.\n",
      "Training epoch 7 batch 575 with loss 1.7408721446990967, accuracy 0.10185185074806213.\n",
      "Training epoch 7 batch 576 with loss 1.6960875988006592, accuracy 0.29603174328804016.\n",
      "Training epoch 7 batch 577 with loss 1.7834287881851196, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 578 with loss 1.7392902374267578, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 579 with loss 1.8176000118255615, accuracy 0.28333333134651184.\n",
      "Training epoch 7 batch 580 with loss 1.8558294773101807, accuracy 0.03333333507180214.\n",
      "Training epoch 7 batch 581 with loss 1.847622275352478, accuracy 0.0.\n",
      "Training epoch 7 batch 582 with loss 1.904004693031311, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 583 with loss 1.836899757385254, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 584 with loss 1.823918342590332, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 585 with loss 1.8447201251983643, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 586 with loss 1.8089402914047241, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 587 with loss 1.8044017553329468, accuracy 0.1488095223903656.\n",
      "Training epoch 7 batch 588 with loss 1.641736388206482, accuracy 0.25.\n",
      "Training epoch 7 batch 589 with loss 1.667907953262329, accuracy 0.33888888359069824.\n",
      "Training epoch 7 batch 590 with loss 1.8001034259796143, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 591 with loss 1.8868739604949951, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 592 with loss 1.7308717966079712, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 593 with loss 1.6737697124481201, accuracy 0.4027777910232544.\n",
      "Training epoch 7 batch 594 with loss 1.8482414484024048, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 595 with loss 1.864082932472229, accuracy 0.24166667461395264.\n",
      "Training epoch 7 batch 596 with loss 1.8643286228179932, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 597 with loss 1.7195310592651367, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 598 with loss 1.811132788658142, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 599 with loss 1.6980676651000977, accuracy 0.0714285746216774.\n",
      "Training epoch 7 batch 600 with loss 1.8860849142074585, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 601 with loss 1.921140432357788, accuracy 0.17222222685813904.\n",
      "Training epoch 7 batch 602 with loss 1.8440444469451904, accuracy 0.125.\n",
      "Training epoch 7 batch 603 with loss 1.8739351034164429, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 604 with loss 1.915260910987854, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 605 with loss 1.7245362997055054, accuracy 0.4027777910232544.\n",
      "Training epoch 7 batch 606 with loss 1.8779093027114868, accuracy 0.02777777798473835.\n",
      "Training epoch 7 batch 607 with loss 1.7818409204483032, accuracy 0.25.\n",
      "Training epoch 7 batch 608 with loss 1.7025063037872314, accuracy 0.25555557012557983.\n",
      "Training epoch 7 batch 609 with loss 1.728567123413086, accuracy 0.23333334922790527.\n",
      "Training epoch 7 batch 610 with loss 1.711464524269104, accuracy 0.3472222089767456.\n",
      "Training epoch 7 batch 611 with loss 1.8784902095794678, accuracy 0.10000000149011612.\n",
      "Training epoch 7 batch 612 with loss 1.8416774272918701, accuracy 0.125.\n",
      "Training epoch 7 batch 613 with loss 1.8174747228622437, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 614 with loss 1.759922981262207, accuracy 0.347222238779068.\n",
      "Training epoch 7 batch 615 with loss 1.8138757944107056, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 616 with loss 1.8510258197784424, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 617 with loss 1.9082008600234985, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 618 with loss 1.843297004699707, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 619 with loss 1.8614526987075806, accuracy 0.22777777910232544.\n",
      "Training epoch 7 batch 620 with loss 1.7485466003417969, accuracy 0.18611112236976624.\n",
      "Training epoch 7 batch 621 with loss 1.8167228698730469, accuracy 0.25555557012557983.\n",
      "Training epoch 7 batch 622 with loss 1.7860171794891357, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 623 with loss 1.819767951965332, accuracy 0.125.\n",
      "Training epoch 7 batch 624 with loss 1.792602300643921, accuracy 0.22500000894069672.\n",
      "Training epoch 7 batch 625 with loss 1.7229535579681396, accuracy 0.32698413729667664.\n",
      "Training epoch 7 batch 626 with loss 1.7043955326080322, accuracy 0.3333333432674408.\n",
      "Training epoch 7 batch 627 with loss 1.7052961587905884, accuracy 0.347222238779068.\n",
      "Training epoch 7 batch 628 with loss 1.8313194513320923, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 629 with loss 1.8198573589324951, accuracy 0.19166666269302368.\n",
      "Training epoch 7 batch 630 with loss 1.8724406957626343, accuracy 0.222222238779068.\n",
      "Training epoch 7 batch 631 with loss 1.7562134265899658, accuracy 0.2361111044883728.\n",
      "Training epoch 7 batch 632 with loss 1.75510573387146, accuracy 0.210317462682724.\n",
      "Training epoch 7 batch 633 with loss 1.7531054019927979, accuracy 0.125.\n",
      "Training epoch 7 batch 634 with loss 1.695418357849121, accuracy 0.4472222328186035.\n",
      "Training epoch 7 batch 635 with loss 1.7613985538482666, accuracy 0.03703703731298447.\n",
      "Training epoch 7 batch 636 with loss 1.807988166809082, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 637 with loss 1.7725589275360107, accuracy 0.2916666567325592.\n",
      "Training epoch 7 batch 638 with loss 1.8238455057144165, accuracy 0.375.\n",
      "Training epoch 7 batch 639 with loss 1.7923858165740967, accuracy 0.2738095223903656.\n",
      "Training epoch 7 batch 640 with loss 1.821537733078003, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 641 with loss 1.7178272008895874, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 642 with loss 1.7257401943206787, accuracy 0.2361111044883728.\n",
      "Training epoch 7 batch 643 with loss 1.8126304149627686, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 644 with loss 1.8095719814300537, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 645 with loss 1.7884199619293213, accuracy 0.1031746044754982.\n",
      "Training epoch 7 batch 646 with loss 1.7484300136566162, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 647 with loss 1.8918380737304688, accuracy 0.29722222685813904.\n",
      "Training epoch 7 batch 648 with loss 1.8146693706512451, accuracy 0.3611111044883728.\n",
      "Training epoch 7 batch 649 with loss 1.738673210144043, accuracy 0.2847222089767456.\n",
      "Training epoch 7 batch 650 with loss 1.7325515747070312, accuracy 0.3166666626930237.\n",
      "Training epoch 7 batch 651 with loss 1.788903832435608, accuracy 0.15925925970077515.\n",
      "Training epoch 7 batch 652 with loss 1.8363803625106812, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 653 with loss 1.8863712549209595, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 654 with loss 1.7933480739593506, accuracy 0.2888889014720917.\n",
      "Training epoch 7 batch 655 with loss 1.8253971338272095, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 656 with loss 1.7053101062774658, accuracy 0.261904776096344.\n",
      "Training epoch 7 batch 657 with loss 1.7764304876327515, accuracy 0.3333333432674408.\n",
      "Training epoch 7 batch 658 with loss 1.8173202276229858, accuracy 0.39722222089767456.\n",
      "Training epoch 7 batch 659 with loss 1.8140414953231812, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 660 with loss 1.8352340459823608, accuracy 0.236111119389534.\n",
      "Training epoch 7 batch 661 with loss 1.8339998722076416, accuracy 0.222222238779068.\n",
      "Training epoch 7 batch 662 with loss 1.8556029796600342, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 663 with loss 1.7745050191879272, accuracy 0.1041666716337204.\n",
      "Training epoch 7 batch 664 with loss 1.72963547706604, accuracy 0.31666669249534607.\n",
      "Training epoch 7 batch 665 with loss 1.9007508754730225, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 666 with loss 1.9513031244277954, accuracy 0.0.\n",
      "Training epoch 7 batch 667 with loss 1.740992784500122, accuracy 0.15555556118488312.\n",
      "Training epoch 7 batch 668 with loss 1.7724802494049072, accuracy 0.2430555522441864.\n",
      "Training epoch 7 batch 669 with loss 1.7696807384490967, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 670 with loss 1.737062692642212, accuracy 0.44761908054351807.\n",
      "Training epoch 7 batch 671 with loss 1.767195463180542, accuracy 0.23333333432674408.\n",
      "Training epoch 7 batch 672 with loss 1.7213588953018188, accuracy 0.17777778208255768.\n",
      "Training epoch 7 batch 673 with loss 1.7587172985076904, accuracy 0.0625.\n",
      "Training epoch 7 batch 674 with loss 1.6898705959320068, accuracy 0.347222238779068.\n",
      "Training epoch 7 batch 675 with loss 1.8181684017181396, accuracy 0.23333333432674408.\n",
      "Training epoch 7 batch 676 with loss 1.8262159824371338, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 677 with loss 1.742218255996704, accuracy 0.35476192831993103.\n",
      "Training epoch 7 batch 678 with loss 1.7746785879135132, accuracy 0.2611111104488373.\n",
      "Training epoch 7 batch 679 with loss 1.8194433450698853, accuracy 0.2888889014720917.\n",
      "Training epoch 7 batch 680 with loss 1.748528242111206, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 681 with loss 1.7701215744018555, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 682 with loss 1.7419780492782593, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 683 with loss 1.8692777156829834, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 684 with loss 1.831560730934143, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 685 with loss 1.6560004949569702, accuracy 0.26851850748062134.\n",
      "Training epoch 7 batch 686 with loss 1.7557411193847656, accuracy 0.2083333283662796.\n",
      "Training epoch 7 batch 687 with loss 1.7201954126358032, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 688 with loss 1.614373803138733, accuracy 0.4888889193534851.\n",
      "Training epoch 7 batch 689 with loss 1.686868667602539, accuracy 0.4833333492279053.\n",
      "Training epoch 7 batch 690 with loss 1.7637964487075806, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 691 with loss 1.7828912734985352, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 692 with loss 1.774442434310913, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 693 with loss 1.7298433780670166, accuracy 0.17936508357524872.\n",
      "Training epoch 7 batch 694 with loss 1.735356330871582, accuracy 0.3611111044883728.\n",
      "Training epoch 7 batch 695 with loss 1.7324345111846924, accuracy 0.3571428656578064.\n",
      "Training epoch 7 batch 696 with loss 1.8314344882965088, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 697 with loss 1.8348214626312256, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 698 with loss 1.837662696838379, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 699 with loss 1.7507116794586182, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 700 with loss 1.8289915323257446, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 701 with loss 1.8756263256072998, accuracy 0.2666666805744171.\n",
      "Training epoch 7 batch 702 with loss 1.6912803649902344, accuracy 0.23888888955116272.\n",
      "Training epoch 7 batch 703 with loss 1.8892866373062134, accuracy 0.05714286118745804.\n",
      "Training epoch 7 batch 704 with loss 1.811597466468811, accuracy 0.1349206417798996.\n",
      "Training epoch 7 batch 705 with loss 1.6898040771484375, accuracy 0.2936508059501648.\n",
      "Training epoch 7 batch 706 with loss 1.7089430093765259, accuracy 0.35555559396743774.\n",
      "Training epoch 7 batch 707 with loss 1.8353111743927002, accuracy 0.125.\n",
      "Training epoch 7 batch 708 with loss 1.8347139358520508, accuracy 0.23888888955116272.\n",
      "Training epoch 7 batch 709 with loss 1.9105727672576904, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 710 with loss 1.7665348052978516, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 711 with loss 1.7659794092178345, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 712 with loss 1.7923616170883179, accuracy 0.1587301641702652.\n",
      "Training epoch 7 batch 713 with loss 1.8485498428344727, accuracy 0.190476194024086.\n",
      "Training epoch 7 batch 714 with loss 1.8957221508026123, accuracy 0.125.\n",
      "Training epoch 7 batch 715 with loss 1.8704607486724854, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 716 with loss 1.7408649921417236, accuracy 0.15555556118488312.\n",
      "Training epoch 7 batch 717 with loss 1.815003752708435, accuracy 0.1805555671453476.\n",
      "Training epoch 7 batch 718 with loss 1.872422456741333, accuracy 0.05416666716337204.\n",
      "Training epoch 7 batch 719 with loss 1.7457205057144165, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 720 with loss 1.7403310537338257, accuracy 0.3361111283302307.\n",
      "Training epoch 7 batch 721 with loss 1.7970155477523804, accuracy 0.18611112236976624.\n",
      "Training epoch 7 batch 722 with loss 1.8172248601913452, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 723 with loss 1.7047113180160522, accuracy 0.3055555820465088.\n",
      "Training epoch 7 batch 724 with loss 1.8040764331817627, accuracy 0.2142857313156128.\n",
      "Training epoch 7 batch 725 with loss 1.7664167881011963, accuracy 0.21388889849185944.\n",
      "Training epoch 7 batch 726 with loss 1.7527673244476318, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 727 with loss 1.8625513315200806, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 728 with loss 1.8531811237335205, accuracy 0.25925928354263306.\n",
      "Training epoch 7 batch 729 with loss 1.8121896982192993, accuracy 0.25555557012557983.\n",
      "Training epoch 7 batch 730 with loss 1.6884031295776367, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 731 with loss 1.80764639377594, accuracy 0.3611111044883728.\n",
      "Training epoch 7 batch 732 with loss 1.6764329671859741, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 733 with loss 1.790313959121704, accuracy 0.17777778208255768.\n",
      "Training epoch 7 batch 734 with loss 1.8486454486846924, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 735 with loss 1.743477463722229, accuracy 0.3055555820465088.\n",
      "Training epoch 7 batch 736 with loss 1.7336275577545166, accuracy 0.15833334624767303.\n",
      "Training epoch 7 batch 737 with loss 1.8382047414779663, accuracy 0.1031746044754982.\n",
      "Training epoch 7 batch 738 with loss 1.828682541847229, accuracy 0.03333333507180214.\n",
      "Training epoch 7 batch 739 with loss 1.8125511407852173, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 740 with loss 1.865298867225647, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 741 with loss 1.906022071838379, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 742 with loss 1.8345731496810913, accuracy 0.24166667461395264.\n",
      "Training epoch 7 batch 743 with loss 1.8216018676757812, accuracy 0.24166667461395264.\n",
      "Training epoch 7 batch 744 with loss 1.7528988122940063, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 745 with loss 1.8465286493301392, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 746 with loss 1.8072564601898193, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 747 with loss 1.8198902606964111, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 748 with loss 1.8198750019073486, accuracy 0.0625.\n",
      "Training epoch 7 batch 749 with loss 1.6607509851455688, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 750 with loss 1.8138583898544312, accuracy 0.06666667014360428.\n",
      "Training epoch 7 batch 751 with loss 1.8185056447982788, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 752 with loss 1.906440019607544, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 753 with loss 1.767432451248169, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 754 with loss 1.765038251876831, accuracy 0.25.\n",
      "Training epoch 7 batch 755 with loss 1.799281120300293, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 756 with loss 1.899171233177185, accuracy 0.25.\n",
      "Training epoch 7 batch 757 with loss 1.8606563806533813, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 758 with loss 1.8698310852050781, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 759 with loss 1.85325026512146, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 760 with loss 1.8658405542373657, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 761 with loss 1.769580602645874, accuracy 0.2916666865348816.\n",
      "Training epoch 7 batch 762 with loss 1.6933116912841797, accuracy 0.3583333492279053.\n",
      "Training epoch 7 batch 763 with loss 1.7504796981811523, accuracy 0.0892857164144516.\n",
      "Training epoch 7 batch 764 with loss 1.7840410470962524, accuracy 0.03333333507180214.\n",
      "Training epoch 7 batch 765 with loss 1.7268972396850586, accuracy 0.3583333492279053.\n",
      "Training epoch 7 batch 766 with loss 1.6662542819976807, accuracy 0.4583333730697632.\n",
      "Training epoch 7 batch 767 with loss 1.7693071365356445, accuracy 0.19166666269302368.\n",
      "Training epoch 7 batch 768 with loss 1.7819305658340454, accuracy 0.02380952425301075.\n",
      "Training epoch 7 batch 769 with loss 1.6866058111190796, accuracy 0.5666666626930237.\n",
      "Training epoch 7 batch 770 with loss 1.8238840103149414, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 771 with loss 1.7751452922821045, accuracy 0.06666667014360428.\n",
      "Training epoch 7 batch 772 with loss 1.8149292469024658, accuracy 0.14166666567325592.\n",
      "Training epoch 7 batch 773 with loss 1.7898190021514893, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 774 with loss 1.8047456741333008, accuracy 0.3472222089767456.\n",
      "Training epoch 7 batch 775 with loss 1.7879877090454102, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 776 with loss 1.7138608694076538, accuracy 0.25.\n",
      "Training epoch 7 batch 777 with loss 1.854320764541626, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 778 with loss 1.7866268157958984, accuracy 0.236111119389534.\n",
      "Training epoch 7 batch 779 with loss 1.5887387990951538, accuracy 0.5305556058883667.\n",
      "Training epoch 7 batch 780 with loss 1.7435516119003296, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 781 with loss 1.9506709575653076, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 782 with loss 1.7145735025405884, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 783 with loss 1.7127044200897217, accuracy 0.10000000894069672.\n",
      "Training epoch 7 batch 784 with loss 1.7473208904266357, accuracy 0.24166667461395264.\n",
      "Training epoch 7 batch 785 with loss 1.8240900039672852, accuracy 0.1071428656578064.\n",
      "Training epoch 7 batch 786 with loss 1.821467638015747, accuracy 0.03333333507180214.\n",
      "Training epoch 7 batch 787 with loss 1.7105159759521484, accuracy 0.375.\n",
      "Training epoch 7 batch 788 with loss 1.842916488647461, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 789 with loss 1.8300930261611938, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 790 with loss 1.821532964706421, accuracy 0.25.\n",
      "Training epoch 7 batch 791 with loss 1.7403227090835571, accuracy 0.1865079402923584.\n",
      "Training epoch 7 batch 792 with loss 1.900343894958496, accuracy 0.10833333432674408.\n",
      "Training epoch 7 batch 793 with loss 1.7608410120010376, accuracy 0.21944445371627808.\n",
      "Training epoch 7 batch 794 with loss 1.8184922933578491, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 795 with loss 1.753379225730896, accuracy 0.17222222685813904.\n",
      "Training epoch 7 batch 796 with loss 1.7998813390731812, accuracy 0.40000003576278687.\n",
      "Training epoch 7 batch 797 with loss 1.798870325088501, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 798 with loss 1.6866661310195923, accuracy 0.1964285671710968.\n",
      "Training epoch 7 batch 799 with loss 1.7721493244171143, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 800 with loss 1.7640199661254883, accuracy 0.4722222685813904.\n",
      "Training epoch 7 batch 801 with loss 1.7502801418304443, accuracy 0.22777777910232544.\n",
      "Training epoch 7 batch 802 with loss 1.9131462574005127, accuracy 0.06666667014360428.\n",
      "Training epoch 7 batch 803 with loss 1.8037288188934326, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 804 with loss 1.6380523443222046, accuracy 0.3333333432674408.\n",
      "Training epoch 7 batch 805 with loss 1.9395164251327515, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 806 with loss 1.7855348587036133, accuracy 0.2111111283302307.\n",
      "Training epoch 7 batch 807 with loss 1.8418447971343994, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 808 with loss 1.7766097784042358, accuracy 0.2658730149269104.\n",
      "Training epoch 7 batch 809 with loss 1.8237253427505493, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 810 with loss 1.6868950128555298, accuracy 0.3305555582046509.\n",
      "Training epoch 7 batch 811 with loss 1.831178069114685, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 812 with loss 1.851810097694397, accuracy 0.10833333432674408.\n",
      "Training epoch 7 batch 813 with loss 1.8183300495147705, accuracy 0.18333333730697632.\n",
      "Training epoch 7 batch 814 with loss 1.8916656970977783, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 815 with loss 1.880340337753296, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 816 with loss 1.8844293355941772, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 817 with loss 1.7758643627166748, accuracy 0.1825396865606308.\n",
      "Training epoch 7 batch 818 with loss 1.7494580745697021, accuracy 0.3541666865348816.\n",
      "Training epoch 7 batch 819 with loss 1.8427832126617432, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 820 with loss 1.7942228317260742, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 821 with loss 1.8666632175445557, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 822 with loss 1.701120376586914, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 823 with loss 1.8532747030258179, accuracy 0.125.\n",
      "Training epoch 7 batch 824 with loss 1.8462547063827515, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 825 with loss 1.7760543823242188, accuracy 0.3166666626930237.\n",
      "Training epoch 7 batch 826 with loss 1.7511918544769287, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 827 with loss 1.8289715051651, accuracy 0.13333334028720856.\n",
      "Training epoch 7 batch 828 with loss 1.896759033203125, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 829 with loss 1.72090744972229, accuracy 0.2152777761220932.\n",
      "Training epoch 7 batch 830 with loss 1.820688009262085, accuracy 0.03333333507180214.\n",
      "Training epoch 7 batch 831 with loss 1.859818696975708, accuracy 0.15833333134651184.\n",
      "Training epoch 7 batch 832 with loss 1.817195177078247, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 833 with loss 1.7862141132354736, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 834 with loss 1.6810414791107178, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 835 with loss 1.6929346323013306, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 836 with loss 1.7381250858306885, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 837 with loss 1.7437794208526611, accuracy 0.25.\n",
      "Training epoch 7 batch 838 with loss 1.7453806400299072, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 839 with loss 1.8261038064956665, accuracy 0.22777777910232544.\n",
      "Training epoch 7 batch 840 with loss 1.7533533573150635, accuracy 0.2916666865348816.\n",
      "Training epoch 7 batch 841 with loss 1.7827436923980713, accuracy 0.17658731341362.\n",
      "Training epoch 7 batch 842 with loss 1.6935679912567139, accuracy 0.24166667461395264.\n",
      "Training epoch 7 batch 843 with loss 1.8534501791000366, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 844 with loss 1.7400754690170288, accuracy 0.3055555522441864.\n",
      "Training epoch 7 batch 845 with loss 1.8022394180297852, accuracy 0.40833336114883423.\n",
      "Training epoch 7 batch 846 with loss 1.7428474426269531, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 847 with loss 1.662092924118042, accuracy 0.38055557012557983.\n",
      "Training epoch 7 batch 848 with loss 1.7011899948120117, accuracy 0.3583333492279053.\n",
      "Training epoch 7 batch 849 with loss 1.8284368515014648, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 850 with loss 1.7730334997177124, accuracy 0.2291666716337204.\n",
      "Training epoch 7 batch 851 with loss 1.9281022548675537, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 852 with loss 1.7566486597061157, accuracy 0.4166666567325592.\n",
      "Training epoch 7 batch 853 with loss 1.8049991130828857, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 854 with loss 1.8251844644546509, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 855 with loss 1.861372709274292, accuracy 0.065476194024086.\n",
      "Training epoch 7 batch 856 with loss 1.8930301666259766, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 857 with loss 1.8473103046417236, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 858 with loss 1.7801246643066406, accuracy 0.22777777910232544.\n",
      "Training epoch 7 batch 859 with loss 1.8164570331573486, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 860 with loss 1.7396650314331055, accuracy 0.22777777910232544.\n",
      "Training epoch 7 batch 861 with loss 1.8304929733276367, accuracy 0.29722222685813904.\n",
      "Training epoch 7 batch 862 with loss 1.7763410806655884, accuracy 0.3333333432674408.\n",
      "Training epoch 7 batch 863 with loss 1.7854372262954712, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 864 with loss 1.761605978012085, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 865 with loss 1.6712493896484375, accuracy 0.30158731341362.\n",
      "Training epoch 7 batch 866 with loss 1.7334554195404053, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 867 with loss 1.7356163263320923, accuracy 0.319444477558136.\n",
      "Training epoch 7 batch 868 with loss 1.6504356861114502, accuracy 0.33888888359069824.\n",
      "Training epoch 7 batch 869 with loss 1.9246686697006226, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 870 with loss 1.8229528665542603, accuracy 0.0793650820851326.\n",
      "Training epoch 7 batch 871 with loss 1.6478351354599, accuracy 0.2666666805744171.\n",
      "Training epoch 7 batch 872 with loss 1.7914478778839111, accuracy 0.3333333432674408.\n",
      "Training epoch 7 batch 873 with loss 1.8219783306121826, accuracy 0.1587301641702652.\n",
      "Training epoch 7 batch 874 with loss 1.6901357173919678, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 875 with loss 1.8200048208236694, accuracy 0.19166667759418488.\n",
      "Training epoch 7 batch 876 with loss 1.6902984380722046, accuracy 0.28333336114883423.\n",
      "Training epoch 7 batch 877 with loss 1.834442377090454, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 878 with loss 1.7566173076629639, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 879 with loss 1.8348004817962646, accuracy 0.3263888955116272.\n",
      "Training epoch 7 batch 880 with loss 1.7258694171905518, accuracy 0.4444444179534912.\n",
      "Training epoch 7 batch 881 with loss 1.7880640029907227, accuracy 0.2281746119260788.\n",
      "Training epoch 7 batch 882 with loss 1.8605092763900757, accuracy 0.125.\n",
      "Training epoch 7 batch 883 with loss 1.7008851766586304, accuracy 0.15833333134651184.\n",
      "Training epoch 7 batch 884 with loss 1.7551721334457397, accuracy 0.26944446563720703.\n",
      "Training epoch 7 batch 885 with loss 1.8598178625106812, accuracy 0.1736111044883728.\n",
      "Training epoch 7 batch 886 with loss 1.8090040683746338, accuracy 0.10277777910232544.\n",
      "Training epoch 7 batch 887 with loss 1.7945493459701538, accuracy 0.28333333134651184.\n",
      "Training epoch 7 batch 888 with loss 1.7602134943008423, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 889 with loss 1.803778052330017, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 890 with loss 1.8721908330917358, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 891 with loss 1.7664028406143188, accuracy 0.15833334624767303.\n",
      "Training epoch 7 batch 892 with loss 1.8313188552856445, accuracy 0.19603174924850464.\n",
      "Training epoch 7 batch 893 with loss 1.7842295169830322, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 894 with loss 1.8565161228179932, accuracy 0.02083333395421505.\n",
      "Training epoch 7 batch 895 with loss 1.7759021520614624, accuracy 0.2750000059604645.\n",
      "Training epoch 7 batch 896 with loss 1.847050428390503, accuracy 0.125.\n",
      "Training epoch 7 batch 897 with loss 1.747145414352417, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 898 with loss 1.803391456604004, accuracy 0.10476191341876984.\n",
      "Training epoch 7 batch 899 with loss 1.7090133428573608, accuracy 0.222222238779068.\n",
      "Training epoch 7 batch 900 with loss 1.9097923040390015, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 901 with loss 1.7486858367919922, accuracy 0.15555556118488312.\n",
      "Training epoch 7 batch 902 with loss 1.7615283727645874, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 903 with loss 1.8218443393707275, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 904 with loss 1.8327200412750244, accuracy 0.08095238357782364.\n",
      "Training epoch 7 batch 905 with loss 1.784484624862671, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 906 with loss 1.7623786926269531, accuracy 0.1805555671453476.\n",
      "Training epoch 7 batch 907 with loss 1.7975835800170898, accuracy 0.29722222685813904.\n",
      "Training epoch 7 batch 908 with loss 1.7924734354019165, accuracy 0.2936508059501648.\n",
      "Training epoch 7 batch 909 with loss 1.9946982860565186, accuracy 0.0.\n",
      "Training epoch 7 batch 910 with loss 1.797741174697876, accuracy 0.19761905074119568.\n",
      "Training epoch 7 batch 911 with loss 1.7503528594970703, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 912 with loss 1.751592993736267, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 913 with loss 1.8362665176391602, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 914 with loss 1.8393762111663818, accuracy 0.31111112236976624.\n",
      "Training epoch 7 batch 915 with loss 1.8660863637924194, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 916 with loss 1.7866857051849365, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 917 with loss 1.7567545175552368, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 918 with loss 1.8003114461898804, accuracy 0.2666666805744171.\n",
      "Training epoch 7 batch 919 with loss 1.7652838230133057, accuracy 0.25.\n",
      "Training epoch 7 batch 920 with loss 1.7368621826171875, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 921 with loss 1.8857204914093018, accuracy 0.125.\n",
      "Training epoch 7 batch 922 with loss 1.8236334323883057, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 923 with loss 1.761391282081604, accuracy 0.3722222149372101.\n",
      "Training epoch 7 batch 924 with loss 1.7388455867767334, accuracy 0.2750000059604645.\n",
      "Training epoch 7 batch 925 with loss 1.7378298044204712, accuracy 0.1765872985124588.\n",
      "Training epoch 7 batch 926 with loss 1.6066808700561523, accuracy 0.3432539701461792.\n",
      "Training epoch 7 batch 927 with loss 1.6703506708145142, accuracy 0.34166666865348816.\n",
      "Training epoch 7 batch 928 with loss 1.9415353536605835, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 929 with loss 1.8484694957733154, accuracy 0.20000001788139343.\n",
      "Training epoch 7 batch 930 with loss 1.765114426612854, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 931 with loss 1.8594000339508057, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 932 with loss 1.8971296548843384, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 933 with loss 1.7895135879516602, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 934 with loss 1.9329074621200562, accuracy 0.11666666716337204.\n",
      "Training epoch 7 batch 935 with loss 1.882103681564331, accuracy 0.0.\n",
      "Training epoch 7 batch 936 with loss 1.6831462383270264, accuracy 0.444444477558136.\n",
      "Training epoch 7 batch 937 with loss 1.7655360698699951, accuracy 0.319444477558136.\n",
      "Training epoch 7 batch 938 with loss 1.820863962173462, accuracy 0.19166666269302368.\n",
      "Training epoch 7 batch 939 with loss 1.7391064167022705, accuracy 0.4027777910232544.\n",
      "Training epoch 7 batch 940 with loss 1.8666795492172241, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 941 with loss 1.8229906558990479, accuracy 0.24523809552192688.\n",
      "Training epoch 7 batch 942 with loss 1.7598177194595337, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 943 with loss 1.733963966369629, accuracy 0.3194444477558136.\n",
      "Training epoch 7 batch 944 with loss 1.8305103778839111, accuracy 0.222222238779068.\n",
      "Training epoch 7 batch 945 with loss 1.9280611276626587, accuracy 0.06666667014360428.\n",
      "Training epoch 7 batch 946 with loss 1.8822944164276123, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 947 with loss 1.7594696283340454, accuracy 0.125.\n",
      "Training epoch 7 batch 948 with loss 1.805877685546875, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 949 with loss 1.7593551874160767, accuracy 0.30277779698371887.\n",
      "Training epoch 7 batch 950 with loss 1.8344780206680298, accuracy 0.19166666269302368.\n",
      "Training epoch 7 batch 951 with loss 1.8834669589996338, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 952 with loss 1.7406679391860962, accuracy 0.17777778208255768.\n",
      "Training epoch 7 batch 953 with loss 1.8165429830551147, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 954 with loss 1.7410352230072021, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 955 with loss 1.8026031255722046, accuracy 0.17222222685813904.\n",
      "Training epoch 7 batch 956 with loss 1.816742181777954, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 957 with loss 1.6753208637237549, accuracy 0.1825396865606308.\n",
      "Training epoch 7 batch 958 with loss 1.8300310373306274, accuracy 0.14166666567325592.\n",
      "Training epoch 7 batch 959 with loss 1.7702686786651611, accuracy 0.125.\n",
      "Training epoch 7 batch 960 with loss 1.7275784015655518, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 961 with loss 1.8671518564224243, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 962 with loss 1.7182376384735107, accuracy 0.25.\n",
      "Training epoch 7 batch 963 with loss 1.6262308359146118, accuracy 0.24166667461395264.\n",
      "Training epoch 7 batch 964 with loss 1.6657003164291382, accuracy 0.2430555671453476.\n",
      "Training epoch 7 batch 965 with loss 1.723907470703125, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 966 with loss 1.7352654933929443, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 967 with loss 1.6992149353027344, accuracy 0.2361111044883728.\n",
      "Training epoch 7 batch 968 with loss 1.7367897033691406, accuracy 0.1805555671453476.\n",
      "Training epoch 7 batch 969 with loss 1.8721383810043335, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 970 with loss 1.8528000116348267, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 971 with loss 1.650235891342163, accuracy 0.550000011920929.\n",
      "Training epoch 7 batch 972 with loss 1.7977405786514282, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 973 with loss 1.8862369060516357, accuracy 0.2460317462682724.\n",
      "Training epoch 7 batch 974 with loss 1.7884562015533447, accuracy 0.15833333134651184.\n",
      "Training epoch 7 batch 975 with loss 1.7049026489257812, accuracy 0.3875000476837158.\n",
      "Training epoch 7 batch 976 with loss 1.792906403541565, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 977 with loss 1.7737375497817993, accuracy 0.15555556118488312.\n",
      "Training epoch 7 batch 978 with loss 1.7704204320907593, accuracy 0.2611111104488373.\n",
      "Training epoch 7 batch 979 with loss 1.7733532190322876, accuracy 0.0694444477558136.\n",
      "Training epoch 7 batch 980 with loss 1.6581106185913086, accuracy 0.236111119389534.\n",
      "Training epoch 7 batch 981 with loss 1.7378337383270264, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 982 with loss 1.7941806316375732, accuracy 0.15833334624767303.\n",
      "Training epoch 7 batch 983 with loss 1.7481775283813477, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 984 with loss 1.7679319381713867, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 985 with loss 1.886407494544983, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 986 with loss 1.788251519203186, accuracy 0.24166667461395264.\n",
      "Training epoch 7 batch 987 with loss 1.7931292057037354, accuracy 0.2916666567325592.\n",
      "Training epoch 7 batch 988 with loss 1.6347935199737549, accuracy 0.4166666865348816.\n",
      "Training epoch 7 batch 989 with loss 1.8019386529922485, accuracy 0.15555556118488312.\n",
      "Training epoch 7 batch 990 with loss 1.7641007900238037, accuracy 0.2916666567325592.\n",
      "Training epoch 7 batch 991 with loss 1.706621766090393, accuracy 0.2797619104385376.\n",
      "Training epoch 7 batch 992 with loss 1.7982664108276367, accuracy 0.21666666865348816.\n",
      "Training epoch 7 batch 993 with loss 1.7845523357391357, accuracy 0.222222238779068.\n",
      "Training epoch 7 batch 994 with loss 1.7456884384155273, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 995 with loss 1.7433151006698608, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 996 with loss 1.8253036737442017, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 997 with loss 1.6684637069702148, accuracy 0.2599206268787384.\n",
      "Training epoch 7 batch 998 with loss 1.7227191925048828, accuracy 0.46666669845581055.\n",
      "Training epoch 7 batch 999 with loss 1.697037935256958, accuracy 0.3611111342906952.\n",
      "Training epoch 7 batch 1000 with loss 1.7971919775009155, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 1001 with loss 1.853750467300415, accuracy 0.06666667014360428.\n",
      "Training epoch 7 batch 1002 with loss 1.8454866409301758, accuracy 0.09444444626569748.\n",
      "Training epoch 7 batch 1003 with loss 1.7975276708602905, accuracy 0.32499998807907104.\n",
      "Training epoch 7 batch 1004 with loss 1.8648402690887451, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 1005 with loss 1.6670732498168945, accuracy 0.28611111640930176.\n",
      "Training epoch 7 batch 1006 with loss 1.763827919960022, accuracy 0.3333333432674408.\n",
      "Training epoch 7 batch 1007 with loss 1.8279545307159424, accuracy 0.18333333730697632.\n",
      "Training epoch 7 batch 1008 with loss 1.782721757888794, accuracy 0.2658730149269104.\n",
      "Training epoch 7 batch 1009 with loss 1.7117340564727783, accuracy 0.125.\n",
      "Training epoch 7 batch 1010 with loss 1.920384168624878, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 1011 with loss 1.919033408164978, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 1012 with loss 1.8705151081085205, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 1013 with loss 1.7792116403579712, accuracy 0.277777761220932.\n",
      "Training epoch 7 batch 1014 with loss 1.7841739654541016, accuracy 0.21111111342906952.\n",
      "Training epoch 7 batch 1015 with loss 1.7947313785552979, accuracy 0.1587301641702652.\n",
      "Training epoch 7 batch 1016 with loss 1.6825634241104126, accuracy 0.4333333671092987.\n",
      "Training epoch 7 batch 1017 with loss 1.7213490009307861, accuracy 0.1458333283662796.\n",
      "Training epoch 7 batch 1018 with loss 1.6793344020843506, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1019 with loss 1.9623037576675415, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 1020 with loss 1.7257671356201172, accuracy 0.236111119389534.\n",
      "Training epoch 7 batch 1021 with loss 1.7941421270370483, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 1022 with loss 1.6639257669448853, accuracy 0.3611111342906952.\n",
      "Training epoch 7 batch 1023 with loss 1.6730878353118896, accuracy 0.30000001192092896.\n",
      "Training epoch 7 batch 1024 with loss 1.8391631841659546, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 1025 with loss 1.7643604278564453, accuracy 0.23333333432674408.\n",
      "Training epoch 7 batch 1026 with loss 1.868773102760315, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 1027 with loss 1.8038692474365234, accuracy 0.24166667461395264.\n",
      "Training epoch 7 batch 1028 with loss 1.8159992694854736, accuracy 0.0.\n",
      "Training epoch 7 batch 1029 with loss 1.68606698513031, accuracy 0.5055555701255798.\n",
      "Training epoch 7 batch 1030 with loss 1.9126869440078735, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 1031 with loss 1.8218355178833008, accuracy 0.13650794327259064.\n",
      "Training epoch 7 batch 1032 with loss 1.7275161743164062, accuracy 0.36666667461395264.\n",
      "Training epoch 7 batch 1033 with loss 1.7468233108520508, accuracy 0.24166667461395264.\n",
      "Training epoch 7 batch 1034 with loss 1.7867505550384521, accuracy 0.1805555671453476.\n",
      "Training epoch 7 batch 1035 with loss 1.8541309833526611, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1036 with loss 1.8468494415283203, accuracy 0.21388888359069824.\n",
      "Training epoch 7 batch 1037 with loss 1.80877685546875, accuracy 0.10833333432674408.\n",
      "Training epoch 7 batch 1038 with loss 1.8328349590301514, accuracy 0.18888889253139496.\n",
      "Training epoch 7 batch 1039 with loss 1.8415113687515259, accuracy 0.2361111044883728.\n",
      "Training epoch 7 batch 1040 with loss 1.8074588775634766, accuracy 0.1587301641702652.\n",
      "Training epoch 7 batch 1041 with loss 1.7717177867889404, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 1042 with loss 1.7465461492538452, accuracy 0.125.\n",
      "Training epoch 7 batch 1043 with loss 1.810600996017456, accuracy 0.10000000149011612.\n",
      "Training epoch 7 batch 1044 with loss 1.7781686782836914, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 1045 with loss 1.8503936529159546, accuracy 0.3333333432674408.\n",
      "Training epoch 7 batch 1046 with loss 1.8470909595489502, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1047 with loss 1.8676687479019165, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 1048 with loss 1.8197400569915771, accuracy 0.2916666865348816.\n",
      "Training epoch 7 batch 1049 with loss 1.7087234258651733, accuracy 0.222222238779068.\n",
      "Training epoch 7 batch 1050 with loss 1.7442529201507568, accuracy 0.23333334922790527.\n",
      "Training epoch 7 batch 1051 with loss 1.7289596796035767, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 1052 with loss 1.8434813022613525, accuracy 0.18333333730697632.\n",
      "Training epoch 7 batch 1053 with loss 1.8510568141937256, accuracy 0.10833333432674408.\n",
      "Training epoch 7 batch 1054 with loss 1.7710132598876953, accuracy 0.3888888955116272.\n",
      "Training epoch 7 batch 1055 with loss 1.8309332132339478, accuracy 0.347222238779068.\n",
      "Training epoch 7 batch 1056 with loss 1.758556604385376, accuracy 0.1130952388048172.\n",
      "Training epoch 7 batch 1057 with loss 1.6787521839141846, accuracy 0.13333334028720856.\n",
      "Training epoch 7 batch 1058 with loss 1.7931737899780273, accuracy 0.1597222238779068.\n",
      "Training epoch 7 batch 1059 with loss 1.7617934942245483, accuracy 0.347222238779068.\n",
      "Training epoch 7 batch 1060 with loss 1.8683319091796875, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 1061 with loss 1.8140404224395752, accuracy 0.31111112236976624.\n",
      "Training epoch 7 batch 1062 with loss 1.8902924060821533, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 1063 with loss 1.764350175857544, accuracy 0.16388890147209167.\n",
      "Training epoch 7 batch 1064 with loss 1.7960987091064453, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 1065 with loss 1.8245925903320312, accuracy 0.15833333134651184.\n",
      "Training epoch 7 batch 1066 with loss 1.7981112003326416, accuracy 0.0892857164144516.\n",
      "Training epoch 7 batch 1067 with loss 1.772679090499878, accuracy 0.3611111044883728.\n",
      "Training epoch 7 batch 1068 with loss 1.8156980276107788, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 1069 with loss 1.899511694908142, accuracy 0.06666667014360428.\n",
      "Training epoch 7 batch 1070 with loss 1.7615753412246704, accuracy 0.1349206417798996.\n",
      "Training epoch 7 batch 1071 with loss 1.7441036701202393, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1072 with loss 1.7920945882797241, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 1073 with loss 1.8080682754516602, accuracy 0.25.\n",
      "Training epoch 7 batch 1074 with loss 1.7407169342041016, accuracy 0.3492063581943512.\n",
      "Training epoch 7 batch 1075 with loss 1.814448356628418, accuracy 0.3214285969734192.\n",
      "Training epoch 7 batch 1076 with loss 1.7968015670776367, accuracy 0.26944446563720703.\n",
      "Training epoch 7 batch 1077 with loss 1.7442073822021484, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 1078 with loss 1.7958580255508423, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 1079 with loss 1.7534488439559937, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 1080 with loss 1.7436596155166626, accuracy 0.31111112236976624.\n",
      "Training epoch 7 batch 1081 with loss 1.926717758178711, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 1082 with loss 1.7456134557724, accuracy 0.2944444417953491.\n",
      "Training epoch 7 batch 1083 with loss 1.625105619430542, accuracy 0.43888890743255615.\n",
      "Training epoch 7 batch 1084 with loss 1.7634124755859375, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 1085 with loss 1.8382470607757568, accuracy 0.17500001192092896.\n",
      "Training epoch 7 batch 1086 with loss 1.808969497680664, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 1087 with loss 1.6955817937850952, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 1088 with loss 1.7306108474731445, accuracy 0.190476194024086.\n",
      "Training epoch 7 batch 1089 with loss 1.7761762142181396, accuracy 0.18333333730697632.\n",
      "Training epoch 7 batch 1090 with loss 1.8200066089630127, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 1091 with loss 1.8020851612091064, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 1092 with loss 1.9259560108184814, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 1093 with loss 1.8867648839950562, accuracy 0.15833333134651184.\n",
      "Training epoch 7 batch 1094 with loss 1.8460702896118164, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1095 with loss 1.6914230585098267, accuracy 0.3916666805744171.\n",
      "Training epoch 7 batch 1096 with loss 1.764301061630249, accuracy 0.15555556118488312.\n",
      "Training epoch 7 batch 1097 with loss 1.855487585067749, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 1098 with loss 1.7222378253936768, accuracy 0.25.\n",
      "Training epoch 7 batch 1099 with loss 1.7808068990707397, accuracy 0.23333334922790527.\n",
      "Training epoch 7 batch 1100 with loss 1.9038219451904297, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 1101 with loss 1.828357458114624, accuracy 0.1865079402923584.\n",
      "Training epoch 7 batch 1102 with loss 1.7215169668197632, accuracy 0.20595237612724304.\n",
      "Training epoch 7 batch 1103 with loss 1.8520129919052124, accuracy 0.02777777798473835.\n",
      "Training epoch 7 batch 1104 with loss 1.8047367334365845, accuracy 0.1349206417798996.\n",
      "Training epoch 7 batch 1105 with loss 1.7620599269866943, accuracy 0.21388889849185944.\n",
      "Training epoch 7 batch 1106 with loss 1.6721004247665405, accuracy 0.3541666865348816.\n",
      "Training epoch 7 batch 1107 with loss 1.7732967138290405, accuracy 0.36269843578338623.\n",
      "Training epoch 7 batch 1108 with loss 1.8374812602996826, accuracy 0.02777777798473835.\n",
      "Training epoch 7 batch 1109 with loss 1.9114227294921875, accuracy 0.125.\n",
      "Training epoch 7 batch 1110 with loss 1.8301010131835938, accuracy 0.1964285671710968.\n",
      "Training epoch 7 batch 1111 with loss 1.7679067850112915, accuracy 0.1805555671453476.\n",
      "Training epoch 7 batch 1112 with loss 1.8729737997055054, accuracy 0.1785714328289032.\n",
      "Training epoch 7 batch 1113 with loss 1.8581126928329468, accuracy 0.2142857164144516.\n",
      "Training epoch 7 batch 1114 with loss 1.8225727081298828, accuracy 0.277777761220932.\n",
      "Training epoch 7 batch 1115 with loss 1.8918691873550415, accuracy 0.0694444477558136.\n",
      "Training epoch 7 batch 1116 with loss 1.7634201049804688, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 1117 with loss 1.836755394935608, accuracy 0.17222222685813904.\n",
      "Training epoch 7 batch 1118 with loss 1.790740728378296, accuracy 0.09047619253396988.\n",
      "Training epoch 7 batch 1119 with loss 1.825676679611206, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 1120 with loss 1.844761848449707, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 1121 with loss 1.8925930261611938, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 1122 with loss 1.8105461597442627, accuracy 0.17658731341362.\n",
      "Training epoch 7 batch 1123 with loss 1.810243844985962, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 1124 with loss 1.8285106420516968, accuracy 0.17222222685813904.\n",
      "Training epoch 7 batch 1125 with loss 1.8791906833648682, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 1126 with loss 1.7432479858398438, accuracy 0.1130952388048172.\n",
      "Training epoch 7 batch 1127 with loss 1.8386322259902954, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1128 with loss 1.7754156589508057, accuracy 0.16388890147209167.\n",
      "Training epoch 7 batch 1129 with loss 1.7376130819320679, accuracy 0.24444445967674255.\n",
      "Training epoch 7 batch 1130 with loss 1.8351128101348877, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 1131 with loss 1.7779594659805298, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 1132 with loss 1.7894611358642578, accuracy 0.190476194024086.\n",
      "Training epoch 7 batch 1133 with loss 1.9371557235717773, accuracy 0.03333333507180214.\n",
      "Training epoch 7 batch 1134 with loss 1.8418605327606201, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 1135 with loss 1.7738231420516968, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 1136 with loss 1.737696647644043, accuracy 0.2666666805744171.\n",
      "Training epoch 7 batch 1137 with loss 1.7446157932281494, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 1138 with loss 1.882287621498108, accuracy 0.0694444477558136.\n",
      "Training epoch 7 batch 1139 with loss 1.6915924549102783, accuracy 0.3333333432674408.\n",
      "Training epoch 7 batch 1140 with loss 1.7503010034561157, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 1141 with loss 1.7964792251586914, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 1142 with loss 1.7760015726089478, accuracy 0.22083333134651184.\n",
      "Training epoch 7 batch 1143 with loss 1.7879749536514282, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 1144 with loss 1.7379591464996338, accuracy 0.37222224473953247.\n",
      "Training epoch 7 batch 1145 with loss 1.7350270748138428, accuracy 0.1071428582072258.\n",
      "Training epoch 7 batch 1146 with loss 1.7965621948242188, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 1147 with loss 1.8847086429595947, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 1148 with loss 1.8992815017700195, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1149 with loss 1.7269262075424194, accuracy 0.3333333432674408.\n",
      "Training epoch 7 batch 1150 with loss 1.6317284107208252, accuracy 0.3958333432674408.\n",
      "Training epoch 7 batch 1151 with loss 1.9184945821762085, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 1152 with loss 1.645622968673706, accuracy 0.29722222685813904.\n",
      "Training epoch 7 batch 1153 with loss 1.7020009756088257, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 1154 with loss 1.6774389743804932, accuracy 0.2944444417953491.\n",
      "Training epoch 7 batch 1155 with loss 1.8549859523773193, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 1156 with loss 1.7338021993637085, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 1157 with loss 1.760514259338379, accuracy 0.2797619104385376.\n",
      "Training epoch 7 batch 1158 with loss 1.8474384546279907, accuracy 0.0476190485060215.\n",
      "Training epoch 7 batch 1159 with loss 1.7804521322250366, accuracy 0.21666666865348816.\n",
      "Training epoch 7 batch 1160 with loss 1.9171230792999268, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1161 with loss 1.7705309391021729, accuracy 0.2916666567325592.\n",
      "Training epoch 7 batch 1162 with loss 1.8529630899429321, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 1163 with loss 1.7654300928115845, accuracy 0.125.\n",
      "Training epoch 7 batch 1164 with loss 1.7512136697769165, accuracy 0.2152777910232544.\n",
      "Training epoch 7 batch 1165 with loss 1.872979760169983, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 1166 with loss 1.8207069635391235, accuracy 0.16919192671775818.\n",
      "Training epoch 7 batch 1167 with loss 1.9329423904418945, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 1168 with loss 1.8853257894515991, accuracy 0.25.\n",
      "Training epoch 7 batch 1169 with loss 1.7796709537506104, accuracy 0.3333333432674408.\n",
      "Training epoch 7 batch 1170 with loss 1.8647292852401733, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 1171 with loss 1.8761560916900635, accuracy 0.21388888359069824.\n",
      "Training epoch 7 batch 1172 with loss 1.7250674962997437, accuracy 0.4166666865348816.\n",
      "Training epoch 7 batch 1173 with loss 1.8206861019134521, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1174 with loss 1.7085154056549072, accuracy 0.31111112236976624.\n",
      "Training epoch 7 batch 1175 with loss 1.767225980758667, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 1176 with loss 1.8133618831634521, accuracy 0.10833333432674408.\n",
      "Training epoch 7 batch 1177 with loss 1.6523447036743164, accuracy 0.5.\n",
      "Training epoch 7 batch 1178 with loss 1.6752281188964844, accuracy 0.3888888955116272.\n",
      "Training epoch 7 batch 1179 with loss 1.7985999584197998, accuracy 0.10000000894069672.\n",
      "Training epoch 7 batch 1180 with loss 1.8536310195922852, accuracy 0.33888888359069824.\n",
      "Training epoch 7 batch 1181 with loss 1.8033654689788818, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 1182 with loss 1.8063478469848633, accuracy 0.25.\n",
      "Training epoch 7 batch 1183 with loss 1.879704475402832, accuracy 0.0.\n",
      "Training epoch 7 batch 1184 with loss 1.8430306911468506, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 1185 with loss 1.8360888957977295, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 1186 with loss 1.8182140588760376, accuracy 0.2936508059501648.\n",
      "Training epoch 7 batch 1187 with loss 1.7407112121582031, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 1188 with loss 1.7623164653778076, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 1189 with loss 1.8885704278945923, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1190 with loss 1.8031184673309326, accuracy 0.21944445371627808.\n",
      "Training epoch 7 batch 1191 with loss 1.789920449256897, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 1192 with loss 1.8462562561035156, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 1193 with loss 1.754315733909607, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 1194 with loss 1.823604941368103, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1195 with loss 1.7380383014678955, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 1196 with loss 1.7727943658828735, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 1197 with loss 1.7278372049331665, accuracy 0.2888889014720917.\n",
      "Training epoch 7 batch 1198 with loss 1.828112006187439, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 1199 with loss 1.8487342596054077, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 1200 with loss 1.7794725894927979, accuracy 0.09880952537059784.\n",
      "Training epoch 7 batch 1201 with loss 1.8957160711288452, accuracy 0.03333333507180214.\n",
      "Training epoch 7 batch 1202 with loss 1.786574363708496, accuracy 0.17222222685813904.\n",
      "Training epoch 7 batch 1203 with loss 1.7779595851898193, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 1204 with loss 1.7186940908432007, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 1205 with loss 1.7800155878067017, accuracy 0.24722221493721008.\n",
      "Training epoch 7 batch 1206 with loss 1.7555307149887085, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 1207 with loss 1.8285157680511475, accuracy 0.0625.\n",
      "Training epoch 7 batch 1208 with loss 1.8524497747421265, accuracy 0.1319444477558136.\n",
      "Training epoch 7 batch 1209 with loss 1.8397296667099, accuracy 0.125.\n",
      "Training epoch 7 batch 1210 with loss 1.8089616298675537, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 1211 with loss 1.8046417236328125, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 1212 with loss 1.7903127670288086, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 1213 with loss 1.718156099319458, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 1214 with loss 1.8470547199249268, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 1215 with loss 1.8732831478118896, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 1216 with loss 1.8022003173828125, accuracy 0.13333334028720856.\n",
      "Training epoch 7 batch 1217 with loss 1.8073772192001343, accuracy 0.15833333134651184.\n",
      "Training epoch 7 batch 1218 with loss 1.7962315082550049, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1219 with loss 1.6987817287445068, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 1220 with loss 1.7611026763916016, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1221 with loss 1.7843172550201416, accuracy 0.28333333134651184.\n",
      "Training epoch 7 batch 1222 with loss 1.7239118814468384, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 1223 with loss 1.7220605611801147, accuracy 0.402777761220932.\n",
      "Training epoch 7 batch 1224 with loss 1.8892176151275635, accuracy 0.22777777910232544.\n",
      "Training epoch 7 batch 1225 with loss 1.8137553930282593, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 1226 with loss 1.7834059000015259, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 1227 with loss 1.926401138305664, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 1228 with loss 1.810249924659729, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 1229 with loss 1.761171579360962, accuracy 0.125.\n",
      "Training epoch 7 batch 1230 with loss 1.7692022323608398, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 1231 with loss 1.7007839679718018, accuracy 0.2666666805744171.\n",
      "Training epoch 7 batch 1232 with loss 1.78194260597229, accuracy 0.20000001788139343.\n",
      "Training epoch 7 batch 1233 with loss 1.9767048358917236, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 1234 with loss 1.8709659576416016, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 1235 with loss 1.7895839214324951, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 1236 with loss 1.7364161014556885, accuracy 0.25.\n",
      "Training epoch 7 batch 1237 with loss 1.911807656288147, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 1238 with loss 1.8341976404190063, accuracy 0.28333333134651184.\n",
      "Training epoch 7 batch 1239 with loss 1.848733901977539, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 1240 with loss 1.7296638488769531, accuracy 0.25.\n",
      "Training epoch 7 batch 1241 with loss 1.8075149059295654, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 1242 with loss 1.8348404169082642, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 1243 with loss 1.695418357849121, accuracy 0.3055555522441864.\n",
      "Training epoch 7 batch 1244 with loss 1.7481342554092407, accuracy 0.17222222685813904.\n",
      "Training epoch 7 batch 1245 with loss 1.7940683364868164, accuracy 0.1587301641702652.\n",
      "Training epoch 7 batch 1246 with loss 1.7581733465194702, accuracy 0.3305555582046509.\n",
      "Training epoch 7 batch 1247 with loss 1.720144510269165, accuracy 0.21666666865348816.\n",
      "Training epoch 7 batch 1248 with loss 1.7503812313079834, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 1249 with loss 1.7885633707046509, accuracy 0.45000001788139343.\n",
      "Training epoch 7 batch 1250 with loss 1.7195669412612915, accuracy 0.1349206417798996.\n",
      "Training epoch 7 batch 1251 with loss 1.7470123767852783, accuracy 0.4138889014720917.\n",
      "Training epoch 7 batch 1252 with loss 1.896794080734253, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 1253 with loss 1.8209311962127686, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1254 with loss 1.727834939956665, accuracy 0.23055556416511536.\n",
      "Training epoch 7 batch 1255 with loss 1.77275812625885, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 1256 with loss 1.8313586711883545, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 1257 with loss 1.7042686939239502, accuracy 0.2708333432674408.\n",
      "Training epoch 7 batch 1258 with loss 1.812747597694397, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 1259 with loss 1.8497183322906494, accuracy 0.3222222328186035.\n",
      "Training epoch 7 batch 1260 with loss 1.850469946861267, accuracy 0.11269842088222504.\n",
      "Training epoch 7 batch 1261 with loss 1.8879339694976807, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 1262 with loss 1.8421531915664673, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 1263 with loss 1.740736722946167, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 1264 with loss 1.8315658569335938, accuracy 0.10833333432674408.\n",
      "Training epoch 7 batch 1265 with loss 1.7303956747055054, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 1266 with loss 1.7266309261322021, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 1267 with loss 1.947350263595581, accuracy 0.06111111491918564.\n",
      "Training epoch 7 batch 1268 with loss 1.823849081993103, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 1269 with loss 1.7719221115112305, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 1270 with loss 1.9337984323501587, accuracy 0.1071428582072258.\n",
      "Training epoch 7 batch 1271 with loss 1.7726202011108398, accuracy 0.17777778208255768.\n",
      "Training epoch 7 batch 1272 with loss 1.656827688217163, accuracy 0.25.\n",
      "Training epoch 7 batch 1273 with loss 1.6525156497955322, accuracy 0.3194444477558136.\n",
      "Training epoch 7 batch 1274 with loss 1.8678267002105713, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1275 with loss 1.8096427917480469, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 1276 with loss 1.8821052312850952, accuracy 0.1805555671453476.\n",
      "Training epoch 7 batch 1277 with loss 1.7936540842056274, accuracy 0.2698412835597992.\n",
      "Training epoch 7 batch 1278 with loss 1.8548113107681274, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 1279 with loss 1.8630965948104858, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 1280 with loss 1.7756321430206299, accuracy 0.26944446563720703.\n",
      "Training epoch 7 batch 1281 with loss 1.7799527645111084, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 1282 with loss 1.835150122642517, accuracy 0.0694444477558136.\n",
      "Training epoch 7 batch 1283 with loss 1.8089287281036377, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 1284 with loss 1.8208955526351929, accuracy 0.21984127163887024.\n",
      "Training epoch 7 batch 1285 with loss 1.657867193222046, accuracy 0.25555557012557983.\n",
      "Training epoch 7 batch 1286 with loss 1.7570924758911133, accuracy 0.18333333730697632.\n",
      "Training epoch 7 batch 1287 with loss 1.7242933511734009, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1288 with loss 1.8379558324813843, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 1289 with loss 1.8457618951797485, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 1290 with loss 1.781664252281189, accuracy 0.4583333730697632.\n",
      "Training epoch 7 batch 1291 with loss 1.8408746719360352, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 1292 with loss 1.7704145908355713, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 1293 with loss 1.8305275440216064, accuracy 0.125.\n",
      "Training epoch 7 batch 1294 with loss 1.7849773168563843, accuracy 0.25.\n",
      "Training epoch 7 batch 1295 with loss 1.7345592975616455, accuracy 0.25555554032325745.\n",
      "Training epoch 7 batch 1296 with loss 1.8769763708114624, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 1297 with loss 1.787493109703064, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1298 with loss 1.7680927515029907, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 1299 with loss 1.763744592666626, accuracy 0.25.\n",
      "Training epoch 7 batch 1300 with loss 1.8211421966552734, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1301 with loss 1.730952501296997, accuracy 0.16388890147209167.\n",
      "Training epoch 7 batch 1302 with loss 1.8663288354873657, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 1303 with loss 1.7727015018463135, accuracy 0.18611112236976624.\n",
      "Training epoch 7 batch 1304 with loss 1.7659860849380493, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 1305 with loss 1.8494060039520264, accuracy 0.2738095223903656.\n",
      "Training epoch 7 batch 1306 with loss 1.6997668743133545, accuracy 0.2698412835597992.\n",
      "Training epoch 7 batch 1307 with loss 1.8224948644638062, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 1308 with loss 1.8373630046844482, accuracy 0.25555557012557983.\n",
      "Training epoch 7 batch 1309 with loss 1.8611739873886108, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1310 with loss 1.8287509679794312, accuracy 0.125.\n",
      "Training epoch 7 batch 1311 with loss 1.8904173374176025, accuracy 0.16388890147209167.\n",
      "Training epoch 7 batch 1312 with loss 1.8749845027923584, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 1313 with loss 1.7278623580932617, accuracy 0.23888888955116272.\n",
      "Training epoch 7 batch 1314 with loss 1.8396775722503662, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 1315 with loss 1.7597780227661133, accuracy 0.1031746044754982.\n",
      "Training epoch 7 batch 1316 with loss 1.8536584377288818, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 1317 with loss 1.8172296285629272, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1318 with loss 1.8333280086517334, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1319 with loss 1.8857097625732422, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 1320 with loss 1.7740116119384766, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 1321 with loss 1.9087934494018555, accuracy 0.02777777798473835.\n",
      "Training epoch 7 batch 1322 with loss 1.7201259136199951, accuracy 0.14444445073604584.\n",
      "Training epoch 7 batch 1323 with loss 1.7835924625396729, accuracy 0.19603174924850464.\n",
      "Training epoch 7 batch 1324 with loss 1.78290593624115, accuracy 0.24722221493721008.\n",
      "Training epoch 7 batch 1325 with loss 1.873086929321289, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1326 with loss 1.855825424194336, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 1327 with loss 1.6571128368377686, accuracy 0.4749999940395355.\n",
      "Training epoch 7 batch 1328 with loss 1.8424659967422485, accuracy 0.0.\n",
      "Training epoch 7 batch 1329 with loss 1.8520581722259521, accuracy 0.28333336114883423.\n",
      "Training epoch 7 batch 1330 with loss 1.7397229671478271, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 1331 with loss 1.7964420318603516, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 1332 with loss 1.670353651046753, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1333 with loss 1.801967978477478, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 1334 with loss 1.7219572067260742, accuracy 0.2361111044883728.\n",
      "Training epoch 7 batch 1335 with loss 1.8344863653182983, accuracy 0.25.\n",
      "Training epoch 7 batch 1336 with loss 1.857972502708435, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 1337 with loss 1.8472039699554443, accuracy 0.2805555760860443.\n",
      "Training epoch 7 batch 1338 with loss 1.8838939666748047, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 1339 with loss 1.7339404821395874, accuracy 0.25555557012557983.\n",
      "Training epoch 7 batch 1340 with loss 1.8306833505630493, accuracy 0.21944445371627808.\n",
      "Training epoch 7 batch 1341 with loss 1.7483437061309814, accuracy 0.23333334922790527.\n",
      "Training epoch 7 batch 1342 with loss 1.7547441720962524, accuracy 0.2944444417953491.\n",
      "Training epoch 7 batch 1343 with loss 1.7680559158325195, accuracy 0.24722224473953247.\n",
      "Training epoch 7 batch 1344 with loss 1.7342824935913086, accuracy 0.19761905074119568.\n",
      "Training epoch 7 batch 1345 with loss 1.7009456157684326, accuracy 0.2888889014720917.\n",
      "Training epoch 7 batch 1346 with loss 1.8508787155151367, accuracy 0.1269841343164444.\n",
      "Training epoch 7 batch 1347 with loss 1.8954193592071533, accuracy 0.1488095223903656.\n",
      "Training epoch 7 batch 1348 with loss 1.7612059116363525, accuracy 0.2370370626449585.\n",
      "Training epoch 7 batch 1349 with loss 1.9102332592010498, accuracy 0.1805555522441864.\n",
      "Training epoch 7 batch 1350 with loss 1.8372135162353516, accuracy 0.33888891339302063.\n",
      "Training epoch 7 batch 1351 with loss 1.775954008102417, accuracy 0.10000000894069672.\n",
      "Training epoch 7 batch 1352 with loss 1.7775558233261108, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 1353 with loss 1.8331096172332764, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 1354 with loss 1.8258578777313232, accuracy 0.2083333283662796.\n",
      "Training epoch 7 batch 1355 with loss 1.8431085348129272, accuracy 0.22777777910232544.\n",
      "Training epoch 7 batch 1356 with loss 1.7829830646514893, accuracy 0.3333333432674408.\n",
      "Training epoch 7 batch 1357 with loss 1.8023443222045898, accuracy 0.24166668951511383.\n",
      "Training epoch 7 batch 1358 with loss 1.8270384073257446, accuracy 0.20000001788139343.\n",
      "Training epoch 7 batch 1359 with loss 1.8160171508789062, accuracy 0.14166668057441711.\n",
      "Training epoch 7 batch 1360 with loss 1.8850730657577515, accuracy 0.17222222685813904.\n",
      "Training epoch 7 batch 1361 with loss 1.8409233093261719, accuracy 0.1527777910232544.\n",
      "Training epoch 7 batch 1362 with loss 1.765599250793457, accuracy 0.3253968358039856.\n",
      "Training epoch 7 batch 1363 with loss 1.7314202785491943, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 1364 with loss 1.9443680047988892, accuracy 0.26944446563720703.\n",
      "Training epoch 7 batch 1365 with loss 1.858490228652954, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 1366 with loss 1.8456194400787354, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 1367 with loss 1.8514198064804077, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1368 with loss 1.7360700368881226, accuracy 0.3166666626930237.\n",
      "Training epoch 7 batch 1369 with loss 1.86626398563385, accuracy 0.2638888955116272.\n",
      "Training epoch 7 batch 1370 with loss 1.812593698501587, accuracy 0.1865079402923584.\n",
      "Training epoch 7 batch 1371 with loss 1.8197171688079834, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 1372 with loss 1.841503381729126, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 1373 with loss 1.8423089981079102, accuracy 0.23333334922790527.\n",
      "Training epoch 7 batch 1374 with loss 1.8017688989639282, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1375 with loss 1.6649748086929321, accuracy 0.28333333134651184.\n",
      "Training epoch 7 batch 1376 with loss 1.8144471645355225, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 1377 with loss 1.609718680381775, accuracy 0.5271428823471069.\n",
      "Training epoch 7 batch 1378 with loss 1.7385501861572266, accuracy 0.1488095223903656.\n",
      "Training epoch 7 batch 1379 with loss 1.85421621799469, accuracy 0.125.\n",
      "Training epoch 7 batch 1380 with loss 1.7950811386108398, accuracy 0.125.\n",
      "Training epoch 7 batch 1381 with loss 1.7817084789276123, accuracy 0.2916666865348816.\n",
      "Training epoch 7 batch 1382 with loss 1.816656470298767, accuracy 0.2460317611694336.\n",
      "Training epoch 7 batch 1383 with loss 1.8042093515396118, accuracy 0.0793650820851326.\n",
      "Training epoch 7 batch 1384 with loss 1.8579477071762085, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 1385 with loss 1.8039048910140991, accuracy 0.4222222566604614.\n",
      "Training epoch 7 batch 1386 with loss 1.8042653799057007, accuracy 0.0476190485060215.\n",
      "Training epoch 7 batch 1387 with loss 1.8573148250579834, accuracy 0.0.\n",
      "Training epoch 7 batch 1388 with loss 1.6467411518096924, accuracy 0.3253968358039856.\n",
      "Training epoch 7 batch 1389 with loss 1.8164079189300537, accuracy 0.09444445371627808.\n",
      "Training epoch 7 batch 1390 with loss 1.8383071422576904, accuracy 0.17222222685813904.\n",
      "Training epoch 7 batch 1391 with loss 1.735091209411621, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 1392 with loss 1.6770973205566406, accuracy 0.4444444477558136.\n",
      "Training epoch 7 batch 1393 with loss 1.805008888244629, accuracy 0.3571428656578064.\n",
      "Training epoch 7 batch 1394 with loss 1.9171257019042969, accuracy 0.03333333507180214.\n",
      "Training epoch 7 batch 1395 with loss 1.8728539943695068, accuracy 0.2916666567325592.\n",
      "Training epoch 7 batch 1396 with loss 1.7859302759170532, accuracy 0.33095240592956543.\n",
      "Training epoch 7 batch 1397 with loss 1.7741864919662476, accuracy 0.10833333432674408.\n",
      "Training epoch 7 batch 1398 with loss 1.8329265117645264, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 1399 with loss 1.7466247081756592, accuracy 0.02777777798473835.\n",
      "Training epoch 7 batch 1400 with loss 1.7881253957748413, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1401 with loss 1.7835452556610107, accuracy 0.22777777910232544.\n",
      "Training epoch 7 batch 1402 with loss 1.854802131652832, accuracy 0.2361111044883728.\n",
      "Training epoch 7 batch 1403 with loss 1.7558962106704712, accuracy 0.2750000059604645.\n",
      "Training epoch 7 batch 1404 with loss 1.8281495571136475, accuracy 0.20000000298023224.\n",
      "Training epoch 7 batch 1405 with loss 1.9287000894546509, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 1406 with loss 1.774916410446167, accuracy 0.2611111104488373.\n",
      "Training epoch 7 batch 1407 with loss 1.849501371383667, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 1408 with loss 1.8175853490829468, accuracy 0.12222222983837128.\n",
      "Training epoch 7 batch 1409 with loss 1.8815282583236694, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 1410 with loss 1.8940410614013672, accuracy 0.1527777761220932.\n",
      "Training epoch 7 batch 1411 with loss 1.8567804098129272, accuracy 0.17777778208255768.\n",
      "Training epoch 7 batch 1412 with loss 1.8312240839004517, accuracy 0.11666667461395264.\n",
      "Training epoch 7 batch 1413 with loss 1.8481121063232422, accuracy 0.19166666269302368.\n",
      "Training epoch 7 batch 1414 with loss 1.7986923456192017, accuracy 0.4861111044883728.\n",
      "Training epoch 7 batch 1415 with loss 1.7628681659698486, accuracy 0.15000000596046448.\n",
      "Training epoch 7 batch 1416 with loss 1.8150339126586914, accuracy 0.12083333730697632.\n",
      "Training epoch 7 batch 1417 with loss 1.842005729675293, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 1418 with loss 1.7901735305786133, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1419 with loss 1.7242313623428345, accuracy 0.3055555820465088.\n",
      "Training epoch 7 batch 1420 with loss 1.8415724039077759, accuracy 0.1130952388048172.\n",
      "Training epoch 7 batch 1421 with loss 1.891053557395935, accuracy 0.0793650820851326.\n",
      "Training epoch 7 batch 1422 with loss 1.7100909948349, accuracy 0.1547619104385376.\n",
      "Training epoch 7 batch 1423 with loss 1.8522720336914062, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 1424 with loss 1.886414885520935, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1425 with loss 1.8105567693710327, accuracy 0.07500000298023224.\n",
      "Training epoch 7 batch 1426 with loss 1.8334680795669556, accuracy 0.1210317462682724.\n",
      "Training epoch 7 batch 1427 with loss 1.8188117742538452, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1428 with loss 1.8763244152069092, accuracy 0.0793650820851326.\n",
      "Training epoch 7 batch 1429 with loss 1.837179183959961, accuracy 0.1865079402923584.\n",
      "Training epoch 7 batch 1430 with loss 1.798314094543457, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 1431 with loss 1.8720331192016602, accuracy 0.10000000894069672.\n",
      "Training epoch 7 batch 1432 with loss 1.7429736852645874, accuracy 0.25555554032325745.\n",
      "Training epoch 7 batch 1433 with loss 1.79768967628479, accuracy 0.1210317462682724.\n",
      "Training epoch 7 batch 1434 with loss 1.8726047277450562, accuracy 0.22777777910232544.\n",
      "Training epoch 7 batch 1435 with loss 1.8730300664901733, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 1436 with loss 1.7574962377548218, accuracy 0.3055555820465088.\n",
      "Training epoch 7 batch 1437 with loss 1.827716588973999, accuracy 0.14166668057441711.\n",
      "Training epoch 7 batch 1438 with loss 1.7757484912872314, accuracy 0.23333334922790527.\n",
      "Training epoch 7 batch 1439 with loss 1.7366302013397217, accuracy 0.3670634925365448.\n",
      "Training epoch 7 batch 1440 with loss 1.7779792547225952, accuracy 0.27142858505249023.\n",
      "Training epoch 7 batch 1441 with loss 1.8329143524169922, accuracy 0.18611112236976624.\n",
      "Training epoch 7 batch 1442 with loss 1.841478943824768, accuracy 0.2361111342906952.\n",
      "Training epoch 7 batch 1443 with loss 1.6783891916275024, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1444 with loss 1.794129729270935, accuracy 0.3166666626930237.\n",
      "Training epoch 7 batch 1445 with loss 1.7646849155426025, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1446 with loss 1.7018200159072876, accuracy 0.34166666865348816.\n",
      "Training epoch 7 batch 1447 with loss 1.6894804239273071, accuracy 0.17777778208255768.\n",
      "Training epoch 7 batch 1448 with loss 1.770227074623108, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 1449 with loss 1.8127567768096924, accuracy 0.2876984179019928.\n",
      "Training epoch 7 batch 1450 with loss 1.841587781906128, accuracy 0.125.\n",
      "Training epoch 7 batch 1451 with loss 1.8297405242919922, accuracy 0.09047619253396988.\n",
      "Training epoch 7 batch 1452 with loss 1.7539821863174438, accuracy 0.22777777910232544.\n",
      "Training epoch 7 batch 1453 with loss 1.7791849374771118, accuracy 0.13055555522441864.\n",
      "Training epoch 7 batch 1454 with loss 1.7427247762680054, accuracy 0.37222224473953247.\n",
      "Training epoch 7 batch 1455 with loss 1.8514015674591064, accuracy 0.3333333432674408.\n",
      "Training epoch 7 batch 1456 with loss 1.7780628204345703, accuracy 0.32698413729667664.\n",
      "Training epoch 7 batch 1457 with loss 1.798895239830017, accuracy 0.125.\n",
      "Training epoch 7 batch 1458 with loss 1.8047268390655518, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1459 with loss 1.787858247756958, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 1460 with loss 1.760244607925415, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 1461 with loss 1.7941524982452393, accuracy 0.3769841194152832.\n",
      "Training epoch 7 batch 1462 with loss 1.7562811374664307, accuracy 0.1041666716337204.\n",
      "Training epoch 7 batch 1463 with loss 1.8886682987213135, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1464 with loss 1.715462327003479, accuracy 0.22777777910232544.\n",
      "Training epoch 7 batch 1465 with loss 1.823143720626831, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 1466 with loss 1.7662591934204102, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 1467 with loss 1.833526372909546, accuracy 0.0763888880610466.\n",
      "Training epoch 7 batch 1468 with loss 1.7567403316497803, accuracy 0.2777777910232544.\n",
      "Training epoch 7 batch 1469 with loss 1.8228824138641357, accuracy 0.03333333507180214.\n",
      "Training epoch 7 batch 1470 with loss 1.8341929912567139, accuracy 0.1388888955116272.\n",
      "Training epoch 7 batch 1471 with loss 1.7734622955322266, accuracy 0.3750000298023224.\n",
      "Training epoch 7 batch 1472 with loss 1.9088674783706665, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 1473 with loss 1.765756607055664, accuracy 0.08888889104127884.\n",
      "Training epoch 7 batch 1474 with loss 1.7616018056869507, accuracy 0.3333333432674408.\n",
      "Training epoch 7 batch 1475 with loss 1.8300771713256836, accuracy 0.0833333358168602.\n",
      "Training epoch 7 batch 1476 with loss 1.8017387390136719, accuracy 0.25.\n",
      "Training epoch 7 batch 1477 with loss 1.8068304061889648, accuracy 0.0555555559694767.\n",
      "Training epoch 7 batch 1478 with loss 1.7798845767974854, accuracy 0.20555555820465088.\n",
      "Training epoch 7 batch 1479 with loss 1.7605838775634766, accuracy 0.1111111119389534.\n",
      "Training epoch 7 batch 1480 with loss 1.7904889583587646, accuracy 0.2083333432674408.\n",
      "Training epoch 7 batch 1481 with loss 1.8585355281829834, accuracy 0.16388890147209167.\n",
      "Training epoch 7 batch 1482 with loss 1.9196827411651611, accuracy 0.0972222238779068.\n",
      "Training epoch 7 batch 1483 with loss 1.7739410400390625, accuracy 0.0.\n",
      "Training epoch 7 batch 1484 with loss 1.8772449493408203, accuracy 0.17222222685813904.\n",
      "Training epoch 7 batch 1485 with loss 1.8936212062835693, accuracy 0.0.\n",
      "Training epoch 7 batch 1486 with loss 1.8196680545806885, accuracy 0.16388890147209167.\n",
      "Training epoch 7 batch 1487 with loss 1.8364627361297607, accuracy 0.255952388048172.\n",
      "Training epoch 7 batch 1488 with loss 1.7252317667007446, accuracy 0.33888888359069824.\n",
      "Training epoch 7 batch 1489 with loss 1.742413878440857, accuracy 0.3194444477558136.\n",
      "Training epoch 7 batch 1490 with loss 1.8690423965454102, accuracy 0.22777777910232544.\n",
      "Training epoch 7 batch 1491 with loss 1.8330339193344116, accuracy 0.0416666679084301.\n",
      "Training epoch 7 batch 1492 with loss 1.800959825515747, accuracy 0.2222222238779068.\n",
      "Training epoch 7 batch 1493 with loss 1.7752412557601929, accuracy 0.2666666805744171.\n",
      "Training epoch 7 batch 1494 with loss 1.7726552486419678, accuracy 0.1944444477558136.\n",
      "Training epoch 7 batch 1495 with loss 1.8097078800201416, accuracy 0.02777777798473835.\n",
      "Training epoch 7 batch 1496 with loss 1.734580636024475, accuracy 0.3888888955116272.\n",
      "Training epoch 7 batch 1497 with loss 1.8706966638565063, accuracy 0.1071428582072258.\n",
      "Training epoch 7 batch 1498 with loss 1.7621389627456665, accuracy 0.1666666716337204.\n",
      "Training epoch 7 batch 1499 with loss 1.8151960372924805, accuracy 0.2222222238779068.\n",
      "Test batch 0 with loss 1.8380695581436157 and accuracy 0.12222222983837128.\n",
      "Test batch 1 with loss 1.806108832359314 and accuracy 0.15000000596046448.\n",
      "Test batch 2 with loss 1.8597097396850586 and accuracy 0.31666669249534607.\n",
      "Test batch 3 with loss 1.8284612894058228 and accuracy 0.10833333432674408.\n",
      "Test batch 4 with loss 1.765796422958374 and accuracy 0.1111111119389534.\n",
      "Test batch 5 with loss 1.8982524871826172 and accuracy 0.25.\n",
      "Test batch 6 with loss 1.7516275644302368 and accuracy 0.2361111044883728.\n",
      "Test batch 7 with loss 1.8964998722076416 and accuracy 0.11666667461395264.\n",
      "Test batch 8 with loss 1.770936369895935 and accuracy 0.1666666716337204.\n",
      "Test batch 9 with loss 1.833209753036499 and accuracy 0.1805555522441864.\n",
      "Test batch 10 with loss 1.7576768398284912 and accuracy 0.20555555820465088.\n",
      "Test batch 11 with loss 1.8174164295196533 and accuracy 0.10277777910232544.\n",
      "Test batch 12 with loss 1.9255836009979248 and accuracy 0.1388888955116272.\n",
      "Test batch 13 with loss 1.7671375274658203 and accuracy 0.32500001788139343.\n",
      "Test batch 14 with loss 1.8592382669448853 and accuracy 0.0972222238779068.\n",
      "Test batch 15 with loss 1.846727967262268 and accuracy 0.15555556118488312.\n",
      "Test batch 16 with loss 1.7723073959350586 and accuracy 0.0972222238779068.\n",
      "Test batch 17 with loss 1.9038928747177124 and accuracy 0.1388888955116272.\n",
      "Test batch 18 with loss 1.9040067195892334 and accuracy 0.08888889104127884.\n",
      "Test batch 19 with loss 1.8025569915771484 and accuracy 0.0694444477558136.\n",
      "Test batch 20 with loss 1.8735096454620361 and accuracy 0.12222222983837128.\n",
      "Test batch 21 with loss 1.7787898778915405 and accuracy 0.23333334922790527.\n",
      "Test batch 22 with loss 1.719925880432129 and accuracy 0.26944446563720703.\n",
      "Test batch 23 with loss 1.9064381122589111 and accuracy 0.06111111491918564.\n",
      "Test batch 24 with loss 1.7707821130752563 and accuracy 0.1666666716337204.\n",
      "Test batch 25 with loss 1.6701593399047852 and accuracy 0.5055555701255798.\n",
      "Test batch 26 with loss 1.8158212900161743 and accuracy 0.3333333432674408.\n",
      "Test batch 27 with loss 1.8299620151519775 and accuracy 0.2003968358039856.\n",
      "Test batch 28 with loss 1.865386962890625 and accuracy 0.125.\n",
      "Test batch 29 with loss 1.7225103378295898 and accuracy 0.4166666567325592.\n",
      "Test batch 30 with loss 1.8150594234466553 and accuracy 0.1805555522441864.\n",
      "Test batch 31 with loss 1.8234421014785767 and accuracy 0.111111119389534.\n",
      "Test batch 32 with loss 1.795941948890686 and accuracy 0.18611112236976624.\n",
      "Test batch 33 with loss 1.9181114435195923 and accuracy 0.20000000298023224.\n",
      "Test batch 34 with loss 1.920358657836914 and accuracy 0.0.\n",
      "Test batch 35 with loss 1.8118667602539062 and accuracy 0.2222222238779068.\n",
      "Test batch 36 with loss 1.8342325687408447 and accuracy 0.0.\n",
      "Test batch 37 with loss 1.8588451147079468 and accuracy 0.1805555671453476.\n",
      "Test batch 38 with loss 1.7793242931365967 and accuracy 0.2638888955116272.\n",
      "Test batch 39 with loss 1.9047409296035767 and accuracy 0.11666666716337204.\n",
      "Test batch 40 with loss 1.81606924533844 and accuracy 0.10833333432674408.\n",
      "Test batch 41 with loss 1.91470205783844 and accuracy 0.0833333358168602.\n",
      "Test batch 42 with loss 1.855919599533081 and accuracy 0.06666667014360428.\n",
      "Test batch 43 with loss 1.8119710683822632 and accuracy 0.24722222983837128.\n",
      "Test batch 44 with loss 1.84929621219635 and accuracy 0.15555556118488312.\n",
      "Test batch 45 with loss 1.9352365732192993 and accuracy 0.0694444477558136.\n",
      "Test batch 46 with loss 1.692050576210022 and accuracy 0.38333335518836975.\n",
      "Test batch 47 with loss 1.7942607402801514 and accuracy 0.13055555522441864.\n",
      "Test batch 48 with loss 1.8327019214630127 and accuracy 0.10277777910232544.\n",
      "Test batch 49 with loss 1.793861746788025 and accuracy 0.2361111044883728.\n",
      "Test batch 50 with loss 1.9071356058120728 and accuracy 0.0833333358168602.\n",
      "Test batch 51 with loss 1.8274052143096924 and accuracy 0.1805555522441864.\n",
      "Test batch 52 with loss 1.7079569101333618 and accuracy 0.222222238779068.\n",
      "Test batch 53 with loss 1.9556457996368408 and accuracy 0.0555555559694767.\n",
      "Test batch 54 with loss 1.8903977870941162 and accuracy 0.0972222238779068.\n",
      "Test batch 55 with loss 1.8096373081207275 and accuracy 0.329365074634552.\n",
      "Test batch 56 with loss 1.8319190740585327 and accuracy 0.22777777910232544.\n",
      "Test batch 57 with loss 1.917650818824768 and accuracy 0.02380952425301075.\n",
      "Test batch 58 with loss 1.8346410989761353 and accuracy 0.1666666716337204.\n",
      "Test batch 59 with loss 1.8667049407958984 and accuracy 0.0972222238779068.\n",
      "Test batch 60 with loss 1.8694835901260376 and accuracy 0.1388888955116272.\n",
      "Test batch 61 with loss 1.8156623840332031 and accuracy 0.22777777910232544.\n",
      "Test batch 62 with loss 1.8256393671035767 and accuracy 0.1388888955116272.\n",
      "Test batch 63 with loss 1.8671118021011353 and accuracy 0.0.\n",
      "Test batch 64 with loss 1.8887898921966553 and accuracy 0.15833334624767303.\n",
      "Test batch 65 with loss 1.8330316543579102 and accuracy 0.13055555522441864.\n",
      "Test batch 66 with loss 1.8954534530639648 and accuracy 0.0555555559694767.\n",
      "Test batch 67 with loss 1.8036956787109375 and accuracy 0.08888889104127884.\n",
      "Test batch 68 with loss 1.9056249856948853 and accuracy 0.03333333507180214.\n",
      "Test batch 69 with loss 1.8110700845718384 and accuracy 0.15833333134651184.\n",
      "Test batch 70 with loss 1.8462905883789062 and accuracy 0.1587301641702652.\n",
      "Test batch 71 with loss 1.839172124862671 and accuracy 0.11666667461395264.\n",
      "Test batch 72 with loss 1.9307937622070312 and accuracy 0.1527777761220932.\n",
      "Test batch 73 with loss 1.884616494178772 and accuracy 0.1666666716337204.\n",
      "Test batch 74 with loss 1.9125016927719116 and accuracy 0.0555555559694767.\n",
      "Test batch 75 with loss 1.7487561702728271 and accuracy 0.3333333134651184.\n",
      "Test batch 76 with loss 1.801264762878418 and accuracy 0.19166666269302368.\n",
      "Test batch 77 with loss 1.7620570659637451 and accuracy 0.19166666269302368.\n",
      "Test batch 78 with loss 1.7240400314331055 and accuracy 0.15000000596046448.\n",
      "Test batch 79 with loss 1.699702262878418 and accuracy 0.3611111044883728.\n",
      "Test batch 80 with loss 1.7309894561767578 and accuracy 0.402777761220932.\n",
      "Test batch 81 with loss 1.8089733123779297 and accuracy 0.0555555559694767.\n",
      "Test batch 82 with loss 1.8308290243148804 and accuracy 0.1527777761220932.\n",
      "Test batch 83 with loss 1.8102309703826904 and accuracy 0.1388888955116272.\n",
      "Test batch 84 with loss 1.758880376815796 and accuracy 0.1944444477558136.\n",
      "Test batch 85 with loss 1.8139429092407227 and accuracy 0.0833333358168602.\n",
      "Test batch 86 with loss 1.8791488409042358 and accuracy 0.02777777798473835.\n",
      "Test batch 87 with loss 1.9406543970108032 and accuracy 0.1111111119389534.\n",
      "Test batch 88 with loss 1.7932660579681396 and accuracy 0.1805555671453476.\n",
      "Test batch 89 with loss 1.920864462852478 and accuracy 0.0694444477558136.\n",
      "Test batch 90 with loss 1.800803542137146 and accuracy 0.0892857164144516.\n",
      "Test batch 91 with loss 1.8609850406646729 and accuracy 0.14444445073604584.\n",
      "Test batch 92 with loss 1.752436876296997 and accuracy 0.3055555522441864.\n",
      "Test batch 93 with loss 1.7179157733917236 and accuracy 0.13425925374031067.\n",
      "Test batch 94 with loss 1.697359323501587 and accuracy 0.24523809552192688.\n",
      "Test batch 95 with loss 1.8761374950408936 and accuracy 0.0833333358168602.\n",
      "Test batch 96 with loss 1.7368824481964111 and accuracy 0.3611111342906952.\n",
      "Test batch 97 with loss 1.7282285690307617 and accuracy 0.3222222328186035.\n",
      "Test batch 98 with loss 1.8352636098861694 and accuracy 0.15833333134651184.\n",
      "Test batch 99 with loss 1.7169997692108154 and accuracy 0.2611111104488373.\n",
      "Test batch 100 with loss 1.7847903966903687 and accuracy 0.25.\n",
      "Test batch 101 with loss 1.7849369049072266 and accuracy 0.261904776096344.\n",
      "Test batch 102 with loss 1.9149091243743896 and accuracy 0.17222222685813904.\n",
      "Test batch 103 with loss 1.9434541463851929 and accuracy 0.0972222238779068.\n",
      "Test batch 104 with loss 1.9308322668075562 and accuracy 0.0972222238779068.\n",
      "Test batch 105 with loss 1.9359019994735718 and accuracy 0.1210317462682724.\n",
      "Test batch 106 with loss 1.902012825012207 and accuracy 0.0416666679084301.\n",
      "Test batch 107 with loss 1.9464305639266968 and accuracy 0.0833333358168602.\n",
      "Test batch 108 with loss 1.803113579750061 and accuracy 0.17222222685813904.\n",
      "Test batch 109 with loss 1.808434247970581 and accuracy 0.402777761220932.\n",
      "Test batch 110 with loss 1.9061429500579834 and accuracy 0.1111111119389534.\n",
      "Test batch 111 with loss 1.8369823694229126 and accuracy 0.3083333373069763.\n",
      "Test batch 112 with loss 1.961362600326538 and accuracy 0.0416666679084301.\n",
      "Test batch 113 with loss 1.809950590133667 and accuracy 0.0833333358168602.\n",
      "Test batch 114 with loss 1.831432580947876 and accuracy 0.25555557012557983.\n",
      "Test batch 115 with loss 1.7876522541046143 and accuracy 0.25.\n",
      "Test batch 116 with loss 1.7654516696929932 and accuracy 0.07500000298023224.\n",
      "Test batch 117 with loss 1.8331769704818726 and accuracy 0.08888889104127884.\n",
      "Test batch 118 with loss 1.8569433689117432 and accuracy 0.1388888955116272.\n",
      "Test batch 119 with loss 1.7551215887069702 and accuracy 0.2361111044883728.\n",
      "Test batch 120 with loss 1.774972915649414 and accuracy 0.10000000894069672.\n",
      "Test batch 121 with loss 1.8731170892715454 and accuracy 0.14444445073604584.\n",
      "Test batch 122 with loss 1.741192102432251 and accuracy 0.3333333432674408.\n",
      "Test batch 123 with loss 1.8853120803833008 and accuracy 0.03333333507180214.\n",
      "Test batch 124 with loss 1.8415788412094116 and accuracy 0.144841268658638.\n",
      "Test batch 125 with loss 1.8520097732543945 and accuracy 0.1944444477558136.\n",
      "Test batch 126 with loss 1.8390398025512695 and accuracy 0.20555555820465088.\n",
      "Test batch 127 with loss 1.7175344228744507 and accuracy 0.1944444477558136.\n",
      "Test batch 128 with loss 1.876930594444275 and accuracy 0.1388888955116272.\n",
      "Test batch 129 with loss 1.8649389743804932 and accuracy 0.1388888955116272.\n",
      "Test batch 130 with loss 1.8726869821548462 and accuracy 0.17222222685813904.\n",
      "Test batch 131 with loss 1.9607959985733032 and accuracy 0.03333333507180214.\n",
      "Test batch 132 with loss 1.791345238685608 and accuracy 0.1527777761220932.\n",
      "Test batch 133 with loss 1.8525654077529907 and accuracy 0.1944444477558136.\n",
      "Test batch 134 with loss 1.8843494653701782 and accuracy 0.2800000011920929.\n",
      "Test batch 135 with loss 1.7103557586669922 and accuracy 0.1944444477558136.\n",
      "Test batch 136 with loss 1.8258411884307861 and accuracy 0.130952388048172.\n",
      "Test batch 137 with loss 1.8009042739868164 and accuracy 0.11666667461395264.\n",
      "Test batch 138 with loss 1.8939298391342163 and accuracy 0.0972222238779068.\n",
      "Test batch 139 with loss 1.897301435470581 and accuracy 0.14444445073604584.\n",
      "Test batch 140 with loss 1.786468505859375 and accuracy 0.13055555522441864.\n",
      "Test batch 141 with loss 1.9180786609649658 and accuracy 0.1527777761220932.\n",
      "Test batch 142 with loss 1.8815193176269531 and accuracy 0.1031746044754982.\n",
      "Test batch 143 with loss 1.7912352085113525 and accuracy 0.22777777910232544.\n",
      "Test batch 144 with loss 1.8790429830551147 and accuracy 0.1626984179019928.\n",
      "Test batch 145 with loss 1.8411400318145752 and accuracy 0.125.\n",
      "Test batch 146 with loss 1.9064109325408936 and accuracy 0.0694444477558136.\n",
      "Test batch 147 with loss 1.8274266719818115 and accuracy 0.0833333358168602.\n",
      "Test batch 148 with loss 1.808010458946228 and accuracy 0.1666666716337204.\n",
      "Test batch 149 with loss 1.9000743627548218 and accuracy 0.0972222238779068.\n",
      "Test batch 150 with loss 1.8311898708343506 and accuracy 0.26944446563720703.\n",
      "Test batch 151 with loss 1.8978742361068726 and accuracy 0.03333333507180214.\n",
      "Test batch 152 with loss 1.764197587966919 and accuracy 0.2750000059604645.\n",
      "Test batch 153 with loss 1.7541463375091553 and accuracy 0.1527777761220932.\n",
      "Test batch 154 with loss 1.7635418176651 and accuracy 0.23333333432674408.\n",
      "Test batch 155 with loss 1.9233745336532593 and accuracy 0.125.\n",
      "Test batch 156 with loss 1.8740026950836182 and accuracy 0.22777777910232544.\n",
      "Test batch 157 with loss 1.8188183307647705 and accuracy 0.1111111119389534.\n",
      "Test batch 158 with loss 1.7867889404296875 and accuracy 0.1666666716337204.\n",
      "Test batch 159 with loss 1.9067258834838867 and accuracy 0.02083333395421505.\n",
      "Test batch 160 with loss 1.8702783584594727 and accuracy 0.02380952425301075.\n",
      "Test batch 161 with loss 1.824798345565796 and accuracy 0.1666666716337204.\n",
      "Test batch 162 with loss 1.7578426599502563 and accuracy 0.2341269850730896.\n",
      "Test batch 163 with loss 1.777306318283081 and accuracy 0.17222222685813904.\n",
      "Test batch 164 with loss 1.7459958791732788 and accuracy 0.2958333492279053.\n",
      "Test batch 165 with loss 1.8436673879623413 and accuracy 0.02380952425301075.\n",
      "Test batch 166 with loss 1.7965662479400635 and accuracy 0.24537038803100586.\n",
      "Test batch 167 with loss 1.8331654071807861 and accuracy 0.3214285969734192.\n",
      "Test batch 168 with loss 1.8155819177627563 and accuracy 0.1805555522441864.\n",
      "Test batch 169 with loss 1.8414112329483032 and accuracy 0.25555557012557983.\n",
      "Test batch 170 with loss 1.7635151147842407 and accuracy 0.1071428582072258.\n",
      "Test batch 171 with loss 1.7220395803451538 and accuracy 0.472222238779068.\n",
      "Test batch 172 with loss 1.8376960754394531 and accuracy 0.0833333358168602.\n",
      "Test batch 173 with loss 1.8615427017211914 and accuracy 0.07500000298023224.\n",
      "Test batch 174 with loss 1.8257598876953125 and accuracy 0.375.\n",
      "Test batch 175 with loss 1.7619714736938477 and accuracy 0.40000003576278687.\n",
      "Test batch 176 with loss 1.8634326457977295 and accuracy 0.09444444626569748.\n",
      "Test batch 177 with loss 1.7785431146621704 and accuracy 0.07500000298023224.\n",
      "Test batch 178 with loss 1.8589165210723877 and accuracy 0.3305555582046509.\n",
      "Test batch 179 with loss 1.9051787853240967 and accuracy 0.11666667461395264.\n",
      "Test batch 180 with loss 1.7475755214691162 and accuracy 0.20555555820465088.\n",
      "Test batch 181 with loss 1.7438303232192993 and accuracy 0.2519841492176056.\n",
      "Test batch 182 with loss 1.8252983093261719 and accuracy 0.15555556118488312.\n",
      "Test batch 183 with loss 1.7875711917877197 and accuracy 0.2797619104385376.\n",
      "Test batch 184 with loss 1.81289541721344 and accuracy 0.12222222983837128.\n",
      "Test batch 185 with loss 1.904376745223999 and accuracy 0.0416666679084301.\n",
      "Test batch 186 with loss 1.8386272192001343 and accuracy 0.02777777798473835.\n",
      "Test batch 187 with loss 1.7457622289657593 and accuracy 0.20370370149612427.\n",
      "Test batch 188 with loss 1.8539358377456665 and accuracy 0.06666667014360428.\n",
      "Test batch 189 with loss 1.8430535793304443 and accuracy 0.06111111491918564.\n",
      "Test batch 190 with loss 1.8153927326202393 and accuracy 0.13055555522441864.\n",
      "Test batch 191 with loss 1.6674394607543945 and accuracy 0.3075396716594696.\n",
      "Test batch 192 with loss 1.7317755222320557 and accuracy 0.2527777850627899.\n",
      "Test batch 193 with loss 1.9256712198257446 and accuracy 0.0555555559694767.\n",
      "Test batch 194 with loss 1.8053362369537354 and accuracy 0.12222222983837128.\n",
      "Test batch 195 with loss 1.8235670328140259 and accuracy 0.1944444477558136.\n",
      "Test batch 196 with loss 1.9926226139068604 and accuracy 0.03333333507180214.\n",
      "Test batch 197 with loss 1.7840087413787842 and accuracy 0.21944445371627808.\n",
      "Test batch 198 with loss 1.7842563390731812 and accuracy 0.1666666716337204.\n",
      "Test batch 199 with loss 1.8491557836532593 and accuracy 0.236111119389534.\n",
      "Test batch 200 with loss 1.7839162349700928 and accuracy 0.02777777798473835.\n",
      "Test batch 201 with loss 1.8040084838867188 and accuracy 0.2777777910232544.\n",
      "Test batch 202 with loss 1.77143132686615 and accuracy 0.1388888955116272.\n",
      "Test batch 203 with loss 1.7039144039154053 and accuracy 0.4222222566604614.\n",
      "Test batch 204 with loss 1.7274948358535767 and accuracy 0.3750000298023224.\n",
      "Test batch 205 with loss 1.8236780166625977 and accuracy 0.13333334028720856.\n",
      "Test batch 206 with loss 1.8597986698150635 and accuracy 0.1349206417798996.\n",
      "Test batch 207 with loss 1.839665412902832 and accuracy 0.3710317313671112.\n",
      "Test batch 208 with loss 1.8363338708877563 and accuracy 0.02777777798473835.\n",
      "Test batch 209 with loss 1.8474525213241577 and accuracy 0.0416666679084301.\n",
      "Test batch 210 with loss 1.8079588413238525 and accuracy 0.0793650820851326.\n",
      "Test batch 211 with loss 1.8193399906158447 and accuracy 0.24166667461395264.\n",
      "Test batch 212 with loss 1.800050139427185 and accuracy 0.18888890743255615.\n",
      "Test batch 213 with loss 1.8187541961669922 and accuracy 0.3333333432674408.\n",
      "Test batch 214 with loss 1.8401778936386108 and accuracy 0.20555555820465088.\n",
      "Test batch 215 with loss 1.8525222539901733 and accuracy 0.2611111104488373.\n",
      "Test batch 216 with loss 1.957044005393982 and accuracy 0.0.\n",
      "Test batch 217 with loss 1.7818453311920166 and accuracy 0.0972222238779068.\n",
      "Test batch 218 with loss 1.7630083560943604 and accuracy 0.3055555522441864.\n",
      "Test batch 219 with loss 1.8931798934936523 and accuracy 0.24166667461395264.\n",
      "Test batch 220 with loss 1.8543589115142822 and accuracy 0.2083333432674408.\n",
      "Test batch 221 with loss 1.8080852031707764 and accuracy 0.236111119389534.\n",
      "Test batch 222 with loss 1.8104082345962524 and accuracy 0.3055555820465088.\n",
      "Test batch 223 with loss 1.8976424932479858 and accuracy 0.14444445073604584.\n",
      "Test batch 224 with loss 1.808595895767212 and accuracy 0.0833333358168602.\n",
      "Test batch 225 with loss 1.865899682044983 and accuracy 0.1111111119389534.\n",
      "Test batch 226 with loss 1.821129560470581 and accuracy 0.05000000447034836.\n",
      "Test batch 227 with loss 1.8365411758422852 and accuracy 0.11666667461395264.\n",
      "Test batch 228 with loss 1.8896268606185913 and accuracy 0.0486111119389534.\n",
      "Test batch 229 with loss 1.8271551132202148 and accuracy 0.16388888657093048.\n",
      "Test batch 230 with loss 1.8784339427947998 and accuracy 0.0.\n",
      "Test batch 231 with loss 1.8090651035308838 and accuracy 0.1805555522441864.\n",
      "Test batch 232 with loss 1.8148428201675415 and accuracy 0.13055555522441864.\n",
      "Test batch 233 with loss 1.758087396621704 and accuracy 0.1071428582072258.\n",
      "Test batch 234 with loss 1.8636038303375244 and accuracy 0.15833333134651184.\n",
      "Test batch 235 with loss 1.791255235671997 and accuracy 0.15555556118488312.\n",
      "Test batch 236 with loss 1.6813428401947021 and accuracy 0.2611111104488373.\n",
      "Test batch 237 with loss 1.850664496421814 and accuracy 0.2083333283662796.\n",
      "Test batch 238 with loss 1.8869695663452148 and accuracy 0.2083333432674408.\n",
      "Test batch 239 with loss 1.810644507408142 and accuracy 0.1666666865348816.\n",
      "Test batch 240 with loss 1.8986022472381592 and accuracy 0.0555555559694767.\n",
      "Test batch 241 with loss 1.8881056308746338 and accuracy 0.0833333358168602.\n",
      "Test batch 242 with loss 1.7201118469238281 and accuracy 0.3027777671813965.\n",
      "Test batch 243 with loss 1.90676748752594 and accuracy 0.0486111119389534.\n",
      "Test batch 244 with loss 1.8713144063949585 and accuracy 0.0694444477558136.\n",
      "Test batch 245 with loss 1.7596986293792725 and accuracy 0.25.\n",
      "Test batch 246 with loss 1.7264610528945923 and accuracy 0.1388888955116272.\n",
      "Test batch 247 with loss 1.8921377658843994 and accuracy 0.0793650820851326.\n",
      "Test batch 248 with loss 1.851926565170288 and accuracy 0.08888889104127884.\n",
      "Test batch 249 with loss 1.7581474781036377 and accuracy 0.25555557012557983.\n",
      "Test batch 250 with loss 1.8525612354278564 and accuracy 0.0833333358168602.\n",
      "Test batch 251 with loss 1.8405393362045288 and accuracy 0.1111111119389534.\n",
      "Test batch 252 with loss 1.809075117111206 and accuracy 0.11666667461395264.\n",
      "Test batch 253 with loss 1.888559103012085 and accuracy 0.0833333358168602.\n",
      "Test batch 254 with loss 1.8835418224334717 and accuracy 0.0694444477558136.\n",
      "Test batch 255 with loss 2.0528383255004883 and accuracy 0.0.\n",
      "Test batch 256 with loss 1.8459246158599854 and accuracy 0.03333333507180214.\n",
      "Test batch 257 with loss 1.7979333400726318 and accuracy 0.1666666716337204.\n",
      "Test batch 258 with loss 1.8540761470794678 and accuracy 0.1111111119389534.\n",
      "Test batch 259 with loss 1.8335211277008057 and accuracy 0.10833333432674408.\n",
      "Test batch 260 with loss 1.814087152481079 and accuracy 0.2281745970249176.\n",
      "Test batch 261 with loss 1.7903974056243896 and accuracy 0.25.\n",
      "Test batch 262 with loss 1.7277555465698242 and accuracy 0.222222238779068.\n",
      "Test batch 263 with loss 1.9233442544937134 and accuracy 0.02777777798473835.\n",
      "Test batch 264 with loss 1.802817940711975 and accuracy 0.3055555522441864.\n",
      "Test batch 265 with loss 1.8541793823242188 and accuracy 0.07500000298023224.\n",
      "Test batch 266 with loss 1.858147382736206 and accuracy 0.0972222238779068.\n",
      "Test batch 267 with loss 1.8621981143951416 and accuracy 0.0972222238779068.\n",
      "Test batch 268 with loss 1.7908226251602173 and accuracy 0.2611111104488373.\n",
      "Test batch 269 with loss 1.764845848083496 and accuracy 0.2750000059604645.\n",
      "Test batch 270 with loss 1.7758653163909912 and accuracy 0.2222222238779068.\n",
      "Test batch 271 with loss 1.8805522918701172 and accuracy 0.02777777798473835.\n",
      "Test batch 272 with loss 1.801807165145874 and accuracy 0.29722222685813904.\n",
      "Test batch 273 with loss 1.8727633953094482 and accuracy 0.25.\n",
      "Test batch 274 with loss 1.9044818878173828 and accuracy 0.31309524178504944.\n",
      "Test batch 275 with loss 1.7421016693115234 and accuracy 0.23333334922790527.\n",
      "Test batch 276 with loss 1.810712218284607 and accuracy 0.25.\n",
      "Test batch 277 with loss 1.7804336547851562 and accuracy 0.22777777910232544.\n",
      "Test batch 278 with loss 1.8001747131347656 and accuracy 0.06111111491918564.\n",
      "Test batch 279 with loss 1.8444684743881226 and accuracy 0.0833333358168602.\n",
      "Test batch 280 with loss 1.7756831645965576 and accuracy 0.03333333507180214.\n",
      "Test batch 281 with loss 1.9096673727035522 and accuracy 0.0416666679084301.\n",
      "Test batch 282 with loss 1.8666515350341797 and accuracy 0.08888889104127884.\n",
      "Test batch 283 with loss 1.837317705154419 and accuracy 0.0972222238779068.\n",
      "Test batch 284 with loss 1.8963168859481812 and accuracy 0.3083333373069763.\n",
      "Test batch 285 with loss 1.8622335195541382 and accuracy 0.16388890147209167.\n",
      "Test batch 286 with loss 1.8444181680679321 and accuracy 0.0555555559694767.\n",
      "Test batch 287 with loss 1.7792714834213257 and accuracy 0.0833333358168602.\n",
      "Test batch 288 with loss 1.810398817062378 and accuracy 0.2083333432674408.\n",
      "Test batch 289 with loss 1.8611787557601929 and accuracy 0.0833333358168602.\n",
      "Test batch 290 with loss 1.812255620956421 and accuracy 0.09444444626569748.\n",
      "Test batch 291 with loss 1.9406135082244873 and accuracy 0.11666667461395264.\n",
      "Test batch 292 with loss 1.812029480934143 and accuracy 0.2611111104488373.\n",
      "Test batch 293 with loss 1.8786605596542358 and accuracy 0.2083333283662796.\n",
      "Test batch 294 with loss 1.8159799575805664 and accuracy 0.125.\n",
      "Test batch 295 with loss 1.7444041967391968 and accuracy 0.17222222685813904.\n",
      "Test batch 296 with loss 1.7861754894256592 and accuracy 0.2240740954875946.\n",
      "Test batch 297 with loss 1.7757511138916016 and accuracy 0.25.\n",
      "Test batch 298 with loss 1.8462769985198975 and accuracy 0.1388888955116272.\n",
      "Test batch 299 with loss 1.7741222381591797 and accuracy 0.1388888955116272.\n",
      "Test batch 300 with loss 1.7932236194610596 and accuracy 0.3055555522441864.\n",
      "Test batch 301 with loss 1.868137001991272 and accuracy 0.2222222238779068.\n",
      "Test batch 302 with loss 1.6670844554901123 and accuracy 0.38611114025115967.\n",
      "Test batch 303 with loss 1.7023121118545532 and accuracy 0.2222222238779068.\n",
      "Test batch 304 with loss 1.7889455556869507 and accuracy 0.17222222685813904.\n",
      "Test batch 305 with loss 1.8254804611206055 and accuracy 0.07500000298023224.\n",
      "Test batch 306 with loss 1.817487120628357 and accuracy 0.125.\n",
      "Test batch 307 with loss 1.811825156211853 and accuracy 0.13055555522441864.\n",
      "Test batch 308 with loss 1.8641763925552368 and accuracy 0.0555555559694767.\n",
      "Test batch 309 with loss 1.975545883178711 and accuracy 0.0.\n",
      "Test batch 310 with loss 1.7878427505493164 and accuracy 0.08888889104127884.\n",
      "Test batch 311 with loss 1.7396354675292969 and accuracy 0.15079365670681.\n",
      "Test batch 312 with loss 1.779616355895996 and accuracy 0.24166665971279144.\n",
      "Test batch 313 with loss 1.860041856765747 and accuracy 0.1388888955116272.\n",
      "Test batch 314 with loss 1.8252776861190796 and accuracy 0.11666666716337204.\n",
      "Test batch 315 with loss 1.771113395690918 and accuracy 0.24722221493721008.\n",
      "Test batch 316 with loss 1.8722589015960693 and accuracy 0.1388888955116272.\n",
      "Test batch 317 with loss 1.8873507976531982 and accuracy 0.125.\n",
      "Test batch 318 with loss 1.6907179355621338 and accuracy 0.347222238779068.\n",
      "Test batch 319 with loss 1.7997467517852783 and accuracy 0.1071428582072258.\n",
      "Test batch 320 with loss 1.8341251611709595 and accuracy 0.1388888955116272.\n",
      "Test batch 321 with loss 1.7901675701141357 and accuracy 0.11666666716337204.\n",
      "Test batch 322 with loss 1.7848390340805054 and accuracy 0.22499999403953552.\n",
      "Test batch 323 with loss 1.7620576620101929 and accuracy 0.4027777910232544.\n",
      "Test batch 324 with loss 1.7778364419937134 and accuracy 0.460317462682724.\n",
      "Test batch 325 with loss 1.7950592041015625 and accuracy 0.347222238779068.\n",
      "Test batch 326 with loss 1.7282953262329102 and accuracy 0.3948412537574768.\n",
      "Test batch 327 with loss 1.9217859506607056 and accuracy 0.0555555559694767.\n",
      "Test batch 328 with loss 1.727685570716858 and accuracy 0.1944444477558136.\n",
      "Test batch 329 with loss 1.8377149105072021 and accuracy 0.0833333358168602.\n",
      "Test batch 330 with loss 1.8299331665039062 and accuracy 0.11666667461395264.\n",
      "Test batch 331 with loss 1.8849868774414062 and accuracy 0.1666666716337204.\n",
      "Test batch 332 with loss 1.8954594135284424 and accuracy 0.0833333358168602.\n",
      "Test batch 333 with loss 1.778824806213379 and accuracy 0.25.\n",
      "Test batch 334 with loss 1.9544410705566406 and accuracy 0.0.\n",
      "Test batch 335 with loss 1.868627905845642 and accuracy 0.0555555559694767.\n",
      "Test batch 336 with loss 1.8803730010986328 and accuracy 0.1388888955116272.\n",
      "Test batch 337 with loss 1.7882448434829712 and accuracy 0.3083333373069763.\n",
      "Test batch 338 with loss 1.741421103477478 and accuracy 0.2638888955116272.\n",
      "Test batch 339 with loss 1.8116058111190796 and accuracy 0.2777777910232544.\n",
      "Test batch 340 with loss 1.7815027236938477 and accuracy 0.22333334386348724.\n",
      "Test batch 341 with loss 1.8427902460098267 and accuracy 0.0833333358168602.\n",
      "Test batch 342 with loss 1.852760672569275 and accuracy 0.125.\n",
      "Test batch 343 with loss 1.7836878299713135 and accuracy 0.3472222089767456.\n",
      "Test batch 344 with loss 1.7878198623657227 and accuracy 0.3333333432674408.\n",
      "Test batch 345 with loss 1.8882791996002197 and accuracy 0.125.\n",
      "Test batch 346 with loss 1.8407223224639893 and accuracy 0.05714286118745804.\n",
      "Test batch 347 with loss 1.7590062618255615 and accuracy 0.3055555522441864.\n",
      "Test batch 348 with loss 1.8148874044418335 and accuracy 0.03333333507180214.\n",
      "Test batch 349 with loss 1.7917420864105225 and accuracy 0.1805555522441864.\n",
      "Test batch 350 with loss 1.8081945180892944 and accuracy 0.25.\n",
      "Test batch 351 with loss 1.8579403162002563 and accuracy 0.1388888955116272.\n",
      "Test batch 352 with loss 1.9036000967025757 and accuracy 0.0833333358168602.\n",
      "Test batch 353 with loss 1.7886419296264648 and accuracy 0.17222222685813904.\n",
      "Test batch 354 with loss 1.8726584911346436 and accuracy 0.0972222238779068.\n",
      "Test batch 355 with loss 1.8505100011825562 and accuracy 0.1388888955116272.\n",
      "Test batch 356 with loss 1.768625259399414 and accuracy 0.25.\n",
      "Test batch 357 with loss 1.8398412466049194 and accuracy 0.1180555522441864.\n",
      "Test batch 358 with loss 1.8552753925323486 and accuracy 0.0833333358168602.\n",
      "Test batch 359 with loss 1.8945815563201904 and accuracy 0.0793650820851326.\n",
      "Test batch 360 with loss 1.8147404193878174 and accuracy 0.18611110746860504.\n",
      "Test batch 361 with loss 1.8517862558364868 and accuracy 0.2321428656578064.\n",
      "Test batch 362 with loss 1.9144871234893799 and accuracy 0.1666666716337204.\n",
      "Test batch 363 with loss 1.9347270727157593 and accuracy 0.1944444477558136.\n",
      "Test batch 364 with loss 1.7883485555648804 and accuracy 0.10833333432674408.\n",
      "Test batch 365 with loss 1.8487335443496704 and accuracy 0.02777777798473835.\n",
      "Test batch 366 with loss 1.8782600164413452 and accuracy 0.1071428582072258.\n",
      "Test batch 367 with loss 1.8923803567886353 and accuracy 0.15000000596046448.\n",
      "Test batch 368 with loss 1.8292694091796875 and accuracy 0.1111111119389534.\n",
      "Test batch 369 with loss 1.8564002513885498 and accuracy 0.06666667014360428.\n",
      "Test batch 370 with loss 1.8586397171020508 and accuracy 0.19166666269302368.\n",
      "Test batch 371 with loss 1.7719271183013916 and accuracy 0.31111112236976624.\n",
      "Test batch 372 with loss 1.8770233392715454 and accuracy 0.125.\n",
      "Test batch 373 with loss 1.654354453086853 and accuracy 0.1805555522441864.\n",
      "Test batch 374 with loss 1.8054742813110352 and accuracy 0.15833333134651184.\n",
      "Test batch 375 with loss 1.7770096063613892 and accuracy 0.18611112236976624.\n",
      "Test batch 376 with loss 1.8068716526031494 and accuracy 0.1944444477558136.\n",
      "Test batch 377 with loss 1.758283019065857 and accuracy 0.2638888955116272.\n",
      "Test batch 378 with loss 1.8378727436065674 and accuracy 0.24166667461395264.\n",
      "Test batch 379 with loss 1.8917443752288818 and accuracy 0.0555555559694767.\n",
      "Test batch 380 with loss 1.8568742275238037 and accuracy 0.1111111119389534.\n",
      "Test batch 381 with loss 1.9300661087036133 and accuracy 0.05714286118745804.\n",
      "Test batch 382 with loss 1.7524690628051758 and accuracy 0.25.\n",
      "Test batch 383 with loss 1.8441822528839111 and accuracy 0.0416666679084301.\n",
      "Test batch 384 with loss 1.9337736368179321 and accuracy 0.0555555559694767.\n",
      "Test batch 385 with loss 1.978630781173706 and accuracy 0.0555555559694767.\n",
      "Test batch 386 with loss 1.8704395294189453 and accuracy 0.1388888955116272.\n",
      "Test batch 387 with loss 1.7418005466461182 and accuracy 0.33888888359069824.\n",
      "Test batch 388 with loss 1.879572868347168 and accuracy 0.07500000298023224.\n",
      "Test batch 389 with loss 1.7998462915420532 and accuracy 0.1458333432674408.\n",
      "Test batch 390 with loss 1.8118658065795898 and accuracy 0.2083333432674408.\n",
      "Test batch 391 with loss 1.92681086063385 and accuracy 0.0555555559694767.\n",
      "Test batch 392 with loss 1.7601979970932007 and accuracy 0.2777777910232544.\n",
      "Test batch 393 with loss 1.7879064083099365 and accuracy 0.125.\n",
      "Test batch 394 with loss 1.8308000564575195 and accuracy 0.1071428582072258.\n",
      "Test batch 395 with loss 1.8195489645004272 and accuracy 0.144841268658638.\n",
      "Test batch 396 with loss 1.807375192642212 and accuracy 0.06111111491918564.\n",
      "Test batch 397 with loss 1.8421920537948608 and accuracy 0.08095238357782364.\n",
      "Test batch 398 with loss 1.8984193801879883 and accuracy 0.125.\n",
      "Test batch 399 with loss 1.817893624305725 and accuracy 0.1805555522441864.\n",
      "Test batch 400 with loss 1.8355792760849 and accuracy 0.4555555582046509.\n",
      "Test batch 401 with loss 1.805238962173462 and accuracy 0.0833333358168602.\n",
      "Test batch 402 with loss 1.783021330833435 and accuracy 0.0833333358168602.\n",
      "Test batch 403 with loss 1.802615761756897 and accuracy 0.0416666679084301.\n",
      "Test batch 404 with loss 1.9098129272460938 and accuracy 0.0833333358168602.\n",
      "Test batch 405 with loss 1.8108465671539307 and accuracy 0.15833333134651184.\n",
      "Test batch 406 with loss 1.6748450994491577 and accuracy 0.2666666805744171.\n",
      "Test batch 407 with loss 1.854841947555542 and accuracy 0.1666666716337204.\n",
      "Test batch 408 with loss 1.897425889968872 and accuracy 0.1666666716337204.\n",
      "Test batch 409 with loss 1.8364959955215454 and accuracy 0.3333333432674408.\n",
      "Test batch 410 with loss 1.7658326625823975 and accuracy 0.11666666716337204.\n",
      "Test batch 411 with loss 1.8123277425765991 and accuracy 0.2777777910232544.\n",
      "Test batch 412 with loss 1.8306471109390259 and accuracy 0.1944444477558136.\n",
      "Test batch 413 with loss 1.868237853050232 and accuracy 0.0.\n",
      "Test batch 414 with loss 1.771360993385315 and accuracy 0.2222222238779068.\n",
      "Test batch 415 with loss 1.7595551013946533 and accuracy 0.1388888955116272.\n",
      "Test batch 416 with loss 1.8592790365219116 and accuracy 0.125.\n",
      "Test batch 417 with loss 1.7591146230697632 and accuracy 0.3333333432674408.\n",
      "Test batch 418 with loss 1.7986526489257812 and accuracy 0.2222222238779068.\n",
      "Test batch 419 with loss 1.8274179697036743 and accuracy 0.07500000298023224.\n",
      "Test batch 420 with loss 1.7695083618164062 and accuracy 0.30158731341362.\n",
      "Test batch 421 with loss 1.7833389043807983 and accuracy 0.1527777761220932.\n",
      "Test batch 422 with loss 1.8070427179336548 and accuracy 0.17222222685813904.\n",
      "Test batch 423 with loss 1.9058319330215454 and accuracy 0.03333333507180214.\n",
      "Test batch 424 with loss 1.8999927043914795 and accuracy 0.0833333358168602.\n",
      "Test batch 425 with loss 1.7634865045547485 and accuracy 0.08888889104127884.\n",
      "Test batch 426 with loss 1.8496513366699219 and accuracy 0.0416666679084301.\n",
      "Test batch 427 with loss 1.8809916973114014 and accuracy 0.02777777798473835.\n",
      "Test batch 428 with loss 1.7773529291152954 and accuracy 0.1666666716337204.\n",
      "Test batch 429 with loss 1.9271700382232666 and accuracy 0.03333333507180214.\n",
      "Test batch 430 with loss 1.8442552089691162 and accuracy 0.26944443583488464.\n",
      "Test batch 431 with loss 1.8399569988250732 and accuracy 0.02777777798473835.\n",
      "Test batch 432 with loss 1.789564847946167 and accuracy 0.1666666716337204.\n",
      "Test batch 433 with loss 1.8351951837539673 and accuracy 0.11666667461395264.\n",
      "Test batch 434 with loss 1.876672387123108 and accuracy 0.19761905074119568.\n",
      "Test batch 435 with loss 1.7664566040039062 and accuracy 0.3055555820465088.\n",
      "Test batch 436 with loss 1.8837831020355225 and accuracy 0.0416666679084301.\n",
      "Test batch 437 with loss 1.8796882629394531 and accuracy 0.20000000298023224.\n",
      "Test batch 438 with loss 1.8180038928985596 and accuracy 0.15833333134651184.\n",
      "Test batch 439 with loss 1.792365312576294 and accuracy 0.10833333432674408.\n",
      "Test batch 440 with loss 1.8469432592391968 and accuracy 0.19166666269302368.\n",
      "Test batch 441 with loss 1.8397163152694702 and accuracy 0.08888889104127884.\n",
      "Test batch 442 with loss 1.8883785009384155 and accuracy 0.2777777910232544.\n",
      "Test batch 443 with loss 1.8426105976104736 and accuracy 0.16428571939468384.\n",
      "Test batch 444 with loss 1.6816942691802979 and accuracy 0.31111112236976624.\n",
      "Test batch 445 with loss 1.8519103527069092 and accuracy 0.08888889104127884.\n",
      "Test batch 446 with loss 1.8080084323883057 and accuracy 0.02777777798473835.\n",
      "Test batch 447 with loss 1.8626896142959595 and accuracy 0.03333333507180214.\n",
      "Test batch 448 with loss 1.804748296737671 and accuracy 0.1666666716337204.\n",
      "Test batch 449 with loss 1.8361132144927979 and accuracy 0.21111111342906952.\n",
      "Test batch 450 with loss 1.841979742050171 and accuracy 0.0.\n",
      "Test batch 451 with loss 1.872968077659607 and accuracy 0.03333333507180214.\n",
      "Test batch 452 with loss 1.8417291641235352 and accuracy 0.10000000894069672.\n",
      "Test batch 453 with loss 1.8317382335662842 and accuracy 0.16388888657093048.\n",
      "Test batch 454 with loss 1.7732925415039062 and accuracy 0.2460317462682724.\n",
      "Test batch 455 with loss 1.71530020236969 and accuracy 0.2611111104488373.\n",
      "Test batch 456 with loss 1.8748910427093506 and accuracy 0.125.\n",
      "Test batch 457 with loss 1.792506456375122 and accuracy 0.3769841492176056.\n",
      "Test batch 458 with loss 1.8029407262802124 and accuracy 0.1041666716337204.\n",
      "Test batch 459 with loss 1.8388290405273438 and accuracy 0.2460317611694336.\n",
      "Test batch 460 with loss 1.7655155658721924 and accuracy 0.25555557012557983.\n",
      "Test batch 461 with loss 1.8798840045928955 and accuracy 0.0555555559694767.\n",
      "Test batch 462 with loss 1.7898184061050415 and accuracy 0.1666666716337204.\n",
      "Test batch 463 with loss 1.8142178058624268 and accuracy 0.03333333507180214.\n",
      "Test batch 464 with loss 1.8222564458847046 and accuracy 0.10833333432674408.\n",
      "Test batch 465 with loss 1.7777683734893799 and accuracy 0.2222222238779068.\n",
      "Test batch 466 with loss 1.8611198663711548 and accuracy 0.10833333432674408.\n",
      "Test batch 467 with loss 1.8946834802627563 and accuracy 0.25.\n",
      "Test batch 468 with loss 1.713910698890686 and accuracy 0.21111111342906952.\n",
      "Test batch 469 with loss 1.9104175567626953 and accuracy 0.0416666679084301.\n",
      "Test batch 470 with loss 1.9074490070343018 and accuracy 0.0793650820851326.\n",
      "Test batch 471 with loss 1.8103485107421875 and accuracy 0.17777778208255768.\n",
      "Test batch 472 with loss 1.8697353601455688 and accuracy 0.13333334028720856.\n",
      "Test batch 473 with loss 1.718634009361267 and accuracy 0.31111112236976624.\n",
      "Test batch 474 with loss 1.8820343017578125 and accuracy 0.07500000298023224.\n",
      "Test batch 475 with loss 1.8347057104110718 and accuracy 0.125.\n",
      "Test batch 476 with loss 1.836073875427246 and accuracy 0.0892857164144516.\n",
      "Test batch 477 with loss 1.875846266746521 and accuracy 0.0972222238779068.\n",
      "Test batch 478 with loss 1.7418066263198853 and accuracy 0.2380952537059784.\n",
      "Test batch 479 with loss 1.911123514175415 and accuracy 0.0972222238779068.\n",
      "Test batch 480 with loss 1.9180148839950562 and accuracy 0.1666666716337204.\n",
      "Test batch 481 with loss 1.8131177425384521 and accuracy 0.2083333432674408.\n",
      "Test batch 482 with loss 1.9009244441986084 and accuracy 0.0.\n",
      "Test batch 483 with loss 1.8609154224395752 and accuracy 0.14444445073604584.\n",
      "Test batch 484 with loss 1.8106956481933594 and accuracy 0.1527777761220932.\n",
      "Test batch 485 with loss 1.8854076862335205 and accuracy 0.11666667461395264.\n",
      "Test batch 486 with loss 1.7983310222625732 and accuracy 0.12222222983837128.\n",
      "Test batch 487 with loss 1.8813002109527588 and accuracy 0.0833333358168602.\n",
      "Test batch 488 with loss 1.775818109512329 and accuracy 0.36666667461395264.\n",
      "Test batch 489 with loss 1.8785194158554077 and accuracy 0.0.\n",
      "Test batch 490 with loss 1.8794492483139038 and accuracy 0.0892857164144516.\n",
      "Test batch 491 with loss 1.7838464975357056 and accuracy 0.15555556118488312.\n",
      "Test batch 492 with loss 1.822607398033142 and accuracy 0.125.\n",
      "Test batch 493 with loss 1.840911865234375 and accuracy 0.0555555559694767.\n",
      "Test batch 494 with loss 1.8505585193634033 and accuracy 0.0833333358168602.\n",
      "Test batch 495 with loss 1.820136308670044 and accuracy 0.1805555522441864.\n",
      "Test batch 496 with loss 1.8090324401855469 and accuracy 0.14444445073604584.\n",
      "Test batch 497 with loss 1.8482506275177002 and accuracy 0.1666666716337204.\n",
      "Test batch 498 with loss 1.804656744003296 and accuracy 0.0833333358168602.\n",
      "Test batch 499 with loss 1.9050309658050537 and accuracy 0.0.\n",
      "Training epoch 8 batch 0 with loss 1.7390111684799194, accuracy 0.3444444537162781.\n",
      "Training epoch 8 batch 1 with loss 1.7773351669311523, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 2 with loss 1.8347969055175781, accuracy 0.12222222983837128.\n",
      "Training epoch 8 batch 3 with loss 1.7442442178726196, accuracy 0.32499998807907104.\n",
      "Training epoch 8 batch 4 with loss 1.7044563293457031, accuracy 0.2152777761220932.\n",
      "Training epoch 8 batch 5 with loss 1.7177293300628662, accuracy 0.21666668355464935.\n",
      "Training epoch 8 batch 6 with loss 1.7014678716659546, accuracy 0.3583333492279053.\n",
      "Training epoch 8 batch 7 with loss 1.8308366537094116, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 8 with loss 1.7096154689788818, accuracy 0.2916666567325592.\n",
      "Training epoch 8 batch 9 with loss 1.7527358531951904, accuracy 0.3611111044883728.\n",
      "Training epoch 8 batch 10 with loss 1.749706506729126, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 11 with loss 1.7875372171401978, accuracy 0.22380952537059784.\n",
      "Training epoch 8 batch 12 with loss 1.8385026454925537, accuracy 0.1805555671453476.\n",
      "Training epoch 8 batch 13 with loss 1.7651573419570923, accuracy 0.38650792837142944.\n",
      "Training epoch 8 batch 14 with loss 1.8305022716522217, accuracy 0.22777777910232544.\n",
      "Training epoch 8 batch 15 with loss 1.8097047805786133, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 16 with loss 1.763331413269043, accuracy 0.2916666567325592.\n",
      "Training epoch 8 batch 17 with loss 1.7953910827636719, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 18 with loss 1.749656081199646, accuracy 0.30000001192092896.\n",
      "Training epoch 8 batch 19 with loss 1.7625764608383179, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 20 with loss 1.7793195247650146, accuracy 0.3194444477558136.\n",
      "Training epoch 8 batch 21 with loss 1.7666237354278564, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 22 with loss 1.8019087314605713, accuracy 0.1726190447807312.\n",
      "Training epoch 8 batch 23 with loss 1.6984453201293945, accuracy 0.2916666865348816.\n",
      "Training epoch 8 batch 24 with loss 1.7837156057357788, accuracy 0.4027777910232544.\n",
      "Training epoch 8 batch 25 with loss 1.8331390619277954, accuracy 0.1071428582072258.\n",
      "Training epoch 8 batch 26 with loss 1.7750205993652344, accuracy 0.1825396865606308.\n",
      "Training epoch 8 batch 27 with loss 1.8170254230499268, accuracy 0.1547619104385376.\n",
      "Training epoch 8 batch 28 with loss 1.70333731174469, accuracy 0.2976190447807312.\n",
      "Training epoch 8 batch 29 with loss 1.6834726333618164, accuracy 0.277777761220932.\n",
      "Training epoch 8 batch 30 with loss 1.8249547481536865, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 31 with loss 1.9429690837860107, accuracy 0.02777777798473835.\n",
      "Training epoch 8 batch 32 with loss 1.7449382543563843, accuracy 0.3333333432674408.\n",
      "Training epoch 8 batch 33 with loss 1.7986888885498047, accuracy 0.23333334922790527.\n",
      "Training epoch 8 batch 34 with loss 1.7976467609405518, accuracy 0.3819444477558136.\n",
      "Training epoch 8 batch 35 with loss 1.8393030166625977, accuracy 0.19166666269302368.\n",
      "Training epoch 8 batch 36 with loss 1.8457987308502197, accuracy 0.10000000894069672.\n",
      "Training epoch 8 batch 37 with loss 1.839123010635376, accuracy 0.31111112236976624.\n",
      "Training epoch 8 batch 38 with loss 1.7588316202163696, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 39 with loss 1.8409007787704468, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 40 with loss 1.714585542678833, accuracy 0.28333333134651184.\n",
      "Training epoch 8 batch 41 with loss 1.5917011499404907, accuracy 0.2579365074634552.\n",
      "Training epoch 8 batch 42 with loss 1.8213834762573242, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 43 with loss 1.655591607093811, accuracy 0.44166669249534607.\n",
      "Training epoch 8 batch 44 with loss 1.7927719354629517, accuracy 0.19166666269302368.\n",
      "Training epoch 8 batch 45 with loss 1.6893479824066162, accuracy 0.3444444537162781.\n",
      "Training epoch 8 batch 46 with loss 1.762984275817871, accuracy 0.3611111044883728.\n",
      "Training epoch 8 batch 47 with loss 1.6998313665390015, accuracy 0.23333333432674408.\n",
      "Training epoch 8 batch 48 with loss 1.7848527431488037, accuracy 0.2666666805744171.\n",
      "Training epoch 8 batch 49 with loss 1.69451904296875, accuracy 0.2888889014720917.\n",
      "Training epoch 8 batch 50 with loss 1.7758941650390625, accuracy 0.144841268658638.\n",
      "Training epoch 8 batch 51 with loss 1.6916707754135132, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 52 with loss 1.8205225467681885, accuracy 0.11666667461395264.\n",
      "Training epoch 8 batch 53 with loss 1.7186739444732666, accuracy 0.18703705072402954.\n",
      "Training epoch 8 batch 54 with loss 1.7878313064575195, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 55 with loss 1.798744559288025, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 56 with loss 1.7396736145019531, accuracy 0.16388890147209167.\n",
      "Training epoch 8 batch 57 with loss 1.7508809566497803, accuracy 0.236111119389534.\n",
      "Training epoch 8 batch 58 with loss 1.7278738021850586, accuracy 0.2611111104488373.\n",
      "Training epoch 8 batch 59 with loss 1.8153846263885498, accuracy 0.25.\n",
      "Training epoch 8 batch 60 with loss 1.8002197742462158, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 61 with loss 1.6368560791015625, accuracy 0.2958333492279053.\n",
      "Training epoch 8 batch 62 with loss 1.854996681213379, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 63 with loss 1.7650972604751587, accuracy 0.18333333730697632.\n",
      "Training epoch 8 batch 64 with loss 1.7804021835327148, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 65 with loss 1.7634150981903076, accuracy 0.236111119389534.\n",
      "Training epoch 8 batch 66 with loss 1.7725054025650024, accuracy 0.2361111044883728.\n",
      "Training epoch 8 batch 67 with loss 1.7653672695159912, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 68 with loss 1.678558349609375, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 69 with loss 1.671696424484253, accuracy 0.43611112236976624.\n",
      "Training epoch 8 batch 70 with loss 1.7081657648086548, accuracy 0.3083333373069763.\n",
      "Training epoch 8 batch 71 with loss 1.8092384338378906, accuracy 0.2777777910232544.\n",
      "Training epoch 8 batch 72 with loss 1.7985200881958008, accuracy 0.14444445073604584.\n",
      "Training epoch 8 batch 73 with loss 1.7079105377197266, accuracy 0.3194444477558136.\n",
      "Training epoch 8 batch 74 with loss 1.6969177722930908, accuracy 0.1587301641702652.\n",
      "Training epoch 8 batch 75 with loss 1.8036400079727173, accuracy 0.1130952388048172.\n",
      "Training epoch 8 batch 76 with loss 1.8188235759735107, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 77 with loss 1.7586063146591187, accuracy 0.14166666567325592.\n",
      "Training epoch 8 batch 78 with loss 1.771230697631836, accuracy 0.39444443583488464.\n",
      "Training epoch 8 batch 79 with loss 1.7959768772125244, accuracy 0.12222222983837128.\n",
      "Training epoch 8 batch 80 with loss 1.743711233139038, accuracy 0.3194444477558136.\n",
      "Training epoch 8 batch 81 with loss 1.7963144779205322, accuracy 0.236111119389534.\n",
      "Training epoch 8 batch 82 with loss 1.8011096715927124, accuracy 0.1269841343164444.\n",
      "Training epoch 8 batch 83 with loss 1.796592354774475, accuracy 0.284722238779068.\n",
      "Training epoch 8 batch 84 with loss 1.9173206090927124, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 85 with loss 1.841521978378296, accuracy 0.03333333507180214.\n",
      "Training epoch 8 batch 86 with loss 1.7122089862823486, accuracy 0.29722222685813904.\n",
      "Training epoch 8 batch 87 with loss 1.7013847827911377, accuracy 0.11269841343164444.\n",
      "Training epoch 8 batch 88 with loss 1.7505019903182983, accuracy 0.38055554032325745.\n",
      "Training epoch 8 batch 89 with loss 1.628997802734375, accuracy 0.2269841432571411.\n",
      "Training epoch 8 batch 90 with loss 1.7334493398666382, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 91 with loss 1.8198760747909546, accuracy 0.03333333507180214.\n",
      "Training epoch 8 batch 92 with loss 1.7835556268692017, accuracy 0.23333334922790527.\n",
      "Training epoch 8 batch 93 with loss 1.661376953125, accuracy 0.4611110985279083.\n",
      "Training epoch 8 batch 94 with loss 1.7226927280426025, accuracy 0.28333333134651184.\n",
      "Training epoch 8 batch 95 with loss 1.7474696636199951, accuracy 0.21111111342906952.\n",
      "Training epoch 8 batch 96 with loss 1.681433081626892, accuracy 0.3611111044883728.\n",
      "Training epoch 8 batch 97 with loss 1.7379188537597656, accuracy 0.32499998807907104.\n",
      "Training epoch 8 batch 98 with loss 1.7543634176254272, accuracy 0.25.\n",
      "Training epoch 8 batch 99 with loss 1.725745439529419, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 100 with loss 1.697758436203003, accuracy 0.38809525966644287.\n",
      "Training epoch 8 batch 101 with loss 1.6680635213851929, accuracy 0.23148149251937866.\n",
      "Training epoch 8 batch 102 with loss 1.647705078125, accuracy 0.1964285671710968.\n",
      "Training epoch 8 batch 103 with loss 1.6966445446014404, accuracy 0.19166666269302368.\n",
      "Training epoch 8 batch 104 with loss 1.8209282159805298, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 105 with loss 1.822624921798706, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 106 with loss 1.629988431930542, accuracy 0.2916666865348816.\n",
      "Training epoch 8 batch 107 with loss 1.8150787353515625, accuracy 0.07870370149612427.\n",
      "Training epoch 8 batch 108 with loss 1.8547664880752563, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 109 with loss 1.7336876392364502, accuracy 0.12222222983837128.\n",
      "Training epoch 8 batch 110 with loss 1.824673056602478, accuracy 0.17499999701976776.\n",
      "Training epoch 8 batch 111 with loss 1.8729826211929321, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 112 with loss 1.6883834600448608, accuracy 0.39722225069999695.\n",
      "Training epoch 8 batch 113 with loss 1.797359824180603, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 114 with loss 1.8249444961547852, accuracy 0.0694444477558136.\n",
      "Training epoch 8 batch 115 with loss 1.6585843563079834, accuracy 0.347222238779068.\n",
      "Training epoch 8 batch 116 with loss 1.811768889427185, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 117 with loss 1.7163136005401611, accuracy 0.25555557012557983.\n",
      "Training epoch 8 batch 118 with loss 1.8686511516571045, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 119 with loss 1.7646872997283936, accuracy 0.18518519401550293.\n",
      "Training epoch 8 batch 120 with loss 1.8606818914413452, accuracy 0.125.\n",
      "Training epoch 8 batch 121 with loss 1.6340004205703735, accuracy 0.3361111283302307.\n",
      "Training epoch 8 batch 122 with loss 1.7335513830184937, accuracy 0.2658730149269104.\n",
      "Training epoch 8 batch 123 with loss 1.7903715372085571, accuracy 0.18888890743255615.\n",
      "Training epoch 8 batch 124 with loss 1.698608160018921, accuracy 0.21666666865348816.\n",
      "Training epoch 8 batch 125 with loss 1.785517692565918, accuracy 0.28333333134651184.\n",
      "Training epoch 8 batch 126 with loss 1.719925880432129, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 127 with loss 1.6711785793304443, accuracy 0.4444444477558136.\n",
      "Training epoch 8 batch 128 with loss 1.765263319015503, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 129 with loss 1.7463992834091187, accuracy 0.20595237612724304.\n",
      "Training epoch 8 batch 130 with loss 1.65622079372406, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 131 with loss 1.8067519664764404, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 132 with loss 1.795682668685913, accuracy 0.19166666269302368.\n",
      "Training epoch 8 batch 133 with loss 1.758252501487732, accuracy 0.15555556118488312.\n",
      "Training epoch 8 batch 134 with loss 1.788593053817749, accuracy 0.4444444179534912.\n",
      "Training epoch 8 batch 135 with loss 1.8444114923477173, accuracy 0.25.\n",
      "Training epoch 8 batch 136 with loss 1.7708221673965454, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 137 with loss 1.747710943222046, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 138 with loss 1.7435853481292725, accuracy 0.19166667759418488.\n",
      "Training epoch 8 batch 139 with loss 1.812523603439331, accuracy 0.02777777798473835.\n",
      "Training epoch 8 batch 140 with loss 1.7479438781738281, accuracy 0.3222222328186035.\n",
      "Training epoch 8 batch 141 with loss 1.8064429759979248, accuracy 0.3333333432674408.\n",
      "Training epoch 8 batch 142 with loss 1.7268857955932617, accuracy 0.25.\n",
      "Training epoch 8 batch 143 with loss 1.665649652481079, accuracy 0.28611111640930176.\n",
      "Training epoch 8 batch 144 with loss 1.6350510120391846, accuracy 0.3055555820465088.\n",
      "Training epoch 8 batch 145 with loss 1.7351605892181396, accuracy 0.31309524178504944.\n",
      "Training epoch 8 batch 146 with loss 1.697522521018982, accuracy 0.2083333432674408.\n",
      "Training epoch 8 batch 147 with loss 1.8328211307525635, accuracy 0.065476194024086.\n",
      "Training epoch 8 batch 148 with loss 1.8074262142181396, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 149 with loss 1.7052295207977295, accuracy 0.23888888955116272.\n",
      "Training epoch 8 batch 150 with loss 1.7293405532836914, accuracy 0.23888888955116272.\n",
      "Training epoch 8 batch 151 with loss 1.6958385705947876, accuracy 0.3472222089767456.\n",
      "Training epoch 8 batch 152 with loss 1.7167612314224243, accuracy 0.24444445967674255.\n",
      "Training epoch 8 batch 153 with loss 1.7208490371704102, accuracy 0.3444444239139557.\n",
      "Training epoch 8 batch 154 with loss 1.7800191640853882, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 155 with loss 1.79000985622406, accuracy 0.347222238779068.\n",
      "Training epoch 8 batch 156 with loss 1.7709537744522095, accuracy 0.06666667014360428.\n",
      "Training epoch 8 batch 157 with loss 1.8732073307037354, accuracy 0.17777778208255768.\n",
      "Training epoch 8 batch 158 with loss 1.8659372329711914, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 159 with loss 1.7686822414398193, accuracy 0.3333333432674408.\n",
      "Training epoch 8 batch 160 with loss 1.7550735473632812, accuracy 0.21666666865348816.\n",
      "Training epoch 8 batch 161 with loss 1.866590142250061, accuracy 0.11666667461395264.\n",
      "Training epoch 8 batch 162 with loss 1.7349650859832764, accuracy 0.2916666567325592.\n",
      "Training epoch 8 batch 163 with loss 1.8071186542510986, accuracy 0.3055555820465088.\n",
      "Training epoch 8 batch 164 with loss 1.7796894311904907, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 165 with loss 1.7902650833129883, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 166 with loss 1.8047584295272827, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 167 with loss 1.7663990259170532, accuracy 0.20000001788139343.\n",
      "Training epoch 8 batch 168 with loss 1.7824151515960693, accuracy 0.1488095223903656.\n",
      "Training epoch 8 batch 169 with loss 1.8199245929718018, accuracy 0.2420634925365448.\n",
      "Training epoch 8 batch 170 with loss 1.9011424779891968, accuracy 0.20000001788139343.\n",
      "Training epoch 8 batch 171 with loss 1.7923368215560913, accuracy 0.2083333432674408.\n",
      "Training epoch 8 batch 172 with loss 1.846173882484436, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 173 with loss 1.749176263809204, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 174 with loss 1.6939932107925415, accuracy 0.3305555582046509.\n",
      "Training epoch 8 batch 175 with loss 1.7268339395523071, accuracy 0.09444444626569748.\n",
      "Training epoch 8 batch 176 with loss 1.8016420602798462, accuracy 0.1031746044754982.\n",
      "Training epoch 8 batch 177 with loss 1.8028147220611572, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 178 with loss 1.7713792324066162, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 179 with loss 1.6952826976776123, accuracy 0.39444443583488464.\n",
      "Training epoch 8 batch 180 with loss 1.7970235347747803, accuracy 0.25.\n",
      "Training epoch 8 batch 181 with loss 1.713785171508789, accuracy 0.1865079402923584.\n",
      "Training epoch 8 batch 182 with loss 1.7661250829696655, accuracy 0.2611111104488373.\n",
      "Training epoch 8 batch 183 with loss 1.6816701889038086, accuracy 0.3861111104488373.\n",
      "Training epoch 8 batch 184 with loss 1.7104194164276123, accuracy 0.1587301641702652.\n",
      "Training epoch 8 batch 185 with loss 1.826755166053772, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 186 with loss 1.8562389612197876, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 187 with loss 1.646108627319336, accuracy 0.21388888359069824.\n",
      "Training epoch 8 batch 188 with loss 1.7470779418945312, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 189 with loss 1.7701371908187866, accuracy 0.17380952835083008.\n",
      "Training epoch 8 batch 190 with loss 1.8895612955093384, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 191 with loss 1.6826080083847046, accuracy 0.24166667461395264.\n",
      "Training epoch 8 batch 192 with loss 1.811313271522522, accuracy 0.06666667014360428.\n",
      "Training epoch 8 batch 193 with loss 1.7580864429473877, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 194 with loss 1.739974021911621, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 195 with loss 1.7496697902679443, accuracy 0.22499999403953552.\n",
      "Training epoch 8 batch 196 with loss 1.713230848312378, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 197 with loss 1.771338701248169, accuracy 0.2777777910232544.\n",
      "Training epoch 8 batch 198 with loss 1.7487417459487915, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 199 with loss 1.8028230667114258, accuracy 0.35277777910232544.\n",
      "Training epoch 8 batch 200 with loss 1.8715156316757202, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 201 with loss 1.8015406131744385, accuracy 0.31666669249534607.\n",
      "Training epoch 8 batch 202 with loss 1.6601829528808594, accuracy 0.1527777910232544.\n",
      "Training epoch 8 batch 203 with loss 1.849811315536499, accuracy 0.10000000894069672.\n",
      "Training epoch 8 batch 204 with loss 1.6952292919158936, accuracy 0.3333333432674408.\n",
      "Training epoch 8 batch 205 with loss 1.7886040210723877, accuracy 0.1736111044883728.\n",
      "Training epoch 8 batch 206 with loss 1.7335408926010132, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 207 with loss 1.7923412322998047, accuracy 0.130952388048172.\n",
      "Training epoch 8 batch 208 with loss 1.6825170516967773, accuracy 0.3214285671710968.\n",
      "Training epoch 8 batch 209 with loss 1.7341667413711548, accuracy 0.25.\n",
      "Training epoch 8 batch 210 with loss 1.8019304275512695, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 211 with loss 1.7910945415496826, accuracy 0.13055555522441864.\n",
      "Training epoch 8 batch 212 with loss 1.6410505771636963, accuracy 0.2825396955013275.\n",
      "Training epoch 8 batch 213 with loss 1.7336766719818115, accuracy 0.4166666865348816.\n",
      "Training epoch 8 batch 214 with loss 1.7007005214691162, accuracy 0.0793650820851326.\n",
      "Training epoch 8 batch 215 with loss 1.643307089805603, accuracy 0.44999998807907104.\n",
      "Training epoch 8 batch 216 with loss 1.7038156986236572, accuracy 0.30277779698371887.\n",
      "Training epoch 8 batch 217 with loss 1.650838851928711, accuracy 0.31666669249534607.\n",
      "Training epoch 8 batch 218 with loss 1.8269879817962646, accuracy 0.125.\n",
      "Training epoch 8 batch 219 with loss 1.8843729496002197, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 220 with loss 1.7633928060531616, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 221 with loss 1.7554689645767212, accuracy 0.222222238779068.\n",
      "Training epoch 8 batch 222 with loss 1.7804012298583984, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 223 with loss 1.672644853591919, accuracy 0.4277777671813965.\n",
      "Training epoch 8 batch 224 with loss 1.678881049156189, accuracy 0.3027777671813965.\n",
      "Training epoch 8 batch 225 with loss 1.7570854425430298, accuracy 0.14166668057441711.\n",
      "Training epoch 8 batch 226 with loss 1.7144110202789307, accuracy 0.28333333134651184.\n",
      "Training epoch 8 batch 227 with loss 1.6847543716430664, accuracy 0.2976190447807312.\n",
      "Training epoch 8 batch 228 with loss 1.8203223943710327, accuracy 0.14444445073604584.\n",
      "Training epoch 8 batch 229 with loss 1.8120133876800537, accuracy 0.3194444477558136.\n",
      "Training epoch 8 batch 230 with loss 1.874908447265625, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 231 with loss 1.7044252157211304, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 232 with loss 1.7432628870010376, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 233 with loss 1.8034566640853882, accuracy 0.24166667461395264.\n",
      "Training epoch 8 batch 234 with loss 1.7011350393295288, accuracy 0.2599206566810608.\n",
      "Training epoch 8 batch 235 with loss 1.8044145107269287, accuracy 0.21111111342906952.\n",
      "Training epoch 8 batch 236 with loss 1.757871389389038, accuracy 0.32499998807907104.\n",
      "Training epoch 8 batch 237 with loss 1.8319370746612549, accuracy 0.25.\n",
      "Training epoch 8 batch 238 with loss 1.906184196472168, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 239 with loss 1.803652048110962, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 240 with loss 1.8506046533584595, accuracy 0.13611111044883728.\n",
      "Training epoch 8 batch 241 with loss 1.714608907699585, accuracy 0.17777778208255768.\n",
      "Training epoch 8 batch 242 with loss 1.7316522598266602, accuracy 0.2083333432674408.\n",
      "Training epoch 8 batch 243 with loss 1.7625224590301514, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 244 with loss 1.6912177801132202, accuracy 0.4166666567325592.\n",
      "Training epoch 8 batch 245 with loss 1.7398834228515625, accuracy 0.3333333432674408.\n",
      "Training epoch 8 batch 246 with loss 1.7754148244857788, accuracy 0.0694444477558136.\n",
      "Training epoch 8 batch 247 with loss 1.7338577508926392, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 248 with loss 1.852243185043335, accuracy 0.2777777910232544.\n",
      "Training epoch 8 batch 249 with loss 1.741263747215271, accuracy 0.14603175222873688.\n",
      "Training epoch 8 batch 250 with loss 1.8140785694122314, accuracy 0.2666666805744171.\n",
      "Training epoch 8 batch 251 with loss 1.777161955833435, accuracy 0.2361111044883728.\n",
      "Training epoch 8 batch 252 with loss 1.7366021871566772, accuracy 0.23888888955116272.\n",
      "Training epoch 8 batch 253 with loss 1.7519009113311768, accuracy 0.21666666865348816.\n",
      "Training epoch 8 batch 254 with loss 1.7108032703399658, accuracy 0.24722224473953247.\n",
      "Training epoch 8 batch 255 with loss 1.8850160837173462, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 256 with loss 1.7936071157455444, accuracy 0.10000000149011612.\n",
      "Training epoch 8 batch 257 with loss 1.8646831512451172, accuracy 0.02777777798473835.\n",
      "Training epoch 8 batch 258 with loss 1.792946219444275, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 259 with loss 1.7204774618148804, accuracy 0.3958333134651184.\n",
      "Training epoch 8 batch 260 with loss 1.9153163433074951, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 261 with loss 1.7147451639175415, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 262 with loss 1.704993486404419, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 263 with loss 1.7409807443618774, accuracy 0.2888889014720917.\n",
      "Training epoch 8 batch 264 with loss 2.0081238746643066, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 265 with loss 1.7872676849365234, accuracy 0.10000000894069672.\n",
      "Training epoch 8 batch 266 with loss 1.6839864253997803, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 267 with loss 1.7110960483551025, accuracy 0.255952388048172.\n",
      "Training epoch 8 batch 268 with loss 1.7924697399139404, accuracy 0.24166667461395264.\n",
      "Training epoch 8 batch 269 with loss 1.8059669733047485, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 270 with loss 1.7755374908447266, accuracy 0.1458333283662796.\n",
      "Training epoch 8 batch 271 with loss 1.7194169759750366, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 272 with loss 1.8021392822265625, accuracy 0.12222222983837128.\n",
      "Training epoch 8 batch 273 with loss 1.704585075378418, accuracy 0.3583333492279053.\n",
      "Training epoch 8 batch 274 with loss 1.7233648300170898, accuracy 0.3305555582046509.\n",
      "Training epoch 8 batch 275 with loss 1.6855109930038452, accuracy 0.28333333134651184.\n",
      "Training epoch 8 batch 276 with loss 1.7506017684936523, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 277 with loss 1.7707605361938477, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 278 with loss 1.6925201416015625, accuracy 0.22777777910232544.\n",
      "Training epoch 8 batch 279 with loss 1.6821380853652954, accuracy 0.1587301641702652.\n",
      "Training epoch 8 batch 280 with loss 1.7305066585540771, accuracy 0.3611111044883728.\n",
      "Training epoch 8 batch 281 with loss 1.7458724975585938, accuracy 0.29722222685813904.\n",
      "Training epoch 8 batch 282 with loss 1.7434231042861938, accuracy 0.2281746119260788.\n",
      "Training epoch 8 batch 283 with loss 1.8047717809677124, accuracy 0.3166666626930237.\n",
      "Training epoch 8 batch 284 with loss 1.729804277420044, accuracy 0.347222238779068.\n",
      "Training epoch 8 batch 285 with loss 1.8627557754516602, accuracy 0.23333334922790527.\n",
      "Training epoch 8 batch 286 with loss 1.6815271377563477, accuracy 0.5.\n",
      "Training epoch 8 batch 287 with loss 1.8893165588378906, accuracy 0.190476194024086.\n",
      "Training epoch 8 batch 288 with loss 1.696229338645935, accuracy 0.39444443583488464.\n",
      "Training epoch 8 batch 289 with loss 1.731143593788147, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 290 with loss 1.7284059524536133, accuracy 0.2182539701461792.\n",
      "Training epoch 8 batch 291 with loss 1.820291519165039, accuracy 0.28333333134651184.\n",
      "Training epoch 8 batch 292 with loss 1.8120149374008179, accuracy 0.31111112236976624.\n",
      "Training epoch 8 batch 293 with loss 1.9114118814468384, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 294 with loss 1.7956888675689697, accuracy 0.14444445073604584.\n",
      "Training epoch 8 batch 295 with loss 1.7240350246429443, accuracy 0.38055557012557983.\n",
      "Training epoch 8 batch 296 with loss 1.8049885034561157, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 297 with loss 1.7919063568115234, accuracy 0.3194444477558136.\n",
      "Training epoch 8 batch 298 with loss 1.7768962383270264, accuracy 0.0793650820851326.\n",
      "Training epoch 8 batch 299 with loss 1.7507781982421875, accuracy 0.222222238779068.\n",
      "Training epoch 8 batch 300 with loss 1.7114919424057007, accuracy 0.2750000059604645.\n",
      "Training epoch 8 batch 301 with loss 1.7457176446914673, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 302 with loss 1.8362629413604736, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 303 with loss 1.7096898555755615, accuracy 0.3638889193534851.\n",
      "Training epoch 8 batch 304 with loss 1.801706075668335, accuracy 0.1071428656578064.\n",
      "Training epoch 8 batch 305 with loss 1.7694774866104126, accuracy 0.1805555671453476.\n",
      "Training epoch 8 batch 306 with loss 1.795061707496643, accuracy 0.15833334624767303.\n",
      "Training epoch 8 batch 307 with loss 1.844759225845337, accuracy 0.06111111491918564.\n",
      "Training epoch 8 batch 308 with loss 1.7028690576553345, accuracy 0.4027777910232544.\n",
      "Training epoch 8 batch 309 with loss 1.8854706287384033, accuracy 0.11666667461395264.\n",
      "Training epoch 8 batch 310 with loss 1.8024574518203735, accuracy 0.25.\n",
      "Training epoch 8 batch 311 with loss 1.734130620956421, accuracy 0.4972222149372101.\n",
      "Training epoch 8 batch 312 with loss 1.7057396173477173, accuracy 0.3222222328186035.\n",
      "Training epoch 8 batch 313 with loss 1.8200231790542603, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 314 with loss 1.834240198135376, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 315 with loss 1.7286031246185303, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 316 with loss 1.720245361328125, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 317 with loss 1.892626166343689, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 318 with loss 1.6865003108978271, accuracy 0.1458333432674408.\n",
      "Training epoch 8 batch 319 with loss 1.746766448020935, accuracy 0.3194444477558136.\n",
      "Training epoch 8 batch 320 with loss 1.683622121810913, accuracy 0.4555555582046509.\n",
      "Training epoch 8 batch 321 with loss 1.7962344884872437, accuracy 0.15833334624767303.\n",
      "Training epoch 8 batch 322 with loss 1.6450579166412354, accuracy 0.4186507761478424.\n",
      "Training epoch 8 batch 323 with loss 1.8595411777496338, accuracy 0.1805555671453476.\n",
      "Training epoch 8 batch 324 with loss 1.7382217645645142, accuracy 0.1875.\n",
      "Training epoch 8 batch 325 with loss 1.7460769414901733, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 326 with loss 1.7278400659561157, accuracy 0.1865079402923584.\n",
      "Training epoch 8 batch 327 with loss 1.724369764328003, accuracy 0.15833333134651184.\n",
      "Training epoch 8 batch 328 with loss 1.7553472518920898, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 329 with loss 1.8920109272003174, accuracy 0.1130952388048172.\n",
      "Training epoch 8 batch 330 with loss 1.8167930841445923, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 331 with loss 1.9261701107025146, accuracy 0.11666667461395264.\n",
      "Training epoch 8 batch 332 with loss 1.899892807006836, accuracy 0.10277777910232544.\n",
      "Training epoch 8 batch 333 with loss 1.7240135669708252, accuracy 0.4138889014720917.\n",
      "Training epoch 8 batch 334 with loss 1.7502577304840088, accuracy 0.125.\n",
      "Training epoch 8 batch 335 with loss 1.8357837200164795, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 336 with loss 1.8245538473129272, accuracy 0.1041666716337204.\n",
      "Training epoch 8 batch 337 with loss 1.809374213218689, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 338 with loss 1.760144829750061, accuracy 0.11666667461395264.\n",
      "Training epoch 8 batch 339 with loss 1.7558622360229492, accuracy 0.3541666865348816.\n",
      "Training epoch 8 batch 340 with loss 1.8375749588012695, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 341 with loss 1.9253826141357422, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 342 with loss 1.7519127130508423, accuracy 0.2777777910232544.\n",
      "Training epoch 8 batch 343 with loss 1.8631538152694702, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 344 with loss 1.7068617343902588, accuracy 0.22499999403953552.\n",
      "Training epoch 8 batch 345 with loss 1.763733148574829, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 346 with loss 1.8284454345703125, accuracy 0.13055555522441864.\n",
      "Training epoch 8 batch 347 with loss 1.8718923330307007, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 348 with loss 1.7393611669540405, accuracy 0.2777777910232544.\n",
      "Training epoch 8 batch 349 with loss 1.8314075469970703, accuracy 0.125.\n",
      "Training epoch 8 batch 350 with loss 1.7786976099014282, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 351 with loss 1.7968171834945679, accuracy 0.2182539701461792.\n",
      "Training epoch 8 batch 352 with loss 1.6416717767715454, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 353 with loss 1.735083818435669, accuracy 0.43611112236976624.\n",
      "Training epoch 8 batch 354 with loss 1.6907613277435303, accuracy 0.1458333432674408.\n",
      "Training epoch 8 batch 355 with loss 1.7939519882202148, accuracy 0.11666667461395264.\n",
      "Training epoch 8 batch 356 with loss 1.7476170063018799, accuracy 0.2361111044883728.\n",
      "Training epoch 8 batch 357 with loss 1.8779605627059937, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 358 with loss 1.8334176540374756, accuracy 0.0763888880610466.\n",
      "Training epoch 8 batch 359 with loss 1.7165768146514893, accuracy 0.2111111283302307.\n",
      "Training epoch 8 batch 360 with loss 1.8331435918807983, accuracy 0.1349206417798996.\n",
      "Training epoch 8 batch 361 with loss 1.7901222705841064, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 362 with loss 1.7015140056610107, accuracy 0.2658730149269104.\n",
      "Training epoch 8 batch 363 with loss 1.7288051843643188, accuracy 0.2666666805744171.\n",
      "Training epoch 8 batch 364 with loss 1.719952940940857, accuracy 0.28333333134651184.\n",
      "Training epoch 8 batch 365 with loss 1.8072001934051514, accuracy 0.2420634925365448.\n",
      "Training epoch 8 batch 366 with loss 1.7966740131378174, accuracy 0.13055555522441864.\n",
      "Training epoch 8 batch 367 with loss 1.7769927978515625, accuracy 0.2777777910232544.\n",
      "Training epoch 8 batch 368 with loss 1.8878755569458008, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 369 with loss 1.7485849857330322, accuracy 0.39444443583488464.\n",
      "Training epoch 8 batch 370 with loss 1.7455999851226807, accuracy 0.0793650820851326.\n",
      "Training epoch 8 batch 371 with loss 1.722678542137146, accuracy 0.335317462682724.\n",
      "Training epoch 8 batch 372 with loss 1.8010728359222412, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 373 with loss 1.748419165611267, accuracy 0.16388888657093048.\n",
      "Training epoch 8 batch 374 with loss 1.733704924583435, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 375 with loss 1.7139368057250977, accuracy 0.25555557012557983.\n",
      "Training epoch 8 batch 376 with loss 1.7363271713256836, accuracy 0.16388890147209167.\n",
      "Training epoch 8 batch 377 with loss 1.74590265750885, accuracy 0.1785714328289032.\n",
      "Training epoch 8 batch 378 with loss 1.7746953964233398, accuracy 0.0.\n",
      "Training epoch 8 batch 379 with loss 1.7138246297836304, accuracy 0.26944446563720703.\n",
      "Training epoch 8 batch 380 with loss 1.8136783838272095, accuracy 0.22380952537059784.\n",
      "Training epoch 8 batch 381 with loss 1.7822281122207642, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 382 with loss 1.8620643615722656, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 383 with loss 1.7054001092910767, accuracy 0.2242063581943512.\n",
      "Training epoch 8 batch 384 with loss 1.7111968994140625, accuracy 0.2361111044883728.\n",
      "Training epoch 8 batch 385 with loss 1.752661943435669, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 386 with loss 1.7150949239730835, accuracy 0.13333334028720856.\n",
      "Training epoch 8 batch 387 with loss 1.791933298110962, accuracy 0.05714286118745804.\n",
      "Training epoch 8 batch 388 with loss 1.670606017112732, accuracy 0.30317461490631104.\n",
      "Training epoch 8 batch 389 with loss 1.7220245599746704, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 390 with loss 1.8383203744888306, accuracy 0.14444445073604584.\n",
      "Training epoch 8 batch 391 with loss 1.7529523372650146, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 392 with loss 1.6751527786254883, accuracy 0.4583333432674408.\n",
      "Training epoch 8 batch 393 with loss 1.829432725906372, accuracy 0.03333333507180214.\n",
      "Training epoch 8 batch 394 with loss 1.7292006015777588, accuracy 0.3888888955116272.\n",
      "Training epoch 8 batch 395 with loss 1.8438827991485596, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 396 with loss 1.746038794517517, accuracy 0.25555557012557983.\n",
      "Training epoch 8 batch 397 with loss 1.7992961406707764, accuracy 0.23888888955116272.\n",
      "Training epoch 8 batch 398 with loss 1.7523390054702759, accuracy 0.3583333194255829.\n",
      "Training epoch 8 batch 399 with loss 1.72299325466156, accuracy 0.19722223281860352.\n",
      "Training epoch 8 batch 400 with loss 1.7881425619125366, accuracy 0.17658731341362.\n",
      "Training epoch 8 batch 401 with loss 1.7599585056304932, accuracy 0.2666666507720947.\n",
      "Training epoch 8 batch 402 with loss 1.7835050821304321, accuracy 0.24074074625968933.\n",
      "Training epoch 8 batch 403 with loss 1.7314519882202148, accuracy 0.15833333134651184.\n",
      "Training epoch 8 batch 404 with loss 1.7471389770507812, accuracy 0.125.\n",
      "Training epoch 8 batch 405 with loss 1.7776250839233398, accuracy 0.06666667014360428.\n",
      "Training epoch 8 batch 406 with loss 1.8471927642822266, accuracy 0.25.\n",
      "Training epoch 8 batch 407 with loss 1.7180774211883545, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 408 with loss 1.7913726568222046, accuracy 0.2698412835597992.\n",
      "Training epoch 8 batch 409 with loss 1.8181203603744507, accuracy 0.16388890147209167.\n",
      "Training epoch 8 batch 410 with loss 1.6958577632904053, accuracy 0.4138889014720917.\n",
      "Training epoch 8 batch 411 with loss 1.8667455911636353, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 412 with loss 1.7950687408447266, accuracy 0.03333333507180214.\n",
      "Training epoch 8 batch 413 with loss 1.6961227655410767, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 414 with loss 1.805858850479126, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 415 with loss 1.7738144397735596, accuracy 0.095238097012043.\n",
      "Training epoch 8 batch 416 with loss 1.746720552444458, accuracy 0.3583333492279053.\n",
      "Training epoch 8 batch 417 with loss 1.7810920476913452, accuracy 0.13055555522441864.\n",
      "Training epoch 8 batch 418 with loss 1.8415952920913696, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 419 with loss 1.8586584329605103, accuracy 0.20000000298023224.\n",
      "Training epoch 8 batch 420 with loss 1.7317126989364624, accuracy 0.2750000059604645.\n",
      "Training epoch 8 batch 421 with loss 1.9278978109359741, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 422 with loss 1.7913545370101929, accuracy 0.17777778208255768.\n",
      "Training epoch 8 batch 423 with loss 1.8204017877578735, accuracy 0.125.\n",
      "Training epoch 8 batch 424 with loss 1.704450011253357, accuracy 0.31666669249534607.\n",
      "Training epoch 8 batch 425 with loss 1.5678619146347046, accuracy 0.32499998807907104.\n",
      "Training epoch 8 batch 426 with loss 1.7995191812515259, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 427 with loss 1.8177162408828735, accuracy 0.1805555671453476.\n",
      "Training epoch 8 batch 428 with loss 1.8196980953216553, accuracy 0.125.\n",
      "Training epoch 8 batch 429 with loss 1.9307419061660767, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 430 with loss 1.7291978597640991, accuracy 0.3055555820465088.\n",
      "Training epoch 8 batch 431 with loss 1.7021358013153076, accuracy 0.2750000059604645.\n",
      "Training epoch 8 batch 432 with loss 1.9077939987182617, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 433 with loss 1.6773436069488525, accuracy 0.2916666567325592.\n",
      "Training epoch 8 batch 434 with loss 1.6999998092651367, accuracy 0.26944443583488464.\n",
      "Training epoch 8 batch 435 with loss 1.8413788080215454, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 436 with loss 1.7832973003387451, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 437 with loss 1.7055854797363281, accuracy 0.31111112236976624.\n",
      "Training epoch 8 batch 438 with loss 1.9023128747940063, accuracy 0.14444445073604584.\n",
      "Training epoch 8 batch 439 with loss 1.770716905593872, accuracy 0.3444444537162781.\n",
      "Training epoch 8 batch 440 with loss 1.7685054540634155, accuracy 0.23333334922790527.\n",
      "Training epoch 8 batch 441 with loss 1.7221978902816772, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 442 with loss 1.8332087993621826, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 443 with loss 1.7397727966308594, accuracy 0.26944446563720703.\n",
      "Training epoch 8 batch 444 with loss 1.6836923360824585, accuracy 0.32499998807907104.\n",
      "Training epoch 8 batch 445 with loss 1.6510047912597656, accuracy 0.2666666805744171.\n",
      "Training epoch 8 batch 446 with loss 1.7690479755401611, accuracy 0.17777778208255768.\n",
      "Training epoch 8 batch 447 with loss 1.7924606800079346, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 448 with loss 1.8029346466064453, accuracy 0.125.\n",
      "Training epoch 8 batch 449 with loss 1.8320544958114624, accuracy 0.20555555820465088.\n",
      "Training epoch 8 batch 450 with loss 1.7888660430908203, accuracy 0.065476194024086.\n",
      "Training epoch 8 batch 451 with loss 1.7707092761993408, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 452 with loss 1.8207305669784546, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 453 with loss 1.6646919250488281, accuracy 0.18611110746860504.\n",
      "Training epoch 8 batch 454 with loss 1.6826012134552002, accuracy 0.4611111283302307.\n",
      "Training epoch 8 batch 455 with loss 1.9064067602157593, accuracy 0.2876984179019928.\n",
      "Training epoch 8 batch 456 with loss 1.8604209423065186, accuracy 0.15555556118488312.\n",
      "Training epoch 8 batch 457 with loss 1.6981303691864014, accuracy 0.18611112236976624.\n",
      "Training epoch 8 batch 458 with loss 1.7823957204818726, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 459 with loss 1.6613996028900146, accuracy 0.25.\n",
      "Training epoch 8 batch 460 with loss 1.8233163356781006, accuracy 0.25.\n",
      "Training epoch 8 batch 461 with loss 1.7894197702407837, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 462 with loss 1.7530616521835327, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 463 with loss 1.8984870910644531, accuracy 0.03333333507180214.\n",
      "Training epoch 8 batch 464 with loss 1.9054346084594727, accuracy 0.18333333730697632.\n",
      "Training epoch 8 batch 465 with loss 1.7682243585586548, accuracy 0.2361111044883728.\n",
      "Training epoch 8 batch 466 with loss 1.8038837909698486, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 467 with loss 1.756482481956482, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 468 with loss 1.7839040756225586, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 469 with loss 1.827087163925171, accuracy 0.25555557012557983.\n",
      "Training epoch 8 batch 470 with loss 1.7116973400115967, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 471 with loss 1.7729313373565674, accuracy 0.02777777798473835.\n",
      "Training epoch 8 batch 472 with loss 1.7824503183364868, accuracy 0.3305555582046509.\n",
      "Training epoch 8 batch 473 with loss 1.6913011074066162, accuracy 0.11666667461395264.\n",
      "Training epoch 8 batch 474 with loss 1.7698848247528076, accuracy 0.204365074634552.\n",
      "Training epoch 8 batch 475 with loss 1.774614930152893, accuracy 0.2420634925365448.\n",
      "Training epoch 8 batch 476 with loss 1.885337233543396, accuracy 0.2250000238418579.\n",
      "Training epoch 8 batch 477 with loss 1.7374509572982788, accuracy 0.25555554032325745.\n",
      "Training epoch 8 batch 478 with loss 1.7394649982452393, accuracy 0.2003968358039856.\n",
      "Training epoch 8 batch 479 with loss 1.7775253057479858, accuracy 0.21666666865348816.\n",
      "Training epoch 8 batch 480 with loss 1.8775031566619873, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 481 with loss 1.876593828201294, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 482 with loss 1.745735764503479, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 483 with loss 1.761491060256958, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 484 with loss 1.7705421447753906, accuracy 0.23148147761821747.\n",
      "Training epoch 8 batch 485 with loss 1.7860819101333618, accuracy 0.2888889014720917.\n",
      "Training epoch 8 batch 486 with loss 1.8017494678497314, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 487 with loss 1.7540231943130493, accuracy 0.4416666626930237.\n",
      "Training epoch 8 batch 488 with loss 1.7798248529434204, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 489 with loss 1.8356244564056396, accuracy 0.21150794625282288.\n",
      "Training epoch 8 batch 490 with loss 1.6882911920547485, accuracy 0.24722221493721008.\n",
      "Training epoch 8 batch 491 with loss 1.7795454263687134, accuracy 0.144841268658638.\n",
      "Training epoch 8 batch 492 with loss 1.7018373012542725, accuracy 0.125.\n",
      "Training epoch 8 batch 493 with loss 1.8434709310531616, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 494 with loss 1.7522014379501343, accuracy 0.236111119389534.\n",
      "Training epoch 8 batch 495 with loss 1.8629486560821533, accuracy 0.2361111044883728.\n",
      "Training epoch 8 batch 496 with loss 1.8163223266601562, accuracy 0.1726190447807312.\n",
      "Training epoch 8 batch 497 with loss 1.7060474157333374, accuracy 0.14444445073604584.\n",
      "Training epoch 8 batch 498 with loss 1.7425100803375244, accuracy 0.2420634925365448.\n",
      "Training epoch 8 batch 499 with loss 1.7832720279693604, accuracy 0.125.\n",
      "Training epoch 8 batch 500 with loss 1.839487075805664, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 501 with loss 1.791487455368042, accuracy 0.17777778208255768.\n",
      "Training epoch 8 batch 502 with loss 1.7364311218261719, accuracy 0.2519841194152832.\n",
      "Training epoch 8 batch 503 with loss 1.8335466384887695, accuracy 0.47777777910232544.\n",
      "Training epoch 8 batch 504 with loss 1.7865787744522095, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 505 with loss 1.7870734930038452, accuracy 0.12777778506278992.\n",
      "Training epoch 8 batch 506 with loss 1.8293908834457397, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 507 with loss 1.6850378513336182, accuracy 0.21944443881511688.\n",
      "Training epoch 8 batch 508 with loss 1.7278096675872803, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 509 with loss 1.8157031536102295, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 510 with loss 1.8672958612442017, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 511 with loss 1.7724634408950806, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 512 with loss 1.8536720275878906, accuracy 0.204365074634552.\n",
      "Training epoch 8 batch 513 with loss 1.776506781578064, accuracy 0.22777777910232544.\n",
      "Training epoch 8 batch 514 with loss 1.7465429306030273, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 515 with loss 1.7785488367080688, accuracy 0.11666667461395264.\n",
      "Training epoch 8 batch 516 with loss 1.858359932899475, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 517 with loss 1.8590753078460693, accuracy 0.1805555671453476.\n",
      "Training epoch 8 batch 518 with loss 1.7903610467910767, accuracy 0.15833333134651184.\n",
      "Training epoch 8 batch 519 with loss 1.7612793445587158, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 520 with loss 1.8103605508804321, accuracy 0.14444445073604584.\n",
      "Training epoch 8 batch 521 with loss 1.8077799081802368, accuracy 0.190476194024086.\n",
      "Training epoch 8 batch 522 with loss 1.7521730661392212, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 523 with loss 1.7890516519546509, accuracy 0.15555556118488312.\n",
      "Training epoch 8 batch 524 with loss 1.8046996593475342, accuracy 0.18611112236976624.\n",
      "Training epoch 8 batch 525 with loss 1.7485567331314087, accuracy 0.14444445073604584.\n",
      "Training epoch 8 batch 526 with loss 1.8413282632827759, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 527 with loss 1.8021459579467773, accuracy 0.1587301641702652.\n",
      "Training epoch 8 batch 528 with loss 1.6627044677734375, accuracy 0.36666667461395264.\n",
      "Training epoch 8 batch 529 with loss 1.7566674947738647, accuracy 0.3075396716594696.\n",
      "Training epoch 8 batch 530 with loss 1.743945837020874, accuracy 0.3888888955116272.\n",
      "Training epoch 8 batch 531 with loss 1.71799635887146, accuracy 0.3126984238624573.\n",
      "Training epoch 8 batch 532 with loss 1.800092339515686, accuracy 0.3392857313156128.\n",
      "Training epoch 8 batch 533 with loss 1.8553094863891602, accuracy 0.0.\n",
      "Training epoch 8 batch 534 with loss 1.7328529357910156, accuracy 0.17777778208255768.\n",
      "Training epoch 8 batch 535 with loss 1.8106517791748047, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 536 with loss 1.7992922067642212, accuracy 0.1269841343164444.\n",
      "Training epoch 8 batch 537 with loss 1.8482030630111694, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 538 with loss 1.7003774642944336, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 539 with loss 1.8000574111938477, accuracy 0.22777777910232544.\n",
      "Training epoch 8 batch 540 with loss 1.6441243886947632, accuracy 0.5666666626930237.\n",
      "Training epoch 8 batch 541 with loss 1.7360833883285522, accuracy 0.4444444179534912.\n",
      "Training epoch 8 batch 542 with loss 1.7981767654418945, accuracy 0.2777777910232544.\n",
      "Training epoch 8 batch 543 with loss 1.8075942993164062, accuracy 0.02083333395421505.\n",
      "Training epoch 8 batch 544 with loss 1.869938850402832, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 545 with loss 1.8523502349853516, accuracy 0.10277777910232544.\n",
      "Training epoch 8 batch 546 with loss 1.8107421398162842, accuracy 0.2380952537059784.\n",
      "Training epoch 8 batch 547 with loss 1.7508666515350342, accuracy 0.2420634925365448.\n",
      "Training epoch 8 batch 548 with loss 1.7965381145477295, accuracy 0.06666667014360428.\n",
      "Training epoch 8 batch 549 with loss 1.8026615381240845, accuracy 0.13055555522441864.\n",
      "Training epoch 8 batch 550 with loss 1.8327754735946655, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 551 with loss 1.7533804178237915, accuracy 0.05714286118745804.\n",
      "Training epoch 8 batch 552 with loss 1.7611682415008545, accuracy 0.29722222685813904.\n",
      "Training epoch 8 batch 553 with loss 1.6973655223846436, accuracy 0.45000001788139343.\n",
      "Training epoch 8 batch 554 with loss 1.671860694885254, accuracy 0.19761905074119568.\n",
      "Training epoch 8 batch 555 with loss 1.8139457702636719, accuracy 0.2083333432674408.\n",
      "Training epoch 8 batch 556 with loss 1.864831566810608, accuracy 0.20555555820465088.\n",
      "Training epoch 8 batch 557 with loss 1.7880475521087646, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 558 with loss 1.7264999151229858, accuracy 0.2888889014720917.\n",
      "Training epoch 8 batch 559 with loss 1.8850187063217163, accuracy 0.15833333134651184.\n",
      "Training epoch 8 batch 560 with loss 1.8107486963272095, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 561 with loss 1.87703537940979, accuracy 0.0763888880610466.\n",
      "Training epoch 8 batch 562 with loss 1.7733519077301025, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 563 with loss 1.6734075546264648, accuracy 0.3222222328186035.\n",
      "Training epoch 8 batch 564 with loss 1.7044751644134521, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 565 with loss 1.7637193202972412, accuracy 0.4055555760860443.\n",
      "Training epoch 8 batch 566 with loss 1.8418071269989014, accuracy 0.0793650820851326.\n",
      "Training epoch 8 batch 567 with loss 1.889878273010254, accuracy 0.236111119389534.\n",
      "Training epoch 8 batch 568 with loss 1.7912311553955078, accuracy 0.26944446563720703.\n",
      "Training epoch 8 batch 569 with loss 1.7691240310668945, accuracy 0.14444445073604584.\n",
      "Training epoch 8 batch 570 with loss 1.7948347330093384, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 571 with loss 1.802016019821167, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 572 with loss 1.8093140125274658, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 573 with loss 1.8747174739837646, accuracy 0.0793650820851326.\n",
      "Training epoch 8 batch 574 with loss 1.69254469871521, accuracy 0.347222238779068.\n",
      "Training epoch 8 batch 575 with loss 1.7139205932617188, accuracy 0.3402777910232544.\n",
      "Training epoch 8 batch 576 with loss 1.8112688064575195, accuracy 0.13055555522441864.\n",
      "Training epoch 8 batch 577 with loss 1.7795137166976929, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 578 with loss 1.7296594381332397, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 579 with loss 1.7701650857925415, accuracy 0.26944446563720703.\n",
      "Training epoch 8 batch 580 with loss 1.8519052267074585, accuracy 0.06666667014360428.\n",
      "Training epoch 8 batch 581 with loss 1.7983033657073975, accuracy 0.2083333432674408.\n",
      "Training epoch 8 batch 582 with loss 1.9082549810409546, accuracy 0.12222222983837128.\n",
      "Training epoch 8 batch 583 with loss 1.6914355754852295, accuracy 0.23888888955116272.\n",
      "Training epoch 8 batch 584 with loss 1.7949559688568115, accuracy 0.1865079402923584.\n",
      "Training epoch 8 batch 585 with loss 1.7671358585357666, accuracy 0.2380952388048172.\n",
      "Training epoch 8 batch 586 with loss 1.6737680435180664, accuracy 0.45000001788139343.\n",
      "Training epoch 8 batch 587 with loss 1.7375028133392334, accuracy 0.236111119389534.\n",
      "Training epoch 8 batch 588 with loss 1.7453464269638062, accuracy 0.2111111283302307.\n",
      "Training epoch 8 batch 589 with loss 1.8078616857528687, accuracy 0.14166668057441711.\n",
      "Training epoch 8 batch 590 with loss 1.777593970298767, accuracy 0.5277777910232544.\n",
      "Training epoch 8 batch 591 with loss 1.7643636465072632, accuracy 0.329365074634552.\n",
      "Training epoch 8 batch 592 with loss 1.8370202779769897, accuracy 0.02777777798473835.\n",
      "Training epoch 8 batch 593 with loss 1.7653634548187256, accuracy 0.1587301641702652.\n",
      "Training epoch 8 batch 594 with loss 1.7898982763290405, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 595 with loss 1.7947250604629517, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 596 with loss 1.8889204263687134, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 597 with loss 1.8946316242218018, accuracy 0.2083333283662796.\n",
      "Training epoch 8 batch 598 with loss 1.7335455417633057, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 599 with loss 1.779545545578003, accuracy 0.2611111104488373.\n",
      "Training epoch 8 batch 600 with loss 1.8643471002578735, accuracy 0.12222222983837128.\n",
      "Training epoch 8 batch 601 with loss 1.8690683841705322, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 602 with loss 1.7883226871490479, accuracy 0.15555556118488312.\n",
      "Training epoch 8 batch 603 with loss 1.8025099039077759, accuracy 0.24166667461395264.\n",
      "Training epoch 8 batch 604 with loss 1.7337309122085571, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 605 with loss 1.8290798664093018, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 606 with loss 1.808140754699707, accuracy 0.33888888359069824.\n",
      "Training epoch 8 batch 607 with loss 1.7896264791488647, accuracy 0.3083333373069763.\n",
      "Training epoch 8 batch 608 with loss 1.8475936651229858, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 609 with loss 1.875730276107788, accuracy 0.0793650820851326.\n",
      "Training epoch 8 batch 610 with loss 1.7267301082611084, accuracy 0.32777777314186096.\n",
      "Training epoch 8 batch 611 with loss 1.8593944311141968, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 612 with loss 1.749937653541565, accuracy 0.17777778208255768.\n",
      "Training epoch 8 batch 613 with loss 1.7779802083969116, accuracy 0.09444444626569748.\n",
      "Training epoch 8 batch 614 with loss 1.8014005422592163, accuracy 0.2750000059604645.\n",
      "Training epoch 8 batch 615 with loss 1.7382131814956665, accuracy 0.222222238779068.\n",
      "Training epoch 8 batch 616 with loss 1.8189483880996704, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 617 with loss 1.825353980064392, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 618 with loss 1.7996175289154053, accuracy 0.20555555820465088.\n",
      "Training epoch 8 batch 619 with loss 1.7190637588500977, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 620 with loss 1.8445537090301514, accuracy 0.065476194024086.\n",
      "Training epoch 8 batch 621 with loss 1.7064281702041626, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 622 with loss 1.8351271152496338, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 623 with loss 1.7933591604232788, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 624 with loss 1.8198730945587158, accuracy 0.0793650820851326.\n",
      "Training epoch 8 batch 625 with loss 1.8406219482421875, accuracy 0.24166667461395264.\n",
      "Training epoch 8 batch 626 with loss 1.890385389328003, accuracy 0.03333333507180214.\n",
      "Training epoch 8 batch 627 with loss 1.798675298690796, accuracy 0.15833334624767303.\n",
      "Training epoch 8 batch 628 with loss 1.7733163833618164, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 629 with loss 1.7917191982269287, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 630 with loss 1.8822028636932373, accuracy 0.2708333432674408.\n",
      "Training epoch 8 batch 631 with loss 1.7819817066192627, accuracy 0.21388888359069824.\n",
      "Training epoch 8 batch 632 with loss 1.8067563772201538, accuracy 0.16388890147209167.\n",
      "Training epoch 8 batch 633 with loss 1.8646070957183838, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 634 with loss 1.7585655450820923, accuracy 0.1964285671710968.\n",
      "Training epoch 8 batch 635 with loss 1.9487802982330322, accuracy 0.125.\n",
      "Training epoch 8 batch 636 with loss 1.7358394861221313, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 637 with loss 1.8696540594100952, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 638 with loss 1.8357006311416626, accuracy 0.2658730149269104.\n",
      "Training epoch 8 batch 639 with loss 1.6718666553497314, accuracy 0.4000000059604645.\n",
      "Training epoch 8 batch 640 with loss 1.7881189584732056, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 641 with loss 1.6955829858779907, accuracy 0.2750000059604645.\n",
      "Training epoch 8 batch 642 with loss 1.79543137550354, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 643 with loss 1.9211540222167969, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 644 with loss 1.7352615594863892, accuracy 0.23333334922790527.\n",
      "Training epoch 8 batch 645 with loss 1.718043327331543, accuracy 0.19166666269302368.\n",
      "Training epoch 8 batch 646 with loss 1.7505548000335693, accuracy 0.25.\n",
      "Training epoch 8 batch 647 with loss 1.7408866882324219, accuracy 0.2361111044883728.\n",
      "Training epoch 8 batch 648 with loss 1.8589375019073486, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 649 with loss 1.7628612518310547, accuracy 0.22500000894069672.\n",
      "Training epoch 8 batch 650 with loss 1.7857112884521484, accuracy 0.3444444537162781.\n",
      "Training epoch 8 batch 651 with loss 1.9412113428115845, accuracy 0.19166666269302368.\n",
      "Training epoch 8 batch 652 with loss 1.7664680480957031, accuracy 0.13055555522441864.\n",
      "Training epoch 8 batch 653 with loss 1.81195068359375, accuracy 0.15833333134651184.\n",
      "Training epoch 8 batch 654 with loss 1.7306913137435913, accuracy 0.15555556118488312.\n",
      "Training epoch 8 batch 655 with loss 1.8457998037338257, accuracy 0.3791666626930237.\n",
      "Training epoch 8 batch 656 with loss 1.7132011651992798, accuracy 0.2083333432674408.\n",
      "Training epoch 8 batch 657 with loss 1.8661901950836182, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 658 with loss 1.8631412982940674, accuracy 0.25.\n",
      "Training epoch 8 batch 659 with loss 1.770572304725647, accuracy 0.222222238779068.\n",
      "Training epoch 8 batch 660 with loss 1.7299197912216187, accuracy 0.1587301641702652.\n",
      "Training epoch 8 batch 661 with loss 1.780281662940979, accuracy 0.20555555820465088.\n",
      "Training epoch 8 batch 662 with loss 1.796344518661499, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 663 with loss 1.8782762289047241, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 664 with loss 1.8440465927124023, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 665 with loss 1.9390161037445068, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 666 with loss 1.845094084739685, accuracy 0.2777777910232544.\n",
      "Training epoch 8 batch 667 with loss 1.8186842203140259, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 668 with loss 1.7465009689331055, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 669 with loss 1.8079723119735718, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 670 with loss 1.7967160940170288, accuracy 0.21388889849185944.\n",
      "Training epoch 8 batch 671 with loss 1.8248618841171265, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 672 with loss 1.8856786489486694, accuracy 0.0476190485060215.\n",
      "Training epoch 8 batch 673 with loss 1.764993667602539, accuracy 0.2599206268787384.\n",
      "Training epoch 8 batch 674 with loss 1.7269573211669922, accuracy 0.37222224473953247.\n",
      "Training epoch 8 batch 675 with loss 1.7623399496078491, accuracy 0.2849206328392029.\n",
      "Training epoch 8 batch 676 with loss 1.7977250814437866, accuracy 0.13055555522441864.\n",
      "Training epoch 8 batch 677 with loss 1.7069677114486694, accuracy 0.2182539701461792.\n",
      "Training epoch 8 batch 678 with loss 1.8334099054336548, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 679 with loss 1.763601303100586, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 680 with loss 1.6870334148406982, accuracy 0.375.\n",
      "Training epoch 8 batch 681 with loss 1.7949435710906982, accuracy 0.23333333432674408.\n",
      "Training epoch 8 batch 682 with loss 1.7881091833114624, accuracy 0.3888888955116272.\n",
      "Training epoch 8 batch 683 with loss 1.6938766241073608, accuracy 0.22500000894069672.\n",
      "Training epoch 8 batch 684 with loss 1.8618704080581665, accuracy 0.25.\n",
      "Training epoch 8 batch 685 with loss 1.760270357131958, accuracy 0.3611111342906952.\n",
      "Training epoch 8 batch 686 with loss 1.765535593032837, accuracy 0.16825397312641144.\n",
      "Training epoch 8 batch 687 with loss 1.7939651012420654, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 688 with loss 1.781825065612793, accuracy 0.10277777910232544.\n",
      "Training epoch 8 batch 689 with loss 1.8232835531234741, accuracy 0.06666667014360428.\n",
      "Training epoch 8 batch 690 with loss 1.7150195837020874, accuracy 0.37222224473953247.\n",
      "Training epoch 8 batch 691 with loss 1.75287663936615, accuracy 0.4404761791229248.\n",
      "Training epoch 8 batch 692 with loss 1.7072408199310303, accuracy 0.6015872955322266.\n",
      "Training epoch 8 batch 693 with loss 1.7044029235839844, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 694 with loss 1.7831720113754272, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 695 with loss 1.74270498752594, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 696 with loss 1.7948009967803955, accuracy 0.2142857313156128.\n",
      "Training epoch 8 batch 697 with loss 1.7287429571151733, accuracy 0.3444444537162781.\n",
      "Training epoch 8 batch 698 with loss 1.7987512350082397, accuracy 0.2420634925365448.\n",
      "Training epoch 8 batch 699 with loss 1.8302600383758545, accuracy 0.18611110746860504.\n",
      "Training epoch 8 batch 700 with loss 1.7303043603897095, accuracy 0.3333333432674408.\n",
      "Training epoch 8 batch 701 with loss 1.9158416986465454, accuracy 0.1527777910232544.\n",
      "Training epoch 8 batch 702 with loss 1.8441047668457031, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 703 with loss 1.7686121463775635, accuracy 0.24722224473953247.\n",
      "Training epoch 8 batch 704 with loss 1.9183721542358398, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 705 with loss 1.7953611612319946, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 706 with loss 1.7084672451019287, accuracy 0.3444444537162781.\n",
      "Training epoch 8 batch 707 with loss 1.7770040035247803, accuracy 0.2083333432674408.\n",
      "Training epoch 8 batch 708 with loss 1.7672456502914429, accuracy 0.3861111104488373.\n",
      "Training epoch 8 batch 709 with loss 1.8329938650131226, accuracy 0.125.\n",
      "Training epoch 8 batch 710 with loss 1.7330005168914795, accuracy 0.24722221493721008.\n",
      "Training epoch 8 batch 711 with loss 1.8008668422698975, accuracy 0.06666667014360428.\n",
      "Training epoch 8 batch 712 with loss 1.6798219680786133, accuracy 0.2966666519641876.\n",
      "Training epoch 8 batch 713 with loss 1.7551803588867188, accuracy 0.3611111044883728.\n",
      "Training epoch 8 batch 714 with loss 1.7386188507080078, accuracy 0.20000001788139343.\n",
      "Training epoch 8 batch 715 with loss 1.7763290405273438, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 716 with loss 1.8267772197723389, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 717 with loss 1.76386296749115, accuracy 0.3194444477558136.\n",
      "Training epoch 8 batch 718 with loss 1.799863576889038, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 719 with loss 1.7670224905014038, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 720 with loss 1.7974345684051514, accuracy 0.23333334922790527.\n",
      "Training epoch 8 batch 721 with loss 1.8836119174957275, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 722 with loss 1.6706264019012451, accuracy 0.444444477558136.\n",
      "Training epoch 8 batch 723 with loss 1.6569830179214478, accuracy 0.25.\n",
      "Training epoch 8 batch 724 with loss 1.8633571863174438, accuracy 0.0714285746216774.\n",
      "Training epoch 8 batch 725 with loss 1.862221360206604, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 726 with loss 1.8229587078094482, accuracy 0.25555557012557983.\n",
      "Training epoch 8 batch 727 with loss 1.6919610500335693, accuracy 0.4694444537162781.\n",
      "Training epoch 8 batch 728 with loss 1.7539052963256836, accuracy 0.2361111044883728.\n",
      "Training epoch 8 batch 729 with loss 1.8754558563232422, accuracy 0.2321428656578064.\n",
      "Training epoch 8 batch 730 with loss 1.848572015762329, accuracy 0.17777778208255768.\n",
      "Training epoch 8 batch 731 with loss 1.6856985092163086, accuracy 0.20555555820465088.\n",
      "Training epoch 8 batch 732 with loss 1.7483949661254883, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 733 with loss 1.78799307346344, accuracy 0.17777778208255768.\n",
      "Training epoch 8 batch 734 with loss 1.8149211406707764, accuracy 0.1349206417798996.\n",
      "Training epoch 8 batch 735 with loss 1.8045819997787476, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 736 with loss 1.6957728862762451, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 737 with loss 1.776342749595642, accuracy 0.125.\n",
      "Training epoch 8 batch 738 with loss 1.7389024496078491, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 739 with loss 1.8690106868743896, accuracy 0.16388888657093048.\n",
      "Training epoch 8 batch 740 with loss 1.7239768505096436, accuracy 0.4222222566604614.\n",
      "Training epoch 8 batch 741 with loss 1.7079728841781616, accuracy 0.3888888955116272.\n",
      "Training epoch 8 batch 742 with loss 1.837947130203247, accuracy 0.12222222983837128.\n",
      "Training epoch 8 batch 743 with loss 1.6985321044921875, accuracy 0.25.\n",
      "Training epoch 8 batch 744 with loss 1.8005704879760742, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 745 with loss 1.8288930654525757, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 746 with loss 1.9204908609390259, accuracy 0.14000000059604645.\n",
      "Training epoch 8 batch 747 with loss 1.870044469833374, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 748 with loss 1.8542648553848267, accuracy 0.21388888359069824.\n",
      "Training epoch 8 batch 749 with loss 1.737778663635254, accuracy 0.432539701461792.\n",
      "Training epoch 8 batch 750 with loss 1.8230810165405273, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 751 with loss 1.8947219848632812, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 752 with loss 1.8391361236572266, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 753 with loss 1.9133665561676025, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 754 with loss 1.8348915576934814, accuracy 0.125.\n",
      "Training epoch 8 batch 755 with loss 1.779362440109253, accuracy 0.222222238779068.\n",
      "Training epoch 8 batch 756 with loss 1.8792426586151123, accuracy 0.05416666716337204.\n",
      "Training epoch 8 batch 757 with loss 1.7544959783554077, accuracy 0.2083333432674408.\n",
      "Training epoch 8 batch 758 with loss 1.730656385421753, accuracy 0.18611110746860504.\n",
      "Training epoch 8 batch 759 with loss 1.76448655128479, accuracy 0.2738095223903656.\n",
      "Training epoch 8 batch 760 with loss 1.8659368753433228, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 761 with loss 1.7024688720703125, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 762 with loss 1.8501478433609009, accuracy 0.03703703731298447.\n",
      "Training epoch 8 batch 763 with loss 1.7708101272583008, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 764 with loss 1.749699354171753, accuracy 0.0763888880610466.\n",
      "Training epoch 8 batch 765 with loss 1.8486862182617188, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 766 with loss 1.7073980569839478, accuracy 0.27222222089767456.\n",
      "Training epoch 8 batch 767 with loss 1.837379813194275, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 768 with loss 1.7861210107803345, accuracy 0.16388888657093048.\n",
      "Training epoch 8 batch 769 with loss 1.7267019748687744, accuracy 0.125.\n",
      "Training epoch 8 batch 770 with loss 1.7257922887802124, accuracy 0.3888888955116272.\n",
      "Training epoch 8 batch 771 with loss 1.7954998016357422, accuracy 0.3583333492279053.\n",
      "Training epoch 8 batch 772 with loss 1.8817039728164673, accuracy 0.125.\n",
      "Training epoch 8 batch 773 with loss 1.7031707763671875, accuracy 0.21388888359069824.\n",
      "Training epoch 8 batch 774 with loss 1.8012373447418213, accuracy 0.1527777910232544.\n",
      "Training epoch 8 batch 775 with loss 1.918722152709961, accuracy 0.22777777910232544.\n",
      "Training epoch 8 batch 776 with loss 1.868720293045044, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 777 with loss 1.7585117816925049, accuracy 0.2777777910232544.\n",
      "Training epoch 8 batch 778 with loss 1.7929611206054688, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 779 with loss 1.733686089515686, accuracy 0.24166667461395264.\n",
      "Training epoch 8 batch 780 with loss 1.7975257635116577, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 781 with loss 1.7701337337493896, accuracy 0.10277777910232544.\n",
      "Training epoch 8 batch 782 with loss 1.7717781066894531, accuracy 0.13333334028720856.\n",
      "Training epoch 8 batch 783 with loss 1.7297732830047607, accuracy 0.4277777671813965.\n",
      "Training epoch 8 batch 784 with loss 1.7893226146697998, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 785 with loss 1.7295598983764648, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 786 with loss 1.7612804174423218, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 787 with loss 1.747086763381958, accuracy 0.0694444477558136.\n",
      "Training epoch 8 batch 788 with loss 1.8151895999908447, accuracy 0.17777778208255768.\n",
      "Training epoch 8 batch 789 with loss 1.7871055603027344, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 790 with loss 1.8463748693466187, accuracy 0.2777777910232544.\n",
      "Training epoch 8 batch 791 with loss 1.7154104709625244, accuracy 0.21523809432983398.\n",
      "Training epoch 8 batch 792 with loss 1.6371856927871704, accuracy 0.2896825671195984.\n",
      "Training epoch 8 batch 793 with loss 1.7621688842773438, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 794 with loss 1.7876068353652954, accuracy 0.2111111283302307.\n",
      "Training epoch 8 batch 795 with loss 1.7723318338394165, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 796 with loss 1.7997667789459229, accuracy 0.26944443583488464.\n",
      "Training epoch 8 batch 797 with loss 1.798784852027893, accuracy 0.12222222983837128.\n",
      "Training epoch 8 batch 798 with loss 1.8196296691894531, accuracy 0.210317462682724.\n",
      "Training epoch 8 batch 799 with loss 1.8967679738998413, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 800 with loss 1.7645072937011719, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 801 with loss 1.7305065393447876, accuracy 0.236111119389534.\n",
      "Training epoch 8 batch 802 with loss 1.7974555492401123, accuracy 0.3194444477558136.\n",
      "Training epoch 8 batch 803 with loss 1.754858374595642, accuracy 0.20555555820465088.\n",
      "Training epoch 8 batch 804 with loss 1.7638393640518188, accuracy 0.25.\n",
      "Training epoch 8 batch 805 with loss 1.7856476306915283, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 806 with loss 1.8941529989242554, accuracy 0.222222238779068.\n",
      "Training epoch 8 batch 807 with loss 1.7127033472061157, accuracy 0.4000000059604645.\n",
      "Training epoch 8 batch 808 with loss 1.7071113586425781, accuracy 0.25555557012557983.\n",
      "Training epoch 8 batch 809 with loss 1.6668074131011963, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 810 with loss 1.8291279077529907, accuracy 0.20555555820465088.\n",
      "Training epoch 8 batch 811 with loss 1.8178701400756836, accuracy 0.12222222983837128.\n",
      "Training epoch 8 batch 812 with loss 1.7373058795928955, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 813 with loss 1.8662563562393188, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 814 with loss 1.8525707721710205, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 815 with loss 1.8450206518173218, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 816 with loss 1.807938575744629, accuracy 0.02777777798473835.\n",
      "Training epoch 8 batch 817 with loss 1.7276500463485718, accuracy 0.2361111044883728.\n",
      "Training epoch 8 batch 818 with loss 1.7331304550170898, accuracy 0.3166666626930237.\n",
      "Training epoch 8 batch 819 with loss 1.8373820781707764, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 820 with loss 1.7858482599258423, accuracy 0.3392857015132904.\n",
      "Training epoch 8 batch 821 with loss 1.752699613571167, accuracy 0.2916666567325592.\n",
      "Training epoch 8 batch 822 with loss 1.825718879699707, accuracy 0.25.\n",
      "Training epoch 8 batch 823 with loss 1.7561252117156982, accuracy 0.2626984119415283.\n",
      "Training epoch 8 batch 824 with loss 1.801754355430603, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 825 with loss 1.8758739233016968, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 826 with loss 1.8175169229507446, accuracy 0.2611111104488373.\n",
      "Training epoch 8 batch 827 with loss 1.8352779150009155, accuracy 0.25555557012557983.\n",
      "Training epoch 8 batch 828 with loss 1.7050062417984009, accuracy 0.347222238779068.\n",
      "Training epoch 8 batch 829 with loss 1.7992464303970337, accuracy 0.19166666269302368.\n",
      "Training epoch 8 batch 830 with loss 1.780544638633728, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 831 with loss 1.8587124347686768, accuracy 0.06111111491918564.\n",
      "Training epoch 8 batch 832 with loss 1.7736942768096924, accuracy 0.20000000298023224.\n",
      "Training epoch 8 batch 833 with loss 1.7662353515625, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 834 with loss 1.6681146621704102, accuracy 0.40000003576278687.\n",
      "Training epoch 8 batch 835 with loss 1.9312412738800049, accuracy 0.125.\n",
      "Training epoch 8 batch 836 with loss 1.8723037242889404, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 837 with loss 1.8429739475250244, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 838 with loss 1.7543973922729492, accuracy 0.13055555522441864.\n",
      "Training epoch 8 batch 839 with loss 1.8818591833114624, accuracy 0.03333333507180214.\n",
      "Training epoch 8 batch 840 with loss 1.8957884311676025, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 841 with loss 1.8878049850463867, accuracy 0.03333333507180214.\n",
      "Training epoch 8 batch 842 with loss 1.8370329141616821, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 843 with loss 1.7687793970108032, accuracy 0.3027777671813965.\n",
      "Training epoch 8 batch 844 with loss 1.638573408126831, accuracy 0.3055555820465088.\n",
      "Training epoch 8 batch 845 with loss 1.8615024089813232, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 846 with loss 1.7681995630264282, accuracy 0.1875000149011612.\n",
      "Training epoch 8 batch 847 with loss 1.829980492591858, accuracy 0.2916666567325592.\n",
      "Training epoch 8 batch 848 with loss 1.802752137184143, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 849 with loss 1.8762906789779663, accuracy 0.02380952425301075.\n",
      "Training epoch 8 batch 850 with loss 1.8449604511260986, accuracy 0.09259259700775146.\n",
      "Training epoch 8 batch 851 with loss 1.781063437461853, accuracy 0.1547619104385376.\n",
      "Training epoch 8 batch 852 with loss 1.8795278072357178, accuracy 0.14444445073604584.\n",
      "Training epoch 8 batch 853 with loss 1.7957818508148193, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 854 with loss 1.7861328125, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 855 with loss 1.7062060832977295, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 856 with loss 1.7991482019424438, accuracy 0.20555555820465088.\n",
      "Training epoch 8 batch 857 with loss 1.8373171091079712, accuracy 0.18333333730697632.\n",
      "Training epoch 8 batch 858 with loss 1.68312668800354, accuracy 0.4861111342906952.\n",
      "Training epoch 8 batch 859 with loss 1.9048839807510376, accuracy 0.14603175222873688.\n",
      "Training epoch 8 batch 860 with loss 1.7206649780273438, accuracy 0.2976190745830536.\n",
      "Training epoch 8 batch 861 with loss 1.7679437398910522, accuracy 0.0476190485060215.\n",
      "Training epoch 8 batch 862 with loss 1.7385082244873047, accuracy 0.13650794327259064.\n",
      "Training epoch 8 batch 863 with loss 1.7445892095565796, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 864 with loss 1.891265630722046, accuracy 0.2083333432674408.\n",
      "Training epoch 8 batch 865 with loss 1.706865668296814, accuracy 0.25555557012557983.\n",
      "Training epoch 8 batch 866 with loss 1.7856385707855225, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 867 with loss 1.8506929874420166, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 868 with loss 1.8282263278961182, accuracy 0.472222238779068.\n",
      "Training epoch 8 batch 869 with loss 1.9436779022216797, accuracy 0.0.\n",
      "Training epoch 8 batch 870 with loss 1.780453085899353, accuracy 0.23333334922790527.\n",
      "Training epoch 8 batch 871 with loss 1.8445930480957031, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 872 with loss 1.7857402563095093, accuracy 0.0694444477558136.\n",
      "Training epoch 8 batch 873 with loss 1.7701889276504517, accuracy 0.20555555820465088.\n",
      "Training epoch 8 batch 874 with loss 1.827075719833374, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 875 with loss 1.7240667343139648, accuracy 0.1785714328289032.\n",
      "Training epoch 8 batch 876 with loss 1.8002309799194336, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 877 with loss 1.7345659732818604, accuracy 0.210317462682724.\n",
      "Training epoch 8 batch 878 with loss 1.7321497201919556, accuracy 0.2361111044883728.\n",
      "Training epoch 8 batch 879 with loss 1.7926117181777954, accuracy 0.06666667014360428.\n",
      "Training epoch 8 batch 880 with loss 1.786260962486267, accuracy 0.125.\n",
      "Training epoch 8 batch 881 with loss 1.8043111562728882, accuracy 0.3166666626930237.\n",
      "Training epoch 8 batch 882 with loss 1.7801614999771118, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 883 with loss 1.6550394296646118, accuracy 0.22777777910232544.\n",
      "Training epoch 8 batch 884 with loss 1.7891204357147217, accuracy 0.11666667461395264.\n",
      "Training epoch 8 batch 885 with loss 1.7923084497451782, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 886 with loss 1.9482234716415405, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 887 with loss 1.8022695779800415, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 888 with loss 1.7636947631835938, accuracy 0.1805555671453476.\n",
      "Training epoch 8 batch 889 with loss 1.7309458255767822, accuracy 0.18611110746860504.\n",
      "Training epoch 8 batch 890 with loss 1.8008817434310913, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 891 with loss 1.7191085815429688, accuracy 0.25.\n",
      "Training epoch 8 batch 892 with loss 1.7601318359375, accuracy 0.20555555820465088.\n",
      "Training epoch 8 batch 893 with loss 1.7627944946289062, accuracy 0.40833333134651184.\n",
      "Training epoch 8 batch 894 with loss 1.7705551385879517, accuracy 0.1488095223903656.\n",
      "Training epoch 8 batch 895 with loss 1.8778088092803955, accuracy 0.06666667014360428.\n",
      "Training epoch 8 batch 896 with loss 1.8221800327301025, accuracy 0.1597222238779068.\n",
      "Training epoch 8 batch 897 with loss 1.7099273204803467, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 898 with loss 1.845383644104004, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 899 with loss 1.60825514793396, accuracy 0.37222224473953247.\n",
      "Training epoch 8 batch 900 with loss 1.927821159362793, accuracy 0.0694444477558136.\n",
      "Training epoch 8 batch 901 with loss 1.6816049814224243, accuracy 0.19761905074119568.\n",
      "Training epoch 8 batch 902 with loss 1.7247403860092163, accuracy 0.4583333730697632.\n",
      "Training epoch 8 batch 903 with loss 1.8317883014678955, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 904 with loss 1.8423187732696533, accuracy 0.20000000298023224.\n",
      "Training epoch 8 batch 905 with loss 1.8124334812164307, accuracy 0.1626984179019928.\n",
      "Training epoch 8 batch 906 with loss 1.8296788930892944, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 907 with loss 1.683892011642456, accuracy 0.3166666626930237.\n",
      "Training epoch 8 batch 908 with loss 1.686457633972168, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 909 with loss 1.801795244216919, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 910 with loss 1.8487741947174072, accuracy 0.03333333507180214.\n",
      "Training epoch 8 batch 911 with loss 1.7056810855865479, accuracy 0.125.\n",
      "Training epoch 8 batch 912 with loss 1.727303147315979, accuracy 0.28333333134651184.\n",
      "Training epoch 8 batch 913 with loss 1.8167165517807007, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 914 with loss 1.7605245113372803, accuracy 0.39444443583488464.\n",
      "Training epoch 8 batch 915 with loss 1.7637176513671875, accuracy 0.2738095223903656.\n",
      "Training epoch 8 batch 916 with loss 1.7373816967010498, accuracy 0.2777777910232544.\n",
      "Training epoch 8 batch 917 with loss 1.757305383682251, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 918 with loss 1.863970398902893, accuracy 0.02380952425301075.\n",
      "Training epoch 8 batch 919 with loss 1.7431843280792236, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 920 with loss 1.8377784490585327, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 921 with loss 1.7189104557037354, accuracy 0.20000000298023224.\n",
      "Training epoch 8 batch 922 with loss 1.7429920434951782, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 923 with loss 1.8662983179092407, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 924 with loss 1.7571954727172852, accuracy 0.3194444477558136.\n",
      "Training epoch 8 batch 925 with loss 1.8470138311386108, accuracy 0.065476194024086.\n",
      "Training epoch 8 batch 926 with loss 1.8556241989135742, accuracy 0.11666667461395264.\n",
      "Training epoch 8 batch 927 with loss 1.881157636642456, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 928 with loss 1.9070895910263062, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 929 with loss 1.8143093585968018, accuracy 0.25.\n",
      "Training epoch 8 batch 930 with loss 1.7444959878921509, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 931 with loss 1.822055459022522, accuracy 0.25555557012557983.\n",
      "Training epoch 8 batch 932 with loss 1.895632028579712, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 933 with loss 1.8254972696304321, accuracy 0.1547619104385376.\n",
      "Training epoch 8 batch 934 with loss 1.726747751235962, accuracy 0.25.\n",
      "Training epoch 8 batch 935 with loss 1.823767066001892, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 936 with loss 1.7433573007583618, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 937 with loss 1.7726469039916992, accuracy 0.18214285373687744.\n",
      "Training epoch 8 batch 938 with loss 1.8414132595062256, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 939 with loss 1.8532928228378296, accuracy 0.1626984179019928.\n",
      "Training epoch 8 batch 940 with loss 1.7787635326385498, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 941 with loss 1.8400894403457642, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 942 with loss 1.847947359085083, accuracy 0.1031746044754982.\n",
      "Training epoch 8 batch 943 with loss 1.9384149312973022, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 944 with loss 1.798901915550232, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 945 with loss 1.685809850692749, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 946 with loss 1.8401960134506226, accuracy 0.26944443583488464.\n",
      "Training epoch 8 batch 947 with loss 1.8444976806640625, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 948 with loss 1.6996324062347412, accuracy 0.24074074625968933.\n",
      "Training epoch 8 batch 949 with loss 1.7801345586776733, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 950 with loss 1.8789241313934326, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 951 with loss 1.840968132019043, accuracy 0.13055555522441864.\n",
      "Training epoch 8 batch 952 with loss 1.812329888343811, accuracy 0.3333333432674408.\n",
      "Training epoch 8 batch 953 with loss 1.9150139093399048, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 954 with loss 1.8070091009140015, accuracy 0.02777777798473835.\n",
      "Training epoch 8 batch 955 with loss 1.8192027807235718, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 956 with loss 1.7998111248016357, accuracy 0.18333333730697632.\n",
      "Training epoch 8 batch 957 with loss 1.7896592617034912, accuracy 0.29666668176651.\n",
      "Training epoch 8 batch 958 with loss 1.835662603378296, accuracy 0.236111119389534.\n",
      "Training epoch 8 batch 959 with loss 1.766174077987671, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 960 with loss 1.7329206466674805, accuracy 0.25.\n",
      "Training epoch 8 batch 961 with loss 1.8133955001831055, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 962 with loss 1.8358023166656494, accuracy 0.2777777910232544.\n",
      "Training epoch 8 batch 963 with loss 1.8178812265396118, accuracy 0.23999999463558197.\n",
      "Training epoch 8 batch 964 with loss 1.7783145904541016, accuracy 0.24722222983837128.\n",
      "Training epoch 8 batch 965 with loss 1.7904036045074463, accuracy 0.16944444179534912.\n",
      "Training epoch 8 batch 966 with loss 1.7569818496704102, accuracy 0.3849206566810608.\n",
      "Training epoch 8 batch 967 with loss 1.726770043373108, accuracy 0.2003968209028244.\n",
      "Training epoch 8 batch 968 with loss 1.7620271444320679, accuracy 0.13425925374031067.\n",
      "Training epoch 8 batch 969 with loss 1.727313756942749, accuracy 0.31111112236976624.\n",
      "Training epoch 8 batch 970 with loss 1.794013261795044, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 971 with loss 1.6838667392730713, accuracy 0.15595239400863647.\n",
      "Training epoch 8 batch 972 with loss 1.827383041381836, accuracy 0.20555555820465088.\n",
      "Training epoch 8 batch 973 with loss 1.7968332767486572, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 974 with loss 1.711521863937378, accuracy 0.3333333432674408.\n",
      "Training epoch 8 batch 975 with loss 1.8001558780670166, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 976 with loss 1.7580687999725342, accuracy 0.222222238779068.\n",
      "Training epoch 8 batch 977 with loss 1.76996648311615, accuracy 0.380952388048172.\n",
      "Training epoch 8 batch 978 with loss 1.8623580932617188, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 979 with loss 1.8138405084609985, accuracy 0.25.\n",
      "Training epoch 8 batch 980 with loss 1.7728450298309326, accuracy 0.2420634925365448.\n",
      "Training epoch 8 batch 981 with loss 1.6582794189453125, accuracy 0.18333333730697632.\n",
      "Training epoch 8 batch 982 with loss 1.7835460901260376, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 983 with loss 1.7584564685821533, accuracy 0.12261904776096344.\n",
      "Training epoch 8 batch 984 with loss 1.8667494058609009, accuracy 0.1031746044754982.\n",
      "Training epoch 8 batch 985 with loss 1.8474222421646118, accuracy 0.03333333507180214.\n",
      "Training epoch 8 batch 986 with loss 1.876890778541565, accuracy 0.06111111491918564.\n",
      "Training epoch 8 batch 987 with loss 1.8029369115829468, accuracy 0.20000000298023224.\n",
      "Training epoch 8 batch 988 with loss 1.7834802865982056, accuracy 0.3500000238418579.\n",
      "Training epoch 8 batch 989 with loss 1.8753430843353271, accuracy 0.1349206417798996.\n",
      "Training epoch 8 batch 990 with loss 1.7898023128509521, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 991 with loss 1.9185796976089478, accuracy 0.190476194024086.\n",
      "Training epoch 8 batch 992 with loss 1.7660856246948242, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 993 with loss 1.8423194885253906, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 994 with loss 1.7462438344955444, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 995 with loss 1.7255407571792603, accuracy 0.23888888955116272.\n",
      "Training epoch 8 batch 996 with loss 1.7127342224121094, accuracy 0.23055554926395416.\n",
      "Training epoch 8 batch 997 with loss 1.7757923603057861, accuracy 0.2142857164144516.\n",
      "Training epoch 8 batch 998 with loss 1.8646776676177979, accuracy 0.2738095223903656.\n",
      "Training epoch 8 batch 999 with loss 1.7374365329742432, accuracy 0.3333333432674408.\n",
      "Training epoch 8 batch 1000 with loss 1.732295274734497, accuracy 0.49722224473953247.\n",
      "Training epoch 8 batch 1001 with loss 1.8516972064971924, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 1002 with loss 1.8219757080078125, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 1003 with loss 1.8742153644561768, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 1004 with loss 1.7432115077972412, accuracy 0.24166667461395264.\n",
      "Training epoch 8 batch 1005 with loss 1.7949278354644775, accuracy 0.28333336114883423.\n",
      "Training epoch 8 batch 1006 with loss 1.7748922109603882, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 1007 with loss 1.6766866445541382, accuracy 0.18611110746860504.\n",
      "Training epoch 8 batch 1008 with loss 1.765048623085022, accuracy 0.2460317462682724.\n",
      "Training epoch 8 batch 1009 with loss 1.897006630897522, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 1010 with loss 1.7637287378311157, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 1011 with loss 1.7864147424697876, accuracy 0.1686507910490036.\n",
      "Training epoch 8 batch 1012 with loss 1.8354854583740234, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 1013 with loss 1.7329238653182983, accuracy 0.2599206268787384.\n",
      "Training epoch 8 batch 1014 with loss 1.8436088562011719, accuracy 0.1458333432674408.\n",
      "Training epoch 8 batch 1015 with loss 1.7594053745269775, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1016 with loss 1.8473255634307861, accuracy 0.125.\n",
      "Training epoch 8 batch 1017 with loss 1.7707427740097046, accuracy 0.21388889849185944.\n",
      "Training epoch 8 batch 1018 with loss 1.7925783395767212, accuracy 0.3333333432674408.\n",
      "Training epoch 8 batch 1019 with loss 1.7211440801620483, accuracy 0.2460317611694336.\n",
      "Training epoch 8 batch 1020 with loss 1.7509740591049194, accuracy 0.15833333134651184.\n",
      "Training epoch 8 batch 1021 with loss 1.8084743022918701, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 1022 with loss 1.7819169759750366, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 1023 with loss 1.74365234375, accuracy 0.375.\n",
      "Training epoch 8 batch 1024 with loss 1.8406940698623657, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 1025 with loss 1.7184581756591797, accuracy 0.20555555820465088.\n",
      "Training epoch 8 batch 1026 with loss 1.7681608200073242, accuracy 0.18333333730697632.\n",
      "Training epoch 8 batch 1027 with loss 1.7877540588378906, accuracy 0.10000000894069672.\n",
      "Training epoch 8 batch 1028 with loss 1.7152801752090454, accuracy 0.30000001192092896.\n",
      "Training epoch 8 batch 1029 with loss 1.693723440170288, accuracy 0.2777777910232544.\n",
      "Training epoch 8 batch 1030 with loss 1.8708908557891846, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 1031 with loss 1.8042566776275635, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1032 with loss 1.7576881647109985, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 1033 with loss 1.8762239217758179, accuracy 0.23333334922790527.\n",
      "Training epoch 8 batch 1034 with loss 1.8639119863510132, accuracy 0.02777777798473835.\n",
      "Training epoch 8 batch 1035 with loss 1.7694342136383057, accuracy 0.1626984179019928.\n",
      "Training epoch 8 batch 1036 with loss 1.8068993091583252, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 1037 with loss 1.7990338802337646, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 1038 with loss 1.7969095706939697, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 1039 with loss 1.7351477146148682, accuracy 0.17777778208255768.\n",
      "Training epoch 8 batch 1040 with loss 1.880321741104126, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 1041 with loss 1.8077003955841064, accuracy 0.22083333134651184.\n",
      "Training epoch 8 batch 1042 with loss 1.8974087238311768, accuracy 0.02380952425301075.\n",
      "Training epoch 8 batch 1043 with loss 1.8039087057113647, accuracy 0.08750000596046448.\n",
      "Training epoch 8 batch 1044 with loss 1.7105686664581299, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1045 with loss 1.832532525062561, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 1046 with loss 1.7970459461212158, accuracy 0.21388889849185944.\n",
      "Training epoch 8 batch 1047 with loss 1.6398239135742188, accuracy 0.4125000238418579.\n",
      "Training epoch 8 batch 1048 with loss 1.7416152954101562, accuracy 0.21388889849185944.\n",
      "Training epoch 8 batch 1049 with loss 1.8199697732925415, accuracy 0.3194444477558136.\n",
      "Training epoch 8 batch 1050 with loss 1.883752465248108, accuracy 0.1071428582072258.\n",
      "Training epoch 8 batch 1051 with loss 1.9004557132720947, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1052 with loss 1.8586070537567139, accuracy 0.2460317462682724.\n",
      "Training epoch 8 batch 1053 with loss 1.7940483093261719, accuracy 0.1805555671453476.\n",
      "Training epoch 8 batch 1054 with loss 1.8440990447998047, accuracy 0.20555555820465088.\n",
      "Training epoch 8 batch 1055 with loss 1.835752248764038, accuracy 0.13055555522441864.\n",
      "Training epoch 8 batch 1056 with loss 1.741572618484497, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1057 with loss 1.7304785251617432, accuracy 0.3194444477558136.\n",
      "Training epoch 8 batch 1058 with loss 1.7610485553741455, accuracy 0.23055556416511536.\n",
      "Training epoch 8 batch 1059 with loss 1.7703931331634521, accuracy 0.28333333134651184.\n",
      "Training epoch 8 batch 1060 with loss 1.8464462757110596, accuracy 0.03333333507180214.\n",
      "Training epoch 8 batch 1061 with loss 1.8841816186904907, accuracy 0.3166666626930237.\n",
      "Training epoch 8 batch 1062 with loss 1.812119483947754, accuracy 0.25.\n",
      "Training epoch 8 batch 1063 with loss 1.8087332248687744, accuracy 0.25833335518836975.\n",
      "Training epoch 8 batch 1064 with loss 1.868945837020874, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 1065 with loss 1.9085699319839478, accuracy 0.0.\n",
      "Training epoch 8 batch 1066 with loss 1.7416553497314453, accuracy 0.32500001788139343.\n",
      "Training epoch 8 batch 1067 with loss 1.7706979513168335, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 1068 with loss 1.7464275360107422, accuracy 0.15833333134651184.\n",
      "Training epoch 8 batch 1069 with loss 1.8102185726165771, accuracy 0.12222222983837128.\n",
      "Training epoch 8 batch 1070 with loss 1.9369951486587524, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 1071 with loss 1.7903343439102173, accuracy 0.2003968209028244.\n",
      "Training epoch 8 batch 1072 with loss 1.7084563970565796, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 1073 with loss 1.8541330099105835, accuracy 0.09444444626569748.\n",
      "Training epoch 8 batch 1074 with loss 1.8622303009033203, accuracy 0.2750000059604645.\n",
      "Training epoch 8 batch 1075 with loss 1.7811486721038818, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 1076 with loss 1.8777034282684326, accuracy 0.0.\n",
      "Training epoch 8 batch 1077 with loss 1.7994756698608398, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 1078 with loss 1.840819001197815, accuracy 0.125.\n",
      "Training epoch 8 batch 1079 with loss 1.7327430248260498, accuracy 0.2083333432674408.\n",
      "Training epoch 8 batch 1080 with loss 1.7507492303848267, accuracy 0.23888888955116272.\n",
      "Training epoch 8 batch 1081 with loss 1.7795228958129883, accuracy 0.2083333432674408.\n",
      "Training epoch 8 batch 1082 with loss 1.8230721950531006, accuracy 0.0.\n",
      "Training epoch 8 batch 1083 with loss 1.788865327835083, accuracy 0.38055557012557983.\n",
      "Training epoch 8 batch 1084 with loss 1.8715150356292725, accuracy 0.0.\n",
      "Training epoch 8 batch 1085 with loss 1.83538019657135, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 1086 with loss 1.7838834524154663, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1087 with loss 1.7567956447601318, accuracy 0.24722224473953247.\n",
      "Training epoch 8 batch 1088 with loss 1.7587175369262695, accuracy 0.15833334624767303.\n",
      "Training epoch 8 batch 1089 with loss 1.6679041385650635, accuracy 0.2916666567325592.\n",
      "Training epoch 8 batch 1090 with loss 1.8128255605697632, accuracy 0.3333333432674408.\n",
      "Training epoch 8 batch 1091 with loss 1.7540886402130127, accuracy 0.236111119389534.\n",
      "Training epoch 8 batch 1092 with loss 1.752785325050354, accuracy 0.2986111342906952.\n",
      "Training epoch 8 batch 1093 with loss 1.7833423614501953, accuracy 0.11666666716337204.\n",
      "Training epoch 8 batch 1094 with loss 1.9005225896835327, accuracy 0.1488095223903656.\n",
      "Training epoch 8 batch 1095 with loss 1.6761844158172607, accuracy 0.42500001192092896.\n",
      "Training epoch 8 batch 1096 with loss 1.758631706237793, accuracy 0.2380952388048172.\n",
      "Training epoch 8 batch 1097 with loss 1.7529926300048828, accuracy 0.20000000298023224.\n",
      "Training epoch 8 batch 1098 with loss 1.848724603652954, accuracy 0.29722222685813904.\n",
      "Training epoch 8 batch 1099 with loss 1.7568910121917725, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 1100 with loss 1.7656217813491821, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1101 with loss 1.7577073574066162, accuracy 0.13611111044883728.\n",
      "Training epoch 8 batch 1102 with loss 1.7557741403579712, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1103 with loss 1.669942855834961, accuracy 0.5.\n",
      "Training epoch 8 batch 1104 with loss 1.7657369375228882, accuracy 0.18611110746860504.\n",
      "Training epoch 8 batch 1105 with loss 1.85746169090271, accuracy 0.28333333134651184.\n",
      "Training epoch 8 batch 1106 with loss 1.8084592819213867, accuracy 0.1408730149269104.\n",
      "Training epoch 8 batch 1107 with loss 1.8407764434814453, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 1108 with loss 1.882387399673462, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 1109 with loss 1.7771196365356445, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 1110 with loss 1.7866188287734985, accuracy 0.13055555522441864.\n",
      "Training epoch 8 batch 1111 with loss 1.7708604335784912, accuracy 0.3194444477558136.\n",
      "Training epoch 8 batch 1112 with loss 1.804330587387085, accuracy 0.25.\n",
      "Training epoch 8 batch 1113 with loss 1.717432975769043, accuracy 0.4047619104385376.\n",
      "Training epoch 8 batch 1114 with loss 1.8070204257965088, accuracy 0.2182539701461792.\n",
      "Training epoch 8 batch 1115 with loss 1.7657434940338135, accuracy 0.18611110746860504.\n",
      "Training epoch 8 batch 1116 with loss 1.9343528747558594, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 1117 with loss 1.7456138134002686, accuracy 0.42222222685813904.\n",
      "Training epoch 8 batch 1118 with loss 1.7920236587524414, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 1119 with loss 1.8606011867523193, accuracy 0.2888889014720917.\n",
      "Training epoch 8 batch 1120 with loss 1.8121141195297241, accuracy 0.13055555522441864.\n",
      "Training epoch 8 batch 1121 with loss 1.6895910501480103, accuracy 0.30158731341362.\n",
      "Training epoch 8 batch 1122 with loss 1.8903106451034546, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 1123 with loss 1.8714138269424438, accuracy 0.06111111491918564.\n",
      "Training epoch 8 batch 1124 with loss 1.7356189489364624, accuracy 0.3055555820465088.\n",
      "Training epoch 8 batch 1125 with loss 1.8787816762924194, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 1126 with loss 1.7621774673461914, accuracy 0.1349206417798996.\n",
      "Training epoch 8 batch 1127 with loss 1.8073890209197998, accuracy 0.329365074634552.\n",
      "Training epoch 8 batch 1128 with loss 1.8114216327667236, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 1129 with loss 1.771383285522461, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1130 with loss 1.8767235279083252, accuracy 0.1031746044754982.\n",
      "Training epoch 8 batch 1131 with loss 1.786716103553772, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 1132 with loss 1.846925973892212, accuracy 0.2916666865348816.\n",
      "Training epoch 8 batch 1133 with loss 1.74448561668396, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 1134 with loss 1.8439292907714844, accuracy 0.2182539701461792.\n",
      "Training epoch 8 batch 1135 with loss 1.7176494598388672, accuracy 0.2083333283662796.\n",
      "Training epoch 8 batch 1136 with loss 1.7465912103652954, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 1137 with loss 1.7351939678192139, accuracy 0.26944443583488464.\n",
      "Training epoch 8 batch 1138 with loss 1.6988866329193115, accuracy 0.39444443583488464.\n",
      "Training epoch 8 batch 1139 with loss 1.8576778173446655, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 1140 with loss 1.9798634052276611, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 1141 with loss 1.7045719623565674, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 1142 with loss 1.8820213079452515, accuracy 0.0694444477558136.\n",
      "Training epoch 8 batch 1143 with loss 1.7972314357757568, accuracy 0.3194444477558136.\n",
      "Training epoch 8 batch 1144 with loss 1.755921721458435, accuracy 0.15555556118488312.\n",
      "Training epoch 8 batch 1145 with loss 1.838710069656372, accuracy 0.065476194024086.\n",
      "Training epoch 8 batch 1146 with loss 1.6985269784927368, accuracy 0.42222222685813904.\n",
      "Training epoch 8 batch 1147 with loss 1.7207447290420532, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 1148 with loss 1.8158700466156006, accuracy 0.2420634925365448.\n",
      "Training epoch 8 batch 1149 with loss 1.7904293537139893, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 1150 with loss 1.793288230895996, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 1151 with loss 1.9442853927612305, accuracy 0.0.\n",
      "Training epoch 8 batch 1152 with loss 1.7624143362045288, accuracy 0.24722221493721008.\n",
      "Training epoch 8 batch 1153 with loss 1.7391166687011719, accuracy 0.17777778208255768.\n",
      "Training epoch 8 batch 1154 with loss 1.726637840270996, accuracy 0.28333333134651184.\n",
      "Training epoch 8 batch 1155 with loss 1.7947108745574951, accuracy 0.2182539701461792.\n",
      "Training epoch 8 batch 1156 with loss 1.7663860321044922, accuracy 0.236111119389534.\n",
      "Training epoch 8 batch 1157 with loss 1.7441799640655518, accuracy 0.1805555671453476.\n",
      "Training epoch 8 batch 1158 with loss 1.8497979640960693, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 1159 with loss 1.791649580001831, accuracy 0.2083333283662796.\n",
      "Training epoch 8 batch 1160 with loss 1.7973432540893555, accuracy 0.10000000149011612.\n",
      "Training epoch 8 batch 1161 with loss 1.7347233295440674, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1162 with loss 1.840816855430603, accuracy 0.10000000149011612.\n",
      "Training epoch 8 batch 1163 with loss 1.8086551427841187, accuracy 0.2916666567325592.\n",
      "Training epoch 8 batch 1164 with loss 1.7222917079925537, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 1165 with loss 1.810495376586914, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 1166 with loss 1.8251724243164062, accuracy 0.11666667461395264.\n",
      "Training epoch 8 batch 1167 with loss 1.8854583501815796, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 1168 with loss 1.805008888244629, accuracy 0.2738095223903656.\n",
      "Training epoch 8 batch 1169 with loss 1.889116883277893, accuracy 0.130952388048172.\n",
      "Training epoch 8 batch 1170 with loss 1.8400671482086182, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1171 with loss 1.8467543125152588, accuracy 0.2738095223903656.\n",
      "Training epoch 8 batch 1172 with loss 1.7151168584823608, accuracy 0.347222238779068.\n",
      "Training epoch 8 batch 1173 with loss 1.86080801486969, accuracy 0.2420634925365448.\n",
      "Training epoch 8 batch 1174 with loss 1.8026187419891357, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 1175 with loss 1.9168624877929688, accuracy 0.0.\n",
      "Training epoch 8 batch 1176 with loss 1.7671006917953491, accuracy 0.3253968358039856.\n",
      "Training epoch 8 batch 1177 with loss 1.736722707748413, accuracy 0.29722222685813904.\n",
      "Training epoch 8 batch 1178 with loss 1.7146103382110596, accuracy 0.29722222685813904.\n",
      "Training epoch 8 batch 1179 with loss 1.713155746459961, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 1180 with loss 1.8470178842544556, accuracy 0.1319444477558136.\n",
      "Training epoch 8 batch 1181 with loss 1.6965802907943726, accuracy 0.2888889014720917.\n",
      "Training epoch 8 batch 1182 with loss 1.8268890380859375, accuracy 0.3214285671710968.\n",
      "Training epoch 8 batch 1183 with loss 1.8463127613067627, accuracy 0.125.\n",
      "Training epoch 8 batch 1184 with loss 1.7873449325561523, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 1185 with loss 1.7292429208755493, accuracy 0.2916666865348816.\n",
      "Training epoch 8 batch 1186 with loss 1.7566583156585693, accuracy 0.255952388048172.\n",
      "Training epoch 8 batch 1187 with loss 1.7695391178131104, accuracy 0.26944446563720703.\n",
      "Training epoch 8 batch 1188 with loss 1.796439528465271, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 1189 with loss 1.6651939153671265, accuracy 0.4277777671813965.\n",
      "Training epoch 8 batch 1190 with loss 1.8327869176864624, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 1191 with loss 1.6435896158218384, accuracy 0.3380952477455139.\n",
      "Training epoch 8 batch 1192 with loss 1.85520339012146, accuracy 0.111111119389534.\n",
      "Training epoch 8 batch 1193 with loss 1.7913309335708618, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 1194 with loss 1.8476063013076782, accuracy 0.2777777910232544.\n",
      "Training epoch 8 batch 1195 with loss 1.8158702850341797, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 1196 with loss 1.7354484796524048, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 1197 with loss 1.8675616979599, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 1198 with loss 1.6422498226165771, accuracy 0.4000000059604645.\n",
      "Training epoch 8 batch 1199 with loss 1.8614866733551025, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 1200 with loss 1.7730505466461182, accuracy 0.2944444417953491.\n",
      "Training epoch 8 batch 1201 with loss 1.8376579284667969, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 1202 with loss 1.8932937383651733, accuracy 0.11666667461395264.\n",
      "Training epoch 8 batch 1203 with loss 1.7658847570419312, accuracy 0.1875.\n",
      "Training epoch 8 batch 1204 with loss 1.7871990203857422, accuracy 0.20158730447292328.\n",
      "Training epoch 8 batch 1205 with loss 1.8071463108062744, accuracy 0.19206349551677704.\n",
      "Training epoch 8 batch 1206 with loss 1.8507713079452515, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 1207 with loss 1.9307172298431396, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 1208 with loss 1.8191397190093994, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 1209 with loss 1.9218555688858032, accuracy 0.0.\n",
      "Training epoch 8 batch 1210 with loss 1.819776177406311, accuracy 0.23333334922790527.\n",
      "Training epoch 8 batch 1211 with loss 1.871474027633667, accuracy 0.03333333507180214.\n",
      "Training epoch 8 batch 1212 with loss 1.7659753561019897, accuracy 0.4791666865348816.\n",
      "Training epoch 8 batch 1213 with loss 1.8197126388549805, accuracy 0.18333333730697632.\n",
      "Training epoch 8 batch 1214 with loss 1.7304826974868774, accuracy 0.30158731341362.\n",
      "Training epoch 8 batch 1215 with loss 1.846767783164978, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 1216 with loss 1.7595916986465454, accuracy 0.236111119389534.\n",
      "Training epoch 8 batch 1217 with loss 1.856595754623413, accuracy 0.0694444477558136.\n",
      "Training epoch 8 batch 1218 with loss 1.7028026580810547, accuracy 0.30416667461395264.\n",
      "Training epoch 8 batch 1219 with loss 1.738452672958374, accuracy 0.2797619104385376.\n",
      "Training epoch 8 batch 1220 with loss 1.7096278667449951, accuracy 0.4000000059604645.\n",
      "Training epoch 8 batch 1221 with loss 1.8361742496490479, accuracy 0.3777778148651123.\n",
      "Training epoch 8 batch 1222 with loss 1.733123540878296, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 1223 with loss 1.7707469463348389, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 1224 with loss 1.7241718769073486, accuracy 0.210317462682724.\n",
      "Training epoch 8 batch 1225 with loss 1.7570940256118774, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 1226 with loss 1.7645399570465088, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 1227 with loss 1.7581055164337158, accuracy 0.23333334922790527.\n",
      "Training epoch 8 batch 1228 with loss 1.8482974767684937, accuracy 0.125.\n",
      "Training epoch 8 batch 1229 with loss 1.950160026550293, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 1230 with loss 1.6727025508880615, accuracy 0.24722221493721008.\n",
      "Training epoch 8 batch 1231 with loss 1.771381139755249, accuracy 0.12037037312984467.\n",
      "Training epoch 8 batch 1232 with loss 1.7281547784805298, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 1233 with loss 1.904862642288208, accuracy 0.0.\n",
      "Training epoch 8 batch 1234 with loss 1.8379714488983154, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 1235 with loss 1.8331657648086548, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 1236 with loss 1.8209078311920166, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 1237 with loss 1.776267647743225, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 1238 with loss 1.7545171976089478, accuracy 0.28333336114883423.\n",
      "Training epoch 8 batch 1239 with loss 1.7427517175674438, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 1240 with loss 1.8814489841461182, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 1241 with loss 1.8244686126708984, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 1242 with loss 1.726658821105957, accuracy 0.30158731341362.\n",
      "Training epoch 8 batch 1243 with loss 1.7734743356704712, accuracy 0.14722222089767456.\n",
      "Training epoch 8 batch 1244 with loss 1.7595773935317993, accuracy 0.33888888359069824.\n",
      "Training epoch 8 batch 1245 with loss 1.766442060470581, accuracy 0.06111111491918564.\n",
      "Training epoch 8 batch 1246 with loss 1.9106113910675049, accuracy 0.22380952537059784.\n",
      "Training epoch 8 batch 1247 with loss 1.9085986614227295, accuracy 0.0793650820851326.\n",
      "Training epoch 8 batch 1248 with loss 1.712775468826294, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 1249 with loss 1.783010482788086, accuracy 0.2083333432674408.\n",
      "Training epoch 8 batch 1250 with loss 1.8130934238433838, accuracy 0.2750000059604645.\n",
      "Training epoch 8 batch 1251 with loss 1.7897226810455322, accuracy 0.335317462682724.\n",
      "Training epoch 8 batch 1252 with loss 1.7874670028686523, accuracy 0.2361111044883728.\n",
      "Training epoch 8 batch 1253 with loss 1.6564851999282837, accuracy 0.2083333432674408.\n",
      "Training epoch 8 batch 1254 with loss 1.777108907699585, accuracy 0.125.\n",
      "Training epoch 8 batch 1255 with loss 1.8279809951782227, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1256 with loss 1.9299618005752563, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 1257 with loss 1.6589736938476562, accuracy 0.23333334922790527.\n",
      "Training epoch 8 batch 1258 with loss 1.9838812351226807, accuracy 0.0.\n",
      "Training epoch 8 batch 1259 with loss 1.7944415807724, accuracy 0.15555556118488312.\n",
      "Training epoch 8 batch 1260 with loss 1.9033985137939453, accuracy 0.09444444626569748.\n",
      "Training epoch 8 batch 1261 with loss 1.7100147008895874, accuracy 0.222222238779068.\n",
      "Training epoch 8 batch 1262 with loss 1.8240089416503906, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 1263 with loss 1.8837741613388062, accuracy 0.0476190485060215.\n",
      "Training epoch 8 batch 1264 with loss 1.8280212879180908, accuracy 0.15833333134651184.\n",
      "Training epoch 8 batch 1265 with loss 1.9225784540176392, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 1266 with loss 1.902451515197754, accuracy 0.05714286118745804.\n",
      "Training epoch 8 batch 1267 with loss 1.9243491888046265, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 1268 with loss 1.8438246250152588, accuracy 0.222222238779068.\n",
      "Training epoch 8 batch 1269 with loss 1.8190845251083374, accuracy 0.06018518656492233.\n",
      "Training epoch 8 batch 1270 with loss 1.7716395854949951, accuracy 0.3571428656578064.\n",
      "Training epoch 8 batch 1271 with loss 1.7450891733169556, accuracy 0.20555555820465088.\n",
      "Training epoch 8 batch 1272 with loss 1.8161462545394897, accuracy 0.15833333134651184.\n",
      "Training epoch 8 batch 1273 with loss 1.8516845703125, accuracy 0.2916666865348816.\n",
      "Training epoch 8 batch 1274 with loss 1.7166646718978882, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 1275 with loss 1.7526792287826538, accuracy 0.0694444477558136.\n",
      "Training epoch 8 batch 1276 with loss 1.8262449502944946, accuracy 0.02380952425301075.\n",
      "Training epoch 8 batch 1277 with loss 1.8316682577133179, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 1278 with loss 1.843518614768982, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 1279 with loss 1.9101107120513916, accuracy 0.15833334624767303.\n",
      "Training epoch 8 batch 1280 with loss 1.75921630859375, accuracy 0.13055555522441864.\n",
      "Training epoch 8 batch 1281 with loss 1.8285512924194336, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 1282 with loss 1.7931121587753296, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 1283 with loss 1.749236822128296, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 1284 with loss 1.8598934412002563, accuracy 0.11666667461395264.\n",
      "Training epoch 8 batch 1285 with loss 1.8715600967407227, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 1286 with loss 1.8256549835205078, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 1287 with loss 1.7367805242538452, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 1288 with loss 1.7950031757354736, accuracy 0.15833333134651184.\n",
      "Training epoch 8 batch 1289 with loss 1.8517696857452393, accuracy 0.15833333134651184.\n",
      "Training epoch 8 batch 1290 with loss 1.9035813808441162, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 1291 with loss 1.8259493112564087, accuracy 0.17777778208255768.\n",
      "Training epoch 8 batch 1292 with loss 1.7230972051620483, accuracy 0.1825396865606308.\n",
      "Training epoch 8 batch 1293 with loss 1.7203044891357422, accuracy 0.2916666865348816.\n",
      "Training epoch 8 batch 1294 with loss 1.7595058679580688, accuracy 0.18333333730697632.\n",
      "Training epoch 8 batch 1295 with loss 1.87958562374115, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 1296 with loss 1.770146131515503, accuracy 0.1805555671453476.\n",
      "Training epoch 8 batch 1297 with loss 1.8776689767837524, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 1298 with loss 1.679960012435913, accuracy 0.402777761220932.\n",
      "Training epoch 8 batch 1299 with loss 1.803753137588501, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1300 with loss 1.8504295349121094, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 1301 with loss 1.8670543432235718, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 1302 with loss 1.8851563930511475, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 1303 with loss 1.7208232879638672, accuracy 0.29722222685813904.\n",
      "Training epoch 8 batch 1304 with loss 1.8264738321304321, accuracy 0.2916666865348816.\n",
      "Training epoch 8 batch 1305 with loss 1.8031362295150757, accuracy 0.14047619700431824.\n",
      "Training epoch 8 batch 1306 with loss 1.7820030450820923, accuracy 0.09444444626569748.\n",
      "Training epoch 8 batch 1307 with loss 1.7449724674224854, accuracy 0.0625.\n",
      "Training epoch 8 batch 1308 with loss 1.7335689067840576, accuracy 0.444444477558136.\n",
      "Training epoch 8 batch 1309 with loss 1.7763307094573975, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1310 with loss 1.9130690097808838, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 1311 with loss 1.7923784255981445, accuracy 0.25.\n",
      "Training epoch 8 batch 1312 with loss 1.8465769290924072, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 1313 with loss 1.6727205514907837, accuracy 0.3222222328186035.\n",
      "Training epoch 8 batch 1314 with loss 1.9107879400253296, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 1315 with loss 1.8380295038223267, accuracy 0.0793650820851326.\n",
      "Training epoch 8 batch 1316 with loss 1.8236576318740845, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 1317 with loss 1.7065613269805908, accuracy 0.30158731341362.\n",
      "Training epoch 8 batch 1318 with loss 1.8161348104476929, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 1319 with loss 1.716896414756775, accuracy 0.25.\n",
      "Training epoch 8 batch 1320 with loss 1.874892234802246, accuracy 0.06666667014360428.\n",
      "Training epoch 8 batch 1321 with loss 1.732505202293396, accuracy 0.19166666269302368.\n",
      "Training epoch 8 batch 1322 with loss 1.9192543029785156, accuracy 0.0.\n",
      "Training epoch 8 batch 1323 with loss 1.7508394718170166, accuracy 0.33888888359069824.\n",
      "Training epoch 8 batch 1324 with loss 1.8062629699707031, accuracy 0.12222222983837128.\n",
      "Training epoch 8 batch 1325 with loss 1.7942092418670654, accuracy 0.16388888657093048.\n",
      "Training epoch 8 batch 1326 with loss 1.8715566396713257, accuracy 0.065476194024086.\n",
      "Training epoch 8 batch 1327 with loss 1.8263574838638306, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 1328 with loss 1.7458765506744385, accuracy 0.22777777910232544.\n",
      "Training epoch 8 batch 1329 with loss 1.7865946292877197, accuracy 0.31111112236976624.\n",
      "Training epoch 8 batch 1330 with loss 1.747886300086975, accuracy 0.3619047701358795.\n",
      "Training epoch 8 batch 1331 with loss 1.803964614868164, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 1332 with loss 1.705243706703186, accuracy 0.222222238779068.\n",
      "Training epoch 8 batch 1333 with loss 1.719763159751892, accuracy 0.111111119389534.\n",
      "Training epoch 8 batch 1334 with loss 1.889264464378357, accuracy 0.03333333507180214.\n",
      "Training epoch 8 batch 1335 with loss 1.898389458656311, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 1336 with loss 1.8942391872406006, accuracy 0.125.\n",
      "Training epoch 8 batch 1337 with loss 1.8788080215454102, accuracy 0.06666667014360428.\n",
      "Training epoch 8 batch 1338 with loss 1.7453992366790771, accuracy 0.4444444477558136.\n",
      "Training epoch 8 batch 1339 with loss 1.7566372156143188, accuracy 0.1349206417798996.\n",
      "Training epoch 8 batch 1340 with loss 1.8308613300323486, accuracy 0.3611111044883728.\n",
      "Training epoch 8 batch 1341 with loss 1.7928123474121094, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 1342 with loss 1.6576893329620361, accuracy 0.39603176712989807.\n",
      "Training epoch 8 batch 1343 with loss 1.7386665344238281, accuracy 0.2111111283302307.\n",
      "Training epoch 8 batch 1344 with loss 1.7837823629379272, accuracy 0.2916666567325592.\n",
      "Training epoch 8 batch 1345 with loss 1.6973145008087158, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 1346 with loss 1.913913369178772, accuracy 0.03333333507180214.\n",
      "Training epoch 8 batch 1347 with loss 1.8044359683990479, accuracy 0.1818181872367859.\n",
      "Training epoch 8 batch 1348 with loss 1.8399803638458252, accuracy 0.15833333134651184.\n",
      "Training epoch 8 batch 1349 with loss 1.8140465021133423, accuracy 0.21111111342906952.\n",
      "Training epoch 8 batch 1350 with loss 1.8232946395874023, accuracy 0.125.\n",
      "Training epoch 8 batch 1351 with loss 1.8519293069839478, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 1352 with loss 1.744135856628418, accuracy 0.28333333134651184.\n",
      "Training epoch 8 batch 1353 with loss 1.8040339946746826, accuracy 0.15555556118488312.\n",
      "Training epoch 8 batch 1354 with loss 1.7080628871917725, accuracy 0.23333333432674408.\n",
      "Training epoch 8 batch 1355 with loss 1.7558040618896484, accuracy 0.2750000059604645.\n",
      "Training epoch 8 batch 1356 with loss 1.7704112529754639, accuracy 0.24761906266212463.\n",
      "Training epoch 8 batch 1357 with loss 1.8482248783111572, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 1358 with loss 1.7982795238494873, accuracy 0.130952388048172.\n",
      "Training epoch 8 batch 1359 with loss 1.8501043319702148, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 1360 with loss 1.8023639917373657, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 1361 with loss 1.8122098445892334, accuracy 0.1527777761220932.\n",
      "Training epoch 8 batch 1362 with loss 1.8449407815933228, accuracy 0.125.\n",
      "Training epoch 8 batch 1363 with loss 1.788903832435608, accuracy 0.15740740299224854.\n",
      "Training epoch 8 batch 1364 with loss 1.7651093006134033, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1365 with loss 1.8005712032318115, accuracy 0.11666667461395264.\n",
      "Training epoch 8 batch 1366 with loss 1.8297656774520874, accuracy 0.118055559694767.\n",
      "Training epoch 8 batch 1367 with loss 1.8147592544555664, accuracy 0.12222222983837128.\n",
      "Training epoch 8 batch 1368 with loss 1.8300302028656006, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 1369 with loss 1.7432012557983398, accuracy 0.23333334922790527.\n",
      "Training epoch 8 batch 1370 with loss 1.8429057598114014, accuracy 0.1111111119389534.\n",
      "Training epoch 8 batch 1371 with loss 1.7298357486724854, accuracy 0.40000003576278687.\n",
      "Training epoch 8 batch 1372 with loss 1.873230218887329, accuracy 0.1805555671453476.\n",
      "Training epoch 8 batch 1373 with loss 1.8015934228897095, accuracy 0.2083333432674408.\n",
      "Training epoch 8 batch 1374 with loss 1.8559157848358154, accuracy 0.02777777798473835.\n",
      "Training epoch 8 batch 1375 with loss 1.8352863788604736, accuracy 0.1349206417798996.\n",
      "Training epoch 8 batch 1376 with loss 1.7846262454986572, accuracy 0.2944444417953491.\n",
      "Training epoch 8 batch 1377 with loss 1.801047682762146, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 1378 with loss 1.8225523233413696, accuracy 0.222222238779068.\n",
      "Training epoch 8 batch 1379 with loss 1.7665421962738037, accuracy 0.1825396865606308.\n",
      "Training epoch 8 batch 1380 with loss 1.91072678565979, accuracy 0.09444444626569748.\n",
      "Training epoch 8 batch 1381 with loss 1.9463253021240234, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 1382 with loss 1.7721627950668335, accuracy 0.222222238779068.\n",
      "Training epoch 8 batch 1383 with loss 1.7149524688720703, accuracy 0.144841268658638.\n",
      "Training epoch 8 batch 1384 with loss 1.7281593084335327, accuracy 0.44206351041793823.\n",
      "Training epoch 8 batch 1385 with loss 1.6996467113494873, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 1386 with loss 1.753875494003296, accuracy 0.11666667461395264.\n",
      "Training epoch 8 batch 1387 with loss 1.6652719974517822, accuracy 0.3499999940395355.\n",
      "Training epoch 8 batch 1388 with loss 1.861914873123169, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 1389 with loss 1.8232723474502563, accuracy 0.0694444477558136.\n",
      "Training epoch 8 batch 1390 with loss 1.7074196338653564, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 1391 with loss 1.6541963815689087, accuracy 0.25555557012557983.\n",
      "Training epoch 8 batch 1392 with loss 1.8417932987213135, accuracy 0.05000000074505806.\n",
      "Training epoch 8 batch 1393 with loss 1.824841856956482, accuracy 0.1599999964237213.\n",
      "Training epoch 8 batch 1394 with loss 1.7663424015045166, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 1395 with loss 1.7560926675796509, accuracy 0.25833332538604736.\n",
      "Training epoch 8 batch 1396 with loss 1.7433115243911743, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 1397 with loss 1.7852623462677002, accuracy 0.3611111044883728.\n",
      "Training epoch 8 batch 1398 with loss 1.8060848712921143, accuracy 0.15833333134651184.\n",
      "Training epoch 8 batch 1399 with loss 1.745311975479126, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 1400 with loss 1.8097553253173828, accuracy 0.11666667461395264.\n",
      "Training epoch 8 batch 1401 with loss 1.703338384628296, accuracy 0.2638888955116272.\n",
      "Training epoch 8 batch 1402 with loss 1.7797313928604126, accuracy 0.25.\n",
      "Training epoch 8 batch 1403 with loss 1.6643717288970947, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 1404 with loss 1.8217544555664062, accuracy 0.10000000894069672.\n",
      "Training epoch 8 batch 1405 with loss 1.8183873891830444, accuracy 0.222222238779068.\n",
      "Training epoch 8 batch 1406 with loss 1.8220294713974, accuracy 0.0972222238779068.\n",
      "Training epoch 8 batch 1407 with loss 1.8356730937957764, accuracy 0.2777777910232544.\n",
      "Training epoch 8 batch 1408 with loss 1.7399650812149048, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 1409 with loss 1.8410060405731201, accuracy 0.1071428582072258.\n",
      "Training epoch 8 batch 1410 with loss 1.873501181602478, accuracy 0.018518518656492233.\n",
      "Training epoch 8 batch 1411 with loss 1.9110733270645142, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 1412 with loss 1.8238741159439087, accuracy 0.4166666865348816.\n",
      "Training epoch 8 batch 1413 with loss 1.7648394107818604, accuracy 0.2611111104488373.\n",
      "Training epoch 8 batch 1414 with loss 1.8511295318603516, accuracy 0.22777777910232544.\n",
      "Training epoch 8 batch 1415 with loss 1.7563470602035522, accuracy 0.1269841343164444.\n",
      "Training epoch 8 batch 1416 with loss 1.8065992593765259, accuracy 0.1031746044754982.\n",
      "Training epoch 8 batch 1417 with loss 1.716979742050171, accuracy 0.4166666865348816.\n",
      "Training epoch 8 batch 1418 with loss 1.7745803594589233, accuracy 0.3222222328186035.\n",
      "Training epoch 8 batch 1419 with loss 1.7480052709579468, accuracy 0.22500000894069672.\n",
      "Training epoch 8 batch 1420 with loss 1.8333772420883179, accuracy 0.02777777798473835.\n",
      "Training epoch 8 batch 1421 with loss 1.7577502727508545, accuracy 0.23333333432674408.\n",
      "Training epoch 8 batch 1422 with loss 1.899827241897583, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 1423 with loss 1.7373466491699219, accuracy 0.13333334028720856.\n",
      "Training epoch 8 batch 1424 with loss 1.880239725112915, accuracy 0.08888889104127884.\n",
      "Training epoch 8 batch 1425 with loss 1.7523705959320068, accuracy 0.29722222685813904.\n",
      "Training epoch 8 batch 1426 with loss 1.8446840047836304, accuracy 0.07500000298023224.\n",
      "Training epoch 8 batch 1427 with loss 1.7080100774765015, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 1428 with loss 1.7813323736190796, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 1429 with loss 1.7516101598739624, accuracy 0.27936509251594543.\n",
      "Training epoch 8 batch 1430 with loss 1.8599016666412354, accuracy 0.03333333507180214.\n",
      "Training epoch 8 batch 1431 with loss 1.837914228439331, accuracy 0.0793650820851326.\n",
      "Training epoch 8 batch 1432 with loss 1.6367063522338867, accuracy 0.3055555522441864.\n",
      "Training epoch 8 batch 1433 with loss 1.876997947692871, accuracy 0.125.\n",
      "Training epoch 8 batch 1434 with loss 1.7087337970733643, accuracy 0.4055555462837219.\n",
      "Training epoch 8 batch 1435 with loss 1.846818208694458, accuracy 0.11269842088222504.\n",
      "Training epoch 8 batch 1436 with loss 1.9439054727554321, accuracy 0.0694444477558136.\n",
      "Training epoch 8 batch 1437 with loss 1.724329948425293, accuracy 0.2361111044883728.\n",
      "Training epoch 8 batch 1438 with loss 1.842534065246582, accuracy 0.2976190447807312.\n",
      "Training epoch 8 batch 1439 with loss 1.7171189785003662, accuracy 0.2666666805744171.\n",
      "Training epoch 8 batch 1440 with loss 1.7636702060699463, accuracy 0.2916666567325592.\n",
      "Training epoch 8 batch 1441 with loss 1.7392473220825195, accuracy 0.1547619104385376.\n",
      "Training epoch 8 batch 1442 with loss 1.7764194011688232, accuracy 0.06111111491918564.\n",
      "Training epoch 8 batch 1443 with loss 1.851259469985962, accuracy 0.25.\n",
      "Training epoch 8 batch 1444 with loss 1.8878368139266968, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 1445 with loss 1.6665267944335938, accuracy 0.3888888657093048.\n",
      "Training epoch 8 batch 1446 with loss 1.8657258749008179, accuracy 0.3777777850627899.\n",
      "Training epoch 8 batch 1447 with loss 1.7286396026611328, accuracy 0.25.\n",
      "Training epoch 8 batch 1448 with loss 1.8618202209472656, accuracy 0.1388888955116272.\n",
      "Training epoch 8 batch 1449 with loss 1.8815228939056396, accuracy 0.09351852536201477.\n",
      "Training epoch 8 batch 1450 with loss 1.7338991165161133, accuracy 0.3571428656578064.\n",
      "Training epoch 8 batch 1451 with loss 1.7340233325958252, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 1452 with loss 1.8592567443847656, accuracy 0.0.\n",
      "Training epoch 8 batch 1453 with loss 1.731278657913208, accuracy 0.125.\n",
      "Training epoch 8 batch 1454 with loss 1.8277794122695923, accuracy 0.35277777910232544.\n",
      "Training epoch 8 batch 1455 with loss 1.755955457687378, accuracy 0.3888888955116272.\n",
      "Training epoch 8 batch 1456 with loss 1.7119125127792358, accuracy 0.36666667461395264.\n",
      "Training epoch 8 batch 1457 with loss 1.7897863388061523, accuracy 0.2460317462682724.\n",
      "Training epoch 8 batch 1458 with loss 1.7423591613769531, accuracy 0.23333334922790527.\n",
      "Training epoch 8 batch 1459 with loss 1.7250200510025024, accuracy 0.30277779698371887.\n",
      "Training epoch 8 batch 1460 with loss 1.8187980651855469, accuracy 0.2611111104488373.\n",
      "Training epoch 8 batch 1461 with loss 1.9001230001449585, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 1462 with loss 1.867064118385315, accuracy 0.28333336114883423.\n",
      "Training epoch 8 batch 1463 with loss 1.8989681005477905, accuracy 0.03333333507180214.\n",
      "Training epoch 8 batch 1464 with loss 1.9693851470947266, accuracy 0.07407407462596893.\n",
      "Training epoch 8 batch 1465 with loss 1.7585614919662476, accuracy 0.190476194024086.\n",
      "Training epoch 8 batch 1466 with loss 1.6714718341827393, accuracy 0.2202380895614624.\n",
      "Training epoch 8 batch 1467 with loss 1.7751903533935547, accuracy 0.2658730149269104.\n",
      "Training epoch 8 batch 1468 with loss 1.8520361185073853, accuracy 0.0416666679084301.\n",
      "Training epoch 8 batch 1469 with loss 1.8855419158935547, accuracy 0.0555555559694767.\n",
      "Training epoch 8 batch 1470 with loss 1.8238954544067383, accuracy 0.17777778208255768.\n",
      "Training epoch 8 batch 1471 with loss 1.7622277736663818, accuracy 0.26944443583488464.\n",
      "Training epoch 8 batch 1472 with loss 1.7846637964248657, accuracy 0.19722223281860352.\n",
      "Training epoch 8 batch 1473 with loss 1.7524864673614502, accuracy 0.3888889253139496.\n",
      "Training epoch 8 batch 1474 with loss 1.932936668395996, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1475 with loss 1.729079008102417, accuracy 0.2083333432674408.\n",
      "Training epoch 8 batch 1476 with loss 1.6390094757080078, accuracy 0.2658730149269104.\n",
      "Training epoch 8 batch 1477 with loss 1.7660131454467773, accuracy 0.20555555820465088.\n",
      "Training epoch 8 batch 1478 with loss 1.793270468711853, accuracy 0.2420634925365448.\n",
      "Training epoch 8 batch 1479 with loss 1.856347680091858, accuracy 0.2222222238779068.\n",
      "Training epoch 8 batch 1480 with loss 1.8102201223373413, accuracy 0.1805555522441864.\n",
      "Training epoch 8 batch 1481 with loss 1.896997094154358, accuracy 0.20555555820465088.\n",
      "Training epoch 8 batch 1482 with loss 1.7423244714736938, accuracy 0.15000000596046448.\n",
      "Training epoch 8 batch 1483 with loss 1.7031465768814087, accuracy 0.402777761220932.\n",
      "Training epoch 8 batch 1484 with loss 1.739168405532837, accuracy 0.1587301641702652.\n",
      "Training epoch 8 batch 1485 with loss 1.8066644668579102, accuracy 0.2361111044883728.\n",
      "Training epoch 8 batch 1486 with loss 1.7289750576019287, accuracy 0.24761904776096344.\n",
      "Training epoch 8 batch 1487 with loss 1.7928857803344727, accuracy 0.14444445073604584.\n",
      "Training epoch 8 batch 1488 with loss 1.8342955112457275, accuracy 0.10833333432674408.\n",
      "Training epoch 8 batch 1489 with loss 1.7420494556427002, accuracy 0.2083333432674408.\n",
      "Training epoch 8 batch 1490 with loss 1.8616220951080322, accuracy 0.1944444477558136.\n",
      "Training epoch 8 batch 1491 with loss 1.8474401235580444, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 1492 with loss 1.7878919839859009, accuracy 0.17222222685813904.\n",
      "Training epoch 8 batch 1493 with loss 1.7294728755950928, accuracy 0.18333333730697632.\n",
      "Training epoch 8 batch 1494 with loss 1.8686866760253906, accuracy 0.0833333358168602.\n",
      "Training epoch 8 batch 1495 with loss 1.8321945667266846, accuracy 0.12222222983837128.\n",
      "Training epoch 8 batch 1496 with loss 1.8469035625457764, accuracy 0.12222222983837128.\n",
      "Training epoch 8 batch 1497 with loss 1.8258984088897705, accuracy 0.1071428582072258.\n",
      "Training epoch 8 batch 1498 with loss 1.8147847652435303, accuracy 0.1666666716337204.\n",
      "Training epoch 8 batch 1499 with loss 1.9020311832427979, accuracy 0.0416666679084301.\n",
      "Test batch 0 with loss 1.8509842157363892 and accuracy 0.125.\n",
      "Test batch 1 with loss 1.7806447744369507 and accuracy 0.3055555820465088.\n",
      "Test batch 2 with loss 1.8396774530410767 and accuracy 0.10833333432674408.\n",
      "Test batch 3 with loss 1.8376766443252563 and accuracy 0.1071428582072258.\n",
      "Test batch 4 with loss 1.8171418905258179 and accuracy 0.23333334922790527.\n",
      "Test batch 5 with loss 1.7399591207504272 and accuracy 0.1805555671453476.\n",
      "Test batch 6 with loss 1.8424323797225952 and accuracy 0.17777778208255768.\n",
      "Test batch 7 with loss 1.9177615642547607 and accuracy 0.1111111119389534.\n",
      "Test batch 8 with loss 1.8132047653198242 and accuracy 0.2638888955116272.\n",
      "Test batch 9 with loss 1.7975314855575562 and accuracy 0.1349206417798996.\n",
      "Test batch 10 with loss 1.8028160333633423 and accuracy 0.3333333432674408.\n",
      "Test batch 11 with loss 1.8049466609954834 and accuracy 0.20000000298023224.\n",
      "Test batch 12 with loss 1.8109776973724365 and accuracy 0.1805555671453476.\n",
      "Test batch 13 with loss 1.71871817111969 and accuracy 0.10000000149011612.\n",
      "Test batch 14 with loss 1.8596410751342773 and accuracy 0.0555555559694767.\n",
      "Test batch 15 with loss 1.7687174081802368 and accuracy 0.30000001192092896.\n",
      "Test batch 16 with loss 1.8120301961898804 and accuracy 0.21111111342906952.\n",
      "Test batch 17 with loss 1.9107110500335693 and accuracy 0.06666667014360428.\n",
      "Test batch 18 with loss 1.804922342300415 and accuracy 0.13055555522441864.\n",
      "Test batch 19 with loss 1.7803328037261963 and accuracy 0.2361111044883728.\n",
      "Test batch 20 with loss 1.7693662643432617 and accuracy 0.1388888955116272.\n",
      "Test batch 21 with loss 1.810990571975708 and accuracy 0.25.\n",
      "Test batch 22 with loss 1.7624666690826416 and accuracy 0.29722222685813904.\n",
      "Test batch 23 with loss 1.8645719289779663 and accuracy 0.0833333358168602.\n",
      "Test batch 24 with loss 1.82706618309021 and accuracy 0.0972222238779068.\n",
      "Test batch 25 with loss 1.8467096090316772 and accuracy 0.125.\n",
      "Test batch 26 with loss 1.715889573097229 and accuracy 0.3583333492279053.\n",
      "Test batch 27 with loss 1.880735158920288 and accuracy 0.13611111044883728.\n",
      "Test batch 28 with loss 1.8757429122924805 and accuracy 0.10277777910232544.\n",
      "Test batch 29 with loss 1.7704546451568604 and accuracy 0.13055555522441864.\n",
      "Test batch 30 with loss 1.8139721155166626 and accuracy 0.11666667461395264.\n",
      "Test batch 31 with loss 1.9305919408798218 and accuracy 0.1805555522441864.\n",
      "Test batch 32 with loss 1.7593402862548828 and accuracy 0.25.\n",
      "Test batch 33 with loss 1.8453559875488281 and accuracy 0.15000000596046448.\n",
      "Test batch 34 with loss 1.9365894794464111 and accuracy 0.03333333507180214.\n",
      "Test batch 35 with loss 1.8055884838104248 and accuracy 0.12222222983837128.\n",
      "Test batch 36 with loss 1.7977988719940186 and accuracy 0.11666667461395264.\n",
      "Test batch 37 with loss 1.787801742553711 and accuracy 0.2944444715976715.\n",
      "Test batch 38 with loss 1.8801641464233398 and accuracy 0.0892857164144516.\n",
      "Test batch 39 with loss 1.8542988300323486 and accuracy 0.3055555820465088.\n",
      "Test batch 40 with loss 1.6892051696777344 and accuracy 0.2142857313156128.\n",
      "Test batch 41 with loss 1.9156911373138428 and accuracy 0.0.\n",
      "Test batch 42 with loss 1.9367663860321045 and accuracy 0.0555555559694767.\n",
      "Test batch 43 with loss 1.8732662200927734 and accuracy 0.125.\n",
      "Test batch 44 with loss 1.7678810358047485 and accuracy 0.3194444477558136.\n",
      "Test batch 45 with loss 1.9263761043548584 and accuracy 0.0416666679084301.\n",
      "Test batch 46 with loss 1.8579823970794678 and accuracy 0.02777777798473835.\n",
      "Test batch 47 with loss 1.8927295207977295 and accuracy 0.07500000298023224.\n",
      "Test batch 48 with loss 1.8492634296417236 and accuracy 0.1388888955116272.\n",
      "Test batch 49 with loss 1.8359094858169556 and accuracy 0.15000000596046448.\n",
      "Test batch 50 with loss 1.822866439819336 and accuracy 0.0972222238779068.\n",
      "Test batch 51 with loss 1.832726240158081 and accuracy 0.1388888955116272.\n",
      "Test batch 52 with loss 1.9473167657852173 and accuracy 0.0.\n",
      "Test batch 53 with loss 1.8741226196289062 and accuracy 0.1626984179019928.\n",
      "Test batch 54 with loss 1.8860374689102173 and accuracy 0.0833333358168602.\n",
      "Test batch 55 with loss 1.8962171077728271 and accuracy 0.0.\n",
      "Test batch 56 with loss 1.8489971160888672 and accuracy 0.1388888955116272.\n",
      "Test batch 57 with loss 1.7851593494415283 and accuracy 0.125.\n",
      "Test batch 58 with loss 1.8420970439910889 and accuracy 0.1111111119389534.\n",
      "Test batch 59 with loss 1.8278648853302002 and accuracy 0.15833334624767303.\n",
      "Test batch 60 with loss 1.6914455890655518 and accuracy 0.4642857313156128.\n",
      "Test batch 61 with loss 1.7216297388076782 and accuracy 0.2976190447807312.\n",
      "Test batch 62 with loss 1.8457504510879517 and accuracy 0.1388888955116272.\n",
      "Test batch 63 with loss 1.7868162393569946 and accuracy 0.28333336114883423.\n",
      "Test batch 64 with loss 1.8107935190200806 and accuracy 0.0416666679084301.\n",
      "Test batch 65 with loss 1.89846932888031 and accuracy 0.03333333507180214.\n",
      "Test batch 66 with loss 1.917046308517456 and accuracy 0.2777777910232544.\n",
      "Test batch 67 with loss 1.8094203472137451 and accuracy 0.35277777910232544.\n",
      "Test batch 68 with loss 1.841102957725525 and accuracy 0.11666667461395264.\n",
      "Test batch 69 with loss 1.8055789470672607 and accuracy 0.065476194024086.\n",
      "Test batch 70 with loss 1.7479912042617798 and accuracy 0.1349206417798996.\n",
      "Test batch 71 with loss 1.7856203317642212 and accuracy 0.3333333432674408.\n",
      "Test batch 72 with loss 1.747288703918457 and accuracy 0.2988095283508301.\n",
      "Test batch 73 with loss 1.774295449256897 and accuracy 0.09259259700775146.\n",
      "Test batch 74 with loss 1.8180679082870483 and accuracy 0.2361111044883728.\n",
      "Test batch 75 with loss 1.882092833518982 and accuracy 0.1944444477558136.\n",
      "Test batch 76 with loss 1.8596413135528564 and accuracy 0.0555555559694767.\n",
      "Test batch 77 with loss 1.8672573566436768 and accuracy 0.09444444626569748.\n",
      "Test batch 78 with loss 1.8638626337051392 and accuracy 0.1111111119389534.\n",
      "Test batch 79 with loss 1.8428709506988525 and accuracy 0.2876984179019928.\n",
      "Test batch 80 with loss 1.9022728204727173 and accuracy 0.02777777798473835.\n",
      "Test batch 81 with loss 1.7937215566635132 and accuracy 0.1666666716337204.\n",
      "Test batch 82 with loss 1.7868807315826416 and accuracy 0.25.\n",
      "Test batch 83 with loss 1.961326241493225 and accuracy 0.0833333358168602.\n",
      "Test batch 84 with loss 1.8625322580337524 and accuracy 0.25.\n",
      "Test batch 85 with loss 1.8634817600250244 and accuracy 0.12222222983837128.\n",
      "Test batch 86 with loss 1.728965163230896 and accuracy 0.25555557012557983.\n",
      "Test batch 87 with loss 1.8106744289398193 and accuracy 0.33888888359069824.\n",
      "Test batch 88 with loss 1.7886556386947632 and accuracy 0.236111119389534.\n",
      "Test batch 89 with loss 1.7447153329849243 and accuracy 0.125.\n",
      "Test batch 90 with loss 1.6685588359832764 and accuracy 0.3888888955116272.\n",
      "Test batch 91 with loss 1.8303695917129517 and accuracy 0.2460317462682724.\n",
      "Test batch 92 with loss 1.8091821670532227 and accuracy 0.08888889104127884.\n",
      "Test batch 93 with loss 1.8873933553695679 and accuracy 0.0.\n",
      "Test batch 94 with loss 1.8370110988616943 and accuracy 0.1111111119389534.\n",
      "Test batch 95 with loss 1.9195858240127563 and accuracy 0.29722222685813904.\n",
      "Test batch 96 with loss 1.8925056457519531 and accuracy 0.1071428582072258.\n",
      "Test batch 97 with loss 1.8710706233978271 and accuracy 0.1805555522441864.\n",
      "Test batch 98 with loss 1.8549396991729736 and accuracy 0.204365074634552.\n",
      "Test batch 99 with loss 1.7272306680679321 and accuracy 0.29722222685813904.\n",
      "Test batch 100 with loss 1.7295324802398682 and accuracy 0.31388887763023376.\n",
      "Test batch 101 with loss 1.7214791774749756 and accuracy 0.4305555522441864.\n",
      "Test batch 102 with loss 1.8587783575057983 and accuracy 0.0.\n",
      "Test batch 103 with loss 1.7718623876571655 and accuracy 0.3115079402923584.\n",
      "Test batch 104 with loss 1.8490865230560303 and accuracy 0.21944445371627808.\n",
      "Test batch 105 with loss 1.8065719604492188 and accuracy 0.1666666716337204.\n",
      "Test batch 106 with loss 1.791446328163147 and accuracy 0.12222222983837128.\n",
      "Test batch 107 with loss 1.7779592275619507 and accuracy 0.24166667461395264.\n",
      "Test batch 108 with loss 1.8530610799789429 and accuracy 0.0476190485060215.\n",
      "Test batch 109 with loss 1.7816646099090576 and accuracy 0.12037037312984467.\n",
      "Test batch 110 with loss 1.7908931970596313 and accuracy 0.10833333432674408.\n",
      "Test batch 111 with loss 1.9217220544815063 and accuracy 0.11666667461395264.\n",
      "Test batch 112 with loss 1.8710304498672485 and accuracy 0.0833333358168602.\n",
      "Test batch 113 with loss 1.816292405128479 and accuracy 0.02380952425301075.\n",
      "Test batch 114 with loss 1.8429210186004639 and accuracy 0.2638888955116272.\n",
      "Test batch 115 with loss 1.759386658668518 and accuracy 0.14166666567325592.\n",
      "Test batch 116 with loss 1.8225250244140625 and accuracy 0.21944445371627808.\n",
      "Test batch 117 with loss 1.80275559425354 and accuracy 0.1111111119389534.\n",
      "Test batch 118 with loss 1.8564538955688477 and accuracy 0.065476194024086.\n",
      "Test batch 119 with loss 1.7866712808609009 and accuracy 0.2888889014720917.\n",
      "Test batch 120 with loss 1.849749207496643 and accuracy 0.14444445073604584.\n",
      "Test batch 121 with loss 1.8791124820709229 and accuracy 0.0555555559694767.\n",
      "Test batch 122 with loss 1.8157867193222046 and accuracy 0.1626984179019928.\n",
      "Test batch 123 with loss 1.7527272701263428 and accuracy 0.0892857164144516.\n",
      "Test batch 124 with loss 1.810860276222229 and accuracy 0.25.\n",
      "Test batch 125 with loss 1.8231666088104248 and accuracy 0.15000000596046448.\n",
      "Test batch 126 with loss 1.8867387771606445 and accuracy 0.0.\n",
      "Test batch 127 with loss 1.7781744003295898 and accuracy 0.26944446563720703.\n",
      "Test batch 128 with loss 1.8266804218292236 and accuracy 0.222222238779068.\n",
      "Test batch 129 with loss 1.8406410217285156 and accuracy 0.1944444477558136.\n",
      "Test batch 130 with loss 1.7697492837905884 and accuracy 0.2750000059604645.\n",
      "Test batch 131 with loss 1.8889662027359009 and accuracy 0.0625.\n",
      "Test batch 132 with loss 1.8299096822738647 and accuracy 0.1626984179019928.\n",
      "Test batch 133 with loss 1.8569437265396118 and accuracy 0.06666667014360428.\n",
      "Test batch 134 with loss 1.8641170263290405 and accuracy 0.02777777798473835.\n",
      "Test batch 135 with loss 1.9346797466278076 and accuracy 0.03333333507180214.\n",
      "Test batch 136 with loss 1.7528547048568726 and accuracy 0.2777777910232544.\n",
      "Test batch 137 with loss 1.9010303020477295 and accuracy 0.125.\n",
      "Test batch 138 with loss 1.9539787769317627 and accuracy 0.0555555559694767.\n",
      "Test batch 139 with loss 1.9055732488632202 and accuracy 0.1944444477558136.\n",
      "Test batch 140 with loss 1.6899398565292358 and accuracy 0.25.\n",
      "Test batch 141 with loss 1.8719167709350586 and accuracy 0.125.\n",
      "Test batch 142 with loss 1.786476731300354 and accuracy 0.21388889849185944.\n",
      "Test batch 143 with loss 1.7603614330291748 and accuracy 0.0833333358168602.\n",
      "Test batch 144 with loss 1.9096362590789795 and accuracy 0.2083333432674408.\n",
      "Test batch 145 with loss 1.820378303527832 and accuracy 0.0833333358168602.\n",
      "Test batch 146 with loss 1.8494418859481812 and accuracy 0.16388890147209167.\n",
      "Test batch 147 with loss 1.852455735206604 and accuracy 0.29722222685813904.\n",
      "Test batch 148 with loss 1.928602933883667 and accuracy 0.03333333507180214.\n",
      "Test batch 149 with loss 1.7277729511260986 and accuracy 0.20000001788139343.\n",
      "Test batch 150 with loss 1.833196997642517 and accuracy 0.190476194024086.\n",
      "Test batch 151 with loss 1.8895717859268188 and accuracy 0.20000000298023224.\n",
      "Test batch 152 with loss 1.7635927200317383 and accuracy 0.31111112236976624.\n",
      "Test batch 153 with loss 1.7001851797103882 and accuracy 0.23888888955116272.\n",
      "Test batch 154 with loss 1.8008184432983398 and accuracy 0.15555556118488312.\n",
      "Test batch 155 with loss 1.9560718536376953 and accuracy 0.2083333432674408.\n",
      "Test batch 156 with loss 1.762298345565796 and accuracy 0.2750000059604645.\n",
      "Test batch 157 with loss 1.866551399230957 and accuracy 0.10833333432674408.\n",
      "Test batch 158 with loss 1.8518621921539307 and accuracy 0.1041666716337204.\n",
      "Test batch 159 with loss 1.869267463684082 and accuracy 0.0972222238779068.\n",
      "Test batch 160 with loss 1.8390048742294312 and accuracy 0.07500000298023224.\n",
      "Test batch 161 with loss 1.7935171127319336 and accuracy 0.23333334922790527.\n",
      "Test batch 162 with loss 1.798795461654663 and accuracy 0.5361111164093018.\n",
      "Test batch 163 with loss 1.7498838901519775 and accuracy 0.22936508059501648.\n",
      "Test batch 164 with loss 1.8479102849960327 and accuracy 0.1031746044754982.\n",
      "Test batch 165 with loss 1.8006852865219116 and accuracy 0.0763888880610466.\n",
      "Test batch 166 with loss 1.9145466089248657 and accuracy 0.2083333432674408.\n",
      "Test batch 167 with loss 1.8533271551132202 and accuracy 0.0416666679084301.\n",
      "Test batch 168 with loss 1.872908592224121 and accuracy 0.1527777761220932.\n",
      "Test batch 169 with loss 1.822028398513794 and accuracy 0.1180555522441864.\n",
      "Test batch 170 with loss 1.7529453039169312 and accuracy 0.14761905372142792.\n",
      "Test batch 171 with loss 1.8679746389389038 and accuracy 0.15000000596046448.\n",
      "Test batch 172 with loss 1.7735611200332642 and accuracy 0.2777777910232544.\n",
      "Test batch 173 with loss 1.8118877410888672 and accuracy 0.1944444477558136.\n",
      "Test batch 174 with loss 1.8643081188201904 and accuracy 0.0555555559694767.\n",
      "Test batch 175 with loss 1.8834378719329834 and accuracy 0.11666667461395264.\n",
      "Test batch 176 with loss 1.7229055166244507 and accuracy 0.17777778208255768.\n",
      "Test batch 177 with loss 1.8756154775619507 and accuracy 0.0476190485060215.\n",
      "Test batch 178 with loss 1.8648502826690674 and accuracy 0.0972222238779068.\n",
      "Test batch 179 with loss 1.7533859014511108 and accuracy 0.125.\n",
      "Test batch 180 with loss 1.724961519241333 and accuracy 0.25462964177131653.\n",
      "Test batch 181 with loss 1.85459303855896 and accuracy 0.125.\n",
      "Test batch 182 with loss 1.8481590747833252 and accuracy 0.0.\n",
      "Test batch 183 with loss 1.7919540405273438 and accuracy 0.08888889104127884.\n",
      "Test batch 184 with loss 1.8757562637329102 and accuracy 0.1944444477558136.\n",
      "Test batch 185 with loss 1.8819262981414795 and accuracy 0.11666667461395264.\n",
      "Test batch 186 with loss 1.8662083148956299 and accuracy 0.2222222238779068.\n",
      "Test batch 187 with loss 1.779990792274475 and accuracy 0.2361111044883728.\n",
      "Test batch 188 with loss 1.738355278968811 and accuracy 0.3166666626930237.\n",
      "Test batch 189 with loss 1.902971863746643 and accuracy 0.25555557012557983.\n",
      "Test batch 190 with loss 1.864797592163086 and accuracy 0.0416666679084301.\n",
      "Test batch 191 with loss 1.8893235921859741 and accuracy 0.14444445073604584.\n",
      "Test batch 192 with loss 1.7754474878311157 and accuracy 0.2291666865348816.\n",
      "Test batch 193 with loss 1.8142226934432983 and accuracy 0.16388888657093048.\n",
      "Test batch 194 with loss 1.7422149181365967 and accuracy 0.3777777850627899.\n",
      "Test batch 195 with loss 1.8090169429779053 and accuracy 0.1111111119389534.\n",
      "Test batch 196 with loss 1.8206630945205688 and accuracy 0.20000000298023224.\n",
      "Test batch 197 with loss 1.8047008514404297 and accuracy 0.15000000596046448.\n",
      "Test batch 198 with loss 1.8006055355072021 and accuracy 0.25555557012557983.\n",
      "Test batch 199 with loss 1.833730936050415 and accuracy 0.1944444477558136.\n",
      "Test batch 200 with loss 1.8273111581802368 and accuracy 0.1666666716337204.\n",
      "Test batch 201 with loss 1.8534162044525146 and accuracy 0.2666666805744171.\n",
      "Test batch 202 with loss 1.7870088815689087 and accuracy 0.236111119389534.\n",
      "Test batch 203 with loss 1.8950669765472412 and accuracy 0.0833333358168602.\n",
      "Test batch 204 with loss 1.8546149730682373 and accuracy 0.0416666679084301.\n",
      "Test batch 205 with loss 1.868170142173767 and accuracy 0.1944444477558136.\n",
      "Test batch 206 with loss 1.6972935199737549 and accuracy 0.3611111342906952.\n",
      "Test batch 207 with loss 1.774062156677246 and accuracy 0.0972222238779068.\n",
      "Test batch 208 with loss 1.832972526550293 and accuracy 0.1527777761220932.\n",
      "Test batch 209 with loss 1.885864496231079 and accuracy 0.1041666716337204.\n",
      "Test batch 210 with loss 1.9437446594238281 and accuracy 0.0.\n",
      "Test batch 211 with loss 1.7643859386444092 and accuracy 0.26249998807907104.\n",
      "Test batch 212 with loss 1.9597041606903076 and accuracy 0.1666666716337204.\n",
      "Test batch 213 with loss 1.8962554931640625 and accuracy 0.0694444477558136.\n",
      "Test batch 214 with loss 1.7704761028289795 and accuracy 0.20000000298023224.\n",
      "Test batch 215 with loss 1.8327176570892334 and accuracy 0.17777778208255768.\n",
      "Test batch 216 with loss 1.8099844455718994 and accuracy 0.02777777798473835.\n",
      "Test batch 217 with loss 1.8643691539764404 and accuracy 0.1666666716337204.\n",
      "Test batch 218 with loss 1.879172682762146 and accuracy 0.0833333358168602.\n",
      "Test batch 219 with loss 1.8628076314926147 and accuracy 0.11666667461395264.\n",
      "Test batch 220 with loss 1.762478232383728 and accuracy 0.1111111119389534.\n",
      "Test batch 221 with loss 1.8322893381118774 and accuracy 0.22777777910232544.\n",
      "Test batch 222 with loss 1.75198495388031 and accuracy 0.2638888955116272.\n",
      "Test batch 223 with loss 1.8150125741958618 and accuracy 0.03333333507180214.\n",
      "Test batch 224 with loss 1.8845552206039429 and accuracy 0.17222222685813904.\n",
      "Test batch 225 with loss 1.8383241891860962 and accuracy 0.0972222238779068.\n",
      "Test batch 226 with loss 1.7848020792007446 and accuracy 0.329365074634552.\n",
      "Test batch 227 with loss 1.8459618091583252 and accuracy 0.236111119389534.\n",
      "Test batch 228 with loss 1.8780992031097412 and accuracy 0.0972222238779068.\n",
      "Test batch 229 with loss 1.7989040613174438 and accuracy 0.22777777910232544.\n",
      "Test batch 230 with loss 1.7740061283111572 and accuracy 0.3333333432674408.\n",
      "Test batch 231 with loss 1.8755861520767212 and accuracy 0.0.\n",
      "Test batch 232 with loss 1.8678947687149048 and accuracy 0.2142857164144516.\n",
      "Test batch 233 with loss 1.7362000942230225 and accuracy 0.21388888359069824.\n",
      "Test batch 234 with loss 1.7538220882415771 and accuracy 0.2888889014720917.\n",
      "Test batch 235 with loss 1.785840630531311 and accuracy 0.2916666567325592.\n",
      "Test batch 236 with loss 1.815319299697876 and accuracy 0.2738095223903656.\n",
      "Test batch 237 with loss 1.7899322509765625 and accuracy 0.10277777910232544.\n",
      "Test batch 238 with loss 1.855542540550232 and accuracy 0.0833333358168602.\n",
      "Test batch 239 with loss 1.7865842580795288 and accuracy 0.3611111044883728.\n",
      "Test batch 240 with loss 1.9041850566864014 and accuracy 0.03333333507180214.\n",
      "Test batch 241 with loss 1.8714816570281982 and accuracy 0.11269842088222504.\n",
      "Test batch 242 with loss 1.7971748113632202 and accuracy 0.3444444537162781.\n",
      "Test batch 243 with loss 1.8159339427947998 and accuracy 0.1666666716337204.\n",
      "Test batch 244 with loss 1.8230043649673462 and accuracy 0.1111111119389534.\n",
      "Test batch 245 with loss 1.852298378944397 and accuracy 0.14444445073604584.\n",
      "Test batch 246 with loss 1.7469799518585205 and accuracy 0.222222238779068.\n",
      "Test batch 247 with loss 1.7906417846679688 and accuracy 0.02083333395421505.\n",
      "Test batch 248 with loss 1.8682619333267212 and accuracy 0.10000000894069672.\n",
      "Test batch 249 with loss 1.8595731258392334 and accuracy 0.065476194024086.\n",
      "Test batch 250 with loss 1.9503179788589478 and accuracy 0.0.\n",
      "Test batch 251 with loss 1.69159734249115 and accuracy 0.3333333432674408.\n",
      "Test batch 252 with loss 1.7913854122161865 and accuracy 0.1944444477558136.\n",
      "Test batch 253 with loss 1.7319523096084595 and accuracy 0.236111119389534.\n",
      "Test batch 254 with loss 1.8355748653411865 and accuracy 0.21666666865348816.\n",
      "Test batch 255 with loss 1.805678129196167 and accuracy 0.3194444477558136.\n",
      "Test batch 256 with loss 1.816004991531372 and accuracy 0.13650794327259064.\n",
      "Test batch 257 with loss 1.8251914978027344 and accuracy 0.2777777910232544.\n",
      "Test batch 258 with loss 1.774456262588501 and accuracy 0.14444445073604584.\n",
      "Test batch 259 with loss 1.799576997756958 and accuracy 0.222222238779068.\n",
      "Test batch 260 with loss 1.738092064857483 and accuracy 0.2599206268787384.\n",
      "Test batch 261 with loss 1.7892160415649414 and accuracy 0.1349206417798996.\n",
      "Test batch 262 with loss 1.8916947841644287 and accuracy 0.2876984179019928.\n",
      "Test batch 263 with loss 1.9416425228118896 and accuracy 0.07500000298023224.\n",
      "Test batch 264 with loss 1.8930466175079346 and accuracy 0.25.\n",
      "Test batch 265 with loss 1.8017686605453491 and accuracy 0.07500000298023224.\n",
      "Test batch 266 with loss 1.7480767965316772 and accuracy 0.23888888955116272.\n",
      "Test batch 267 with loss 1.8706655502319336 and accuracy 0.0416666679084301.\n",
      "Test batch 268 with loss 1.7976115942001343 and accuracy 0.17222222685813904.\n",
      "Test batch 269 with loss 1.7714589834213257 and accuracy 0.2708333432674408.\n",
      "Test batch 270 with loss 1.7701642513275146 and accuracy 0.3055555522441864.\n",
      "Test batch 271 with loss 1.852307677268982 and accuracy 0.25555557012557983.\n",
      "Test batch 272 with loss 1.9434309005737305 and accuracy 0.06666667014360428.\n",
      "Test batch 273 with loss 1.856149673461914 and accuracy 0.15000000596046448.\n",
      "Test batch 274 with loss 1.8713096380233765 and accuracy 0.03333333507180214.\n",
      "Test batch 275 with loss 1.7697420120239258 and accuracy 0.1111111119389534.\n",
      "Test batch 276 with loss 1.8599870204925537 and accuracy 0.3055555522441864.\n",
      "Test batch 277 with loss 1.8318830728530884 and accuracy 0.0555555559694767.\n",
      "Test batch 278 with loss 1.7673356533050537 and accuracy 0.0555555559694767.\n",
      "Test batch 279 with loss 1.922197699546814 and accuracy 0.130952388048172.\n",
      "Test batch 280 with loss 1.8065364360809326 and accuracy 0.22777777910232544.\n",
      "Test batch 281 with loss 1.8080705404281616 and accuracy 0.1666666716337204.\n",
      "Test batch 282 with loss 1.8234145641326904 and accuracy 0.3333333432674408.\n",
      "Test batch 283 with loss 1.6664899587631226 and accuracy 0.2638888955116272.\n",
      "Test batch 284 with loss 1.9129022359848022 and accuracy 0.10000000894069672.\n",
      "Test batch 285 with loss 1.6939042806625366 and accuracy 0.6111111044883728.\n",
      "Test batch 286 with loss 1.6519205570220947 and accuracy 0.4305555820465088.\n",
      "Test batch 287 with loss 1.86039137840271 and accuracy 0.125.\n",
      "Test batch 288 with loss 1.9512258768081665 and accuracy 0.1388888955116272.\n",
      "Test batch 289 with loss 1.7732231616973877 and accuracy 0.14166666567325592.\n",
      "Test batch 290 with loss 1.701867699623108 and accuracy 0.14166668057441711.\n",
      "Test batch 291 with loss 1.9158910512924194 and accuracy 0.07500000298023224.\n",
      "Test batch 292 with loss 1.7391622066497803 and accuracy 0.375.\n",
      "Test batch 293 with loss 1.7692596912384033 and accuracy 0.2380952537059784.\n",
      "Test batch 294 with loss 1.8508532047271729 and accuracy 0.0416666679084301.\n",
      "Test batch 295 with loss 1.8566306829452515 and accuracy 0.08888889104127884.\n",
      "Test batch 296 with loss 1.7550703287124634 and accuracy 0.18611110746860504.\n",
      "Test batch 297 with loss 1.8907787799835205 and accuracy 0.12222222983837128.\n",
      "Test batch 298 with loss 1.8351852893829346 and accuracy 0.0833333358168602.\n",
      "Test batch 299 with loss 1.930072546005249 and accuracy 0.03333333507180214.\n",
      "Test batch 300 with loss 1.8118302822113037 and accuracy 0.0694444477558136.\n",
      "Test batch 301 with loss 1.8266277313232422 and accuracy 0.19166666269302368.\n",
      "Test batch 302 with loss 1.907090187072754 and accuracy 0.07500000298023224.\n",
      "Test batch 303 with loss 1.8393046855926514 and accuracy 0.2361111044883728.\n",
      "Test batch 304 with loss 1.860957145690918 and accuracy 0.1388888955116272.\n",
      "Test batch 305 with loss 1.8652738332748413 and accuracy 0.1666666716337204.\n",
      "Test batch 306 with loss 1.7377535104751587 and accuracy 0.222222238779068.\n",
      "Test batch 307 with loss 1.8088315725326538 and accuracy 0.2460317462682724.\n",
      "Test batch 308 with loss 1.818627119064331 and accuracy 0.3333333432674408.\n",
      "Test batch 309 with loss 1.7565422058105469 and accuracy 0.0833333358168602.\n",
      "Test batch 310 with loss 1.7832953929901123 and accuracy 0.0416666679084301.\n",
      "Test batch 311 with loss 1.7308712005615234 and accuracy 0.2370370477437973.\n",
      "Test batch 312 with loss 1.8464387655258179 and accuracy 0.15333333611488342.\n",
      "Test batch 313 with loss 1.7523183822631836 and accuracy 0.24166668951511383.\n",
      "Test batch 314 with loss 1.7986619472503662 and accuracy 0.17222222685813904.\n",
      "Test batch 315 with loss 1.845921516418457 and accuracy 0.17222222685813904.\n",
      "Test batch 316 with loss 1.833193063735962 and accuracy 0.10833333432674408.\n",
      "Test batch 317 with loss 1.8362277746200562 and accuracy 0.3333333432674408.\n",
      "Test batch 318 with loss 1.825073480606079 and accuracy 0.1944444477558136.\n",
      "Test batch 319 with loss 1.8354202508926392 and accuracy 0.15000000596046448.\n",
      "Test batch 320 with loss 1.8392517566680908 and accuracy 0.3055555522441864.\n",
      "Test batch 321 with loss 1.8686702251434326 and accuracy 0.12777778506278992.\n",
      "Test batch 322 with loss 1.7418930530548096 and accuracy 0.1666666716337204.\n",
      "Test batch 323 with loss 1.7423810958862305 and accuracy 0.1626984179019928.\n",
      "Test batch 324 with loss 1.7786591053009033 and accuracy 0.1488095223903656.\n",
      "Test batch 325 with loss 1.8666698932647705 and accuracy 0.1527777761220932.\n",
      "Test batch 326 with loss 1.8739134073257446 and accuracy 0.20370370149612427.\n",
      "Test batch 327 with loss 1.7732445001602173 and accuracy 0.16388888657093048.\n",
      "Test batch 328 with loss 1.8183389902114868 and accuracy 0.2430555522441864.\n",
      "Test batch 329 with loss 1.7888638973236084 and accuracy 0.22777777910232544.\n",
      "Test batch 330 with loss 1.808821439743042 and accuracy 0.3333333432674408.\n",
      "Test batch 331 with loss 1.8206062316894531 and accuracy 0.03333333507180214.\n",
      "Test batch 332 with loss 1.7789866924285889 and accuracy 0.3888888955116272.\n",
      "Test batch 333 with loss 1.8862783908843994 and accuracy 0.11666667461395264.\n",
      "Test batch 334 with loss 1.9011030197143555 and accuracy 0.0416666679084301.\n",
      "Test batch 335 with loss 1.738573431968689 and accuracy 0.2460317611694336.\n",
      "Test batch 336 with loss 1.793755292892456 and accuracy 0.19166667759418488.\n",
      "Test batch 337 with loss 1.8114954233169556 and accuracy 0.3055555820465088.\n",
      "Test batch 338 with loss 1.89340078830719 and accuracy 0.2182539701461792.\n",
      "Test batch 339 with loss 1.742841362953186 and accuracy 0.4333333373069763.\n",
      "Test batch 340 with loss 1.7871599197387695 and accuracy 0.17777778208255768.\n",
      "Test batch 341 with loss 1.8209288120269775 and accuracy 0.1388888955116272.\n",
      "Test batch 342 with loss 1.900470495223999 and accuracy 0.0.\n",
      "Test batch 343 with loss 1.8380889892578125 and accuracy 0.06666667014360428.\n",
      "Test batch 344 with loss 1.8816678524017334 and accuracy 0.02777777798473835.\n",
      "Test batch 345 with loss 1.8533437252044678 and accuracy 0.1805555671453476.\n",
      "Test batch 346 with loss 1.7475534677505493 and accuracy 0.2222222238779068.\n",
      "Test batch 347 with loss 1.8442974090576172 and accuracy 0.14444445073604584.\n",
      "Test batch 348 with loss 1.8109543323516846 and accuracy 0.3948412835597992.\n",
      "Test batch 349 with loss 1.8317241668701172 and accuracy 0.25.\n",
      "Test batch 350 with loss 1.864936113357544 and accuracy 0.0555555559694767.\n",
      "Test batch 351 with loss 1.8208640813827515 and accuracy 0.0833333358168602.\n",
      "Test batch 352 with loss 1.7754619121551514 and accuracy 0.0714285746216774.\n",
      "Test batch 353 with loss 1.823556661605835 and accuracy 0.30000001192092896.\n",
      "Test batch 354 with loss 1.8447612524032593 and accuracy 0.1388888955116272.\n",
      "Test batch 355 with loss 1.7353630065917969 and accuracy 0.1666666716337204.\n",
      "Test batch 356 with loss 1.875718116760254 and accuracy 0.02083333395421505.\n",
      "Test batch 357 with loss 1.8672659397125244 and accuracy 0.15833333134651184.\n",
      "Test batch 358 with loss 1.8066158294677734 and accuracy 0.125.\n",
      "Test batch 359 with loss 1.8896162509918213 and accuracy 0.1666666716337204.\n",
      "Test batch 360 with loss 1.8678324222564697 and accuracy 0.08095238357782364.\n",
      "Test batch 361 with loss 1.8765842914581299 and accuracy 0.0972222238779068.\n",
      "Test batch 362 with loss 1.7442295551300049 and accuracy 0.33888888359069824.\n",
      "Test batch 363 with loss 1.9755685329437256 and accuracy 0.0416666679084301.\n",
      "Test batch 364 with loss 1.931829810142517 and accuracy 0.02777777798473835.\n",
      "Test batch 365 with loss 1.8300670385360718 and accuracy 0.17777778208255768.\n",
      "Test batch 366 with loss 1.8013442754745483 and accuracy 0.0833333358168602.\n",
      "Test batch 367 with loss 1.8493478298187256 and accuracy 0.17222222685813904.\n",
      "Test batch 368 with loss 1.8085275888442993 and accuracy 0.25.\n",
      "Test batch 369 with loss 1.8945255279541016 and accuracy 0.0972222238779068.\n",
      "Test batch 370 with loss 1.7863575220108032 and accuracy 0.10833333432674408.\n",
      "Test batch 371 with loss 1.73334538936615 and accuracy 0.1726190447807312.\n",
      "Test batch 372 with loss 1.8759266138076782 and accuracy 0.1111111119389534.\n",
      "Test batch 373 with loss 1.8876628875732422 and accuracy 0.08888889104127884.\n",
      "Test batch 374 with loss 1.8861534595489502 and accuracy 0.065476194024086.\n",
      "Test batch 375 with loss 1.80538010597229 and accuracy 0.0555555559694767.\n",
      "Test batch 376 with loss 1.959337592124939 and accuracy 0.0416666679084301.\n",
      "Test batch 377 with loss 1.8395559787750244 and accuracy 0.0833333358168602.\n",
      "Test batch 378 with loss 1.739885687828064 and accuracy 0.32870370149612427.\n",
      "Test batch 379 with loss 1.8344335556030273 and accuracy 0.11666666716337204.\n",
      "Test batch 380 with loss 1.7829259634017944 and accuracy 0.190476194024086.\n",
      "Test batch 381 with loss 1.9015105962753296 and accuracy 0.08888889104127884.\n",
      "Test batch 382 with loss 1.832874059677124 and accuracy 0.17222222685813904.\n",
      "Test batch 383 with loss 1.8310731649398804 and accuracy 0.25.\n",
      "Test batch 384 with loss 1.8474937677383423 and accuracy 0.17222222685813904.\n",
      "Test batch 385 with loss 1.940524697303772 and accuracy 0.06111111491918564.\n",
      "Test batch 386 with loss 1.9003410339355469 and accuracy 0.0833333358168602.\n",
      "Test batch 387 with loss 1.8774375915527344 and accuracy 0.236111119389534.\n",
      "Test batch 388 with loss 1.8391023874282837 and accuracy 0.15000000596046448.\n",
      "Test batch 389 with loss 1.8691112995147705 and accuracy 0.3194444477558136.\n",
      "Test batch 390 with loss 1.8546653985977173 and accuracy 0.1527777761220932.\n",
      "Test batch 391 with loss 1.8634722232818604 and accuracy 0.0555555559694767.\n",
      "Test batch 392 with loss 1.7587769031524658 and accuracy 0.125.\n",
      "Test batch 393 with loss 1.7887837886810303 and accuracy 0.3583333492279053.\n",
      "Test batch 394 with loss 1.8874619007110596 and accuracy 0.1111111119389534.\n",
      "Test batch 395 with loss 1.8167707920074463 and accuracy 0.12222222983837128.\n",
      "Test batch 396 with loss 1.8367459774017334 and accuracy 0.1765872985124588.\n",
      "Test batch 397 with loss 1.8412885665893555 and accuracy 0.125.\n",
      "Test batch 398 with loss 1.8183561563491821 and accuracy 0.1814814805984497.\n",
      "Test batch 399 with loss 1.7433607578277588 and accuracy 0.17380952835083008.\n",
      "Test batch 400 with loss 1.9201854467391968 and accuracy 0.1527777761220932.\n",
      "Test batch 401 with loss 1.851070761680603 and accuracy 0.05416666716337204.\n",
      "Test batch 402 with loss 1.8434299230575562 and accuracy 0.06666667014360428.\n",
      "Test batch 403 with loss 1.8115781545639038 and accuracy 0.190476194024086.\n",
      "Test batch 404 with loss 1.831085443496704 and accuracy 0.255952388048172.\n",
      "Test batch 405 with loss 1.9186618328094482 and accuracy 0.1388888955116272.\n",
      "Test batch 406 with loss 1.8681986331939697 and accuracy 0.1111111119389534.\n",
      "Test batch 407 with loss 1.7691549062728882 and accuracy 0.2611111104488373.\n",
      "Test batch 408 with loss 1.7611169815063477 and accuracy 0.19166667759418488.\n",
      "Test batch 409 with loss 1.7320411205291748 and accuracy 0.1666666716337204.\n",
      "Test batch 410 with loss 1.9220130443572998 and accuracy 0.0833333358168602.\n",
      "Test batch 411 with loss 1.798287034034729 and accuracy 0.0833333358168602.\n",
      "Test batch 412 with loss 1.9353176355361938 and accuracy 0.0555555559694767.\n",
      "Test batch 413 with loss 1.77606201171875 and accuracy 0.2708333432674408.\n",
      "Test batch 414 with loss 1.8309810161590576 and accuracy 0.11666667461395264.\n",
      "Test batch 415 with loss 1.86415696144104 and accuracy 0.10833333432674408.\n",
      "Test batch 416 with loss 1.8835842609405518 and accuracy 0.1805555522441864.\n",
      "Test batch 417 with loss 1.8529170751571655 and accuracy 0.3333333432674408.\n",
      "Test batch 418 with loss 1.7075550556182861 and accuracy 0.4305555820465088.\n",
      "Test batch 419 with loss 1.8523766994476318 and accuracy 0.08888889104127884.\n",
      "Test batch 420 with loss 1.792610764503479 and accuracy 0.4583333432674408.\n",
      "Test batch 421 with loss 1.8312809467315674 and accuracy 0.08888889104127884.\n",
      "Test batch 422 with loss 1.796419382095337 and accuracy 0.27936509251594543.\n",
      "Test batch 423 with loss 1.8425037860870361 and accuracy 0.1944444477558136.\n",
      "Test batch 424 with loss 1.6683733463287354 and accuracy 0.5.\n",
      "Test batch 425 with loss 1.79018235206604 and accuracy 0.347222238779068.\n",
      "Test batch 426 with loss 1.7261438369750977 and accuracy 0.44999998807907104.\n",
      "Test batch 427 with loss 1.8866780996322632 and accuracy 0.03333333507180214.\n",
      "Test batch 428 with loss 1.791480302810669 and accuracy 0.1805555522441864.\n",
      "Test batch 429 with loss 1.7727501392364502 and accuracy 0.2750000059604645.\n",
      "Test batch 430 with loss 1.8898378610610962 and accuracy 0.31666669249534607.\n",
      "Test batch 431 with loss 1.8252620697021484 and accuracy 0.14444445073604584.\n",
      "Test batch 432 with loss 1.8498090505599976 and accuracy 0.11666667461395264.\n",
      "Test batch 433 with loss 1.8001277446746826 and accuracy 0.19880953431129456.\n",
      "Test batch 434 with loss 1.855099081993103 and accuracy 0.0833333358168602.\n",
      "Test batch 435 with loss 1.8478692770004272 and accuracy 0.2222222238779068.\n",
      "Test batch 436 with loss 1.9477345943450928 and accuracy 0.02083333395421505.\n",
      "Test batch 437 with loss 1.752481460571289 and accuracy 0.21388889849185944.\n",
      "Test batch 438 with loss 1.870582938194275 and accuracy 0.1111111119389534.\n",
      "Test batch 439 with loss 1.7328054904937744 and accuracy 0.15714286267757416.\n",
      "Test batch 440 with loss 1.9174718856811523 and accuracy 0.08095238357782364.\n",
      "Test batch 441 with loss 1.9475822448730469 and accuracy 0.08888889104127884.\n",
      "Test batch 442 with loss 1.8542273044586182 and accuracy 0.1527777761220932.\n",
      "Test batch 443 with loss 1.8352863788604736 and accuracy 0.0833333358168602.\n",
      "Test batch 444 with loss 1.7708103656768799 and accuracy 0.24722221493721008.\n",
      "Test batch 445 with loss 1.7878408432006836 and accuracy 0.23333334922790527.\n",
      "Test batch 446 with loss 1.8281434774398804 and accuracy 0.1666666716337204.\n",
      "Test batch 447 with loss 1.8862727880477905 and accuracy 0.1527777761220932.\n",
      "Test batch 448 with loss 1.8254423141479492 and accuracy 0.2083333432674408.\n",
      "Test batch 449 with loss 1.915543794631958 and accuracy 0.1805555522441864.\n",
      "Test batch 450 with loss 1.8980348110198975 and accuracy 0.1666666716337204.\n",
      "Test batch 451 with loss 1.8266741037368774 and accuracy 0.125.\n",
      "Test batch 452 with loss 1.8145873546600342 and accuracy 0.10833333432674408.\n",
      "Test batch 453 with loss 1.7279651165008545 and accuracy 0.3333333432674408.\n",
      "Test batch 454 with loss 1.9196302890777588 and accuracy 0.0.\n",
      "Test batch 455 with loss 1.7087106704711914 and accuracy 0.3444444537162781.\n",
      "Test batch 456 with loss 1.8466380834579468 and accuracy 0.18611110746860504.\n",
      "Test batch 457 with loss 1.9198501110076904 and accuracy 0.0.\n",
      "Test batch 458 with loss 1.8402490615844727 and accuracy 0.0793650820851326.\n",
      "Test batch 459 with loss 1.8919357061386108 and accuracy 0.07500000298023224.\n",
      "Test batch 460 with loss 1.834596872329712 and accuracy 0.03333333507180214.\n",
      "Test batch 461 with loss 1.7979021072387695 and accuracy 0.0763888880610466.\n",
      "Test batch 462 with loss 1.7458364963531494 and accuracy 0.1805555522441864.\n",
      "Test batch 463 with loss 1.8470741510391235 and accuracy 0.0833333358168602.\n",
      "Test batch 464 with loss 1.6961561441421509 and accuracy 0.38333332538604736.\n",
      "Test batch 465 with loss 1.8525844812393188 and accuracy 0.07500000298023224.\n",
      "Test batch 466 with loss 1.7872190475463867 and accuracy 0.2666666805744171.\n",
      "Test batch 467 with loss 1.7938247919082642 and accuracy 0.22777777910232544.\n",
      "Test batch 468 with loss 1.892887830734253 and accuracy 0.0972222238779068.\n",
      "Test batch 469 with loss 1.8533481359481812 and accuracy 0.1388888955116272.\n",
      "Test batch 470 with loss 1.8752574920654297 and accuracy 0.1111111119389534.\n",
      "Test batch 471 with loss 1.8663734197616577 and accuracy 0.15000000596046448.\n",
      "Test batch 472 with loss 1.7934213876724243 and accuracy 0.25333333015441895.\n",
      "Test batch 473 with loss 1.9283342361450195 and accuracy 0.0416666679084301.\n",
      "Test batch 474 with loss 1.8212627172470093 and accuracy 0.0694444477558136.\n",
      "Test batch 475 with loss 1.8059196472167969 and accuracy 0.0833333358168602.\n",
      "Test batch 476 with loss 1.8323625326156616 and accuracy 0.14444445073604584.\n",
      "Test batch 477 with loss 1.8748884201049805 and accuracy 0.1388888955116272.\n",
      "Test batch 478 with loss 1.856106162071228 and accuracy 0.15833333134651184.\n",
      "Test batch 479 with loss 1.8034511804580688 and accuracy 0.1527777761220932.\n",
      "Test batch 480 with loss 1.7634357213974 and accuracy 0.23333334922790527.\n",
      "Test batch 481 with loss 1.9701125621795654 and accuracy 0.0416666679084301.\n",
      "Test batch 482 with loss 1.8681771755218506 and accuracy 0.0416666679084301.\n",
      "Test batch 483 with loss 1.8784538507461548 and accuracy 0.08888889104127884.\n",
      "Test batch 484 with loss 1.8623828887939453 and accuracy 0.236111119389534.\n",
      "Test batch 485 with loss 1.8700716495513916 and accuracy 0.20000001788139343.\n",
      "Test batch 486 with loss 1.7641041278839111 and accuracy 0.366666704416275.\n",
      "Test batch 487 with loss 1.8128817081451416 and accuracy 0.2777777910232544.\n",
      "Test batch 488 with loss 1.7664272785186768 and accuracy 0.0555555559694767.\n",
      "Test batch 489 with loss 1.8895190954208374 and accuracy 0.11666667461395264.\n",
      "Test batch 490 with loss 1.8859819173812866 and accuracy 0.06666667014360428.\n",
      "Test batch 491 with loss 1.897411584854126 and accuracy 0.0476190485060215.\n",
      "Test batch 492 with loss 1.7525460720062256 and accuracy 0.17500001192092896.\n",
      "Test batch 493 with loss 1.8601583242416382 and accuracy 0.03333333507180214.\n",
      "Test batch 494 with loss 1.7476612329483032 and accuracy 0.03333333507180214.\n",
      "Test batch 495 with loss 1.8709039688110352 and accuracy 0.2738095223903656.\n",
      "Test batch 496 with loss 1.839056372642517 and accuracy 0.09444445371627808.\n",
      "Test batch 497 with loss 1.9318468570709229 and accuracy 0.0555555559694767.\n",
      "Test batch 498 with loss 1.794277548789978 and accuracy 0.1388888955116272.\n",
      "Test batch 499 with loss 1.8195226192474365 and accuracy 0.20555555820465088.\n",
      "Training epoch 9 batch 0 with loss 1.7421290874481201, accuracy 0.3333333432674408.\n",
      "Training epoch 9 batch 1 with loss 1.7441284656524658, accuracy 0.42222222685813904.\n",
      "Training epoch 9 batch 2 with loss 1.6892290115356445, accuracy 0.2460317462682724.\n",
      "Training epoch 9 batch 3 with loss 1.6355682611465454, accuracy 0.2658730149269104.\n",
      "Training epoch 9 batch 4 with loss 1.7021572589874268, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 5 with loss 1.8539092540740967, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 6 with loss 1.63120436668396, accuracy 0.5388889312744141.\n",
      "Training epoch 9 batch 7 with loss 1.7526471614837646, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 8 with loss 1.689767837524414, accuracy 0.3194444477558136.\n",
      "Training epoch 9 batch 9 with loss 1.7558196783065796, accuracy 0.14166666567325592.\n",
      "Training epoch 9 batch 10 with loss 1.7570915222167969, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 11 with loss 1.818211555480957, accuracy 0.13055555522441864.\n",
      "Training epoch 9 batch 12 with loss 1.6994755268096924, accuracy 0.19166666269302368.\n",
      "Training epoch 9 batch 13 with loss 1.8325872421264648, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 14 with loss 1.6990687847137451, accuracy 0.190476194024086.\n",
      "Training epoch 9 batch 15 with loss 1.8240299224853516, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 16 with loss 1.8222930431365967, accuracy 0.13333334028720856.\n",
      "Training epoch 9 batch 17 with loss 1.7147859334945679, accuracy 0.2063492089509964.\n",
      "Training epoch 9 batch 18 with loss 1.8268547058105469, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 19 with loss 1.884558916091919, accuracy 0.0.\n",
      "Training epoch 9 batch 20 with loss 1.750807523727417, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 21 with loss 1.8735204935073853, accuracy 0.07500000298023224.\n",
      "Training epoch 9 batch 22 with loss 1.6791330575942993, accuracy 0.3083333373069763.\n",
      "Training epoch 9 batch 23 with loss 1.8947827816009521, accuracy 0.0.\n",
      "Training epoch 9 batch 24 with loss 1.69700026512146, accuracy 0.25.\n",
      "Training epoch 9 batch 25 with loss 1.7828330993652344, accuracy 0.43611112236976624.\n",
      "Training epoch 9 batch 26 with loss 1.8013420104980469, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 27 with loss 1.7934678792953491, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 28 with loss 1.7052621841430664, accuracy 0.4027777910232544.\n",
      "Training epoch 9 batch 29 with loss 1.7927942276000977, accuracy 0.08888889104127884.\n",
      "Training epoch 9 batch 30 with loss 1.7419551610946655, accuracy 0.27222222089767456.\n",
      "Training epoch 9 batch 31 with loss 1.6681222915649414, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 32 with loss 1.7648128271102905, accuracy 0.17777778208255768.\n",
      "Training epoch 9 batch 33 with loss 1.7603257894515991, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 34 with loss 1.6752029657363892, accuracy 0.32500001788139343.\n",
      "Training epoch 9 batch 35 with loss 1.8035213947296143, accuracy 0.03333333507180214.\n",
      "Training epoch 9 batch 36 with loss 1.7442238330841064, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 37 with loss 1.7869669198989868, accuracy 0.15833333134651184.\n",
      "Training epoch 9 batch 38 with loss 1.8420616388320923, accuracy 0.2638888955116272.\n",
      "Training epoch 9 batch 39 with loss 1.6612422466278076, accuracy 0.2519841492176056.\n",
      "Training epoch 9 batch 40 with loss 1.7433435916900635, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 41 with loss 1.6550508737564087, accuracy 0.33888888359069824.\n",
      "Training epoch 9 batch 42 with loss 1.760931372642517, accuracy 0.236111119389534.\n",
      "Training epoch 9 batch 43 with loss 1.7352632284164429, accuracy 0.3253968358039856.\n",
      "Training epoch 9 batch 44 with loss 1.7781627178192139, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 45 with loss 1.7234888076782227, accuracy 0.2916666567325592.\n",
      "Training epoch 9 batch 46 with loss 1.8227636814117432, accuracy 0.23888888955116272.\n",
      "Training epoch 9 batch 47 with loss 1.6872695684432983, accuracy 0.4416666626930237.\n",
      "Training epoch 9 batch 48 with loss 1.6726720333099365, accuracy 0.3166666626930237.\n",
      "Training epoch 9 batch 49 with loss 1.616071343421936, accuracy 0.4166666567325592.\n",
      "Training epoch 9 batch 50 with loss 1.6933631896972656, accuracy 0.17777778208255768.\n",
      "Training epoch 9 batch 51 with loss 1.7877756357192993, accuracy 0.125.\n",
      "Training epoch 9 batch 52 with loss 1.8469619750976562, accuracy 0.07500000298023224.\n",
      "Training epoch 9 batch 53 with loss 1.8034003973007202, accuracy 0.0694444477558136.\n",
      "Training epoch 9 batch 54 with loss 1.7019058465957642, accuracy 0.375.\n",
      "Training epoch 9 batch 55 with loss 1.8151590824127197, accuracy 0.125.\n",
      "Training epoch 9 batch 56 with loss 1.7843990325927734, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 57 with loss 1.6439481973648071, accuracy 0.4861111044883728.\n",
      "Training epoch 9 batch 58 with loss 1.7156425714492798, accuracy 0.3333333432674408.\n",
      "Training epoch 9 batch 59 with loss 1.8649237155914307, accuracy 0.15555556118488312.\n",
      "Training epoch 9 batch 60 with loss 1.7221968173980713, accuracy 0.29722222685813904.\n",
      "Training epoch 9 batch 61 with loss 1.7240852117538452, accuracy 0.3611111044883728.\n",
      "Training epoch 9 batch 62 with loss 1.6777732372283936, accuracy 0.36269843578338623.\n",
      "Training epoch 9 batch 63 with loss 1.8461593389511108, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 64 with loss 1.814814805984497, accuracy 0.06666667014360428.\n",
      "Training epoch 9 batch 65 with loss 1.753575086593628, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 66 with loss 1.8433113098144531, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 67 with loss 1.7380033731460571, accuracy 0.20555555820465088.\n",
      "Training epoch 9 batch 68 with loss 1.606003999710083, accuracy 0.21388888359069824.\n",
      "Training epoch 9 batch 69 with loss 1.7371034622192383, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 70 with loss 1.757171630859375, accuracy 0.2916666865348816.\n",
      "Training epoch 9 batch 71 with loss 1.7941067218780518, accuracy 0.03333333507180214.\n",
      "Training epoch 9 batch 72 with loss 1.7629945278167725, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 73 with loss 1.7643216848373413, accuracy 0.19166667759418488.\n",
      "Training epoch 9 batch 74 with loss 1.7796761989593506, accuracy 0.2888889014720917.\n",
      "Training epoch 9 batch 75 with loss 1.8376489877700806, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 76 with loss 1.7281360626220703, accuracy 0.33492064476013184.\n",
      "Training epoch 9 batch 77 with loss 1.835693359375, accuracy 0.15555556118488312.\n",
      "Training epoch 9 batch 78 with loss 1.677747130393982, accuracy 0.2611111104488373.\n",
      "Training epoch 9 batch 79 with loss 1.785821557044983, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 80 with loss 1.783556580543518, accuracy 0.10000000894069672.\n",
      "Training epoch 9 batch 81 with loss 1.7049248218536377, accuracy 0.43611112236976624.\n",
      "Training epoch 9 batch 82 with loss 1.7122137546539307, accuracy 0.3499999940395355.\n",
      "Training epoch 9 batch 83 with loss 1.7565944194793701, accuracy 0.11269841343164444.\n",
      "Training epoch 9 batch 84 with loss 1.8330156803131104, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 85 with loss 1.7644153833389282, accuracy 0.31111112236976624.\n",
      "Training epoch 9 batch 86 with loss 1.708611249923706, accuracy 0.17777778208255768.\n",
      "Training epoch 9 batch 87 with loss 1.7146663665771484, accuracy 0.34166666865348816.\n",
      "Training epoch 9 batch 88 with loss 1.8209320306777954, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 89 with loss 1.7028367519378662, accuracy 0.18333333730697632.\n",
      "Training epoch 9 batch 90 with loss 1.684075951576233, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 91 with loss 1.8143398761749268, accuracy 0.02777777798473835.\n",
      "Training epoch 9 batch 92 with loss 1.7520058155059814, accuracy 0.18333333730697632.\n",
      "Training epoch 9 batch 93 with loss 1.814170479774475, accuracy 0.07500000298023224.\n",
      "Training epoch 9 batch 94 with loss 1.824937105178833, accuracy 0.11574074625968933.\n",
      "Training epoch 9 batch 95 with loss 1.7479829788208008, accuracy 0.25.\n",
      "Training epoch 9 batch 96 with loss 1.7776902914047241, accuracy 0.3194444477558136.\n",
      "Training epoch 9 batch 97 with loss 1.7974437475204468, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 98 with loss 1.8082783222198486, accuracy 0.12222222983837128.\n",
      "Training epoch 9 batch 99 with loss 1.7128766775131226, accuracy 0.21944445371627808.\n",
      "Training epoch 9 batch 100 with loss 1.7490953207015991, accuracy 0.277777761220932.\n",
      "Training epoch 9 batch 101 with loss 1.8152382373809814, accuracy 0.25.\n",
      "Training epoch 9 batch 102 with loss 1.77060067653656, accuracy 0.26547619700431824.\n",
      "Training epoch 9 batch 103 with loss 1.648688554763794, accuracy 0.4861111342906952.\n",
      "Training epoch 9 batch 104 with loss 1.7738109827041626, accuracy 0.1488095223903656.\n",
      "Training epoch 9 batch 105 with loss 1.876169204711914, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 106 with loss 1.8143717050552368, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 107 with loss 1.7609853744506836, accuracy 0.13055555522441864.\n",
      "Training epoch 9 batch 108 with loss 1.794835090637207, accuracy 0.38333332538604736.\n",
      "Training epoch 9 batch 109 with loss 1.7918617725372314, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 110 with loss 1.7344154119491577, accuracy 0.2611111104488373.\n",
      "Training epoch 9 batch 111 with loss 1.744275689125061, accuracy 0.2638888955116272.\n",
      "Training epoch 9 batch 112 with loss 1.7608296871185303, accuracy 0.0694444477558136.\n",
      "Training epoch 9 batch 113 with loss 1.6864608526229858, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 114 with loss 1.77260422706604, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 115 with loss 1.84640634059906, accuracy 0.236111119389534.\n",
      "Training epoch 9 batch 116 with loss 1.7972948551177979, accuracy 0.13333334028720856.\n",
      "Training epoch 9 batch 117 with loss 1.7254585027694702, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 118 with loss 1.7945373058319092, accuracy 0.13055555522441864.\n",
      "Training epoch 9 batch 119 with loss 1.6824438571929932, accuracy 0.15436507761478424.\n",
      "Training epoch 9 batch 120 with loss 1.7351309061050415, accuracy 0.12857143580913544.\n",
      "Training epoch 9 batch 121 with loss 1.840786337852478, accuracy 0.3253968358039856.\n",
      "Training epoch 9 batch 122 with loss 1.7121607065200806, accuracy 0.22539682686328888.\n",
      "Training epoch 9 batch 123 with loss 1.7777713537216187, accuracy 0.190476194024086.\n",
      "Training epoch 9 batch 124 with loss 1.7326809167861938, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 125 with loss 1.7006734609603882, accuracy 0.26944446563720703.\n",
      "Training epoch 9 batch 126 with loss 1.8272209167480469, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 127 with loss 1.794809103012085, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 128 with loss 1.882171392440796, accuracy 0.1805555522441864.\n",
      "Training epoch 9 batch 129 with loss 1.7725746631622314, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 130 with loss 1.775481939315796, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 131 with loss 1.7722381353378296, accuracy 0.1527777910232544.\n",
      "Training epoch 9 batch 132 with loss 1.8298412561416626, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 133 with loss 1.8357508182525635, accuracy 0.02380952425301075.\n",
      "Training epoch 9 batch 134 with loss 1.682204008102417, accuracy 0.3638888895511627.\n",
      "Training epoch 9 batch 135 with loss 1.804625153541565, accuracy 0.17777778208255768.\n",
      "Training epoch 9 batch 136 with loss 1.6997684240341187, accuracy 0.3861111104488373.\n",
      "Training epoch 9 batch 137 with loss 1.7717907428741455, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 138 with loss 1.7333863973617554, accuracy 0.3194444477558136.\n",
      "Training epoch 9 batch 139 with loss 1.7914730310440063, accuracy 0.2638888955116272.\n",
      "Training epoch 9 batch 140 with loss 1.791913390159607, accuracy 0.29722222685813904.\n",
      "Training epoch 9 batch 141 with loss 1.726100206375122, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 142 with loss 1.7603328227996826, accuracy 0.19166667759418488.\n",
      "Training epoch 9 batch 143 with loss 1.698386788368225, accuracy 0.31666669249534607.\n",
      "Training epoch 9 batch 144 with loss 1.8056379556655884, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 145 with loss 1.7184407711029053, accuracy 0.2341269850730896.\n",
      "Training epoch 9 batch 146 with loss 1.834441900253296, accuracy 0.09880952537059784.\n",
      "Training epoch 9 batch 147 with loss 1.8226633071899414, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 148 with loss 1.6972758769989014, accuracy 0.17500001192092896.\n",
      "Training epoch 9 batch 149 with loss 1.713524580001831, accuracy 0.4750000238418579.\n",
      "Training epoch 9 batch 150 with loss 1.6995261907577515, accuracy 0.2865079343318939.\n",
      "Training epoch 9 batch 151 with loss 1.7043476104736328, accuracy 0.1805555522441864.\n",
      "Training epoch 9 batch 152 with loss 1.7180029153823853, accuracy 0.3027777671813965.\n",
      "Training epoch 9 batch 153 with loss 1.6917393207550049, accuracy 0.2111111283302307.\n",
      "Training epoch 9 batch 154 with loss 1.79169499874115, accuracy 0.1805555522441864.\n",
      "Training epoch 9 batch 155 with loss 1.774399995803833, accuracy 0.24166667461395264.\n",
      "Training epoch 9 batch 156 with loss 1.9029735326766968, accuracy 0.08888889104127884.\n",
      "Training epoch 9 batch 157 with loss 1.8693373203277588, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 158 with loss 1.6592216491699219, accuracy 0.24444445967674255.\n",
      "Training epoch 9 batch 159 with loss 1.7197678089141846, accuracy 0.24722221493721008.\n",
      "Training epoch 9 batch 160 with loss 1.7280280590057373, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 161 with loss 1.8230470418930054, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 162 with loss 1.768843412399292, accuracy 0.28333333134651184.\n",
      "Training epoch 9 batch 163 with loss 1.865517258644104, accuracy 0.13650794327259064.\n",
      "Training epoch 9 batch 164 with loss 1.8664045333862305, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 165 with loss 1.8503891229629517, accuracy 0.3055555522441864.\n",
      "Training epoch 9 batch 166 with loss 1.9008255004882812, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 167 with loss 1.7060511112213135, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 168 with loss 1.718077301979065, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 169 with loss 1.8079583644866943, accuracy 0.03333333507180214.\n",
      "Training epoch 9 batch 170 with loss 1.875971794128418, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 171 with loss 1.695492148399353, accuracy 0.36666667461395264.\n",
      "Training epoch 9 batch 172 with loss 1.7453361749649048, accuracy 0.2083333283662796.\n",
      "Training epoch 9 batch 173 with loss 1.6368684768676758, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 174 with loss 1.6890885829925537, accuracy 0.20000001788139343.\n",
      "Training epoch 9 batch 175 with loss 1.725402593612671, accuracy 0.15555556118488312.\n",
      "Training epoch 9 batch 176 with loss 1.8290927410125732, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 177 with loss 1.7860803604125977, accuracy 0.25555554032325745.\n",
      "Training epoch 9 batch 178 with loss 1.8140842914581299, accuracy 0.3194444477558136.\n",
      "Training epoch 9 batch 179 with loss 1.709913969039917, accuracy 0.45277777314186096.\n",
      "Training epoch 9 batch 180 with loss 1.785585641860962, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 181 with loss 1.7534382343292236, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 182 with loss 1.7908928394317627, accuracy 0.3055555820465088.\n",
      "Training epoch 9 batch 183 with loss 1.7472946643829346, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 184 with loss 1.7614662647247314, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 185 with loss 1.7179378271102905, accuracy 0.33888888359069824.\n",
      "Training epoch 9 batch 186 with loss 1.7003198862075806, accuracy 0.2380952537059784.\n",
      "Training epoch 9 batch 187 with loss 1.7315161228179932, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 188 with loss 1.8569494485855103, accuracy 0.10000000149011612.\n",
      "Training epoch 9 batch 189 with loss 1.7408568859100342, accuracy 0.42777779698371887.\n",
      "Training epoch 9 batch 190 with loss 1.559826135635376, accuracy 0.31388890743255615.\n",
      "Training epoch 9 batch 191 with loss 1.8776311874389648, accuracy 0.125.\n",
      "Training epoch 9 batch 192 with loss 1.7760145664215088, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 193 with loss 1.7764955759048462, accuracy 0.2043650895357132.\n",
      "Training epoch 9 batch 194 with loss 1.651490569114685, accuracy 0.5305555462837219.\n",
      "Training epoch 9 batch 195 with loss 1.871778130531311, accuracy 0.1547619104385376.\n",
      "Training epoch 9 batch 196 with loss 1.874593734741211, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 197 with loss 1.7856134176254272, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 198 with loss 1.683769941329956, accuracy 0.21666666865348816.\n",
      "Training epoch 9 batch 199 with loss 1.7329931259155273, accuracy 0.15555556118488312.\n",
      "Training epoch 9 batch 200 with loss 1.6949031352996826, accuracy 0.16984127461910248.\n",
      "Training epoch 9 batch 201 with loss 1.8107216358184814, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 202 with loss 1.7558715343475342, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 203 with loss 1.7314074039459229, accuracy 0.261904776096344.\n",
      "Training epoch 9 batch 204 with loss 1.8062832355499268, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 205 with loss 1.8082669973373413, accuracy 0.35277777910232544.\n",
      "Training epoch 9 batch 206 with loss 1.7646617889404297, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 207 with loss 1.8488051891326904, accuracy 0.125.\n",
      "Training epoch 9 batch 208 with loss 1.696459412574768, accuracy 0.14166666567325592.\n",
      "Training epoch 9 batch 209 with loss 1.8742296695709229, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 210 with loss 1.6973001956939697, accuracy 0.3888888657093048.\n",
      "Training epoch 9 batch 211 with loss 1.7383334636688232, accuracy 0.2083333283662796.\n",
      "Training epoch 9 batch 212 with loss 1.7465639114379883, accuracy 0.22499999403953552.\n",
      "Training epoch 9 batch 213 with loss 1.811934471130371, accuracy 0.25555557012557983.\n",
      "Training epoch 9 batch 214 with loss 1.686159372329712, accuracy 0.12777778506278992.\n",
      "Training epoch 9 batch 215 with loss 1.7468669414520264, accuracy 0.20555555820465088.\n",
      "Training epoch 9 batch 216 with loss 1.7790250778198242, accuracy 0.25.\n",
      "Training epoch 9 batch 217 with loss 1.8542989492416382, accuracy 0.1349206417798996.\n",
      "Training epoch 9 batch 218 with loss 1.6200004816055298, accuracy 0.3611111342906952.\n",
      "Training epoch 9 batch 219 with loss 1.7813819646835327, accuracy 0.1805555522441864.\n",
      "Training epoch 9 batch 220 with loss 1.7331154346466064, accuracy 0.22777777910232544.\n",
      "Training epoch 9 batch 221 with loss 1.769529104232788, accuracy 0.2750000059604645.\n",
      "Training epoch 9 batch 222 with loss 1.7365987300872803, accuracy 0.24583333730697632.\n",
      "Training epoch 9 batch 223 with loss 1.7282114028930664, accuracy 0.2638888955116272.\n",
      "Training epoch 9 batch 224 with loss 1.7409547567367554, accuracy 0.11428572237491608.\n",
      "Training epoch 9 batch 225 with loss 1.7471799850463867, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 226 with loss 1.7365772724151611, accuracy 0.3194444477558136.\n",
      "Training epoch 9 batch 227 with loss 1.842519760131836, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 228 with loss 1.7814229726791382, accuracy 0.08888889104127884.\n",
      "Training epoch 9 batch 229 with loss 1.680276870727539, accuracy 0.36666667461395264.\n",
      "Training epoch 9 batch 230 with loss 1.7455183267593384, accuracy 0.13333334028720856.\n",
      "Training epoch 9 batch 231 with loss 1.6625627279281616, accuracy 0.25.\n",
      "Training epoch 9 batch 232 with loss 1.8189513683319092, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 233 with loss 1.7607905864715576, accuracy 0.2142857313156128.\n",
      "Training epoch 9 batch 234 with loss 1.7452056407928467, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 235 with loss 1.835076928138733, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 236 with loss 1.7439215183258057, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 237 with loss 1.781791090965271, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 238 with loss 1.8565572500228882, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 239 with loss 1.7563203573226929, accuracy 0.1805555671453476.\n",
      "Training epoch 9 batch 240 with loss 1.6823240518569946, accuracy 0.1458333283662796.\n",
      "Training epoch 9 batch 241 with loss 1.8623796701431274, accuracy 0.3472222089767456.\n",
      "Training epoch 9 batch 242 with loss 1.7263380289077759, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 243 with loss 1.72980535030365, accuracy 0.21666666865348816.\n",
      "Training epoch 9 batch 244 with loss 1.8097184896469116, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 245 with loss 1.729770302772522, accuracy 0.3194444477558136.\n",
      "Training epoch 9 batch 246 with loss 1.7660192251205444, accuracy 0.17500001192092896.\n",
      "Training epoch 9 batch 247 with loss 1.7657209634780884, accuracy 0.25.\n",
      "Training epoch 9 batch 248 with loss 1.79635751247406, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 249 with loss 1.7835851907730103, accuracy 0.2666666805744171.\n",
      "Training epoch 9 batch 250 with loss 1.7322860956192017, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 251 with loss 1.7754350900650024, accuracy 0.3472222089767456.\n",
      "Training epoch 9 batch 252 with loss 1.817866325378418, accuracy 0.12222222983837128.\n",
      "Training epoch 9 batch 253 with loss 1.7493371963500977, accuracy 0.07500000298023224.\n",
      "Training epoch 9 batch 254 with loss 1.8040196895599365, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 255 with loss 1.8055709600448608, accuracy 0.25.\n",
      "Training epoch 9 batch 256 with loss 1.8112987279891968, accuracy 0.2460317462682724.\n",
      "Training epoch 9 batch 257 with loss 1.6183831691741943, accuracy 0.3583333492279053.\n",
      "Training epoch 9 batch 258 with loss 1.7577518224716187, accuracy 0.1805555671453476.\n",
      "Training epoch 9 batch 259 with loss 1.6864475011825562, accuracy 0.3611111044883728.\n",
      "Training epoch 9 batch 260 with loss 1.766810417175293, accuracy 0.18888889253139496.\n",
      "Training epoch 9 batch 261 with loss 1.7372547388076782, accuracy 0.23333333432674408.\n",
      "Training epoch 9 batch 262 with loss 1.7599375247955322, accuracy 0.25.\n",
      "Training epoch 9 batch 263 with loss 1.7591676712036133, accuracy 0.1865079402923584.\n",
      "Training epoch 9 batch 264 with loss 1.7574564218521118, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 265 with loss 1.91107177734375, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 266 with loss 1.7469606399536133, accuracy 0.2638888955116272.\n",
      "Training epoch 9 batch 267 with loss 1.8657548427581787, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 268 with loss 1.7692102193832397, accuracy 0.3571428656578064.\n",
      "Training epoch 9 batch 269 with loss 1.7594468593597412, accuracy 0.1865079402923584.\n",
      "Training epoch 9 batch 270 with loss 1.7983735799789429, accuracy 0.329365074634552.\n",
      "Training epoch 9 batch 271 with loss 1.8231804370880127, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 272 with loss 1.801218032836914, accuracy 0.3154761791229248.\n",
      "Training epoch 9 batch 273 with loss 1.701951265335083, accuracy 0.3948412835597992.\n",
      "Training epoch 9 batch 274 with loss 1.7608144283294678, accuracy 0.26250001788139343.\n",
      "Training epoch 9 batch 275 with loss 1.8399410247802734, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 276 with loss 1.6546881198883057, accuracy 0.4861111044883728.\n",
      "Training epoch 9 batch 277 with loss 1.8208343982696533, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 278 with loss 1.8450227975845337, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 279 with loss 1.8176519870758057, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 280 with loss 1.7923014163970947, accuracy 0.2142857313156128.\n",
      "Training epoch 9 batch 281 with loss 1.669308066368103, accuracy 0.5092592835426331.\n",
      "Training epoch 9 batch 282 with loss 1.7075672149658203, accuracy 0.24166667461395264.\n",
      "Training epoch 9 batch 283 with loss 1.8663890361785889, accuracy 0.0.\n",
      "Training epoch 9 batch 284 with loss 1.677120566368103, accuracy 0.255952388048172.\n",
      "Training epoch 9 batch 285 with loss 1.7513198852539062, accuracy 0.07500000298023224.\n",
      "Training epoch 9 batch 286 with loss 1.8030979633331299, accuracy 0.10000000894069672.\n",
      "Training epoch 9 batch 287 with loss 1.8042290210723877, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 288 with loss 1.724426507949829, accuracy 0.12222222983837128.\n",
      "Training epoch 9 batch 289 with loss 1.8049676418304443, accuracy 0.1210317462682724.\n",
      "Training epoch 9 batch 290 with loss 1.7719513177871704, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 291 with loss 1.8338491916656494, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 292 with loss 1.8281958103179932, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 293 with loss 1.7444193363189697, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 294 with loss 1.7652448415756226, accuracy 0.17222222685813904.\n",
      "Training epoch 9 batch 295 with loss 1.624756097793579, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 296 with loss 1.920365571975708, accuracy 0.18611112236976624.\n",
      "Training epoch 9 batch 297 with loss 1.6945310831069946, accuracy 0.3194444477558136.\n",
      "Training epoch 9 batch 298 with loss 1.7826837301254272, accuracy 0.2460317611694336.\n",
      "Training epoch 9 batch 299 with loss 1.7800509929656982, accuracy 0.2380952537059784.\n",
      "Training epoch 9 batch 300 with loss 1.7798277139663696, accuracy 0.17499999701976776.\n",
      "Training epoch 9 batch 301 with loss 1.6996724605560303, accuracy 0.2666666805744171.\n",
      "Training epoch 9 batch 302 with loss 1.8824046850204468, accuracy 0.3083333373069763.\n",
      "Training epoch 9 batch 303 with loss 1.7869561910629272, accuracy 0.3333333432674408.\n",
      "Training epoch 9 batch 304 with loss 1.760694146156311, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 305 with loss 1.9299089908599854, accuracy 0.2916666865348816.\n",
      "Training epoch 9 batch 306 with loss 1.7751439809799194, accuracy 0.3333333432674408.\n",
      "Training epoch 9 batch 307 with loss 1.8210773468017578, accuracy 0.33888891339302063.\n",
      "Training epoch 9 batch 308 with loss 1.7632110118865967, accuracy 0.09444444626569748.\n",
      "Training epoch 9 batch 309 with loss 1.7769949436187744, accuracy 0.15833333134651184.\n",
      "Training epoch 9 batch 310 with loss 1.7566686868667603, accuracy 0.16031746566295624.\n",
      "Training epoch 9 batch 311 with loss 1.7972335815429688, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 312 with loss 1.8994882106781006, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 313 with loss 1.7508392333984375, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 314 with loss 1.6969537734985352, accuracy 0.3083333373069763.\n",
      "Training epoch 9 batch 315 with loss 1.848324179649353, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 316 with loss 1.7820497751235962, accuracy 0.12222222983837128.\n",
      "Training epoch 9 batch 317 with loss 1.7647850513458252, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 318 with loss 1.7669620513916016, accuracy 0.22500000894069672.\n",
      "Training epoch 9 batch 319 with loss 1.6826375722885132, accuracy 0.3166666626930237.\n",
      "Training epoch 9 batch 320 with loss 1.828656792640686, accuracy 0.18888889253139496.\n",
      "Training epoch 9 batch 321 with loss 1.7834587097167969, accuracy 0.190476194024086.\n",
      "Training epoch 9 batch 322 with loss 1.8814560174942017, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 323 with loss 1.8496087789535522, accuracy 0.236111119389534.\n",
      "Training epoch 9 batch 324 with loss 1.7186285257339478, accuracy 0.30000001192092896.\n",
      "Training epoch 9 batch 325 with loss 1.7666711807250977, accuracy 0.15833334624767303.\n",
      "Training epoch 9 batch 326 with loss 1.7297977209091187, accuracy 0.3444444537162781.\n",
      "Training epoch 9 batch 327 with loss 1.7699787616729736, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 328 with loss 1.8163427114486694, accuracy 0.2708333432674408.\n",
      "Training epoch 9 batch 329 with loss 1.8334171772003174, accuracy 0.06666667014360428.\n",
      "Training epoch 9 batch 330 with loss 1.6990430355072021, accuracy 0.20555555820465088.\n",
      "Training epoch 9 batch 331 with loss 1.7696412801742554, accuracy 0.1805555671453476.\n",
      "Training epoch 9 batch 332 with loss 1.7938673496246338, accuracy 0.03333333507180214.\n",
      "Training epoch 9 batch 333 with loss 1.8415638208389282, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 334 with loss 1.7879854440689087, accuracy 0.13333334028720856.\n",
      "Training epoch 9 batch 335 with loss 1.7420785427093506, accuracy 0.22777777910232544.\n",
      "Training epoch 9 batch 336 with loss 1.8278192281723022, accuracy 0.2291666716337204.\n",
      "Training epoch 9 batch 337 with loss 1.8501484394073486, accuracy 0.14166666567325592.\n",
      "Training epoch 9 batch 338 with loss 1.8404600620269775, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 339 with loss 1.7437117099761963, accuracy 0.1041666716337204.\n",
      "Training epoch 9 batch 340 with loss 1.7829744815826416, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 341 with loss 1.7647994756698608, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 342 with loss 1.8059206008911133, accuracy 0.28333336114883423.\n",
      "Training epoch 9 batch 343 with loss 1.8184341192245483, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 344 with loss 1.8181827068328857, accuracy 0.18333333730697632.\n",
      "Training epoch 9 batch 345 with loss 1.6948089599609375, accuracy 0.4059523940086365.\n",
      "Training epoch 9 batch 346 with loss 1.8834145069122314, accuracy 0.0.\n",
      "Training epoch 9 batch 347 with loss 1.7765544652938843, accuracy 0.2083333283662796.\n",
      "Training epoch 9 batch 348 with loss 1.7628204822540283, accuracy 0.1825396865606308.\n",
      "Training epoch 9 batch 349 with loss 1.7169771194458008, accuracy 0.2361111044883728.\n",
      "Training epoch 9 batch 350 with loss 1.7923858165740967, accuracy 0.03333333507180214.\n",
      "Training epoch 9 batch 351 with loss 1.7703447341918945, accuracy 0.3055555820465088.\n",
      "Training epoch 9 batch 352 with loss 1.9083728790283203, accuracy 0.16388890147209167.\n",
      "Training epoch 9 batch 353 with loss 1.70522940158844, accuracy 0.3055555820465088.\n",
      "Training epoch 9 batch 354 with loss 1.8649883270263672, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 355 with loss 1.713292121887207, accuracy 0.29722222685813904.\n",
      "Training epoch 9 batch 356 with loss 1.822767972946167, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 357 with loss 1.7669185400009155, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 358 with loss 1.7154136896133423, accuracy 0.26944446563720703.\n",
      "Training epoch 9 batch 359 with loss 1.7613521814346313, accuracy 0.2111111283302307.\n",
      "Training epoch 9 batch 360 with loss 1.8296489715576172, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 361 with loss 1.7576744556427002, accuracy 0.08888889104127884.\n",
      "Training epoch 9 batch 362 with loss 1.6396996974945068, accuracy 0.3777777850627899.\n",
      "Training epoch 9 batch 363 with loss 1.7782182693481445, accuracy 0.13055555522441864.\n",
      "Training epoch 9 batch 364 with loss 1.777268648147583, accuracy 0.3888888955116272.\n",
      "Training epoch 9 batch 365 with loss 1.780158281326294, accuracy 0.18333333730697632.\n",
      "Training epoch 9 batch 366 with loss 1.8901317119598389, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 367 with loss 1.721653938293457, accuracy 0.3916666805744171.\n",
      "Training epoch 9 batch 368 with loss 1.7657047510147095, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 369 with loss 1.8090250492095947, accuracy 0.125.\n",
      "Training epoch 9 batch 370 with loss 1.7444865703582764, accuracy 0.18611110746860504.\n",
      "Training epoch 9 batch 371 with loss 1.8460935354232788, accuracy 0.10000000894069672.\n",
      "Training epoch 9 batch 372 with loss 1.7116670608520508, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 373 with loss 1.8086566925048828, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 374 with loss 1.857488989830017, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 375 with loss 1.7472941875457764, accuracy 0.18333333730697632.\n",
      "Training epoch 9 batch 376 with loss 1.8584058284759521, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 377 with loss 1.7473576068878174, accuracy 0.31388887763023376.\n",
      "Training epoch 9 batch 378 with loss 1.7409121990203857, accuracy 0.409722238779068.\n",
      "Training epoch 9 batch 379 with loss 1.824981689453125, accuracy 0.21388889849185944.\n",
      "Training epoch 9 batch 380 with loss 1.7097747325897217, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 381 with loss 1.8184988498687744, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 382 with loss 1.752509355545044, accuracy 0.19166667759418488.\n",
      "Training epoch 9 batch 383 with loss 1.7929298877716064, accuracy 0.19166666269302368.\n",
      "Training epoch 9 batch 384 with loss 1.8175767660140991, accuracy 0.3055555522441864.\n",
      "Training epoch 9 batch 385 with loss 1.8048679828643799, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 386 with loss 1.7280642986297607, accuracy 0.125.\n",
      "Training epoch 9 batch 387 with loss 1.8482811450958252, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 388 with loss 1.818058967590332, accuracy 0.2361111044883728.\n",
      "Training epoch 9 batch 389 with loss 1.6936038732528687, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 390 with loss 1.6635520458221436, accuracy 0.25.\n",
      "Training epoch 9 batch 391 with loss 1.7181980609893799, accuracy 0.2888889014720917.\n",
      "Training epoch 9 batch 392 with loss 1.81045663356781, accuracy 0.25555557012557983.\n",
      "Training epoch 9 batch 393 with loss 1.755345106124878, accuracy 0.3194444477558136.\n",
      "Training epoch 9 batch 394 with loss 1.7524887323379517, accuracy 0.07500000298023224.\n",
      "Training epoch 9 batch 395 with loss 1.7816545963287354, accuracy 0.13333334028720856.\n",
      "Training epoch 9 batch 396 with loss 1.7844575643539429, accuracy 0.25.\n",
      "Training epoch 9 batch 397 with loss 1.775827169418335, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 398 with loss 1.8222074508666992, accuracy 0.236111119389534.\n",
      "Training epoch 9 batch 399 with loss 1.7696233987808228, accuracy 0.3333333432674408.\n",
      "Training epoch 9 batch 400 with loss 1.824772596359253, accuracy 0.190476194024086.\n",
      "Training epoch 9 batch 401 with loss 1.848661184310913, accuracy 0.18611112236976624.\n",
      "Training epoch 9 batch 402 with loss 1.7837272882461548, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 403 with loss 1.7798477411270142, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 404 with loss 1.8261457681655884, accuracy 0.23333333432674408.\n",
      "Training epoch 9 batch 405 with loss 1.640027403831482, accuracy 0.42500001192092896.\n",
      "Training epoch 9 batch 406 with loss 1.784688949584961, accuracy 0.15833333134651184.\n",
      "Training epoch 9 batch 407 with loss 1.7744191884994507, accuracy 0.3777777850627899.\n",
      "Training epoch 9 batch 408 with loss 1.811248540878296, accuracy 0.236111119389534.\n",
      "Training epoch 9 batch 409 with loss 1.6846115589141846, accuracy 0.2916666567325592.\n",
      "Training epoch 9 batch 410 with loss 1.701449990272522, accuracy 0.2888889014720917.\n",
      "Training epoch 9 batch 411 with loss 1.8312187194824219, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 412 with loss 1.8252264261245728, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 413 with loss 1.8366835117340088, accuracy 0.22777777910232544.\n",
      "Training epoch 9 batch 414 with loss 1.8649793863296509, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 415 with loss 1.6533699035644531, accuracy 0.4404761791229248.\n",
      "Training epoch 9 batch 416 with loss 1.796614408493042, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 417 with loss 1.8018583059310913, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 418 with loss 1.781937837600708, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 419 with loss 1.7821629047393799, accuracy 0.125.\n",
      "Training epoch 9 batch 420 with loss 1.8817592859268188, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 421 with loss 1.809593915939331, accuracy 0.3055555522441864.\n",
      "Training epoch 9 batch 422 with loss 1.7608600854873657, accuracy 0.18333333730697632.\n",
      "Training epoch 9 batch 423 with loss 1.7419675588607788, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 424 with loss 1.7614774703979492, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 425 with loss 1.6698887348175049, accuracy 0.44999998807907104.\n",
      "Training epoch 9 batch 426 with loss 1.8701457977294922, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 427 with loss 1.7287811040878296, accuracy 0.31111112236976624.\n",
      "Training epoch 9 batch 428 with loss 1.655461311340332, accuracy 0.3888888955116272.\n",
      "Training epoch 9 batch 429 with loss 1.905326247215271, accuracy 0.03333333507180214.\n",
      "Training epoch 9 batch 430 with loss 1.8238357305526733, accuracy 0.0694444477558136.\n",
      "Training epoch 9 batch 431 with loss 1.8310171365737915, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 432 with loss 1.8423620462417603, accuracy 0.07500000298023224.\n",
      "Training epoch 9 batch 433 with loss 1.861901044845581, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 434 with loss 1.8747837543487549, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 435 with loss 1.759116768836975, accuracy 0.19722223281860352.\n",
      "Training epoch 9 batch 436 with loss 1.8409850597381592, accuracy 0.1805555671453476.\n",
      "Training epoch 9 batch 437 with loss 1.7519538402557373, accuracy 0.125.\n",
      "Training epoch 9 batch 438 with loss 1.73760187625885, accuracy 0.24166667461395264.\n",
      "Training epoch 9 batch 439 with loss 1.8832166194915771, accuracy 0.0.\n",
      "Training epoch 9 batch 440 with loss 1.7496287822723389, accuracy 0.1031746044754982.\n",
      "Training epoch 9 batch 441 with loss 1.8557134866714478, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 442 with loss 1.7985570430755615, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 443 with loss 1.756333589553833, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 444 with loss 1.8893287181854248, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 445 with loss 1.7192280292510986, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 446 with loss 1.8648927211761475, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 447 with loss 1.792087197303772, accuracy 0.26944443583488464.\n",
      "Training epoch 9 batch 448 with loss 1.8458753824234009, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 449 with loss 1.7574924230575562, accuracy 0.375.\n",
      "Training epoch 9 batch 450 with loss 1.63833487033844, accuracy 0.2738095223903656.\n",
      "Training epoch 9 batch 451 with loss 1.633568525314331, accuracy 0.25.\n",
      "Training epoch 9 batch 452 with loss 1.9578094482421875, accuracy 0.12222222983837128.\n",
      "Training epoch 9 batch 453 with loss 1.7966177463531494, accuracy 0.47777777910232544.\n",
      "Training epoch 9 batch 454 with loss 1.639574408531189, accuracy 0.46666669845581055.\n",
      "Training epoch 9 batch 455 with loss 1.7650763988494873, accuracy 0.236111119389534.\n",
      "Training epoch 9 batch 456 with loss 1.7442013025283813, accuracy 0.42500001192092896.\n",
      "Training epoch 9 batch 457 with loss 1.9379348754882812, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 458 with loss 1.826332688331604, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 459 with loss 1.7141706943511963, accuracy 0.2888889014720917.\n",
      "Training epoch 9 batch 460 with loss 1.7466129064559937, accuracy 0.23333333432674408.\n",
      "Training epoch 9 batch 461 with loss 1.741633653640747, accuracy 0.4444444477558136.\n",
      "Training epoch 9 batch 462 with loss 1.7809011936187744, accuracy 0.0793650820851326.\n",
      "Training epoch 9 batch 463 with loss 1.7248599529266357, accuracy 0.2750000059604645.\n",
      "Training epoch 9 batch 464 with loss 1.7142822742462158, accuracy 0.2888889014720917.\n",
      "Training epoch 9 batch 465 with loss 1.7956069707870483, accuracy 0.12222222983837128.\n",
      "Training epoch 9 batch 466 with loss 1.7710168361663818, accuracy 0.2599206268787384.\n",
      "Training epoch 9 batch 467 with loss 1.7923297882080078, accuracy 0.17500001192092896.\n",
      "Training epoch 9 batch 468 with loss 1.617047905921936, accuracy 0.35555556416511536.\n",
      "Training epoch 9 batch 469 with loss 1.726933479309082, accuracy 0.25.\n",
      "Training epoch 9 batch 470 with loss 1.7954301834106445, accuracy 0.19166666269302368.\n",
      "Training epoch 9 batch 471 with loss 1.7625318765640259, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 472 with loss 1.7743785381317139, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 473 with loss 1.870621919631958, accuracy 0.24166667461395264.\n",
      "Training epoch 9 batch 474 with loss 1.8307859897613525, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 475 with loss 1.8049061298370361, accuracy 0.3166666626930237.\n",
      "Training epoch 9 batch 476 with loss 1.6871554851531982, accuracy 0.3611111342906952.\n",
      "Training epoch 9 batch 477 with loss 1.7600187063217163, accuracy 0.1805555522441864.\n",
      "Training epoch 9 batch 478 with loss 1.8955901861190796, accuracy 0.02380952425301075.\n",
      "Training epoch 9 batch 479 with loss 1.733664870262146, accuracy 0.12222222983837128.\n",
      "Training epoch 9 batch 480 with loss 1.787560224533081, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 481 with loss 1.8221505880355835, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 482 with loss 1.8219845294952393, accuracy 0.19166666269302368.\n",
      "Training epoch 9 batch 483 with loss 1.7204134464263916, accuracy 0.3166666626930237.\n",
      "Training epoch 9 batch 484 with loss 1.7369333505630493, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 485 with loss 1.8382694721221924, accuracy 0.06666667014360428.\n",
      "Training epoch 9 batch 486 with loss 1.6620903015136719, accuracy 0.18611110746860504.\n",
      "Training epoch 9 batch 487 with loss 1.817299485206604, accuracy 0.36666667461395264.\n",
      "Training epoch 9 batch 488 with loss 1.7870525121688843, accuracy 0.23888888955116272.\n",
      "Training epoch 9 batch 489 with loss 1.9177160263061523, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 490 with loss 1.811460256576538, accuracy 0.3055555820465088.\n",
      "Training epoch 9 batch 491 with loss 1.7583026885986328, accuracy 0.43611112236976624.\n",
      "Training epoch 9 batch 492 with loss 1.8443931341171265, accuracy 0.28333333134651184.\n",
      "Training epoch 9 batch 493 with loss 1.8782918453216553, accuracy 0.0763888880610466.\n",
      "Training epoch 9 batch 494 with loss 1.798387885093689, accuracy 0.18888889253139496.\n",
      "Training epoch 9 batch 495 with loss 1.7595033645629883, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 496 with loss 1.941354513168335, accuracy 0.0.\n",
      "Training epoch 9 batch 497 with loss 1.8145326375961304, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 498 with loss 1.7334293127059937, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 499 with loss 1.8284906148910522, accuracy 0.03333333507180214.\n",
      "Training epoch 9 batch 500 with loss 1.611466646194458, accuracy 0.3499999940395355.\n",
      "Training epoch 9 batch 501 with loss 1.809337854385376, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 502 with loss 1.8649457693099976, accuracy 0.10000000894069672.\n",
      "Training epoch 9 batch 503 with loss 1.8394616842269897, accuracy 0.0892857164144516.\n",
      "Training epoch 9 batch 504 with loss 1.8631995916366577, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 505 with loss 1.806372880935669, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 506 with loss 1.778887391090393, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 507 with loss 1.7553399801254272, accuracy 0.23888888955116272.\n",
      "Training epoch 9 batch 508 with loss 1.7778915166854858, accuracy 0.236111119389534.\n",
      "Training epoch 9 batch 509 with loss 1.8182728290557861, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 510 with loss 1.8402671813964844, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 511 with loss 1.7815183401107788, accuracy 0.3333333432674408.\n",
      "Training epoch 9 batch 512 with loss 1.7356799840927124, accuracy 0.08888889104127884.\n",
      "Training epoch 9 batch 513 with loss 1.8171803951263428, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 514 with loss 1.7659136056900024, accuracy 0.22777777910232544.\n",
      "Training epoch 9 batch 515 with loss 1.855955719947815, accuracy 0.06111111491918564.\n",
      "Training epoch 9 batch 516 with loss 1.8318774700164795, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 517 with loss 1.9846121072769165, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 518 with loss 1.7866531610488892, accuracy 0.0.\n",
      "Training epoch 9 batch 519 with loss 1.678378701210022, accuracy 0.3611111342906952.\n",
      "Training epoch 9 batch 520 with loss 1.8733375072479248, accuracy 0.24166667461395264.\n",
      "Training epoch 9 batch 521 with loss 1.682394027709961, accuracy 0.2738095223903656.\n",
      "Training epoch 9 batch 522 with loss 1.737717628479004, accuracy 0.23333333432674408.\n",
      "Training epoch 9 batch 523 with loss 1.733904242515564, accuracy 0.17936508357524872.\n",
      "Training epoch 9 batch 524 with loss 1.7606914043426514, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 525 with loss 1.806302785873413, accuracy 0.17222222685813904.\n",
      "Training epoch 9 batch 526 with loss 1.6682777404785156, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 527 with loss 1.796010971069336, accuracy 0.15833333134651184.\n",
      "Training epoch 9 batch 528 with loss 1.8556702136993408, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 529 with loss 1.7768837213516235, accuracy 0.22539684176445007.\n",
      "Training epoch 9 batch 530 with loss 1.775159478187561, accuracy 0.03333333507180214.\n",
      "Training epoch 9 batch 531 with loss 1.7471401691436768, accuracy 0.3571428656578064.\n",
      "Training epoch 9 batch 532 with loss 1.8182556629180908, accuracy 0.25.\n",
      "Training epoch 9 batch 533 with loss 1.754105806350708, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 534 with loss 1.7493162155151367, accuracy 0.2888889014720917.\n",
      "Training epoch 9 batch 535 with loss 1.8112964630126953, accuracy 0.125.\n",
      "Training epoch 9 batch 536 with loss 1.8538652658462524, accuracy 0.02777777798473835.\n",
      "Training epoch 9 batch 537 with loss 1.9376182556152344, accuracy 0.02777777798473835.\n",
      "Training epoch 9 batch 538 with loss 1.8105237483978271, accuracy 0.0793650820851326.\n",
      "Training epoch 9 batch 539 with loss 1.6836135387420654, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 540 with loss 1.7300513982772827, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 541 with loss 1.749821662902832, accuracy 0.3571428656578064.\n",
      "Training epoch 9 batch 542 with loss 1.820521354675293, accuracy 0.3333333432674408.\n",
      "Training epoch 9 batch 543 with loss 1.763292908668518, accuracy 0.2944444417953491.\n",
      "Training epoch 9 batch 544 with loss 1.8315538167953491, accuracy 0.22777777910232544.\n",
      "Training epoch 9 batch 545 with loss 1.843491554260254, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 546 with loss 1.7897132635116577, accuracy 0.1825396865606308.\n",
      "Training epoch 9 batch 547 with loss 1.7976230382919312, accuracy 0.2142857313156128.\n",
      "Training epoch 9 batch 548 with loss 1.8849258422851562, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 549 with loss 1.7945270538330078, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 550 with loss 1.6769195795059204, accuracy 0.3611111342906952.\n",
      "Training epoch 9 batch 551 with loss 1.7323524951934814, accuracy 0.25555557012557983.\n",
      "Training epoch 9 batch 552 with loss 1.871683120727539, accuracy 0.1180555522441864.\n",
      "Training epoch 9 batch 553 with loss 1.8130762577056885, accuracy 0.2611111104488373.\n",
      "Training epoch 9 batch 554 with loss 1.791221261024475, accuracy 0.24761904776096344.\n",
      "Training epoch 9 batch 555 with loss 1.7618271112442017, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 556 with loss 1.7827335596084595, accuracy 0.29722222685813904.\n",
      "Training epoch 9 batch 557 with loss 1.743886947631836, accuracy 0.13333334028720856.\n",
      "Training epoch 9 batch 558 with loss 1.954565405845642, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 559 with loss 1.9442682266235352, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 560 with loss 1.7545764446258545, accuracy 0.2611111104488373.\n",
      "Training epoch 9 batch 561 with loss 1.7952282428741455, accuracy 0.25555557012557983.\n",
      "Training epoch 9 batch 562 with loss 1.838195562362671, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 563 with loss 1.911349892616272, accuracy 0.25.\n",
      "Training epoch 9 batch 564 with loss 1.7566429376602173, accuracy 0.3166666626930237.\n",
      "Training epoch 9 batch 565 with loss 1.8623002767562866, accuracy 0.0793650820851326.\n",
      "Training epoch 9 batch 566 with loss 1.7167930603027344, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 567 with loss 1.7906434535980225, accuracy 0.1626984179019928.\n",
      "Training epoch 9 batch 568 with loss 1.9715934991836548, accuracy 0.07999999821186066.\n",
      "Training epoch 9 batch 569 with loss 1.730725884437561, accuracy 0.21388889849185944.\n",
      "Training epoch 9 batch 570 with loss 1.8466771841049194, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 571 with loss 1.7800871133804321, accuracy 0.2916666865348816.\n",
      "Training epoch 9 batch 572 with loss 1.8483139276504517, accuracy 0.07407407462596893.\n",
      "Training epoch 9 batch 573 with loss 1.7267730236053467, accuracy 0.3083333373069763.\n",
      "Training epoch 9 batch 574 with loss 1.7931606769561768, accuracy 0.375.\n",
      "Training epoch 9 batch 575 with loss 1.7315127849578857, accuracy 0.13055555522441864.\n",
      "Training epoch 9 batch 576 with loss 1.7443920373916626, accuracy 0.30000001192092896.\n",
      "Training epoch 9 batch 577 with loss 1.7385116815567017, accuracy 0.2361111044883728.\n",
      "Training epoch 9 batch 578 with loss 1.7842366695404053, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 579 with loss 1.9353373050689697, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 580 with loss 1.7502628564834595, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 581 with loss 1.7364202737808228, accuracy 0.2658730149269104.\n",
      "Training epoch 9 batch 582 with loss 1.8207972049713135, accuracy 0.22777777910232544.\n",
      "Training epoch 9 batch 583 with loss 1.862169623374939, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 584 with loss 1.8198938369750977, accuracy 0.3253968358039856.\n",
      "Training epoch 9 batch 585 with loss 1.7287206649780273, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 586 with loss 1.7615041732788086, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 587 with loss 1.6634094715118408, accuracy 0.3472222089767456.\n",
      "Training epoch 9 batch 588 with loss 1.6071935892105103, accuracy 0.444444477558136.\n",
      "Training epoch 9 batch 589 with loss 1.8485510349273682, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 590 with loss 1.6903877258300781, accuracy 0.402777761220932.\n",
      "Training epoch 9 batch 591 with loss 1.6624139547348022, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 592 with loss 1.794424057006836, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 593 with loss 1.7983471155166626, accuracy 0.261904776096344.\n",
      "Training epoch 9 batch 594 with loss 1.809211015701294, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 595 with loss 1.8269188404083252, accuracy 0.0694444477558136.\n",
      "Training epoch 9 batch 596 with loss 1.8144108057022095, accuracy 0.18333333730697632.\n",
      "Training epoch 9 batch 597 with loss 1.6970021724700928, accuracy 0.33888891339302063.\n",
      "Training epoch 9 batch 598 with loss 1.821598768234253, accuracy 0.0793650820851326.\n",
      "Training epoch 9 batch 599 with loss 1.7441571950912476, accuracy 0.3222222328186035.\n",
      "Training epoch 9 batch 600 with loss 1.6881641149520874, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 601 with loss 1.8028984069824219, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 602 with loss 1.7557640075683594, accuracy 0.3750000298023224.\n",
      "Training epoch 9 batch 603 with loss 1.8190081119537354, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 604 with loss 1.8555529117584229, accuracy 0.2611111104488373.\n",
      "Training epoch 9 batch 605 with loss 1.7207540273666382, accuracy 0.5111111402511597.\n",
      "Training epoch 9 batch 606 with loss 1.7814140319824219, accuracy 0.17222222685813904.\n",
      "Training epoch 9 batch 607 with loss 1.7460132837295532, accuracy 0.2281745970249176.\n",
      "Training epoch 9 batch 608 with loss 1.7452398538589478, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 609 with loss 1.9010846614837646, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 610 with loss 1.7716783285140991, accuracy 0.10000000894069672.\n",
      "Training epoch 9 batch 611 with loss 1.75343918800354, accuracy 0.2638888955116272.\n",
      "Training epoch 9 batch 612 with loss 1.8443164825439453, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 613 with loss 1.809523582458496, accuracy 0.0694444477558136.\n",
      "Training epoch 9 batch 614 with loss 1.7458349466323853, accuracy 0.22777777910232544.\n",
      "Training epoch 9 batch 615 with loss 1.7275216579437256, accuracy 0.09259259700775146.\n",
      "Training epoch 9 batch 616 with loss 1.8349841833114624, accuracy 0.210317462682724.\n",
      "Training epoch 9 batch 617 with loss 1.7922172546386719, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 618 with loss 1.6628456115722656, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 619 with loss 1.8321326971054077, accuracy 0.125.\n",
      "Training epoch 9 batch 620 with loss 1.8485549688339233, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 621 with loss 1.749387502670288, accuracy 0.3055555820465088.\n",
      "Training epoch 9 batch 622 with loss 1.8088626861572266, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 623 with loss 1.8377039432525635, accuracy 0.18333333730697632.\n",
      "Training epoch 9 batch 624 with loss 1.7664563655853271, accuracy 0.13055555522441864.\n",
      "Training epoch 9 batch 625 with loss 1.8435615301132202, accuracy 0.4305555522441864.\n",
      "Training epoch 9 batch 626 with loss 1.848114252090454, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 627 with loss 1.7449709177017212, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 628 with loss 1.6763988733291626, accuracy 0.519444465637207.\n",
      "Training epoch 9 batch 629 with loss 1.7373530864715576, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 630 with loss 1.7647755146026611, accuracy 0.3194444477558136.\n",
      "Training epoch 9 batch 631 with loss 1.8825105428695679, accuracy 0.1269841343164444.\n",
      "Training epoch 9 batch 632 with loss 1.8515352010726929, accuracy 0.03333333507180214.\n",
      "Training epoch 9 batch 633 with loss 1.812579870223999, accuracy 0.22777777910232544.\n",
      "Training epoch 9 batch 634 with loss 1.7169969081878662, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 635 with loss 1.8110301494598389, accuracy 0.17333334684371948.\n",
      "Training epoch 9 batch 636 with loss 1.7730817794799805, accuracy 0.02380952425301075.\n",
      "Training epoch 9 batch 637 with loss 1.6599791049957275, accuracy 0.28333336114883423.\n",
      "Training epoch 9 batch 638 with loss 1.7405903339385986, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 639 with loss 1.8004968166351318, accuracy 0.06111111491918564.\n",
      "Training epoch 9 batch 640 with loss 1.7306993007659912, accuracy 0.3083333373069763.\n",
      "Training epoch 9 batch 641 with loss 1.7837482690811157, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 642 with loss 1.730695366859436, accuracy 0.26944446563720703.\n",
      "Training epoch 9 batch 643 with loss 1.7391713857650757, accuracy 0.2916666567325592.\n",
      "Training epoch 9 batch 644 with loss 1.8820394277572632, accuracy 0.0892857164144516.\n",
      "Training epoch 9 batch 645 with loss 1.719788908958435, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 646 with loss 1.8021434545516968, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 647 with loss 1.7493982315063477, accuracy 0.2916666567325592.\n",
      "Training epoch 9 batch 648 with loss 1.7857757806777954, accuracy 0.21111111342906952.\n",
      "Training epoch 9 batch 649 with loss 1.862012505531311, accuracy 0.02777777798473835.\n",
      "Training epoch 9 batch 650 with loss 1.7060489654541016, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 651 with loss 1.7823143005371094, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 652 with loss 1.7874009609222412, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 653 with loss 1.8339450359344482, accuracy 0.236111119389534.\n",
      "Training epoch 9 batch 654 with loss 1.7239906787872314, accuracy 0.255952388048172.\n",
      "Training epoch 9 batch 655 with loss 1.9197158813476562, accuracy 0.06111111491918564.\n",
      "Training epoch 9 batch 656 with loss 1.6834185123443604, accuracy 0.21388888359069824.\n",
      "Training epoch 9 batch 657 with loss 1.7877219915390015, accuracy 0.14166668057441711.\n",
      "Training epoch 9 batch 658 with loss 1.7529582977294922, accuracy 0.3055555522441864.\n",
      "Training epoch 9 batch 659 with loss 1.698462724685669, accuracy 0.3888888955116272.\n",
      "Training epoch 9 batch 660 with loss 1.6408262252807617, accuracy 0.4000000059604645.\n",
      "Training epoch 9 batch 661 with loss 1.8603607416152954, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 662 with loss 1.7739894390106201, accuracy 0.12222222983837128.\n",
      "Training epoch 9 batch 663 with loss 1.8601211309432983, accuracy 0.0.\n",
      "Training epoch 9 batch 664 with loss 1.7302993535995483, accuracy 0.2611111104488373.\n",
      "Training epoch 9 batch 665 with loss 1.8108810186386108, accuracy 0.28333333134651184.\n",
      "Training epoch 9 batch 666 with loss 1.797609567642212, accuracy 0.19166666269302368.\n",
      "Training epoch 9 batch 667 with loss 1.7945188283920288, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 668 with loss 1.7510524988174438, accuracy 0.23888888955116272.\n",
      "Training epoch 9 batch 669 with loss 1.659764289855957, accuracy 0.4611111283302307.\n",
      "Training epoch 9 batch 670 with loss 1.7882429361343384, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 671 with loss 1.7400243282318115, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 672 with loss 1.7492296695709229, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 673 with loss 1.7810121774673462, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 674 with loss 1.8741047382354736, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 675 with loss 1.7604560852050781, accuracy 0.20555555820465088.\n",
      "Training epoch 9 batch 676 with loss 1.6427257061004639, accuracy 0.3472222089767456.\n",
      "Training epoch 9 batch 677 with loss 1.736270546913147, accuracy 0.13055555522441864.\n",
      "Training epoch 9 batch 678 with loss 1.8372182846069336, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 679 with loss 1.7477222681045532, accuracy 0.4166666865348816.\n",
      "Training epoch 9 batch 680 with loss 1.716723084449768, accuracy 0.5.\n",
      "Training epoch 9 batch 681 with loss 1.7565820217132568, accuracy 0.08750000596046448.\n",
      "Training epoch 9 batch 682 with loss 1.692774772644043, accuracy 0.19761905074119568.\n",
      "Training epoch 9 batch 683 with loss 1.6653032302856445, accuracy 0.3083333373069763.\n",
      "Training epoch 9 batch 684 with loss 1.7710908651351929, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 685 with loss 1.7766492366790771, accuracy 0.25.\n",
      "Training epoch 9 batch 686 with loss 1.6607431173324585, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 687 with loss 1.8467748165130615, accuracy 0.08888889104127884.\n",
      "Training epoch 9 batch 688 with loss 1.8919334411621094, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 689 with loss 1.8418769836425781, accuracy 0.22777777910232544.\n",
      "Training epoch 9 batch 690 with loss 1.6603717803955078, accuracy 0.24920636415481567.\n",
      "Training epoch 9 batch 691 with loss 1.6847898960113525, accuracy 0.46666666865348816.\n",
      "Training epoch 9 batch 692 with loss 1.6771667003631592, accuracy 0.4119047522544861.\n",
      "Training epoch 9 batch 693 with loss 1.8226983547210693, accuracy 0.25.\n",
      "Training epoch 9 batch 694 with loss 1.7380988597869873, accuracy 0.40833333134651184.\n",
      "Training epoch 9 batch 695 with loss 1.762219786643982, accuracy 0.12222222983837128.\n",
      "Training epoch 9 batch 696 with loss 1.7850927114486694, accuracy 0.283730149269104.\n",
      "Training epoch 9 batch 697 with loss 1.8474655151367188, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 698 with loss 1.829748511314392, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 699 with loss 1.7273228168487549, accuracy 0.23333333432674408.\n",
      "Training epoch 9 batch 700 with loss 1.7887623310089111, accuracy 0.1805555522441864.\n",
      "Training epoch 9 batch 701 with loss 1.8382575511932373, accuracy 0.10000000894069672.\n",
      "Training epoch 9 batch 702 with loss 1.7656948566436768, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 703 with loss 1.7796348333358765, accuracy 0.125.\n",
      "Training epoch 9 batch 704 with loss 1.747220754623413, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 705 with loss 1.8452485799789429, accuracy 0.24166667461395264.\n",
      "Training epoch 9 batch 706 with loss 1.8547277450561523, accuracy 0.20000001788139343.\n",
      "Training epoch 9 batch 707 with loss 1.8595969676971436, accuracy 0.08888889104127884.\n",
      "Training epoch 9 batch 708 with loss 1.857033371925354, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 709 with loss 1.8289331197738647, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 710 with loss 1.7841947078704834, accuracy 0.3055555522441864.\n",
      "Training epoch 9 batch 711 with loss 1.8111164569854736, accuracy 0.1041666716337204.\n",
      "Training epoch 9 batch 712 with loss 1.8050390481948853, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 713 with loss 1.8246653079986572, accuracy 0.075396828353405.\n",
      "Training epoch 9 batch 714 with loss 1.7862062454223633, accuracy 0.20000001788139343.\n",
      "Training epoch 9 batch 715 with loss 1.794660210609436, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 716 with loss 1.8583732843399048, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 717 with loss 1.8235588073730469, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 718 with loss 1.7975631952285767, accuracy 0.0694444477558136.\n",
      "Training epoch 9 batch 719 with loss 1.8773682117462158, accuracy 0.17777778208255768.\n",
      "Training epoch 9 batch 720 with loss 1.7603247165679932, accuracy 0.22777777910232544.\n",
      "Training epoch 9 batch 721 with loss 1.7914035320281982, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 722 with loss 1.8419876098632812, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 723 with loss 1.8869777917861938, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 724 with loss 1.7719789743423462, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 725 with loss 1.9052925109863281, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 726 with loss 1.8150556087493896, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 727 with loss 1.7979533672332764, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 728 with loss 1.7422059774398804, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 729 with loss 1.874455213546753, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 730 with loss 1.7788537740707397, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 731 with loss 1.7797000408172607, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 732 with loss 1.8265405893325806, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 733 with loss 1.7689796686172485, accuracy 0.24166667461395264.\n",
      "Training epoch 9 batch 734 with loss 1.860358476638794, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 735 with loss 1.8807296752929688, accuracy 0.0694444477558136.\n",
      "Training epoch 9 batch 736 with loss 1.8043038845062256, accuracy 0.236111119389534.\n",
      "Training epoch 9 batch 737 with loss 1.8031275272369385, accuracy 0.1805555671453476.\n",
      "Training epoch 9 batch 738 with loss 1.7466247081756592, accuracy 0.17222222685813904.\n",
      "Training epoch 9 batch 739 with loss 1.8210242986679077, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 740 with loss 1.7508615255355835, accuracy 0.24722221493721008.\n",
      "Training epoch 9 batch 741 with loss 1.8713772296905518, accuracy 0.0763888880610466.\n",
      "Training epoch 9 batch 742 with loss 1.8191249370574951, accuracy 0.130952388048172.\n",
      "Training epoch 9 batch 743 with loss 1.8543617725372314, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 744 with loss 1.7518165111541748, accuracy 0.125.\n",
      "Training epoch 9 batch 745 with loss 1.7324345111846924, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 746 with loss 1.7387535572052002, accuracy 0.24166667461395264.\n",
      "Training epoch 9 batch 747 with loss 1.7807347774505615, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 748 with loss 1.7050269842147827, accuracy 0.3222222328186035.\n",
      "Training epoch 9 batch 749 with loss 1.8154754638671875, accuracy 0.125.\n",
      "Training epoch 9 batch 750 with loss 1.6981065273284912, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 751 with loss 1.8762552738189697, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 752 with loss 1.6994314193725586, accuracy 0.277777761220932.\n",
      "Training epoch 9 batch 753 with loss 1.617588996887207, accuracy 0.4305555522441864.\n",
      "Training epoch 9 batch 754 with loss 1.8137843608856201, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 755 with loss 1.827275037765503, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 756 with loss 1.7549307346343994, accuracy 0.1805555522441864.\n",
      "Training epoch 9 batch 757 with loss 1.7096103429794312, accuracy 0.29722222685813904.\n",
      "Training epoch 9 batch 758 with loss 1.823339819908142, accuracy 0.10000000149011612.\n",
      "Training epoch 9 batch 759 with loss 1.7677522897720337, accuracy 0.1587301641702652.\n",
      "Training epoch 9 batch 760 with loss 1.7832422256469727, accuracy 0.41428571939468384.\n",
      "Training epoch 9 batch 761 with loss 1.7859652042388916, accuracy 0.25.\n",
      "Training epoch 9 batch 762 with loss 1.694472074508667, accuracy 0.3500000238418579.\n",
      "Training epoch 9 batch 763 with loss 1.6997225284576416, accuracy 0.347222238779068.\n",
      "Training epoch 9 batch 764 with loss 1.9181989431381226, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 765 with loss 1.777801752090454, accuracy 0.17777778208255768.\n",
      "Training epoch 9 batch 766 with loss 1.7304189205169678, accuracy 0.236111119389534.\n",
      "Training epoch 9 batch 767 with loss 1.7038209438323975, accuracy 0.3916666805744171.\n",
      "Training epoch 9 batch 768 with loss 1.797964334487915, accuracy 0.125.\n",
      "Training epoch 9 batch 769 with loss 1.8400605916976929, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 770 with loss 1.799936056137085, accuracy 0.17222222685813904.\n",
      "Training epoch 9 batch 771 with loss 1.6503851413726807, accuracy 0.3722222149372101.\n",
      "Training epoch 9 batch 772 with loss 1.6066560745239258, accuracy 0.23888888955116272.\n",
      "Training epoch 9 batch 773 with loss 1.8083183765411377, accuracy 0.2916666865348816.\n",
      "Training epoch 9 batch 774 with loss 1.709415078163147, accuracy 0.29722222685813904.\n",
      "Training epoch 9 batch 775 with loss 1.7110950946807861, accuracy 0.17500001192092896.\n",
      "Training epoch 9 batch 776 with loss 1.8187118768692017, accuracy 0.32500001788139343.\n",
      "Training epoch 9 batch 777 with loss 1.72481369972229, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 778 with loss 1.8207082748413086, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 779 with loss 1.937246322631836, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 780 with loss 1.7710368633270264, accuracy 0.24722222983837128.\n",
      "Training epoch 9 batch 781 with loss 1.7920253276824951, accuracy 0.30000001192092896.\n",
      "Training epoch 9 batch 782 with loss 1.7604849338531494, accuracy 0.4749999940395355.\n",
      "Training epoch 9 batch 783 with loss 1.7239789962768555, accuracy 0.26111114025115967.\n",
      "Training epoch 9 batch 784 with loss 1.7158876657485962, accuracy 0.07500000298023224.\n",
      "Training epoch 9 batch 785 with loss 1.72846257686615, accuracy 0.2361111342906952.\n",
      "Training epoch 9 batch 786 with loss 1.7970390319824219, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 787 with loss 1.795544981956482, accuracy 0.2460317462682724.\n",
      "Training epoch 9 batch 788 with loss 1.7706016302108765, accuracy 0.2003968209028244.\n",
      "Training epoch 9 batch 789 with loss 1.6958627700805664, accuracy 0.2916666567325592.\n",
      "Training epoch 9 batch 790 with loss 1.8471930027008057, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 791 with loss 1.7503745555877686, accuracy 0.22777779400348663.\n",
      "Training epoch 9 batch 792 with loss 1.7040860652923584, accuracy 0.3452380895614624.\n",
      "Training epoch 9 batch 793 with loss 1.7779061794281006, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 794 with loss 1.7896734476089478, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 795 with loss 1.7254273891448975, accuracy 0.1597222238779068.\n",
      "Training epoch 9 batch 796 with loss 1.8212254047393799, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 797 with loss 1.768654227256775, accuracy 0.12222222983837128.\n",
      "Training epoch 9 batch 798 with loss 1.9103988409042358, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 799 with loss 1.8726236820220947, accuracy 0.16388888657093048.\n",
      "Training epoch 9 batch 800 with loss 1.7949737310409546, accuracy 0.25.\n",
      "Training epoch 9 batch 801 with loss 1.781475305557251, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 802 with loss 1.743107795715332, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 803 with loss 1.8338857889175415, accuracy 0.12222222983837128.\n",
      "Training epoch 9 batch 804 with loss 1.7008056640625, accuracy 0.2708333432674408.\n",
      "Training epoch 9 batch 805 with loss 1.723864197731018, accuracy 0.24166667461395264.\n",
      "Training epoch 9 batch 806 with loss 1.7176620960235596, accuracy 0.24761906266212463.\n",
      "Training epoch 9 batch 807 with loss 1.7821069955825806, accuracy 0.0793650820851326.\n",
      "Training epoch 9 batch 808 with loss 1.8008228540420532, accuracy 0.21984127163887024.\n",
      "Training epoch 9 batch 809 with loss 1.6076523065567017, accuracy 0.2888889014720917.\n",
      "Training epoch 9 batch 810 with loss 1.76675546169281, accuracy 0.14047619700431824.\n",
      "Training epoch 9 batch 811 with loss 1.6950784921646118, accuracy 0.32500001788139343.\n",
      "Training epoch 9 batch 812 with loss 1.7233864068984985, accuracy 0.10000000149011612.\n",
      "Training epoch 9 batch 813 with loss 1.6969308853149414, accuracy 0.32777777314186096.\n",
      "Training epoch 9 batch 814 with loss 1.6988027095794678, accuracy 0.4305555820465088.\n",
      "Training epoch 9 batch 815 with loss 1.8154592514038086, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 816 with loss 1.9197461605072021, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 817 with loss 1.7227805852890015, accuracy 0.20555555820465088.\n",
      "Training epoch 9 batch 818 with loss 1.8243573904037476, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 819 with loss 1.8405555486679077, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 820 with loss 1.793154001235962, accuracy 0.4404761791229248.\n",
      "Training epoch 9 batch 821 with loss 1.7294399738311768, accuracy 0.3055555522441864.\n",
      "Training epoch 9 batch 822 with loss 1.8344815969467163, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 823 with loss 1.7844454050064087, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 824 with loss 1.7495330572128296, accuracy 0.02083333395421505.\n",
      "Training epoch 9 batch 825 with loss 1.8703161478042603, accuracy 0.21666666865348816.\n",
      "Training epoch 9 batch 826 with loss 1.763241171836853, accuracy 0.3611111342906952.\n",
      "Training epoch 9 batch 827 with loss 1.8632150888442993, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 828 with loss 1.777374505996704, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 829 with loss 1.7570583820343018, accuracy 0.277777761220932.\n",
      "Training epoch 9 batch 830 with loss 1.826477289199829, accuracy 0.22777777910232544.\n",
      "Training epoch 9 batch 831 with loss 1.7703516483306885, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 832 with loss 1.6517562866210938, accuracy 0.3365079462528229.\n",
      "Training epoch 9 batch 833 with loss 1.8250997066497803, accuracy 0.25555557012557983.\n",
      "Training epoch 9 batch 834 with loss 1.721639633178711, accuracy 0.2638888955116272.\n",
      "Training epoch 9 batch 835 with loss 1.8504711389541626, accuracy 0.1210317462682724.\n",
      "Training epoch 9 batch 836 with loss 1.7276417016983032, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 837 with loss 1.8018020391464233, accuracy 0.2083333283662796.\n",
      "Training epoch 9 batch 838 with loss 1.7587759494781494, accuracy 0.3194444477558136.\n",
      "Training epoch 9 batch 839 with loss 1.6844637393951416, accuracy 0.3392857313156128.\n",
      "Training epoch 9 batch 840 with loss 1.8317188024520874, accuracy 0.22777777910232544.\n",
      "Training epoch 9 batch 841 with loss 1.5934247970581055, accuracy 0.5833333730697632.\n",
      "Training epoch 9 batch 842 with loss 1.836194396018982, accuracy 0.2944444417953491.\n",
      "Training epoch 9 batch 843 with loss 1.8125507831573486, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 844 with loss 1.792677640914917, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 845 with loss 1.8547319173812866, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 846 with loss 1.8245166540145874, accuracy 0.08888889104127884.\n",
      "Training epoch 9 batch 847 with loss 1.7797237634658813, accuracy 0.3305555582046509.\n",
      "Training epoch 9 batch 848 with loss 1.777377724647522, accuracy 0.125.\n",
      "Training epoch 9 batch 849 with loss 1.8762619495391846, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 850 with loss 1.6940412521362305, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 851 with loss 1.814515471458435, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 852 with loss 1.8691107034683228, accuracy 0.15833333134651184.\n",
      "Training epoch 9 batch 853 with loss 1.9059641361236572, accuracy 0.125.\n",
      "Training epoch 9 batch 854 with loss 1.9224824905395508, accuracy 0.19166667759418488.\n",
      "Training epoch 9 batch 855 with loss 1.787132978439331, accuracy 0.2888889014720917.\n",
      "Training epoch 9 batch 856 with loss 1.7640416622161865, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 857 with loss 1.7698415517807007, accuracy 0.1587301641702652.\n",
      "Training epoch 9 batch 858 with loss 1.883636713027954, accuracy 0.18333333730697632.\n",
      "Training epoch 9 batch 859 with loss 1.854877233505249, accuracy 0.07500000298023224.\n",
      "Training epoch 9 batch 860 with loss 1.8445799350738525, accuracy 0.12222222983837128.\n",
      "Training epoch 9 batch 861 with loss 1.6916797161102295, accuracy 0.3055555522441864.\n",
      "Training epoch 9 batch 862 with loss 1.826115608215332, accuracy 0.39444446563720703.\n",
      "Training epoch 9 batch 863 with loss 1.7973445653915405, accuracy 0.29722222685813904.\n",
      "Training epoch 9 batch 864 with loss 1.7973320484161377, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 865 with loss 1.7324737310409546, accuracy 0.2638888955116272.\n",
      "Training epoch 9 batch 866 with loss 1.809314489364624, accuracy 0.4222222566604614.\n",
      "Training epoch 9 batch 867 with loss 1.9135618209838867, accuracy 0.08888889104127884.\n",
      "Training epoch 9 batch 868 with loss 1.831524133682251, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 869 with loss 1.7759405374526978, accuracy 0.24166667461395264.\n",
      "Training epoch 9 batch 870 with loss 1.773377776145935, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 871 with loss 1.752955436706543, accuracy 0.36666664481163025.\n",
      "Training epoch 9 batch 872 with loss 1.7278236150741577, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 873 with loss 1.786886215209961, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 874 with loss 1.7777307033538818, accuracy 0.2361111044883728.\n",
      "Training epoch 9 batch 875 with loss 1.7990128993988037, accuracy 0.03333333507180214.\n",
      "Training epoch 9 batch 876 with loss 1.8580551147460938, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 877 with loss 1.7566325664520264, accuracy 0.3682539761066437.\n",
      "Training epoch 9 batch 878 with loss 1.7714204788208008, accuracy 0.12222222983837128.\n",
      "Training epoch 9 batch 879 with loss 1.7226232290267944, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 880 with loss 1.8669668436050415, accuracy 0.06666667014360428.\n",
      "Training epoch 9 batch 881 with loss 1.75557541847229, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 882 with loss 1.7301902770996094, accuracy 0.2361111044883728.\n",
      "Training epoch 9 batch 883 with loss 1.7890037298202515, accuracy 0.3055555522441864.\n",
      "Training epoch 9 batch 884 with loss 1.8326126337051392, accuracy 0.17777778208255768.\n",
      "Training epoch 9 batch 885 with loss 1.8649457693099976, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 886 with loss 1.7130568027496338, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 887 with loss 1.8570003509521484, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 888 with loss 1.789839506149292, accuracy 0.125.\n",
      "Training epoch 9 batch 889 with loss 1.923011064529419, accuracy 0.3166666626930237.\n",
      "Training epoch 9 batch 890 with loss 1.7373762130737305, accuracy 0.130952388048172.\n",
      "Training epoch 9 batch 891 with loss 1.7879432439804077, accuracy 0.20555555820465088.\n",
      "Training epoch 9 batch 892 with loss 1.7492843866348267, accuracy 0.2460317462682724.\n",
      "Training epoch 9 batch 893 with loss 1.877934217453003, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 894 with loss 1.7666330337524414, accuracy 0.3638888895511627.\n",
      "Training epoch 9 batch 895 with loss 1.660143494606018, accuracy 0.3194444477558136.\n",
      "Training epoch 9 batch 896 with loss 1.8578382730484009, accuracy 0.15555556118488312.\n",
      "Training epoch 9 batch 897 with loss 1.8384584188461304, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 898 with loss 1.8785350322723389, accuracy 0.20555555820465088.\n",
      "Training epoch 9 batch 899 with loss 1.8252098560333252, accuracy 0.02083333395421505.\n",
      "Training epoch 9 batch 900 with loss 1.8244975805282593, accuracy 0.2430555522441864.\n",
      "Training epoch 9 batch 901 with loss 1.7321622371673584, accuracy 0.4000000059604645.\n",
      "Training epoch 9 batch 902 with loss 1.7908951044082642, accuracy 0.2380952537059784.\n",
      "Training epoch 9 batch 903 with loss 1.7275104522705078, accuracy 0.3166666626930237.\n",
      "Training epoch 9 batch 904 with loss 2.000649929046631, accuracy 0.0.\n",
      "Training epoch 9 batch 905 with loss 1.9028351306915283, accuracy 0.2750000059604645.\n",
      "Training epoch 9 batch 906 with loss 1.822538137435913, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 907 with loss 1.755013108253479, accuracy 0.2460317462682724.\n",
      "Training epoch 9 batch 908 with loss 1.809364676475525, accuracy 0.18333333730697632.\n",
      "Training epoch 9 batch 909 with loss 1.8191667795181274, accuracy 0.2003968358039856.\n",
      "Training epoch 9 batch 910 with loss 1.7997779846191406, accuracy 0.25.\n",
      "Training epoch 9 batch 911 with loss 1.9098663330078125, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 912 with loss 1.8466548919677734, accuracy 0.24166667461395264.\n",
      "Training epoch 9 batch 913 with loss 1.7400811910629272, accuracy 0.3888888955116272.\n",
      "Training epoch 9 batch 914 with loss 1.900058388710022, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 915 with loss 1.8701887130737305, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 916 with loss 1.8298375606536865, accuracy 0.13055555522441864.\n",
      "Training epoch 9 batch 917 with loss 1.775357961654663, accuracy 0.16388888657093048.\n",
      "Training epoch 9 batch 918 with loss 1.7387542724609375, accuracy 0.07500000298023224.\n",
      "Training epoch 9 batch 919 with loss 1.7711864709854126, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 920 with loss 1.8217576742172241, accuracy 0.24166667461395264.\n",
      "Training epoch 9 batch 921 with loss 1.6964832544326782, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 922 with loss 1.8082166910171509, accuracy 0.05714286118745804.\n",
      "Training epoch 9 batch 923 with loss 1.7314141988754272, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 924 with loss 1.8715133666992188, accuracy 0.1071428582072258.\n",
      "Training epoch 9 batch 925 with loss 2.014061450958252, accuracy 0.03333333507180214.\n",
      "Training epoch 9 batch 926 with loss 1.812721610069275, accuracy 0.3055555522441864.\n",
      "Training epoch 9 batch 927 with loss 1.8170177936553955, accuracy 0.2916666567325592.\n",
      "Training epoch 9 batch 928 with loss 1.8636630773544312, accuracy 0.14166666567325592.\n",
      "Training epoch 9 batch 929 with loss 1.8929529190063477, accuracy 0.30000001192092896.\n",
      "Training epoch 9 batch 930 with loss 1.7536500692367554, accuracy 0.06666667014360428.\n",
      "Training epoch 9 batch 931 with loss 1.8534982204437256, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 932 with loss 1.7297881841659546, accuracy 0.26944443583488464.\n",
      "Training epoch 9 batch 933 with loss 1.878974199295044, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 934 with loss 1.7962843179702759, accuracy 0.329365074634552.\n",
      "Training epoch 9 batch 935 with loss 1.8768190145492554, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 936 with loss 1.7264560461044312, accuracy 0.17658731341362.\n",
      "Training epoch 9 batch 937 with loss 1.774317979812622, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 938 with loss 1.8815809488296509, accuracy 0.2571428716182709.\n",
      "Training epoch 9 batch 939 with loss 1.8159892559051514, accuracy 0.3222222328186035.\n",
      "Training epoch 9 batch 940 with loss 1.7211869955062866, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 941 with loss 1.7806644439697266, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 942 with loss 1.8542884588241577, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 943 with loss 1.8951421976089478, accuracy 0.1805555671453476.\n",
      "Training epoch 9 batch 944 with loss 1.7913013696670532, accuracy 0.2944444417953491.\n",
      "Training epoch 9 batch 945 with loss 1.849730134010315, accuracy 0.25555557012557983.\n",
      "Training epoch 9 batch 946 with loss 1.779453992843628, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 947 with loss 1.9671192169189453, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 948 with loss 1.8462769985198975, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 949 with loss 1.837984323501587, accuracy 0.0694444477558136.\n",
      "Training epoch 9 batch 950 with loss 1.8324295282363892, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 951 with loss 1.8733253479003906, accuracy 0.02380952425301075.\n",
      "Training epoch 9 batch 952 with loss 1.8867181539535522, accuracy 0.08888889104127884.\n",
      "Training epoch 9 batch 953 with loss 1.8420588970184326, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 954 with loss 1.7341926097869873, accuracy 0.15833333134651184.\n",
      "Training epoch 9 batch 955 with loss 1.6933692693710327, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 956 with loss 1.7450069189071655, accuracy 0.23888888955116272.\n",
      "Training epoch 9 batch 957 with loss 1.7411000728607178, accuracy 0.38055554032325745.\n",
      "Training epoch 9 batch 958 with loss 1.7622928619384766, accuracy 0.3916666507720947.\n",
      "Training epoch 9 batch 959 with loss 1.854628324508667, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 960 with loss 1.826124906539917, accuracy 0.24166667461395264.\n",
      "Training epoch 9 batch 961 with loss 1.6975809335708618, accuracy 0.21388889849185944.\n",
      "Training epoch 9 batch 962 with loss 1.7588002681732178, accuracy 0.26851850748062134.\n",
      "Training epoch 9 batch 963 with loss 1.7712562084197998, accuracy 0.236111119389534.\n",
      "Training epoch 9 batch 964 with loss 1.8569066524505615, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 965 with loss 1.8456026315689087, accuracy 0.25555557012557983.\n",
      "Training epoch 9 batch 966 with loss 1.8014312982559204, accuracy 0.1964285671710968.\n",
      "Training epoch 9 batch 967 with loss 1.8420007228851318, accuracy 0.2291666567325592.\n",
      "Training epoch 9 batch 968 with loss 1.799640417098999, accuracy 0.1626984179019928.\n",
      "Training epoch 9 batch 969 with loss 1.8920809030532837, accuracy 0.12962962687015533.\n",
      "Training epoch 9 batch 970 with loss 1.7009975910186768, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 971 with loss 1.7883262634277344, accuracy 0.25.\n",
      "Training epoch 9 batch 972 with loss 1.8950846195220947, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 973 with loss 1.706066370010376, accuracy 0.3333333432674408.\n",
      "Training epoch 9 batch 974 with loss 1.6666396856307983, accuracy 0.25.\n",
      "Training epoch 9 batch 975 with loss 1.8197273015975952, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 976 with loss 1.7363011837005615, accuracy 0.1875.\n",
      "Training epoch 9 batch 977 with loss 1.7378981113433838, accuracy 0.16388890147209167.\n",
      "Training epoch 9 batch 978 with loss 1.8283092975616455, accuracy 0.21944445371627808.\n",
      "Training epoch 9 batch 979 with loss 1.8061258792877197, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 980 with loss 1.7581520080566406, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 981 with loss 1.8843543529510498, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 982 with loss 1.6315391063690186, accuracy 0.2638888955116272.\n",
      "Training epoch 9 batch 983 with loss 1.8530813455581665, accuracy 0.17777778208255768.\n",
      "Training epoch 9 batch 984 with loss 1.842201590538025, accuracy 0.15833333134651184.\n",
      "Training epoch 9 batch 985 with loss 1.8090051412582397, accuracy 0.08750000596046448.\n",
      "Training epoch 9 batch 986 with loss 1.7007246017456055, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 987 with loss 1.7932628393173218, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 988 with loss 1.781894326210022, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 989 with loss 1.8665698766708374, accuracy 0.0694444477558136.\n",
      "Training epoch 9 batch 990 with loss 1.864168405532837, accuracy 0.1805555522441864.\n",
      "Training epoch 9 batch 991 with loss 1.7560704946517944, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 992 with loss 1.8903945684432983, accuracy 0.10000000894069672.\n",
      "Training epoch 9 batch 993 with loss 1.7209084033966064, accuracy 0.3777777850627899.\n",
      "Training epoch 9 batch 994 with loss 1.920421838760376, accuracy 0.0.\n",
      "Training epoch 9 batch 995 with loss 1.7715505361557007, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 996 with loss 1.7443042993545532, accuracy 0.2888889014720917.\n",
      "Training epoch 9 batch 997 with loss 1.872605562210083, accuracy 0.10000000149011612.\n",
      "Training epoch 9 batch 998 with loss 1.7532331943511963, accuracy 0.236111119389534.\n",
      "Training epoch 9 batch 999 with loss 1.754077672958374, accuracy 0.144841268658638.\n",
      "Training epoch 9 batch 1000 with loss 1.7300641536712646, accuracy 0.33690476417541504.\n",
      "Training epoch 9 batch 1001 with loss 1.9283511638641357, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 1002 with loss 1.8596588373184204, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1003 with loss 1.7756659984588623, accuracy 0.204365074634552.\n",
      "Training epoch 9 batch 1004 with loss 1.7745498418807983, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1005 with loss 1.8174304962158203, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1006 with loss 1.9760792255401611, accuracy 0.0.\n",
      "Training epoch 9 batch 1007 with loss 1.7672221660614014, accuracy 0.21388888359069824.\n",
      "Training epoch 9 batch 1008 with loss 1.7978719472885132, accuracy 0.3583333492279053.\n",
      "Training epoch 9 batch 1009 with loss 1.7542953491210938, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 1010 with loss 1.8369543552398682, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 1011 with loss 1.794952392578125, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 1012 with loss 1.8584306240081787, accuracy 0.28333333134651184.\n",
      "Training epoch 9 batch 1013 with loss 1.7406151294708252, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 1014 with loss 1.8142019510269165, accuracy 0.17222222685813904.\n",
      "Training epoch 9 batch 1015 with loss 1.9109432697296143, accuracy 0.02083333395421505.\n",
      "Training epoch 9 batch 1016 with loss 1.878392219543457, accuracy 0.0694444477558136.\n",
      "Training epoch 9 batch 1017 with loss 1.8602186441421509, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 1018 with loss 1.8453657627105713, accuracy 0.2182539701461792.\n",
      "Training epoch 9 batch 1019 with loss 1.7812741994857788, accuracy 0.12222222983837128.\n",
      "Training epoch 9 batch 1020 with loss 1.6396312713623047, accuracy 0.3194444477558136.\n",
      "Training epoch 9 batch 1021 with loss 1.9085410833358765, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 1022 with loss 1.752772569656372, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 1023 with loss 1.8313229084014893, accuracy 0.2698412835597992.\n",
      "Training epoch 9 batch 1024 with loss 1.8880383968353271, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 1025 with loss 1.7464878559112549, accuracy 0.3916666805744171.\n",
      "Training epoch 9 batch 1026 with loss 1.8229751586914062, accuracy 0.2013888955116272.\n",
      "Training epoch 9 batch 1027 with loss 1.7292969226837158, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 1028 with loss 1.72793710231781, accuracy 0.2527777850627899.\n",
      "Training epoch 9 batch 1029 with loss 1.6855125427246094, accuracy 0.25.\n",
      "Training epoch 9 batch 1030 with loss 1.683066964149475, accuracy 0.34166669845581055.\n",
      "Training epoch 9 batch 1031 with loss 1.8353769779205322, accuracy 0.17222222685813904.\n",
      "Training epoch 9 batch 1032 with loss 1.8379297256469727, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 1033 with loss 1.7900184392929077, accuracy 0.16388890147209167.\n",
      "Training epoch 9 batch 1034 with loss 1.863154649734497, accuracy 0.13055555522441864.\n",
      "Training epoch 9 batch 1035 with loss 1.7893550395965576, accuracy 0.1805555671453476.\n",
      "Training epoch 9 batch 1036 with loss 1.8623031377792358, accuracy 0.07500000298023224.\n",
      "Training epoch 9 batch 1037 with loss 1.894614577293396, accuracy 0.06111111491918564.\n",
      "Training epoch 9 batch 1038 with loss 1.8193080425262451, accuracy 0.1071428582072258.\n",
      "Training epoch 9 batch 1039 with loss 1.8883936405181885, accuracy 0.08888889104127884.\n",
      "Training epoch 9 batch 1040 with loss 1.7462745904922485, accuracy 0.13055555522441864.\n",
      "Training epoch 9 batch 1041 with loss 1.771990180015564, accuracy 0.08888889104127884.\n",
      "Training epoch 9 batch 1042 with loss 1.7807642221450806, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 1043 with loss 1.7509924173355103, accuracy 0.17222222685813904.\n",
      "Training epoch 9 batch 1044 with loss 1.761810064315796, accuracy 0.23333333432674408.\n",
      "Training epoch 9 batch 1045 with loss 1.8103818893432617, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 1046 with loss 1.8692810535430908, accuracy 0.09880952537059784.\n",
      "Training epoch 9 batch 1047 with loss 1.8255131244659424, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 1048 with loss 1.75298273563385, accuracy 0.347222238779068.\n",
      "Training epoch 9 batch 1049 with loss 1.7394254207611084, accuracy 0.19603174924850464.\n",
      "Training epoch 9 batch 1050 with loss 1.8703445196151733, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 1051 with loss 1.8582398891448975, accuracy 0.17222222685813904.\n",
      "Training epoch 9 batch 1052 with loss 1.7161452770233154, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 1053 with loss 1.764070749282837, accuracy 0.3214285969734192.\n",
      "Training epoch 9 batch 1054 with loss 1.8328393697738647, accuracy 0.20000001788139343.\n",
      "Training epoch 9 batch 1055 with loss 1.7534112930297852, accuracy 0.190476194024086.\n",
      "Training epoch 9 batch 1056 with loss 1.730916976928711, accuracy 0.17916665971279144.\n",
      "Training epoch 9 batch 1057 with loss 1.8050978183746338, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 1058 with loss 1.7709691524505615, accuracy 0.21111111342906952.\n",
      "Training epoch 9 batch 1059 with loss 1.7894401550292969, accuracy 0.24722224473953247.\n",
      "Training epoch 9 batch 1060 with loss 1.8135426044464111, accuracy 0.06666667014360428.\n",
      "Training epoch 9 batch 1061 with loss 1.765586495399475, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 1062 with loss 1.881300687789917, accuracy 0.2182539701461792.\n",
      "Training epoch 9 batch 1063 with loss 1.772449254989624, accuracy 0.18611110746860504.\n",
      "Training epoch 9 batch 1064 with loss 1.76743483543396, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 1065 with loss 1.6687673330307007, accuracy 0.3293651044368744.\n",
      "Training epoch 9 batch 1066 with loss 1.9154770374298096, accuracy 0.10000000894069672.\n",
      "Training epoch 9 batch 1067 with loss 1.9908357858657837, accuracy 0.0.\n",
      "Training epoch 9 batch 1068 with loss 1.8724405765533447, accuracy 0.15833333134651184.\n",
      "Training epoch 9 batch 1069 with loss 1.7806546688079834, accuracy 0.2420634925365448.\n",
      "Training epoch 9 batch 1070 with loss 1.740136742591858, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 1071 with loss 1.8605636358261108, accuracy 0.25.\n",
      "Training epoch 9 batch 1072 with loss 1.8359901905059814, accuracy 0.06666667014360428.\n",
      "Training epoch 9 batch 1073 with loss 1.8682161569595337, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 1074 with loss 1.7762181758880615, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 1075 with loss 1.7852847576141357, accuracy 0.14166666567325592.\n",
      "Training epoch 9 batch 1076 with loss 1.7530568838119507, accuracy 0.22777777910232544.\n",
      "Training epoch 9 batch 1077 with loss 1.7283685207366943, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 1078 with loss 1.7802293300628662, accuracy 0.30873018503189087.\n",
      "Training epoch 9 batch 1079 with loss 1.7088871002197266, accuracy 0.2750000059604645.\n",
      "Training epoch 9 batch 1080 with loss 1.7556486129760742, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 1081 with loss 1.783334493637085, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 1082 with loss 1.8544212579727173, accuracy 0.17777778208255768.\n",
      "Training epoch 9 batch 1083 with loss 1.8247287273406982, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 1084 with loss 1.7997945547103882, accuracy 0.2805555462837219.\n",
      "Training epoch 9 batch 1085 with loss 1.7688219547271729, accuracy 0.28333333134651184.\n",
      "Training epoch 9 batch 1086 with loss 1.803126335144043, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 1087 with loss 1.8071807622909546, accuracy 0.18611110746860504.\n",
      "Training epoch 9 batch 1088 with loss 1.8226072788238525, accuracy 0.15833333134651184.\n",
      "Training epoch 9 batch 1089 with loss 1.9000978469848633, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 1090 with loss 1.7172889709472656, accuracy 0.36666667461395264.\n",
      "Training epoch 9 batch 1091 with loss 1.7036209106445312, accuracy 0.19722223281860352.\n",
      "Training epoch 9 batch 1092 with loss 1.858141541481018, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 1093 with loss 1.8188579082489014, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1094 with loss 1.7757205963134766, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 1095 with loss 1.803400993347168, accuracy 0.2638888955116272.\n",
      "Training epoch 9 batch 1096 with loss 1.7743206024169922, accuracy 0.2916666865348816.\n",
      "Training epoch 9 batch 1097 with loss 1.7384727001190186, accuracy 0.30277779698371887.\n",
      "Training epoch 9 batch 1098 with loss 1.7451133728027344, accuracy 0.19166667759418488.\n",
      "Training epoch 9 batch 1099 with loss 1.855360984802246, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 1100 with loss 1.8240196704864502, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 1101 with loss 1.9135926961898804, accuracy 0.06666667014360428.\n",
      "Training epoch 9 batch 1102 with loss 1.795218825340271, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 1103 with loss 1.8077032566070557, accuracy 0.14166666567325592.\n",
      "Training epoch 9 batch 1104 with loss 1.829837441444397, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 1105 with loss 1.884900450706482, accuracy 0.2876984179019928.\n",
      "Training epoch 9 batch 1106 with loss 1.8215163946151733, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 1107 with loss 1.7456128597259521, accuracy 0.3722222149372101.\n",
      "Training epoch 9 batch 1108 with loss 1.7166038751602173, accuracy 0.2805555462837219.\n",
      "Training epoch 9 batch 1109 with loss 1.8209644556045532, accuracy 0.20555555820465088.\n",
      "Training epoch 9 batch 1110 with loss 1.766236662864685, accuracy 0.2142857164144516.\n",
      "Training epoch 9 batch 1111 with loss 1.8969202041625977, accuracy 0.12037037312984467.\n",
      "Training epoch 9 batch 1112 with loss 1.7801100015640259, accuracy 0.1805555671453476.\n",
      "Training epoch 9 batch 1113 with loss 1.754586935043335, accuracy 0.17222222685813904.\n",
      "Training epoch 9 batch 1114 with loss 1.7508084774017334, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 1115 with loss 1.7917448282241821, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 1116 with loss 1.74471116065979, accuracy 0.2888889014720917.\n",
      "Training epoch 9 batch 1117 with loss 1.7652437686920166, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 1118 with loss 1.834027647972107, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 1119 with loss 1.7841001749038696, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 1120 with loss 1.803682565689087, accuracy 0.30158731341362.\n",
      "Training epoch 9 batch 1121 with loss 1.7634010314941406, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 1122 with loss 1.7483196258544922, accuracy 0.21388888359069824.\n",
      "Training epoch 9 batch 1123 with loss 1.6893266439437866, accuracy 0.1686508059501648.\n",
      "Training epoch 9 batch 1124 with loss 1.8167692422866821, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 1125 with loss 1.8824609518051147, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 1126 with loss 1.824152946472168, accuracy 0.07500000298023224.\n",
      "Training epoch 9 batch 1127 with loss 1.7735869884490967, accuracy 0.39047619700431824.\n",
      "Training epoch 9 batch 1128 with loss 1.7816612720489502, accuracy 0.12777778506278992.\n",
      "Training epoch 9 batch 1129 with loss 1.692869782447815, accuracy 0.3571428656578064.\n",
      "Training epoch 9 batch 1130 with loss 1.8288688659667969, accuracy 0.3194444477558136.\n",
      "Training epoch 9 batch 1131 with loss 1.851605772972107, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 1132 with loss 1.702989935874939, accuracy 0.22499999403953552.\n",
      "Training epoch 9 batch 1133 with loss 1.8314082622528076, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 1134 with loss 1.6787183284759521, accuracy 0.28333333134651184.\n",
      "Training epoch 9 batch 1135 with loss 1.7280433177947998, accuracy 0.13333334028720856.\n",
      "Training epoch 9 batch 1136 with loss 1.770162582397461, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 1137 with loss 1.7847076654434204, accuracy 0.2638888955116272.\n",
      "Training epoch 9 batch 1138 with loss 1.6837308406829834, accuracy 0.16031746566295624.\n",
      "Training epoch 9 batch 1139 with loss 1.8330577611923218, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 1140 with loss 1.7645124197006226, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1141 with loss 1.8796571493148804, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 1142 with loss 1.7311344146728516, accuracy 0.236111119389534.\n",
      "Training epoch 9 batch 1143 with loss 1.8333276510238647, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 1144 with loss 1.7398316860198975, accuracy 0.3361110985279083.\n",
      "Training epoch 9 batch 1145 with loss 1.8182204961776733, accuracy 0.2638888955116272.\n",
      "Training epoch 9 batch 1146 with loss 1.7400680780410767, accuracy 0.3305555582046509.\n",
      "Training epoch 9 batch 1147 with loss 1.7691469192504883, accuracy 0.3166666626930237.\n",
      "Training epoch 9 batch 1148 with loss 1.7912425994873047, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 1149 with loss 1.8390411138534546, accuracy 0.02083333395421505.\n",
      "Training epoch 9 batch 1150 with loss 1.8010733127593994, accuracy 0.18333333730697632.\n",
      "Training epoch 9 batch 1151 with loss 1.765866994857788, accuracy 0.29722222685813904.\n",
      "Training epoch 9 batch 1152 with loss 1.8419466018676758, accuracy 0.07500000298023224.\n",
      "Training epoch 9 batch 1153 with loss 1.7971880435943604, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 1154 with loss 1.6674915552139282, accuracy 0.38055557012557983.\n",
      "Training epoch 9 batch 1155 with loss 1.7162458896636963, accuracy 0.3849206268787384.\n",
      "Training epoch 9 batch 1156 with loss 1.8796775341033936, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 1157 with loss 1.784144639968872, accuracy 0.09880952537059784.\n",
      "Training epoch 9 batch 1158 with loss 1.8505910634994507, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1159 with loss 1.8619331121444702, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 1160 with loss 1.694178819656372, accuracy 0.18611112236976624.\n",
      "Training epoch 9 batch 1161 with loss 1.8338632583618164, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 1162 with loss 1.819814920425415, accuracy 0.2361111044883728.\n",
      "Training epoch 9 batch 1163 with loss 1.642717957496643, accuracy 0.33888891339302063.\n",
      "Training epoch 9 batch 1164 with loss 1.9069868326187134, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1165 with loss 1.827172040939331, accuracy 0.02777777798473835.\n",
      "Training epoch 9 batch 1166 with loss 1.726061224937439, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 1167 with loss 1.7779748439788818, accuracy 0.125.\n",
      "Training epoch 9 batch 1168 with loss 1.903670310974121, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 1169 with loss 1.735656499862671, accuracy 0.3333333432674408.\n",
      "Training epoch 9 batch 1170 with loss 1.7353559732437134, accuracy 0.25.\n",
      "Training epoch 9 batch 1171 with loss 1.814257025718689, accuracy 0.10000000894069672.\n",
      "Training epoch 9 batch 1172 with loss 1.8260494470596313, accuracy 0.2003968358039856.\n",
      "Training epoch 9 batch 1173 with loss 1.8344812393188477, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 1174 with loss 1.6483577489852905, accuracy 0.307539701461792.\n",
      "Training epoch 9 batch 1175 with loss 1.73661208152771, accuracy 0.19722223281860352.\n",
      "Training epoch 9 batch 1176 with loss 1.7334750890731812, accuracy 0.2750000059604645.\n",
      "Training epoch 9 batch 1177 with loss 1.9710743427276611, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 1178 with loss 1.862304925918579, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 1179 with loss 1.7452644109725952, accuracy 0.2321428656578064.\n",
      "Training epoch 9 batch 1180 with loss 1.8191843032836914, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 1181 with loss 1.7472879886627197, accuracy 0.25.\n",
      "Training epoch 9 batch 1182 with loss 1.8046181201934814, accuracy 0.10000000894069672.\n",
      "Training epoch 9 batch 1183 with loss 1.8462955951690674, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 1184 with loss 1.7058433294296265, accuracy 0.1924603134393692.\n",
      "Training epoch 9 batch 1185 with loss 1.8378055095672607, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 1186 with loss 1.8391990661621094, accuracy 0.2916666865348816.\n",
      "Training epoch 9 batch 1187 with loss 1.7785627841949463, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 1188 with loss 1.742491364479065, accuracy 0.3333333432674408.\n",
      "Training epoch 9 batch 1189 with loss 1.7863680124282837, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 1190 with loss 1.7996833324432373, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 1191 with loss 1.7483088970184326, accuracy 0.2638888955116272.\n",
      "Training epoch 9 batch 1192 with loss 1.9107551574707031, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1193 with loss 1.8177753686904907, accuracy 0.31666669249534607.\n",
      "Training epoch 9 batch 1194 with loss 1.9572868347167969, accuracy 0.0.\n",
      "Training epoch 9 batch 1195 with loss 1.8324111700057983, accuracy 0.13055555522441864.\n",
      "Training epoch 9 batch 1196 with loss 1.8232202529907227, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 1197 with loss 1.858994722366333, accuracy 0.17777778208255768.\n",
      "Training epoch 9 batch 1198 with loss 1.8295748233795166, accuracy 0.03333333507180214.\n",
      "Training epoch 9 batch 1199 with loss 1.7289607524871826, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 1200 with loss 1.7909574508666992, accuracy 0.283730149269104.\n",
      "Training epoch 9 batch 1201 with loss 1.7220458984375, accuracy 0.1865079402923584.\n",
      "Training epoch 9 batch 1202 with loss 1.7534459829330444, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 1203 with loss 1.8500553369522095, accuracy 0.19166666269302368.\n",
      "Training epoch 9 batch 1204 with loss 1.7369840145111084, accuracy 0.3472222089767456.\n",
      "Training epoch 9 batch 1205 with loss 1.87965989112854, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1206 with loss 1.806321382522583, accuracy 0.28333333134651184.\n",
      "Training epoch 9 batch 1207 with loss 1.8168224096298218, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 1208 with loss 1.727264642715454, accuracy 0.24444445967674255.\n",
      "Training epoch 9 batch 1209 with loss 1.8375740051269531, accuracy 0.190476194024086.\n",
      "Training epoch 9 batch 1210 with loss 1.7485758066177368, accuracy 0.2797619104385376.\n",
      "Training epoch 9 batch 1211 with loss 1.8064451217651367, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 1212 with loss 1.8613011837005615, accuracy 0.08888889104127884.\n",
      "Training epoch 9 batch 1213 with loss 1.8578917980194092, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 1214 with loss 1.7498794794082642, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 1215 with loss 1.778543472290039, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 1216 with loss 1.7663911581039429, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1217 with loss 1.7141942977905273, accuracy 0.2888889014720917.\n",
      "Training epoch 9 batch 1218 with loss 1.7465527057647705, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 1219 with loss 1.857141137123108, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 1220 with loss 1.8531850576400757, accuracy 0.10972222685813904.\n",
      "Training epoch 9 batch 1221 with loss 1.8403151035308838, accuracy 0.1458333432674408.\n",
      "Training epoch 9 batch 1222 with loss 1.7550321817398071, accuracy 0.22500000894069672.\n",
      "Training epoch 9 batch 1223 with loss 1.7183771133422852, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 1224 with loss 1.7037137746810913, accuracy 0.10277777910232544.\n",
      "Training epoch 9 batch 1225 with loss 1.818250060081482, accuracy 0.1805555671453476.\n",
      "Training epoch 9 batch 1226 with loss 1.7209733724594116, accuracy 0.17222222685813904.\n",
      "Training epoch 9 batch 1227 with loss 1.6945030689239502, accuracy 0.3541666567325592.\n",
      "Training epoch 9 batch 1228 with loss 1.7419296503067017, accuracy 0.15833333134651184.\n",
      "Training epoch 9 batch 1229 with loss 1.8250277042388916, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 1230 with loss 1.8570924997329712, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 1231 with loss 1.8107340335845947, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 1232 with loss 1.6875736713409424, accuracy 0.5277777910232544.\n",
      "Training epoch 9 batch 1233 with loss 1.9953237771987915, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1234 with loss 1.8266770839691162, accuracy 0.06666667014360428.\n",
      "Training epoch 9 batch 1235 with loss 1.7181411981582642, accuracy 0.2888889014720917.\n",
      "Training epoch 9 batch 1236 with loss 1.8758351802825928, accuracy 0.13055555522441864.\n",
      "Training epoch 9 batch 1237 with loss 1.7900712490081787, accuracy 0.3444444537162781.\n",
      "Training epoch 9 batch 1238 with loss 1.891141653060913, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 1239 with loss 1.8559802770614624, accuracy 0.0.\n",
      "Training epoch 9 batch 1240 with loss 1.7928168773651123, accuracy 0.17222222685813904.\n",
      "Training epoch 9 batch 1241 with loss 1.8652822971343994, accuracy 0.25.\n",
      "Training epoch 9 batch 1242 with loss 1.7900121212005615, accuracy 0.2916666567325592.\n",
      "Training epoch 9 batch 1243 with loss 1.7272990942001343, accuracy 0.18333333730697632.\n",
      "Training epoch 9 batch 1244 with loss 1.8279774188995361, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 1245 with loss 1.8505204916000366, accuracy 0.2611111104488373.\n",
      "Training epoch 9 batch 1246 with loss 1.7489172220230103, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 1247 with loss 1.8318551778793335, accuracy 0.2527777850627899.\n",
      "Training epoch 9 batch 1248 with loss 1.7912838459014893, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 1249 with loss 1.8339017629623413, accuracy 0.07500000298023224.\n",
      "Training epoch 9 batch 1250 with loss 1.7409862279891968, accuracy 0.6000000238418579.\n",
      "Training epoch 9 batch 1251 with loss 1.8948123455047607, accuracy 0.06666667014360428.\n",
      "Training epoch 9 batch 1252 with loss 1.8289754390716553, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 1253 with loss 1.864621877670288, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 1254 with loss 1.7704530954360962, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 1255 with loss 1.8371975421905518, accuracy 0.2698412835597992.\n",
      "Training epoch 9 batch 1256 with loss 1.8449974060058594, accuracy 0.15833334624767303.\n",
      "Training epoch 9 batch 1257 with loss 1.8470462560653687, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 1258 with loss 1.8215049505233765, accuracy 0.1210317462682724.\n",
      "Training epoch 9 batch 1259 with loss 1.7671966552734375, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 1260 with loss 1.706891655921936, accuracy 0.2738095223903656.\n",
      "Training epoch 9 batch 1261 with loss 1.7376419305801392, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 1262 with loss 1.7900197505950928, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 1263 with loss 1.825262427330017, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 1264 with loss 1.6265321969985962, accuracy 0.3888888955116272.\n",
      "Training epoch 9 batch 1265 with loss 1.8855693340301514, accuracy 0.09444444626569748.\n",
      "Training epoch 9 batch 1266 with loss 1.8192274570465088, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 1267 with loss 1.9174522161483765, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 1268 with loss 1.938482642173767, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 1269 with loss 1.7542215585708618, accuracy 0.3055555820465088.\n",
      "Training epoch 9 batch 1270 with loss 1.8826539516448975, accuracy 0.03333333507180214.\n",
      "Training epoch 9 batch 1271 with loss 1.7654136419296265, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 1272 with loss 1.7574278116226196, accuracy 0.25.\n",
      "Training epoch 9 batch 1273 with loss 1.8270066976547241, accuracy 0.20000001788139343.\n",
      "Training epoch 9 batch 1274 with loss 1.700343370437622, accuracy 0.261904776096344.\n",
      "Training epoch 9 batch 1275 with loss 1.8345136642456055, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1276 with loss 1.8779287338256836, accuracy 0.125.\n",
      "Training epoch 9 batch 1277 with loss 1.746690034866333, accuracy 0.4000000059604645.\n",
      "Training epoch 9 batch 1278 with loss 1.8021999597549438, accuracy 0.06111111491918564.\n",
      "Training epoch 9 batch 1279 with loss 1.6937296390533447, accuracy 0.3194444477558136.\n",
      "Training epoch 9 batch 1280 with loss 1.7220897674560547, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 1281 with loss 1.7916135787963867, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 1282 with loss 1.846172571182251, accuracy 0.125.\n",
      "Training epoch 9 batch 1283 with loss 1.8026325702667236, accuracy 0.1190476268529892.\n",
      "Training epoch 9 batch 1284 with loss 1.7382957935333252, accuracy 0.15833334624767303.\n",
      "Training epoch 9 batch 1285 with loss 1.8087469339370728, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 1286 with loss 1.8255512714385986, accuracy 0.28333333134651184.\n",
      "Training epoch 9 batch 1287 with loss 1.7794189453125, accuracy 0.19722221791744232.\n",
      "Training epoch 9 batch 1288 with loss 1.8213239908218384, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 1289 with loss 1.8257133960723877, accuracy 0.23888888955116272.\n",
      "Training epoch 9 batch 1290 with loss 1.7680861949920654, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 1291 with loss 1.8721767663955688, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 1292 with loss 1.8987518548965454, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 1293 with loss 1.8398430347442627, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 1294 with loss 1.805692434310913, accuracy 0.13333334028720856.\n",
      "Training epoch 9 batch 1295 with loss 1.786778450012207, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 1296 with loss 1.7101796865463257, accuracy 0.24166667461395264.\n",
      "Training epoch 9 batch 1297 with loss 1.8419559001922607, accuracy 0.1210317462682724.\n",
      "Training epoch 9 batch 1298 with loss 1.8638489246368408, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1299 with loss 1.7668882608413696, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 1300 with loss 1.7460439205169678, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 1301 with loss 1.8818533420562744, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 1302 with loss 1.764626145362854, accuracy 0.15000000596046448.\n",
      "Training epoch 9 batch 1303 with loss 1.80461847782135, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 1304 with loss 1.689146637916565, accuracy 0.4920635223388672.\n",
      "Training epoch 9 batch 1305 with loss 1.8333122730255127, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 1306 with loss 1.7355648279190063, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 1307 with loss 1.7666267156600952, accuracy 0.26250001788139343.\n",
      "Training epoch 9 batch 1308 with loss 1.9070627689361572, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 1309 with loss 1.8056325912475586, accuracy 0.1626984179019928.\n",
      "Training epoch 9 batch 1310 with loss 1.7964096069335938, accuracy 0.1527777761220932.\n",
      "Training epoch 9 batch 1311 with loss 1.726416826248169, accuracy 0.40476194024086.\n",
      "Training epoch 9 batch 1312 with loss 1.7272493839263916, accuracy 0.25.\n",
      "Training epoch 9 batch 1313 with loss 1.688490867614746, accuracy 0.25.\n",
      "Training epoch 9 batch 1314 with loss 1.769852638244629, accuracy 0.1626984179019928.\n",
      "Training epoch 9 batch 1315 with loss 1.8316253423690796, accuracy 0.1210317462682724.\n",
      "Training epoch 9 batch 1316 with loss 1.7597084045410156, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1317 with loss 1.7608518600463867, accuracy 0.1488095223903656.\n",
      "Training epoch 9 batch 1318 with loss 1.8054208755493164, accuracy 0.236111119389534.\n",
      "Training epoch 9 batch 1319 with loss 1.8174737691879272, accuracy 0.31111112236976624.\n",
      "Training epoch 9 batch 1320 with loss 1.6969484090805054, accuracy 0.2777777910232544.\n",
      "Training epoch 9 batch 1321 with loss 1.7631419897079468, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1322 with loss 1.8638769388198853, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 1323 with loss 1.8200404644012451, accuracy 0.32499998807907104.\n",
      "Training epoch 9 batch 1324 with loss 1.834850549697876, accuracy 0.02777777798473835.\n",
      "Training epoch 9 batch 1325 with loss 1.8256126642227173, accuracy 0.0.\n",
      "Training epoch 9 batch 1326 with loss 1.8626213073730469, accuracy 0.25.\n",
      "Training epoch 9 batch 1327 with loss 1.815601110458374, accuracy 0.20555555820465088.\n",
      "Training epoch 9 batch 1328 with loss 1.7860145568847656, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 1329 with loss 1.8650000095367432, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 1330 with loss 1.7402454614639282, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 1331 with loss 1.7781219482421875, accuracy 0.2420634925365448.\n",
      "Training epoch 9 batch 1332 with loss 1.823340654373169, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 1333 with loss 1.7228788137435913, accuracy 0.25.\n",
      "Training epoch 9 batch 1334 with loss 1.724418044090271, accuracy 0.15833334624767303.\n",
      "Training epoch 9 batch 1335 with loss 1.8668146133422852, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 1336 with loss 1.8753856420516968, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 1337 with loss 1.7385133504867554, accuracy 0.3055555522441864.\n",
      "Training epoch 9 batch 1338 with loss 1.6982558965682983, accuracy 0.3948412835597992.\n",
      "Training epoch 9 batch 1339 with loss 1.7746665477752686, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 1340 with loss 1.7166423797607422, accuracy 0.3888888657093048.\n",
      "Training epoch 9 batch 1341 with loss 1.7892982959747314, accuracy 0.21944445371627808.\n",
      "Training epoch 9 batch 1342 with loss 1.7827552556991577, accuracy 0.20555555820465088.\n",
      "Training epoch 9 batch 1343 with loss 1.7518281936645508, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 1344 with loss 1.9705438613891602, accuracy 0.0.\n",
      "Training epoch 9 batch 1345 with loss 1.7130768299102783, accuracy 0.236111119389534.\n",
      "Training epoch 9 batch 1346 with loss 1.7634308338165283, accuracy 0.15555556118488312.\n",
      "Training epoch 9 batch 1347 with loss 1.78069269657135, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 1348 with loss 1.736353874206543, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1349 with loss 1.685070276260376, accuracy 0.3638889193534851.\n",
      "Training epoch 9 batch 1350 with loss 1.811805009841919, accuracy 0.1071428656578064.\n",
      "Training epoch 9 batch 1351 with loss 1.811122179031372, accuracy 0.20555555820465088.\n",
      "Training epoch 9 batch 1352 with loss 1.822545051574707, accuracy 0.28333333134651184.\n",
      "Training epoch 9 batch 1353 with loss 1.853711724281311, accuracy 0.28333333134651184.\n",
      "Training epoch 9 batch 1354 with loss 1.7409883737564087, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 1355 with loss 1.8229386806488037, accuracy 0.25.\n",
      "Training epoch 9 batch 1356 with loss 1.7817490100860596, accuracy 0.2750000059604645.\n",
      "Training epoch 9 batch 1357 with loss 1.7751871347427368, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 1358 with loss 1.7176334857940674, accuracy 0.3611111044883728.\n",
      "Training epoch 9 batch 1359 with loss 1.8119628429412842, accuracy 0.18214285373687744.\n",
      "Training epoch 9 batch 1360 with loss 1.778683066368103, accuracy 0.2638888955116272.\n",
      "Training epoch 9 batch 1361 with loss 1.7625484466552734, accuracy 0.1825396865606308.\n",
      "Training epoch 9 batch 1362 with loss 1.7860805988311768, accuracy 0.13055555522441864.\n",
      "Training epoch 9 batch 1363 with loss 1.7760374546051025, accuracy 0.19722223281860352.\n",
      "Training epoch 9 batch 1364 with loss 1.8505958318710327, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 1365 with loss 1.7372146844863892, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 1366 with loss 1.8357298374176025, accuracy 0.08888889104127884.\n",
      "Training epoch 9 batch 1367 with loss 1.8273369073867798, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1368 with loss 1.8220329284667969, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 1369 with loss 1.8717279434204102, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 1370 with loss 1.656757116317749, accuracy 0.3055555820465088.\n",
      "Training epoch 9 batch 1371 with loss 1.7245137691497803, accuracy 0.1805555522441864.\n",
      "Training epoch 9 batch 1372 with loss 1.884612798690796, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 1373 with loss 1.7480319738388062, accuracy 0.20000001788139343.\n",
      "Training epoch 9 batch 1374 with loss 1.7904460430145264, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 1375 with loss 1.8756935596466064, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 1376 with loss 1.774723768234253, accuracy 0.37222224473953247.\n",
      "Training epoch 9 batch 1377 with loss 1.8375993967056274, accuracy 0.0793650820851326.\n",
      "Training epoch 9 batch 1378 with loss 1.9027109146118164, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 1379 with loss 1.7497354745864868, accuracy 0.25555557012557983.\n",
      "Training epoch 9 batch 1380 with loss 1.8088455200195312, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 1381 with loss 1.8927290439605713, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 1382 with loss 1.7897142171859741, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1383 with loss 1.7759491205215454, accuracy 0.21666666865348816.\n",
      "Training epoch 9 batch 1384 with loss 1.7109262943267822, accuracy 0.2222222238779068.\n",
      "Training epoch 9 batch 1385 with loss 1.8346946239471436, accuracy 0.125.\n",
      "Training epoch 9 batch 1386 with loss 1.7921192646026611, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 1387 with loss 1.8256938457489014, accuracy 0.1805555522441864.\n",
      "Training epoch 9 batch 1388 with loss 1.8413445949554443, accuracy 0.125.\n",
      "Training epoch 9 batch 1389 with loss 1.7601795196533203, accuracy 0.3055555522441864.\n",
      "Training epoch 9 batch 1390 with loss 1.9051567316055298, accuracy 0.190476194024086.\n",
      "Training epoch 9 batch 1391 with loss 1.89663827419281, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 1392 with loss 1.8267042636871338, accuracy 0.125.\n",
      "Training epoch 9 batch 1393 with loss 1.864227294921875, accuracy 0.17222222685813904.\n",
      "Training epoch 9 batch 1394 with loss 1.700145959854126, accuracy 0.380952388048172.\n",
      "Training epoch 9 batch 1395 with loss 1.8217731714248657, accuracy 0.33888888359069824.\n",
      "Training epoch 9 batch 1396 with loss 1.7246885299682617, accuracy 0.17777778208255768.\n",
      "Training epoch 9 batch 1397 with loss 1.7623287439346313, accuracy 0.347222238779068.\n",
      "Training epoch 9 batch 1398 with loss 1.7184908390045166, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 1399 with loss 1.763429045677185, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 1400 with loss 1.7754065990447998, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 1401 with loss 1.7735998630523682, accuracy 0.24166668951511383.\n",
      "Training epoch 9 batch 1402 with loss 1.8958892822265625, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 1403 with loss 1.8064285516738892, accuracy 0.19166666269302368.\n",
      "Training epoch 9 batch 1404 with loss 1.7436020374298096, accuracy 0.236111119389534.\n",
      "Training epoch 9 batch 1405 with loss 1.7701480388641357, accuracy 0.15833333134651184.\n",
      "Training epoch 9 batch 1406 with loss 1.8179973363876343, accuracy 0.2182539701461792.\n",
      "Training epoch 9 batch 1407 with loss 1.8443114757537842, accuracy 0.125.\n",
      "Training epoch 9 batch 1408 with loss 1.7811124324798584, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 1409 with loss 1.931710958480835, accuracy 0.0.\n",
      "Training epoch 9 batch 1410 with loss 1.7981417179107666, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1411 with loss 1.82183837890625, accuracy 0.28333333134651184.\n",
      "Training epoch 9 batch 1412 with loss 1.8293355703353882, accuracy 0.06666667014360428.\n",
      "Training epoch 9 batch 1413 with loss 1.8245000839233398, accuracy 0.13333334028720856.\n",
      "Training epoch 9 batch 1414 with loss 1.6788383722305298, accuracy 0.3484848737716675.\n",
      "Training epoch 9 batch 1415 with loss 1.7621740102767944, accuracy 0.08888889104127884.\n",
      "Training epoch 9 batch 1416 with loss 1.7771480083465576, accuracy 0.3710317611694336.\n",
      "Training epoch 9 batch 1417 with loss 1.7350337505340576, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 1418 with loss 1.9156494140625, accuracy 0.03333333507180214.\n",
      "Training epoch 9 batch 1419 with loss 1.7595869302749634, accuracy 0.2916666865348816.\n",
      "Training epoch 9 batch 1420 with loss 1.7872272729873657, accuracy 0.125.\n",
      "Training epoch 9 batch 1421 with loss 1.8090159893035889, accuracy 0.07500000298023224.\n",
      "Training epoch 9 batch 1422 with loss 1.8077495098114014, accuracy 0.125.\n",
      "Training epoch 9 batch 1423 with loss 1.8270565271377563, accuracy 0.2888889014720917.\n",
      "Training epoch 9 batch 1424 with loss 1.7983264923095703, accuracy 0.2888889014720917.\n",
      "Training epoch 9 batch 1425 with loss 1.768157720565796, accuracy 0.31111112236976624.\n",
      "Training epoch 9 batch 1426 with loss 1.7898744344711304, accuracy 0.2916666865348816.\n",
      "Training epoch 9 batch 1427 with loss 1.8130550384521484, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1428 with loss 1.7945072650909424, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 1429 with loss 1.8248701095581055, accuracy 0.25.\n",
      "Training epoch 9 batch 1430 with loss 1.7307014465332031, accuracy 0.25555557012557983.\n",
      "Training epoch 9 batch 1431 with loss 1.7868928909301758, accuracy 0.19166667759418488.\n",
      "Training epoch 9 batch 1432 with loss 1.7764098644256592, accuracy 0.3194444477558136.\n",
      "Training epoch 9 batch 1433 with loss 1.8308372497558594, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 1434 with loss 1.7320339679718018, accuracy 0.125.\n",
      "Training epoch 9 batch 1435 with loss 1.7208144664764404, accuracy 0.17777778208255768.\n",
      "Training epoch 9 batch 1436 with loss 1.7694110870361328, accuracy 0.33888891339302063.\n",
      "Training epoch 9 batch 1437 with loss 1.8543554544448853, accuracy 0.1666666716337204.\n",
      "Training epoch 9 batch 1438 with loss 1.7646385431289673, accuracy 0.065476194024086.\n",
      "Training epoch 9 batch 1439 with loss 1.8152501583099365, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1440 with loss 1.7554973363876343, accuracy 0.25555557012557983.\n",
      "Training epoch 9 batch 1441 with loss 1.8462450504302979, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 1442 with loss 1.789899468421936, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1443 with loss 1.7716724872589111, accuracy 0.3055555522441864.\n",
      "Training epoch 9 batch 1444 with loss 1.7386033535003662, accuracy 0.0972222238779068.\n",
      "Training epoch 9 batch 1445 with loss 1.7054052352905273, accuracy 0.3888888955116272.\n",
      "Training epoch 9 batch 1446 with loss 1.7755342721939087, accuracy 0.3333333432674408.\n",
      "Training epoch 9 batch 1447 with loss 1.7308765649795532, accuracy 0.2698412835597992.\n",
      "Training epoch 9 batch 1448 with loss 1.772344946861267, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 1449 with loss 1.6942088603973389, accuracy 0.47777777910232544.\n",
      "Training epoch 9 batch 1450 with loss 1.7757413387298584, accuracy 0.1805555522441864.\n",
      "Training epoch 9 batch 1451 with loss 1.8812793493270874, accuracy 0.0.\n",
      "Training epoch 9 batch 1452 with loss 1.8071601390838623, accuracy 0.17777778208255768.\n",
      "Training epoch 9 batch 1453 with loss 1.737269401550293, accuracy 0.3253968358039856.\n",
      "Training epoch 9 batch 1454 with loss 1.83975350856781, accuracy 0.18611112236976624.\n",
      "Training epoch 9 batch 1455 with loss 1.735494613647461, accuracy 0.1944444477558136.\n",
      "Training epoch 9 batch 1456 with loss 1.7453597784042358, accuracy 0.1726190447807312.\n",
      "Training epoch 9 batch 1457 with loss 1.8246467113494873, accuracy 0.10833333432674408.\n",
      "Training epoch 9 batch 1458 with loss 1.6999626159667969, accuracy 0.3500000238418579.\n",
      "Training epoch 9 batch 1459 with loss 1.825143575668335, accuracy 0.18611110746860504.\n",
      "Training epoch 9 batch 1460 with loss 1.7897427082061768, accuracy 0.2666666805744171.\n",
      "Training epoch 9 batch 1461 with loss 1.819769263267517, accuracy 0.23333334922790527.\n",
      "Training epoch 9 batch 1462 with loss 1.8872226476669312, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 1463 with loss 1.68551766872406, accuracy 0.4138888716697693.\n",
      "Training epoch 9 batch 1464 with loss 1.7516765594482422, accuracy 0.3499999940395355.\n",
      "Training epoch 9 batch 1465 with loss 1.796972632408142, accuracy 0.20000000298023224.\n",
      "Training epoch 9 batch 1466 with loss 1.7835452556610107, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 1467 with loss 1.8505510091781616, accuracy 0.0555555559694767.\n",
      "Training epoch 9 batch 1468 with loss 1.8598010540008545, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1469 with loss 1.8191213607788086, accuracy 0.222222238779068.\n",
      "Training epoch 9 batch 1470 with loss 1.8708667755126953, accuracy 0.1071428582072258.\n",
      "Training epoch 9 batch 1471 with loss 1.8934829235076904, accuracy 0.06111111491918564.\n",
      "Training epoch 9 batch 1472 with loss 1.7942085266113281, accuracy 0.2083333432674408.\n",
      "Training epoch 9 batch 1473 with loss 1.7891063690185547, accuracy 0.23333333432674408.\n",
      "Training epoch 9 batch 1474 with loss 1.8459932804107666, accuracy 0.0833333358168602.\n",
      "Training epoch 9 batch 1475 with loss 1.8203678131103516, accuracy 0.125.\n",
      "Training epoch 9 batch 1476 with loss 1.8170934915542603, accuracy 0.0416666679084301.\n",
      "Training epoch 9 batch 1477 with loss 1.7597824335098267, accuracy 0.3333333432674408.\n",
      "Training epoch 9 batch 1478 with loss 1.866611123085022, accuracy 0.28333333134651184.\n",
      "Training epoch 9 batch 1479 with loss 1.7717300653457642, accuracy 0.18611110746860504.\n",
      "Training epoch 9 batch 1480 with loss 1.880950689315796, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 1481 with loss 1.7588398456573486, accuracy 0.2142857313156128.\n",
      "Training epoch 9 batch 1482 with loss 1.7657464742660522, accuracy 0.11666667461395264.\n",
      "Training epoch 9 batch 1483 with loss 1.8308351039886475, accuracy 0.3154762089252472.\n",
      "Training epoch 9 batch 1484 with loss 1.641274094581604, accuracy 0.3166666626930237.\n",
      "Training epoch 9 batch 1485 with loss 1.8520286083221436, accuracy 0.21388888359069824.\n",
      "Training epoch 9 batch 1486 with loss 1.880863904953003, accuracy 0.14444445073604584.\n",
      "Training epoch 9 batch 1487 with loss 1.757288932800293, accuracy 0.29722222685813904.\n",
      "Training epoch 9 batch 1488 with loss 1.807590126991272, accuracy 0.1388888955116272.\n",
      "Training epoch 9 batch 1489 with loss 1.8181835412979126, accuracy 0.1805555522441864.\n",
      "Training epoch 9 batch 1490 with loss 1.8049920797348022, accuracy 0.1111111119389534.\n",
      "Training epoch 9 batch 1491 with loss 1.7967898845672607, accuracy 0.2638888955116272.\n",
      "Training epoch 9 batch 1492 with loss 1.803108811378479, accuracy 0.23333333432674408.\n",
      "Training epoch 9 batch 1493 with loss 1.7718875408172607, accuracy 0.2916666567325592.\n",
      "Training epoch 9 batch 1494 with loss 1.6852575540542603, accuracy 0.347222238779068.\n",
      "Training epoch 9 batch 1495 with loss 1.7985464334487915, accuracy 0.0714285746216774.\n",
      "Training epoch 9 batch 1496 with loss 1.7903110980987549, accuracy 0.06666667014360428.\n",
      "Training epoch 9 batch 1497 with loss 1.7976675033569336, accuracy 0.11269842088222504.\n",
      "Training epoch 9 batch 1498 with loss 1.7706416845321655, accuracy 0.17500001192092896.\n",
      "Training epoch 9 batch 1499 with loss 1.8117691278457642, accuracy 0.0793650820851326.\n",
      "Test batch 0 with loss 1.9935840368270874 and accuracy 0.02380952425301075.\n",
      "Test batch 1 with loss 1.7732784748077393 and accuracy 0.2916666567325592.\n",
      "Test batch 2 with loss 1.8366906642913818 and accuracy 0.0793650820851326.\n",
      "Test batch 3 with loss 1.860769271850586 and accuracy 0.1666666716337204.\n",
      "Test batch 4 with loss 1.7161458730697632 and accuracy 0.2361111044883728.\n",
      "Test batch 5 with loss 1.9322755336761475 and accuracy 0.11666667461395264.\n",
      "Test batch 6 with loss 1.7902910709381104 and accuracy 0.23333333432674408.\n",
      "Test batch 7 with loss 1.7501652240753174 and accuracy 0.4000000059604645.\n",
      "Test batch 8 with loss 1.8689262866973877 and accuracy 0.06666667014360428.\n",
      "Test batch 9 with loss 1.877901315689087 and accuracy 0.08888889104127884.\n",
      "Test batch 10 with loss 1.878064751625061 and accuracy 0.2222222238779068.\n",
      "Test batch 11 with loss 1.8602495193481445 and accuracy 0.0892857164144516.\n",
      "Test batch 12 with loss 1.8191410303115845 and accuracy 0.1527777761220932.\n",
      "Test batch 13 with loss 1.8123267889022827 and accuracy 0.31111112236976624.\n",
      "Test batch 14 with loss 1.782457709312439 and accuracy 0.3055555522441864.\n",
      "Test batch 15 with loss 1.770594835281372 and accuracy 0.22777777910232544.\n",
      "Test batch 16 with loss 1.8450653553009033 and accuracy 0.11666667461395264.\n",
      "Test batch 17 with loss 1.792231559753418 and accuracy 0.2777777910232544.\n",
      "Test batch 18 with loss 1.7887029647827148 and accuracy 0.28333336114883423.\n",
      "Test batch 19 with loss 1.8484547138214111 and accuracy 0.1130952388048172.\n",
      "Test batch 20 with loss 1.8508527278900146 and accuracy 0.0555555559694767.\n",
      "Test batch 21 with loss 1.8205368518829346 and accuracy 0.0833333358168602.\n",
      "Test batch 22 with loss 1.7410234212875366 and accuracy 0.2777777910232544.\n",
      "Test batch 23 with loss 1.8127946853637695 and accuracy 0.10185185074806213.\n",
      "Test batch 24 with loss 1.864587426185608 and accuracy 0.0416666679084301.\n",
      "Test batch 25 with loss 1.850290298461914 and accuracy 0.20000000298023224.\n",
      "Test batch 26 with loss 1.832446813583374 and accuracy 0.03333333507180214.\n",
      "Test batch 27 with loss 1.8956321477890015 and accuracy 0.0416666679084301.\n",
      "Test batch 28 with loss 1.9275264739990234 and accuracy 0.12222222983837128.\n",
      "Test batch 29 with loss 1.8107372522354126 and accuracy 0.18333333730697632.\n",
      "Test batch 30 with loss 1.8757131099700928 and accuracy 0.2083333432674408.\n",
      "Test batch 31 with loss 1.8396562337875366 and accuracy 0.0972222238779068.\n",
      "Test batch 32 with loss 1.815503716468811 and accuracy 0.1666666716337204.\n",
      "Test batch 33 with loss 1.7682851552963257 and accuracy 0.3055555522441864.\n",
      "Test batch 34 with loss 1.8204723596572876 and accuracy 0.1388888955116272.\n",
      "Test batch 35 with loss 1.8747026920318604 and accuracy 0.125.\n",
      "Test batch 36 with loss 1.7946828603744507 and accuracy 0.25.\n",
      "Test batch 37 with loss 1.7451883554458618 and accuracy 0.2944444417953491.\n",
      "Test batch 38 with loss 1.891507863998413 and accuracy 0.1388888955116272.\n",
      "Test batch 39 with loss 1.8026434183120728 and accuracy 0.43611112236976624.\n",
      "Test batch 40 with loss 1.800355315208435 and accuracy 0.2944444417953491.\n",
      "Test batch 41 with loss 1.8271888494491577 and accuracy 0.02777777798473835.\n",
      "Test batch 42 with loss 1.8297755718231201 and accuracy 0.10833333432674408.\n",
      "Test batch 43 with loss 1.7859876155853271 and accuracy 0.20000001788139343.\n",
      "Test batch 44 with loss 1.8171377182006836 and accuracy 0.2222222238779068.\n",
      "Test batch 45 with loss 1.7539901733398438 and accuracy 0.26944443583488464.\n",
      "Test batch 46 with loss 1.7238779067993164 and accuracy 0.0972222238779068.\n",
      "Test batch 47 with loss 1.8068649768829346 and accuracy 0.06666667014360428.\n",
      "Test batch 48 with loss 1.8259468078613281 and accuracy 0.284722238779068.\n",
      "Test batch 49 with loss 1.772496223449707 and accuracy 0.3333333432674408.\n",
      "Test batch 50 with loss 1.7820247411727905 and accuracy 0.2611111104488373.\n",
      "Test batch 51 with loss 1.7591526508331299 and accuracy 0.13333334028720856.\n",
      "Test batch 52 with loss 1.7384027242660522 and accuracy 0.24722221493721008.\n",
      "Test batch 53 with loss 1.924038290977478 and accuracy 0.0833333358168602.\n",
      "Test batch 54 with loss 1.8238977193832397 and accuracy 0.0972222238779068.\n",
      "Test batch 55 with loss 1.878653883934021 and accuracy 0.0416666679084301.\n",
      "Test batch 56 with loss 1.8087589740753174 and accuracy 0.22777777910232544.\n",
      "Test batch 57 with loss 1.9230363368988037 and accuracy 0.21388889849185944.\n",
      "Test batch 58 with loss 1.8332608938217163 and accuracy 0.03703703731298447.\n",
      "Test batch 59 with loss 1.7347466945648193 and accuracy 0.25.\n",
      "Test batch 60 with loss 1.7847251892089844 and accuracy 0.08888889104127884.\n",
      "Test batch 61 with loss 1.8471959829330444 and accuracy 0.08888889104127884.\n",
      "Test batch 62 with loss 1.8445402383804321 and accuracy 0.3472222089767456.\n",
      "Test batch 63 with loss 1.836414098739624 and accuracy 0.20000000298023224.\n",
      "Test batch 64 with loss 1.8112980127334595 and accuracy 0.11666667461395264.\n",
      "Test batch 65 with loss 1.922559142112732 and accuracy 0.0416666679084301.\n",
      "Test batch 66 with loss 1.921730637550354 and accuracy 0.130952388048172.\n",
      "Test batch 67 with loss 1.8820995092391968 and accuracy 0.125.\n",
      "Test batch 68 with loss 1.8222700357437134 and accuracy 0.3222222328186035.\n",
      "Test batch 69 with loss 1.8012727499008179 and accuracy 0.1944444477558136.\n",
      "Test batch 70 with loss 1.8123788833618164 and accuracy 0.25555557012557983.\n",
      "Test batch 71 with loss 1.7947680950164795 and accuracy 0.1666666716337204.\n",
      "Test batch 72 with loss 1.796929121017456 and accuracy 0.2321428656578064.\n",
      "Test batch 73 with loss 1.8340753316879272 and accuracy 0.0833333358168602.\n",
      "Test batch 74 with loss 1.7960363626480103 and accuracy 0.20000000298023224.\n",
      "Test batch 75 with loss 2.0000736713409424 and accuracy 0.1071428582072258.\n",
      "Test batch 76 with loss 1.8845775127410889 and accuracy 0.13055555522441864.\n",
      "Test batch 77 with loss 1.8064196109771729 and accuracy 0.18214285373687744.\n",
      "Test batch 78 with loss 1.9625709056854248 and accuracy 0.0416666679084301.\n",
      "Test batch 79 with loss 1.9241065979003906 and accuracy 0.0416666679084301.\n",
      "Test batch 80 with loss 1.8141205310821533 and accuracy 0.1349206417798996.\n",
      "Test batch 81 with loss 1.9052480459213257 and accuracy 0.1666666716337204.\n",
      "Test batch 82 with loss 1.83632493019104 and accuracy 0.14444445073604584.\n",
      "Test batch 83 with loss 1.7685701847076416 and accuracy 0.2777777910232544.\n",
      "Test batch 84 with loss 1.8218101263046265 and accuracy 0.2777777910232544.\n",
      "Test batch 85 with loss 1.9356015920639038 and accuracy 0.2182539701461792.\n",
      "Test batch 86 with loss 1.8298118114471436 and accuracy 0.25.\n",
      "Test batch 87 with loss 1.8984445333480835 and accuracy 0.15000000596046448.\n",
      "Test batch 88 with loss 1.7833364009857178 and accuracy 0.1666666716337204.\n",
      "Test batch 89 with loss 1.7396742105484009 and accuracy 0.222222238779068.\n",
      "Test batch 90 with loss 1.813537359237671 and accuracy 0.1388888955116272.\n",
      "Test batch 91 with loss 1.8766742944717407 and accuracy 0.1666666716337204.\n",
      "Test batch 92 with loss 1.790612816810608 and accuracy 0.1944444477558136.\n",
      "Test batch 93 with loss 1.829869031906128 and accuracy 0.20555555820465088.\n",
      "Test batch 94 with loss 1.7832921743392944 and accuracy 0.1944444477558136.\n",
      "Test batch 95 with loss 1.8792740106582642 and accuracy 0.0555555559694767.\n",
      "Test batch 96 with loss 1.8080360889434814 and accuracy 0.1388888955116272.\n",
      "Test batch 97 with loss 1.7804206609725952 and accuracy 0.1071428582072258.\n",
      "Test batch 98 with loss 1.8669023513793945 and accuracy 0.20000001788139343.\n",
      "Test batch 99 with loss 1.9368600845336914 and accuracy 0.0972222238779068.\n",
      "Test batch 100 with loss 1.7749130725860596 and accuracy 0.24722224473953247.\n",
      "Test batch 101 with loss 1.794838547706604 and accuracy 0.33888888359069824.\n",
      "Test batch 102 with loss 1.9373438358306885 and accuracy 0.15833333134651184.\n",
      "Test batch 103 with loss 1.9238799810409546 and accuracy 0.02777777798473835.\n",
      "Test batch 104 with loss 1.6998863220214844 and accuracy 0.1666666716337204.\n",
      "Test batch 105 with loss 1.9579250812530518 and accuracy 0.0476190485060215.\n",
      "Test batch 106 with loss 1.8437836170196533 and accuracy 0.1388888955116272.\n",
      "Test batch 107 with loss 1.8011233806610107 and accuracy 0.2222222238779068.\n",
      "Test batch 108 with loss 1.8917490243911743 and accuracy 0.07500000298023224.\n",
      "Test batch 109 with loss 1.8521963357925415 and accuracy 0.1666666716337204.\n",
      "Test batch 110 with loss 1.7560911178588867 and accuracy 0.1130952388048172.\n",
      "Test batch 111 with loss 1.8429927825927734 and accuracy 0.1666666716337204.\n",
      "Test batch 112 with loss 1.824058175086975 and accuracy 0.2361111044883728.\n",
      "Test batch 113 with loss 1.8264353275299072 and accuracy 0.1269841343164444.\n",
      "Test batch 114 with loss 1.860795021057129 and accuracy 0.125.\n",
      "Test batch 115 with loss 1.870830774307251 and accuracy 0.14444445073604584.\n",
      "Test batch 116 with loss 1.801451325416565 and accuracy 0.0972222238779068.\n",
      "Test batch 117 with loss 1.9267947673797607 and accuracy 0.03333333507180214.\n",
      "Test batch 118 with loss 1.840987205505371 and accuracy 0.2638888955116272.\n",
      "Test batch 119 with loss 1.8422359228134155 and accuracy 0.3988095223903656.\n",
      "Test batch 120 with loss 1.802917718887329 and accuracy 0.17222222685813904.\n",
      "Test batch 121 with loss 1.819649338722229 and accuracy 0.1944444477558136.\n",
      "Test batch 122 with loss 1.827016830444336 and accuracy 0.1805555671453476.\n",
      "Test batch 123 with loss 1.712270975112915 and accuracy 0.23333333432674408.\n",
      "Test batch 124 with loss 1.9007381200790405 and accuracy 0.0833333358168602.\n",
      "Test batch 125 with loss 1.8341318368911743 and accuracy 0.13055555522441864.\n",
      "Test batch 126 with loss 1.983353853225708 and accuracy 0.0.\n",
      "Test batch 127 with loss 1.8353010416030884 and accuracy 0.0515873022377491.\n",
      "Test batch 128 with loss 1.8432419300079346 and accuracy 0.10000000894069672.\n",
      "Test batch 129 with loss 1.909264326095581 and accuracy 0.06666667014360428.\n",
      "Test batch 130 with loss 1.9071054458618164 and accuracy 0.1666666716337204.\n",
      "Test batch 131 with loss 1.8583828210830688 and accuracy 0.1527777761220932.\n",
      "Test batch 132 with loss 1.7976621389389038 and accuracy 0.0714285746216774.\n",
      "Test batch 133 with loss 1.8094489574432373 and accuracy 0.1944444477558136.\n",
      "Test batch 134 with loss 1.8678840398788452 and accuracy 0.3611111342906952.\n",
      "Test batch 135 with loss 1.8040332794189453 and accuracy 0.1388888955116272.\n",
      "Test batch 136 with loss 1.8633006811141968 and accuracy 0.02777777798473835.\n",
      "Test batch 137 with loss 1.8136287927627563 and accuracy 0.0555555559694767.\n",
      "Test batch 138 with loss 1.8790245056152344 and accuracy 0.1111111119389534.\n",
      "Test batch 139 with loss 1.8264001607894897 and accuracy 0.25.\n",
      "Test batch 140 with loss 1.811910629272461 and accuracy 0.0555555559694767.\n",
      "Test batch 141 with loss 1.8540856838226318 and accuracy 0.2888889014720917.\n",
      "Test batch 142 with loss 1.7941097021102905 and accuracy 0.0972222238779068.\n",
      "Test batch 143 with loss 1.820835828781128 and accuracy 0.2638888955116272.\n",
      "Test batch 144 with loss 1.8136622905731201 and accuracy 0.125.\n",
      "Test batch 145 with loss 1.8823740482330322 and accuracy 0.08888889104127884.\n",
      "Test batch 146 with loss 1.8235445022583008 and accuracy 0.1805555522441864.\n",
      "Test batch 147 with loss 1.801201581954956 and accuracy 0.25555557012557983.\n",
      "Test batch 148 with loss 1.8613474369049072 and accuracy 0.1666666716337204.\n",
      "Test batch 149 with loss 1.7656915187835693 and accuracy 0.1269841343164444.\n",
      "Test batch 150 with loss 1.71634840965271 and accuracy 0.2083333432674408.\n",
      "Test batch 151 with loss 1.8290760517120361 and accuracy 0.23888888955116272.\n",
      "Test batch 152 with loss 1.8513704538345337 and accuracy 0.0555555559694767.\n",
      "Test batch 153 with loss 1.7943452596664429 and accuracy 0.03703703731298447.\n",
      "Test batch 154 with loss 1.8656028509140015 and accuracy 0.1349206417798996.\n",
      "Test batch 155 with loss 1.7368748188018799 and accuracy 0.3194444477558136.\n",
      "Test batch 156 with loss 1.8021094799041748 and accuracy 0.12037037312984467.\n",
      "Test batch 157 with loss 1.7204248905181885 and accuracy 0.2888889014720917.\n",
      "Test batch 158 with loss 1.750571846961975 and accuracy 0.210317462682724.\n",
      "Test batch 159 with loss 1.7765247821807861 and accuracy 0.19722223281860352.\n",
      "Test batch 160 with loss 1.8891862630844116 and accuracy 0.1488095223903656.\n",
      "Test batch 161 with loss 1.9266431331634521 and accuracy 0.03333333507180214.\n",
      "Test batch 162 with loss 1.7813053131103516 and accuracy 0.17222222685813904.\n",
      "Test batch 163 with loss 1.832486867904663 and accuracy 0.16388888657093048.\n",
      "Test batch 164 with loss 1.766921043395996 and accuracy 0.3194444477558136.\n",
      "Test batch 165 with loss 1.8202766180038452 and accuracy 0.25555557012557983.\n",
      "Test batch 166 with loss 1.805649757385254 and accuracy 0.2083333432674408.\n",
      "Test batch 167 with loss 1.8607101440429688 and accuracy 0.0694444477558136.\n",
      "Test batch 168 with loss 1.7347633838653564 and accuracy 0.277777761220932.\n",
      "Test batch 169 with loss 1.7632697820663452 and accuracy 0.2083333432674408.\n",
      "Test batch 170 with loss 1.8381742238998413 and accuracy 0.17222222685813904.\n",
      "Test batch 171 with loss 1.767854928970337 and accuracy 0.3055555522441864.\n",
      "Test batch 172 with loss 1.7907642126083374 and accuracy 0.3194444477558136.\n",
      "Test batch 173 with loss 1.752826452255249 and accuracy 0.33888888359069824.\n",
      "Test batch 174 with loss 1.8584480285644531 and accuracy 0.1944444477558136.\n",
      "Test batch 175 with loss 1.8674217462539673 and accuracy 0.10277777910232544.\n",
      "Test batch 176 with loss 1.7837692499160767 and accuracy 0.0833333358168602.\n",
      "Test batch 177 with loss 1.8137695789337158 and accuracy 0.14047619700431824.\n",
      "Test batch 178 with loss 1.8757823705673218 and accuracy 0.03333333507180214.\n",
      "Test batch 179 with loss 1.8299604654312134 and accuracy 0.1527777761220932.\n",
      "Test batch 180 with loss 1.8533570766448975 and accuracy 0.0555555559694767.\n",
      "Test batch 181 with loss 1.881455421447754 and accuracy 0.2698412835597992.\n",
      "Test batch 182 with loss 1.8287255764007568 and accuracy 0.1111111119389534.\n",
      "Test batch 183 with loss 1.7929102182388306 and accuracy 0.21388888359069824.\n",
      "Test batch 184 with loss 1.7642170190811157 and accuracy 0.14444445073604584.\n",
      "Test batch 185 with loss 1.8533687591552734 and accuracy 0.1666666716337204.\n",
      "Test batch 186 with loss 1.7299972772598267 and accuracy 0.23888888955116272.\n",
      "Test batch 187 with loss 1.7698733806610107 and accuracy 0.20000000298023224.\n",
      "Test batch 188 with loss 1.8072893619537354 and accuracy 0.15833333134651184.\n",
      "Test batch 189 with loss 1.8285531997680664 and accuracy 0.0416666679084301.\n",
      "Test batch 190 with loss 1.7830702066421509 and accuracy 0.20000000298023224.\n",
      "Test batch 191 with loss 1.8016437292099 and accuracy 0.261904776096344.\n",
      "Test batch 192 with loss 1.8494020700454712 and accuracy 0.1666666716337204.\n",
      "Test batch 193 with loss 1.7975175380706787 and accuracy 0.2738095223903656.\n",
      "Test batch 194 with loss 1.8704497814178467 and accuracy 0.3125.\n",
      "Test batch 195 with loss 1.74809992313385 and accuracy 0.4305555820465088.\n",
      "Test batch 196 with loss 1.9514249563217163 and accuracy 0.0.\n",
      "Test batch 197 with loss 1.7152942419052124 and accuracy 0.15000000596046448.\n",
      "Test batch 198 with loss 1.8250528573989868 and accuracy 0.0476190485060215.\n",
      "Test batch 199 with loss 1.8485653400421143 and accuracy 0.1666666716337204.\n",
      "Test batch 200 with loss 1.7970739603042603 and accuracy 0.08888889104127884.\n",
      "Test batch 201 with loss 1.7987273931503296 and accuracy 0.07500000298023224.\n",
      "Test batch 202 with loss 1.7352607250213623 and accuracy 0.28333333134651184.\n",
      "Test batch 203 with loss 1.815802812576294 and accuracy 0.2638888955116272.\n",
      "Test batch 204 with loss 1.780287742614746 and accuracy 0.21666666865348816.\n",
      "Test batch 205 with loss 1.8273242712020874 and accuracy 0.12777778506278992.\n",
      "Test batch 206 with loss 1.8282073736190796 and accuracy 0.1944444477558136.\n",
      "Test batch 207 with loss 1.6659796237945557 and accuracy 0.2809523940086365.\n",
      "Test batch 208 with loss 1.8352375030517578 and accuracy 0.28333333134651184.\n",
      "Test batch 209 with loss 1.7455562353134155 and accuracy 0.1388888955116272.\n",
      "Test batch 210 with loss 1.8413556814193726 and accuracy 0.1388888955116272.\n",
      "Test batch 211 with loss 1.8559755086898804 and accuracy 0.06666667014360428.\n",
      "Test batch 212 with loss 1.9084182977676392 and accuracy 0.0694444477558136.\n",
      "Test batch 213 with loss 1.757027268409729 and accuracy 0.4027778208255768.\n",
      "Test batch 214 with loss 1.8763328790664673 and accuracy 0.02380952425301075.\n",
      "Test batch 215 with loss 1.8893051147460938 and accuracy 0.2916666865348816.\n",
      "Test batch 216 with loss 1.8176634311676025 and accuracy 0.2013888955116272.\n",
      "Test batch 217 with loss 1.8267494440078735 and accuracy 0.1111111119389534.\n",
      "Test batch 218 with loss 1.8248599767684937 and accuracy 0.1388888955116272.\n",
      "Test batch 219 with loss 1.8987222909927368 and accuracy 0.3333333432674408.\n",
      "Test batch 220 with loss 1.7499189376831055 and accuracy 0.1666666716337204.\n",
      "Test batch 221 with loss 1.809553861618042 and accuracy 0.1111111119389534.\n",
      "Test batch 222 with loss 1.7412059307098389 and accuracy 0.3055555820465088.\n",
      "Test batch 223 with loss 1.8783975839614868 and accuracy 0.06111111491918564.\n",
      "Test batch 224 with loss 1.9048484563827515 and accuracy 0.08888889104127884.\n",
      "Test batch 225 with loss 1.6730997562408447 and accuracy 0.375.\n",
      "Test batch 226 with loss 1.8350229263305664 and accuracy 0.1111111119389534.\n",
      "Test batch 227 with loss 1.801302194595337 and accuracy 0.39444446563720703.\n",
      "Test batch 228 with loss 1.768144965171814 and accuracy 0.1666666716337204.\n",
      "Test batch 229 with loss 1.6300592422485352 and accuracy 0.3450000286102295.\n",
      "Test batch 230 with loss 1.8382850885391235 and accuracy 0.277777761220932.\n",
      "Test batch 231 with loss 1.82655930519104 and accuracy 0.0833333358168602.\n",
      "Test batch 232 with loss 1.7362651824951172 and accuracy 0.37222224473953247.\n",
      "Test batch 233 with loss 1.8194135427474976 and accuracy 0.1388888955116272.\n",
      "Test batch 234 with loss 1.7544918060302734 and accuracy 0.0833333358168602.\n",
      "Test batch 235 with loss 1.8816782236099243 and accuracy 0.0416666679084301.\n",
      "Test batch 236 with loss 1.754279375076294 and accuracy 0.2805555760860443.\n",
      "Test batch 237 with loss 1.8266007900238037 and accuracy 0.1388888955116272.\n",
      "Test batch 238 with loss 1.8036632537841797 and accuracy 0.1805555522441864.\n",
      "Test batch 239 with loss 1.7790483236312866 and accuracy 0.15000000596046448.\n",
      "Test batch 240 with loss 1.7872120141983032 and accuracy 0.2638888955116272.\n",
      "Test batch 241 with loss 1.7810825109481812 and accuracy 0.1527777761220932.\n",
      "Test batch 242 with loss 1.8923988342285156 and accuracy 0.1388888955116272.\n",
      "Test batch 243 with loss 1.8417155742645264 and accuracy 0.1388888955116272.\n",
      "Test batch 244 with loss 1.8595117330551147 and accuracy 0.18333333730697632.\n",
      "Test batch 245 with loss 1.8048779964447021 and accuracy 0.12222222983837128.\n",
      "Test batch 246 with loss 1.8230946063995361 and accuracy 0.11666667461395264.\n",
      "Test batch 247 with loss 1.9240630865097046 and accuracy 0.1071428582072258.\n",
      "Test batch 248 with loss 1.99040949344635 and accuracy 0.0416666679084301.\n",
      "Test batch 249 with loss 1.8560078144073486 and accuracy 0.14444445073604584.\n",
      "Test batch 250 with loss 1.8747724294662476 and accuracy 0.1666666716337204.\n",
      "Test batch 251 with loss 1.872747778892517 and accuracy 0.29722222685813904.\n",
      "Test batch 252 with loss 1.8761554956436157 and accuracy 0.1388888955116272.\n",
      "Test batch 253 with loss 1.7982803583145142 and accuracy 0.36666667461395264.\n",
      "Test batch 254 with loss 1.7816078662872314 and accuracy 0.3611111044883728.\n",
      "Test batch 255 with loss 1.8759434223175049 and accuracy 0.0555555559694767.\n",
      "Test batch 256 with loss 1.7931190729141235 and accuracy 0.375.\n",
      "Test batch 257 with loss 1.9464658498764038 and accuracy 0.2083333432674408.\n",
      "Test batch 258 with loss 1.7416284084320068 and accuracy 0.22777777910232544.\n",
      "Test batch 259 with loss 1.8213071823120117 and accuracy 0.1349206417798996.\n",
      "Test batch 260 with loss 1.7862615585327148 and accuracy 0.1111111119389534.\n",
      "Test batch 261 with loss 1.826796293258667 and accuracy 0.0833333358168602.\n",
      "Test batch 262 with loss 1.968969702720642 and accuracy 0.20000000298023224.\n",
      "Test batch 263 with loss 1.8052676916122437 and accuracy 0.1666666716337204.\n",
      "Test batch 264 with loss 1.681300163269043 and accuracy 0.17222222685813904.\n",
      "Test batch 265 with loss 1.7767620086669922 and accuracy 0.21944445371627808.\n",
      "Test batch 266 with loss 1.7573277950286865 and accuracy 0.125.\n",
      "Test batch 267 with loss 1.8604812622070312 and accuracy 0.1666666716337204.\n",
      "Test batch 268 with loss 1.819157600402832 and accuracy 0.10833333432674408.\n",
      "Test batch 269 with loss 1.7644023895263672 and accuracy 0.20000000298023224.\n",
      "Test batch 270 with loss 1.7247209548950195 and accuracy 0.19166666269302368.\n",
      "Test batch 271 with loss 1.7774499654769897 and accuracy 0.1666666716337204.\n",
      "Test batch 272 with loss 1.8834432363510132 and accuracy 0.0416666679084301.\n",
      "Test batch 273 with loss 1.909031629562378 and accuracy 0.16388890147209167.\n",
      "Test batch 274 with loss 1.9209753274917603 and accuracy 0.0694444477558136.\n",
      "Test batch 275 with loss 1.8011776208877563 and accuracy 0.0555555559694767.\n",
      "Test batch 276 with loss 1.8581383228302002 and accuracy 0.0972222238779068.\n",
      "Test batch 277 with loss 1.9086780548095703 and accuracy 0.1527777761220932.\n",
      "Test batch 278 with loss 1.744192361831665 and accuracy 0.1031746044754982.\n",
      "Test batch 279 with loss 1.8301324844360352 and accuracy 0.2916666865348816.\n",
      "Test batch 280 with loss 1.9070974588394165 and accuracy 0.0416666679084301.\n",
      "Test batch 281 with loss 1.9427225589752197 and accuracy 0.02777777798473835.\n",
      "Test batch 282 with loss 1.888100266456604 and accuracy 0.05714286118745804.\n",
      "Test batch 283 with loss 1.751173734664917 and accuracy 0.17083334922790527.\n",
      "Test batch 284 with loss 1.8225796222686768 and accuracy 0.19603174924850464.\n",
      "Test batch 285 with loss 1.7349421977996826 and accuracy 0.4055555760860443.\n",
      "Test batch 286 with loss 1.892742395401001 and accuracy 0.03333333507180214.\n",
      "Test batch 287 with loss 1.8645679950714111 and accuracy 0.25555557012557983.\n",
      "Test batch 288 with loss 1.855525255203247 and accuracy 0.1944444477558136.\n",
      "Test batch 289 with loss 1.7768123149871826 and accuracy 0.2916666865348816.\n",
      "Test batch 290 with loss 1.9203345775604248 and accuracy 0.1666666716337204.\n",
      "Test batch 291 with loss 1.7702659368515015 and accuracy 0.23333334922790527.\n",
      "Test batch 292 with loss 1.898336410522461 and accuracy 0.12222222983837128.\n",
      "Test batch 293 with loss 1.8869587182998657 and accuracy 0.33750003576278687.\n",
      "Test batch 294 with loss 1.7827787399291992 and accuracy 0.25555557012557983.\n",
      "Test batch 295 with loss 1.8020098209381104 and accuracy 0.22380952537059784.\n",
      "Test batch 296 with loss 1.8732589483261108 and accuracy 0.0.\n",
      "Test batch 297 with loss 1.7672865390777588 and accuracy 0.17222222685813904.\n",
      "Test batch 298 with loss 1.8128598928451538 and accuracy 0.15000000596046448.\n",
      "Test batch 299 with loss 1.8743743896484375 and accuracy 0.0972222238779068.\n",
      "Test batch 300 with loss 1.7941665649414062 and accuracy 0.1944444477558136.\n",
      "Test batch 301 with loss 1.9055900573730469 and accuracy 0.1944444477558136.\n",
      "Test batch 302 with loss 1.7939141988754272 and accuracy 0.1666666716337204.\n",
      "Test batch 303 with loss 1.790123701095581 and accuracy 0.14166666567325592.\n",
      "Test batch 304 with loss 1.7690349817276 and accuracy 0.25833335518836975.\n",
      "Test batch 305 with loss 1.766379714012146 and accuracy 0.25.\n",
      "Test batch 306 with loss 1.8961899280548096 and accuracy 0.17222222685813904.\n",
      "Test batch 307 with loss 1.8077799081802368 and accuracy 0.1388888955116272.\n",
      "Test batch 308 with loss 1.7880817651748657 and accuracy 0.2111111283302307.\n",
      "Test batch 309 with loss 1.7620563507080078 and accuracy 0.3333333432674408.\n",
      "Test batch 310 with loss 1.8306798934936523 and accuracy 0.2638888955116272.\n",
      "Test batch 311 with loss 1.8232381343841553 and accuracy 0.1527777910232544.\n",
      "Test batch 312 with loss 1.8582760095596313 and accuracy 0.0416666679084301.\n",
      "Test batch 313 with loss 1.8753048181533813 and accuracy 0.03333333507180214.\n",
      "Test batch 314 with loss 1.8678178787231445 and accuracy 0.0.\n",
      "Test batch 315 with loss 1.8591798543930054 and accuracy 0.1388888955116272.\n",
      "Test batch 316 with loss 1.8132930994033813 and accuracy 0.0833333358168602.\n",
      "Test batch 317 with loss 1.8498315811157227 and accuracy 0.1597222238779068.\n",
      "Test batch 318 with loss 1.7649815082550049 and accuracy 0.0555555559694767.\n",
      "Test batch 319 with loss 1.7845920324325562 and accuracy 0.0476190485060215.\n",
      "Test batch 320 with loss 1.889925241470337 and accuracy 0.0416666679084301.\n",
      "Test batch 321 with loss 1.818895697593689 and accuracy 0.1944444477558136.\n",
      "Test batch 322 with loss 1.8521875143051147 and accuracy 0.125.\n",
      "Test batch 323 with loss 1.807706594467163 and accuracy 0.32499998807907104.\n",
      "Test batch 324 with loss 1.936349868774414 and accuracy 0.11666667461395264.\n",
      "Test batch 325 with loss 1.8200289011001587 and accuracy 0.24722222983837128.\n",
      "Test batch 326 with loss 1.8381811380386353 and accuracy 0.1388888955116272.\n",
      "Test batch 327 with loss 1.7847191095352173 and accuracy 0.20000000298023224.\n",
      "Test batch 328 with loss 1.8755302429199219 and accuracy 0.222222238779068.\n",
      "Test batch 329 with loss 1.8126838207244873 and accuracy 0.15833334624767303.\n",
      "Test batch 330 with loss 1.8478116989135742 and accuracy 0.06666667014360428.\n",
      "Test batch 331 with loss 1.920661211013794 and accuracy 0.1111111119389534.\n",
      "Test batch 332 with loss 1.7569068670272827 and accuracy 0.13055555522441864.\n",
      "Test batch 333 with loss 1.8496637344360352 and accuracy 0.0.\n",
      "Test batch 334 with loss 1.8944200277328491 and accuracy 0.0416666679084301.\n",
      "Test batch 335 with loss 1.8837493658065796 and accuracy 0.1666666716337204.\n",
      "Test batch 336 with loss 1.7560598850250244 and accuracy 0.20000001788139343.\n",
      "Test batch 337 with loss 1.8829829692840576 and accuracy 0.1666666716337204.\n",
      "Test batch 338 with loss 1.8277883529663086 and accuracy 0.1666666716337204.\n",
      "Test batch 339 with loss 1.7763168811798096 and accuracy 0.20000001788139343.\n",
      "Test batch 340 with loss 1.7685184478759766 and accuracy 0.125.\n",
      "Test batch 341 with loss 1.8847652673721313 and accuracy 0.125.\n",
      "Test batch 342 with loss 1.8338168859481812 and accuracy 0.0972222238779068.\n",
      "Test batch 343 with loss 1.8138761520385742 and accuracy 0.1527777761220932.\n",
      "Test batch 344 with loss 1.9173732995986938 and accuracy 0.0972222238779068.\n",
      "Test batch 345 with loss 1.8950496912002563 and accuracy 0.15833333134651184.\n",
      "Test batch 346 with loss 1.6870578527450562 and accuracy 0.2698412835597992.\n",
      "Test batch 347 with loss 1.7675659656524658 and accuracy 0.2888889014720917.\n",
      "Test batch 348 with loss 1.8073737621307373 and accuracy 0.1527777910232544.\n",
      "Test batch 349 with loss 1.720664620399475 and accuracy 0.27142858505249023.\n",
      "Test batch 350 with loss 1.7965891361236572 and accuracy 0.1666666716337204.\n",
      "Test batch 351 with loss 1.8895076513290405 and accuracy 0.1388888955116272.\n",
      "Test batch 352 with loss 1.8064441680908203 and accuracy 0.24166667461395264.\n",
      "Test batch 353 with loss 1.8389707803726196 and accuracy 0.02380952425301075.\n",
      "Test batch 354 with loss 1.8354060649871826 and accuracy 0.25555554032325745.\n",
      "Test batch 355 with loss 1.9066250324249268 and accuracy 0.06666667014360428.\n",
      "Test batch 356 with loss 1.8795840740203857 and accuracy 0.222222238779068.\n",
      "Test batch 357 with loss 1.8796799182891846 and accuracy 0.10476191341876984.\n",
      "Test batch 358 with loss 1.861212134361267 and accuracy 0.3166666626930237.\n",
      "Test batch 359 with loss 1.7384626865386963 and accuracy 0.17500001192092896.\n",
      "Test batch 360 with loss 1.8188908100128174 and accuracy 0.03333333507180214.\n",
      "Test batch 361 with loss 1.8522052764892578 and accuracy 0.20000000298023224.\n",
      "Test batch 362 with loss 1.8519446849822998 and accuracy 0.2083333432674408.\n",
      "Test batch 363 with loss 1.7285534143447876 and accuracy 0.18611110746860504.\n",
      "Test batch 364 with loss 1.7951242923736572 and accuracy 0.1388888955116272.\n",
      "Test batch 365 with loss 1.8286793231964111 and accuracy 0.0515873022377491.\n",
      "Test batch 366 with loss 1.8349792957305908 and accuracy 0.03333333507180214.\n",
      "Test batch 367 with loss 1.8422343730926514 and accuracy 0.1111111119389534.\n",
      "Test batch 368 with loss 1.8283107280731201 and accuracy 0.1944444477558136.\n",
      "Test batch 369 with loss 1.8341363668441772 and accuracy 0.15833333134651184.\n",
      "Test batch 370 with loss 1.7472206354141235 and accuracy 0.3333333134651184.\n",
      "Test batch 371 with loss 1.9159905910491943 and accuracy 0.125.\n",
      "Test batch 372 with loss 1.8807331323623657 and accuracy 0.10185185074806213.\n",
      "Test batch 373 with loss 1.8442188501358032 and accuracy 0.0476190485060215.\n",
      "Test batch 374 with loss 1.9164842367172241 and accuracy 0.1319444477558136.\n",
      "Test batch 375 with loss 1.810144066810608 and accuracy 0.3472222089767456.\n",
      "Test batch 376 with loss 1.7840999364852905 and accuracy 0.18809524178504944.\n",
      "Test batch 377 with loss 1.7747704982757568 and accuracy 0.111111119389534.\n",
      "Test batch 378 with loss 1.861283540725708 and accuracy 0.0555555559694767.\n",
      "Test batch 379 with loss 1.841160774230957 and accuracy 0.0555555559694767.\n",
      "Test batch 380 with loss 1.7037630081176758 and accuracy 0.3611111044883728.\n",
      "Test batch 381 with loss 1.8731712102890015 and accuracy 0.0714285746216774.\n",
      "Test batch 382 with loss 1.8476804494857788 and accuracy 0.1666666716337204.\n",
      "Test batch 383 with loss 1.8645524978637695 and accuracy 0.1527777910232544.\n",
      "Test batch 384 with loss 1.7910445928573608 and accuracy 0.14444445073604584.\n",
      "Test batch 385 with loss 1.9107847213745117 and accuracy 0.0833333358168602.\n",
      "Test batch 386 with loss 1.7600862979888916 and accuracy 0.08888889104127884.\n",
      "Test batch 387 with loss 1.8894380331039429 and accuracy 0.10833333432674408.\n",
      "Test batch 388 with loss 1.8339906930923462 and accuracy 0.07500000298023224.\n",
      "Test batch 389 with loss 1.879647970199585 and accuracy 0.2083333432674408.\n",
      "Test batch 390 with loss 1.784863829612732 and accuracy 0.2083333432674408.\n",
      "Test batch 391 with loss 1.8077795505523682 and accuracy 0.17222222685813904.\n",
      "Test batch 392 with loss 1.7956287860870361 and accuracy 0.1527777761220932.\n",
      "Test batch 393 with loss 1.8071292638778687 and accuracy 0.0833333358168602.\n",
      "Test batch 394 with loss 1.7461907863616943 and accuracy 0.25833335518836975.\n",
      "Test batch 395 with loss 2.000673770904541 and accuracy 0.1111111119389534.\n",
      "Test batch 396 with loss 1.939003586769104 and accuracy 0.1666666716337204.\n",
      "Test batch 397 with loss 1.7325938940048218 and accuracy 0.18333333730697632.\n",
      "Test batch 398 with loss 1.9050995111465454 and accuracy 0.065476194024086.\n",
      "Test batch 399 with loss 1.7303025722503662 and accuracy 0.2638888955116272.\n",
      "Test batch 400 with loss 1.7825685739517212 and accuracy 0.15833333134651184.\n",
      "Test batch 401 with loss 1.8101985454559326 and accuracy 0.1726190596818924.\n",
      "Test batch 402 with loss 1.8297914266586304 and accuracy 0.0972222238779068.\n",
      "Test batch 403 with loss 1.7722734212875366 and accuracy 0.1944444477558136.\n",
      "Test batch 404 with loss 1.9267915487289429 and accuracy 0.0.\n",
      "Test batch 405 with loss 1.7038949728012085 and accuracy 0.3888888955116272.\n",
      "Test batch 406 with loss 1.7740398645401 and accuracy 0.2222222238779068.\n",
      "Test batch 407 with loss 1.8736608028411865 and accuracy 0.10833333432674408.\n",
      "Test batch 408 with loss 1.8653643131256104 and accuracy 0.0833333358168602.\n",
      "Test batch 409 with loss 1.8880630731582642 and accuracy 0.06018518656492233.\n",
      "Test batch 410 with loss 1.8462793827056885 and accuracy 0.11666667461395264.\n",
      "Test batch 411 with loss 1.8100608587265015 and accuracy 0.1587301641702652.\n",
      "Test batch 412 with loss 1.72812819480896 and accuracy 0.06666667014360428.\n",
      "Test batch 413 with loss 1.7234876155853271 and accuracy 0.3630952537059784.\n",
      "Test batch 414 with loss 1.7673819065093994 and accuracy 0.18333333730697632.\n",
      "Test batch 415 with loss 1.9187662601470947 and accuracy 0.0416666679084301.\n",
      "Test batch 416 with loss 1.8448158502578735 and accuracy 0.125.\n",
      "Test batch 417 with loss 1.8765764236450195 and accuracy 0.0555555559694767.\n",
      "Test batch 418 with loss 1.880061149597168 and accuracy 0.23333334922790527.\n",
      "Test batch 419 with loss 1.8004446029663086 and accuracy 0.1488095223903656.\n",
      "Test batch 420 with loss 1.759791374206543 and accuracy 0.2599206268787384.\n",
      "Test batch 421 with loss 1.7634766101837158 and accuracy 0.21944445371627808.\n",
      "Test batch 422 with loss 1.8421186208724976 and accuracy 0.17777778208255768.\n",
      "Test batch 423 with loss 1.72184157371521 and accuracy 0.2380952537059784.\n",
      "Test batch 424 with loss 1.7935712337493896 and accuracy 0.15740741789340973.\n",
      "Test batch 425 with loss 1.8830888271331787 and accuracy 0.08888889104127884.\n",
      "Test batch 426 with loss 1.746751070022583 and accuracy 0.3888888955116272.\n",
      "Test batch 427 with loss 1.7605278491973877 and accuracy 0.1666666716337204.\n",
      "Test batch 428 with loss 1.7964986562728882 and accuracy 0.25555557012557983.\n",
      "Test batch 429 with loss 1.8700937032699585 and accuracy 0.08888889104127884.\n",
      "Test batch 430 with loss 1.7995271682739258 and accuracy 0.1111111119389534.\n",
      "Test batch 431 with loss 1.8535394668579102 and accuracy 0.1944444477558136.\n",
      "Test batch 432 with loss 1.8564258813858032 and accuracy 0.02380952425301075.\n",
      "Test batch 433 with loss 1.8799043893814087 and accuracy 0.1666666716337204.\n",
      "Test batch 434 with loss 1.8155195713043213 and accuracy 0.09444444626569748.\n",
      "Test batch 435 with loss 1.9040769338607788 and accuracy 0.0.\n",
      "Test batch 436 with loss 1.8312606811523438 and accuracy 0.190476194024086.\n",
      "Test batch 437 with loss 1.724576711654663 and accuracy 0.3333333432674408.\n",
      "Test batch 438 with loss 1.924795389175415 and accuracy 0.0.\n",
      "Test batch 439 with loss 1.8339296579360962 and accuracy 0.0476190485060215.\n",
      "Test batch 440 with loss 1.8563579320907593 and accuracy 0.1597222238779068.\n",
      "Test batch 441 with loss 1.8783385753631592 and accuracy 0.0833333358168602.\n",
      "Test batch 442 with loss 1.8769598007202148 and accuracy 0.0555555559694767.\n",
      "Test batch 443 with loss 1.7732387781143188 and accuracy 0.2611111104488373.\n",
      "Test batch 444 with loss 1.8059842586517334 and accuracy 0.1875.\n",
      "Test batch 445 with loss 1.8654009103775024 and accuracy 0.0793650820851326.\n",
      "Test batch 446 with loss 1.9708433151245117 and accuracy 0.1527777761220932.\n",
      "Test batch 447 with loss 1.844233512878418 and accuracy 0.0476190485060215.\n",
      "Test batch 448 with loss 1.8521045446395874 and accuracy 0.1944444477558136.\n",
      "Test batch 449 with loss 1.8124078512191772 and accuracy 0.20000001788139343.\n",
      "Test batch 450 with loss 1.773419976234436 and accuracy 0.13055555522441864.\n",
      "Test batch 451 with loss 1.8382822275161743 and accuracy 0.0833333358168602.\n",
      "Test batch 452 with loss 1.787936806678772 and accuracy 0.19166666269302368.\n",
      "Test batch 453 with loss 1.7829965353012085 and accuracy 0.16388888657093048.\n",
      "Test batch 454 with loss 1.8496761322021484 and accuracy 0.15555556118488312.\n",
      "Test batch 455 with loss 1.7613626718521118 and accuracy 0.3333333432674408.\n",
      "Test batch 456 with loss 1.8105814456939697 and accuracy 0.32500001788139343.\n",
      "Test batch 457 with loss 1.8434165716171265 and accuracy 0.15000000596046448.\n",
      "Test batch 458 with loss 1.8960771560668945 and accuracy 0.08888889104127884.\n",
      "Test batch 459 with loss 1.734975814819336 and accuracy 0.24166667461395264.\n",
      "Test batch 460 with loss 1.7368800640106201 and accuracy 0.2222222238779068.\n",
      "Test batch 461 with loss 1.8306198120117188 and accuracy 0.1111111119389534.\n",
      "Test batch 462 with loss 1.8690392971038818 and accuracy 0.0833333358168602.\n",
      "Test batch 463 with loss 1.6895768642425537 and accuracy 0.190476194024086.\n",
      "Test batch 464 with loss 1.8099000453948975 and accuracy 0.1111111119389534.\n",
      "Test batch 465 with loss 1.860018014907837 and accuracy 0.0555555559694767.\n",
      "Test batch 466 with loss 1.7383626699447632 and accuracy 0.3166666626930237.\n",
      "Test batch 467 with loss 1.9380521774291992 and accuracy 0.125.\n",
      "Test batch 468 with loss 1.8137762546539307 and accuracy 0.125.\n",
      "Test batch 469 with loss 1.9179389476776123 and accuracy 0.0694444477558136.\n",
      "Test batch 470 with loss 1.8299528360366821 and accuracy 0.3333333432674408.\n",
      "Test batch 471 with loss 1.8474853038787842 and accuracy 0.0833333358168602.\n",
      "Test batch 472 with loss 1.806612253189087 and accuracy 0.3055555820465088.\n",
      "Test batch 473 with loss 1.8798820972442627 and accuracy 0.1388888955116272.\n",
      "Test batch 474 with loss 1.8290042877197266 and accuracy 0.1666666716337204.\n",
      "Test batch 475 with loss 1.7912324666976929 and accuracy 0.1111111119389534.\n",
      "Test batch 476 with loss 1.88458251953125 and accuracy 0.07500000298023224.\n",
      "Test batch 477 with loss 1.8210567235946655 and accuracy 0.380952388048172.\n",
      "Test batch 478 with loss 1.7759193181991577 and accuracy 0.15555556118488312.\n",
      "Test batch 479 with loss 1.9028213024139404 and accuracy 0.12222222983837128.\n",
      "Test batch 480 with loss 1.9270213842391968 and accuracy 0.0.\n",
      "Test batch 481 with loss 1.8096975088119507 and accuracy 0.31111112236976624.\n",
      "Test batch 482 with loss 1.8450425863265991 and accuracy 0.11666667461395264.\n",
      "Test batch 483 with loss 1.8148857355117798 and accuracy 0.11666667461395264.\n",
      "Test batch 484 with loss 1.8285324573516846 and accuracy 0.11666667461395264.\n",
      "Test batch 485 with loss 1.8556829690933228 and accuracy 0.2222222238779068.\n",
      "Test batch 486 with loss 1.8495123386383057 and accuracy 0.15000000596046448.\n",
      "Test batch 487 with loss 1.797538161277771 and accuracy 0.1964285671710968.\n",
      "Test batch 488 with loss 1.7963178157806396 and accuracy 0.2222222238779068.\n",
      "Test batch 489 with loss 1.7699089050292969 and accuracy 0.26944443583488464.\n",
      "Test batch 490 with loss 1.9016424417495728 and accuracy 0.0476190485060215.\n",
      "Test batch 491 with loss 1.8095630407333374 and accuracy 0.21944445371627808.\n",
      "Test batch 492 with loss 1.7314653396606445 and accuracy 0.2222222238779068.\n",
      "Test batch 493 with loss 1.8458631038665771 and accuracy 0.07500000298023224.\n",
      "Test batch 494 with loss 1.913123369216919 and accuracy 0.0972222238779068.\n",
      "Test batch 495 with loss 1.877319097518921 and accuracy 0.1111111119389534.\n",
      "Test batch 496 with loss 1.845781922340393 and accuracy 0.10000000894069672.\n",
      "Test batch 497 with loss 1.8540141582489014 and accuracy 0.19166666269302368.\n",
      "Test batch 498 with loss 2.055884838104248 and accuracy 0.0.\n",
      "Test batch 499 with loss 1.7133973836898804 and accuracy 0.21666666865348816.\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu' # set so 'cuda:xx' if you have a GPU, xx is GPU index. L'entraînement des réseaux de neurones est grandement accéléré par l'utilisation d'un GPU \n",
    "x0 = next(iter(dataloader_train))[0] # [B, T, C]\n",
    "model = DumbModel(x0.size(2), x0.size(1), 6)  # vous instanciez ici votre modèle\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy # définissez la fonction de perte selon laquelle le modèle sera optimisé\n",
    "\n",
    "# en pratique on utilise pas une simple descente de gradient mais une procédure d'optimisation plus sophistiquée qui est implémentée sous la forme d'un objet Optimizer.\n",
    "# Il en existe beaucoup d'optimizers différents, vous pouvez en tester différents, \n",
    "# je vous propose d'utiliser en premier lieu l'algorithme Adam\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "n_epochs = 10 # le nombre d'itérations dans l'entrainement \n",
    "\n",
    "chemin_vers_sauvegarde_model = Path(\"saved_models\")/\"dumb_model\" # chemin vers un fichier où vous sauvegarderez votre modèle après optimisation pour le réutiliser plus tard. \n",
    "\n",
    "model.to(device) # on place le modèle dans le GPU si nécessaire\n",
    "training_loss = []\n",
    "test_loss = []\n",
    "metric = torchmetrics.classification.MulticlassAccuracy(num_classes=6)\n",
    "test_accuracy = []\n",
    "for epoch in range(n_epochs):\n",
    "    training_loss.append([])\n",
    "    i = 0\n",
    "    for batch_x,batch_y in dataloader_train:\n",
    "        batch_x.to(device)\n",
    "        batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_y_predicted = model(batch_x.permute(0, 2, 1)) # in : [B, C, T]; out : [B, 6]\n",
    "        \n",
    "        l = loss(batch_y_predicted, batch_y)\n",
    "        # loggez la loss sur le batch d'entraînement\n",
    "        print(f\"Training epoch {epoch} batch {i} with loss {l}, accuracy {metric(batch_y_predicted, batch_y)}.\")\n",
    "        training_loss[epoch].append(float(l))\n",
    "        i += 1\n",
    "        \n",
    "        l.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    i = 0\n",
    "    test_loss.append([])\n",
    "    test_accuracy.append([])\n",
    "    for batch_x,batch_y in dataloader_valid:\n",
    "        \n",
    "        batch_x.to(device)\n",
    "        batch_y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_y_predicted = model(batch_x.permute(0, 2, 1))  \n",
    "            \n",
    "        # loggez la loss et les métriques sur le batch de validation\n",
    "        l = loss(batch_y_predicted, batch_y)\n",
    "        accuracy = metric(batch_y_predicted, batch_y)\n",
    "        print(f\"Test batch {i} with loss {l} and accuracy {accuracy}.\")\n",
    "        test_loss[epoch].append(float(l))\n",
    "        test_accuracy[epoch].append(float(accuracy))\n",
    "\n",
    "        i += 1\n",
    "torch.save(model, chemin_vers_sauvegarde_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1a455335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(sum(training_loss, []), '.')\n",
    "plt.plot(np.arange(1, n_epochs + 1) * len(dataloader_train), [np.mean(loss_epoch) for loss_epoch in training_loss], '-o', label = \"Loss moyenne sur l'epoch\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.legend()\n",
    "plt.xlabel('# batch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sum(test_loss, []), '.')\n",
    "plt.plot(np.arange(1, n_epochs + 1) * len(dataloader_valid), [np.mean(loss_epoch) for loss_epoch in test_loss], '-o', label = \"Loss moyenne sur l'epoch\")\n",
    "plt.title(\"Test loss\")\n",
    "plt.legend()\n",
    "plt.xlabel('# batch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sum(test_accuracy, []), '.')\n",
    "plt.plot(np.arange(1, n_epochs + 1) * len(dataloader_valid), [np.mean(loss_epoch) for loss_epoch in test_accuracy], '-o', label = \"Accuracy moyenne sur l'epoch\")\n",
    "plt.title(\"Test accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel('# batch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4994ac40",
   "metadata": {},
   "source": [
    "Vérifiez que vous avez bien enregistré votre modèle en fin d'entrainement. Chargez le avec la fonction \n",
    "```python\n",
    "modele = torch.load(...) \n",
    "```\n",
    "et vérifiez que vous pouvez l'utiliser sur des données du problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test batch with loss 1.9162102937698364.\n",
      "Test batch with loss 1.8490830659866333.\n",
      "Test batch with loss 1.7137503623962402.\n",
      "Test batch with loss 1.8939495086669922.\n",
      "Test batch with loss 1.824660062789917.\n",
      "Test batch with loss 1.7863051891326904.\n",
      "Test batch with loss 1.751853585243225.\n",
      "Test batch with loss 1.8758474588394165.\n",
      "Test batch with loss 1.906531810760498.\n",
      "Test batch with loss 1.855523705482483.\n",
      "Test batch with loss 1.8069117069244385.\n",
      "Test batch with loss 1.865397334098816.\n",
      "Test batch with loss 1.7928205728530884.\n",
      "Test batch with loss 1.7804055213928223.\n",
      "Test batch with loss 1.8974405527114868.\n",
      "Test batch with loss 1.847578525543213.\n",
      "Test batch with loss 1.8308601379394531.\n",
      "Test batch with loss 1.7982773780822754.\n",
      "Test batch with loss 1.835289478302002.\n",
      "Test batch with loss 1.7021275758743286.\n",
      "Test batch with loss 1.847933292388916.\n",
      "Test batch with loss 1.777271032333374.\n",
      "Test batch with loss 1.8214532136917114.\n",
      "Test batch with loss 1.8546814918518066.\n",
      "Test batch with loss 1.8456361293792725.\n",
      "Test batch with loss 1.8831318616867065.\n",
      "Test batch with loss 1.8747378587722778.\n",
      "Test batch with loss 1.783947229385376.\n",
      "Test batch with loss 1.876750111579895.\n",
      "Test batch with loss 1.8339537382125854.\n",
      "Test batch with loss 1.8899210691452026.\n",
      "Test batch with loss 1.7937532663345337.\n",
      "Test batch with loss 1.7909682989120483.\n",
      "Test batch with loss 1.7763829231262207.\n",
      "Test batch with loss 1.910140872001648.\n",
      "Test batch with loss 1.8486912250518799.\n",
      "Test batch with loss 1.7710416316986084.\n",
      "Test batch with loss 1.7975019216537476.\n",
      "Test batch with loss 1.8079115152359009.\n",
      "Test batch with loss 1.887978434562683.\n",
      "Test batch with loss 1.8657532930374146.\n",
      "Test batch with loss 1.7776778936386108.\n",
      "Test batch with loss 1.8542572259902954.\n",
      "Test batch with loss 1.851121187210083.\n",
      "Test batch with loss 1.8068125247955322.\n",
      "Test batch with loss 1.892358422279358.\n",
      "Test batch with loss 1.7849401235580444.\n",
      "Test batch with loss 1.7799948453903198.\n",
      "Test batch with loss 1.8551923036575317.\n",
      "Test batch with loss 1.7798805236816406.\n",
      "Test batch with loss 1.7459778785705566.\n",
      "Test batch with loss 1.8656479120254517.\n",
      "Test batch with loss 1.8335206508636475.\n",
      "Test batch with loss 1.7511533498764038.\n",
      "Test batch with loss 1.8252421617507935.\n",
      "Test batch with loss 1.891105055809021.\n",
      "Test batch with loss 1.8318939208984375.\n",
      "Test batch with loss 1.8514262437820435.\n",
      "Test batch with loss 1.9175080060958862.\n",
      "Test batch with loss 1.825242280960083.\n",
      "Test batch with loss 1.8942149877548218.\n",
      "Test batch with loss 1.8924789428710938.\n",
      "Test batch with loss 1.7900450229644775.\n",
      "Test batch with loss 1.8426142930984497.\n",
      "Test batch with loss 1.8817700147628784.\n",
      "Test batch with loss 1.771126389503479.\n",
      "Test batch with loss 1.838722825050354.\n",
      "Test batch with loss 1.8329048156738281.\n",
      "Test batch with loss 1.8039618730545044.\n",
      "Test batch with loss 1.8319350481033325.\n",
      "Test batch with loss 1.846766710281372.\n",
      "Test batch with loss 1.9525567293167114.\n",
      "Test batch with loss 1.8129791021347046.\n",
      "Test batch with loss 1.7412725687026978.\n",
      "Test batch with loss 1.942023754119873.\n",
      "Test batch with loss 1.927883505821228.\n",
      "Test batch with loss 1.8375585079193115.\n",
      "Test batch with loss 1.7513707876205444.\n",
      "Test batch with loss 1.7577518224716187.\n",
      "Test batch with loss 1.8254026174545288.\n",
      "Test batch with loss 1.8836270570755005.\n",
      "Test batch with loss 1.755788803100586.\n",
      "Test batch with loss 1.8138152360916138.\n",
      "Test batch with loss 1.8201371431350708.\n",
      "Test batch with loss 1.8699473142623901.\n",
      "Test batch with loss 1.9148422479629517.\n",
      "Test batch with loss 1.842938780784607.\n",
      "Test batch with loss 1.7922669649124146.\n",
      "Test batch with loss 1.8504184484481812.\n",
      "Test batch with loss 1.8766400814056396.\n",
      "Test batch with loss 1.8069993257522583.\n",
      "Test batch with loss 1.857843041419983.\n",
      "Test batch with loss 1.85202956199646.\n",
      "Test batch with loss 1.7817658185958862.\n",
      "Test batch with loss 1.850480318069458.\n",
      "Test batch with loss 1.8032809495925903.\n",
      "Test batch with loss 1.761633276939392.\n",
      "Test batch with loss 1.71877121925354.\n",
      "Test batch with loss 1.8613077402114868.\n",
      "Test batch with loss 1.8920609951019287.\n",
      "Test batch with loss 1.8278263807296753.\n",
      "Test batch with loss 1.789974331855774.\n",
      "Test batch with loss 1.755144715309143.\n",
      "Test batch with loss 1.8007450103759766.\n",
      "Test batch with loss 1.7397929430007935.\n",
      "Test batch with loss 1.8265498876571655.\n",
      "Test batch with loss 1.817805528640747.\n",
      "Test batch with loss 1.767279863357544.\n",
      "Test batch with loss 1.8128308057785034.\n",
      "Test batch with loss 1.9016145467758179.\n",
      "Test batch with loss 1.9521033763885498.\n",
      "Test batch with loss 1.7960059642791748.\n",
      "Test batch with loss 1.861072063446045.\n",
      "Test batch with loss 1.888999342918396.\n",
      "Test batch with loss 1.8602149486541748.\n",
      "Test batch with loss 1.799710988998413.\n",
      "Test batch with loss 1.8543483018875122.\n",
      "Test batch with loss 1.8534696102142334.\n",
      "Test batch with loss 1.7426875829696655.\n",
      "Test batch with loss 1.8699337244033813.\n",
      "Test batch with loss 1.7365193367004395.\n",
      "Test batch with loss 1.8578031063079834.\n",
      "Test batch with loss 1.8913605213165283.\n",
      "Test batch with loss 1.8478200435638428.\n",
      "Test batch with loss 1.8583298921585083.\n",
      "Test batch with loss 1.8731868267059326.\n",
      "Test batch with loss 1.7684532403945923.\n",
      "Test batch with loss 1.8640706539154053.\n",
      "Test batch with loss 1.7800885438919067.\n",
      "Test batch with loss 1.871703028678894.\n",
      "Test batch with loss 1.862223744392395.\n",
      "Test batch with loss 1.7635688781738281.\n",
      "Test batch with loss 1.8241173028945923.\n",
      "Test batch with loss 1.834578275680542.\n",
      "Test batch with loss 1.8939212560653687.\n",
      "Test batch with loss 1.8867732286453247.\n",
      "Test batch with loss 1.8448314666748047.\n",
      "Test batch with loss 1.7929335832595825.\n",
      "Test batch with loss 1.8345798254013062.\n",
      "Test batch with loss 1.823066234588623.\n",
      "Test batch with loss 1.815693974494934.\n",
      "Test batch with loss 1.7936564683914185.\n",
      "Test batch with loss 1.8201262950897217.\n",
      "Test batch with loss 1.7970024347305298.\n",
      "Test batch with loss 1.817651629447937.\n",
      "Test batch with loss 1.7738909721374512.\n",
      "Test batch with loss 1.8625009059906006.\n",
      "Test batch with loss 1.8009010553359985.\n",
      "Test batch with loss 1.7610313892364502.\n",
      "Test batch with loss 1.761971116065979.\n",
      "Test batch with loss 1.7928290367126465.\n",
      "Test batch with loss 1.7317674160003662.\n",
      "Test batch with loss 1.8606067895889282.\n",
      "Test batch with loss 1.7910003662109375.\n",
      "Test batch with loss 1.8465077877044678.\n",
      "Test batch with loss 1.7501137256622314.\n",
      "Test batch with loss 1.7812589406967163.\n",
      "Test batch with loss 1.7755932807922363.\n",
      "Test batch with loss 1.865265965461731.\n",
      "Test batch with loss 1.7981504201889038.\n",
      "Test batch with loss 1.7785636186599731.\n",
      "Test batch with loss 1.7521533966064453.\n",
      "Test batch with loss 1.805539846420288.\n",
      "Test batch with loss 1.7982109785079956.\n",
      "Test batch with loss 1.7779642343521118.\n",
      "Test batch with loss 1.7724976539611816.\n",
      "Test batch with loss 1.842440128326416.\n",
      "Test batch with loss 1.797027349472046.\n",
      "Test batch with loss 1.9081157445907593.\n",
      "Test batch with loss 1.85367751121521.\n",
      "Test batch with loss 1.7975655794143677.\n",
      "Test batch with loss 1.8544524908065796.\n",
      "Test batch with loss 1.7797044515609741.\n",
      "Test batch with loss 1.8890751600265503.\n",
      "Test batch with loss 1.8515803813934326.\n",
      "Test batch with loss 1.799656629562378.\n",
      "Test batch with loss 1.873406171798706.\n",
      "Test batch with loss 1.8575667142868042.\n",
      "Test batch with loss 1.7800856828689575.\n",
      "Test batch with loss 1.9230797290802002.\n",
      "Test batch with loss 1.8096877336502075.\n",
      "Test batch with loss 1.74227774143219.\n",
      "Test batch with loss 1.929594874382019.\n",
      "Test batch with loss 1.8850942850112915.\n",
      "Test batch with loss 1.8348324298858643.\n",
      "Test batch with loss 1.869486927986145.\n",
      "Test batch with loss 1.7925896644592285.\n",
      "Test batch with loss 1.7984987497329712.\n",
      "Test batch with loss 1.7600769996643066.\n",
      "Test batch with loss 1.829530954360962.\n",
      "Test batch with loss 1.840207815170288.\n",
      "Test batch with loss 1.8005229234695435.\n",
      "Test batch with loss 1.8049482107162476.\n",
      "Test batch with loss 1.810868501663208.\n",
      "Test batch with loss 1.8580023050308228.\n",
      "Test batch with loss 1.9247756004333496.\n",
      "Test batch with loss 1.8064781427383423.\n",
      "Test batch with loss 1.6762083768844604.\n",
      "Test batch with loss 1.844858169555664.\n",
      "Test batch with loss 1.8498328924179077.\n",
      "Test batch with loss 1.8759055137634277.\n",
      "Test batch with loss 1.7882827520370483.\n",
      "Test batch with loss 1.813572645187378.\n",
      "Test batch with loss 1.8066539764404297.\n",
      "Test batch with loss 1.8717236518859863.\n",
      "Test batch with loss 1.830405592918396.\n",
      "Test batch with loss 1.7808946371078491.\n",
      "Test batch with loss 1.8250033855438232.\n",
      "Test batch with loss 1.8180369138717651.\n",
      "Test batch with loss 1.8221046924591064.\n",
      "Test batch with loss 1.9005541801452637.\n",
      "Test batch with loss 1.929185152053833.\n",
      "Test batch with loss 1.786400318145752.\n",
      "Test batch with loss 1.8387031555175781.\n",
      "Test batch with loss 1.8361960649490356.\n",
      "Test batch with loss 1.8472548723220825.\n",
      "Test batch with loss 1.824708342552185.\n",
      "Test batch with loss 1.7968467473983765.\n",
      "Test batch with loss 1.8860840797424316.\n",
      "Test batch with loss 1.8105746507644653.\n",
      "Test batch with loss 1.835155725479126.\n",
      "Test batch with loss 1.7967232465744019.\n",
      "Test batch with loss 1.8304882049560547.\n",
      "Test batch with loss 1.8937151432037354.\n",
      "Test batch with loss 1.8059481382369995.\n",
      "Test batch with loss 1.8899462223052979.\n",
      "Test batch with loss 1.8207253217697144.\n",
      "Test batch with loss 1.7106812000274658.\n",
      "Test batch with loss 1.9704220294952393.\n",
      "Test batch with loss 1.7972629070281982.\n",
      "Test batch with loss 1.8328557014465332.\n",
      "Test batch with loss 1.8037090301513672.\n",
      "Test batch with loss 1.8293513059616089.\n",
      "Test batch with loss 1.8997249603271484.\n",
      "Test batch with loss 1.874125599861145.\n",
      "Test batch with loss 1.8444628715515137.\n",
      "Test batch with loss 1.8773428201675415.\n",
      "Test batch with loss 1.8123931884765625.\n",
      "Test batch with loss 1.8444099426269531.\n",
      "Test batch with loss 1.7606313228607178.\n",
      "Test batch with loss 1.8307896852493286.\n",
      "Test batch with loss 1.806514859199524.\n",
      "Test batch with loss 1.8680628538131714.\n",
      "Test batch with loss 1.9139459133148193.\n",
      "Test batch with loss 1.914064645767212.\n",
      "Test batch with loss 1.8448565006256104.\n",
      "Test batch with loss 1.83535635471344.\n",
      "Test batch with loss 1.875558853149414.\n",
      "Test batch with loss 1.8593966960906982.\n",
      "Test batch with loss 1.8939546346664429.\n",
      "Test batch with loss 1.7600644826889038.\n",
      "Test batch with loss 1.85807204246521.\n",
      "Test batch with loss 1.9028812646865845.\n",
      "Test batch with loss 1.796142816543579.\n",
      "Test batch with loss 1.8635481595993042.\n",
      "Test batch with loss 1.7698594331741333.\n",
      "Test batch with loss 1.8188157081604004.\n",
      "Test batch with loss 1.7952293157577515.\n",
      "Test batch with loss 1.823656678199768.\n",
      "Test batch with loss 1.8452173471450806.\n",
      "Test batch with loss 1.8188730478286743.\n",
      "Test batch with loss 1.7938826084136963.\n",
      "Test batch with loss 1.8008230924606323.\n",
      "Test batch with loss 1.8009397983551025.\n",
      "Test batch with loss 1.7752997875213623.\n",
      "Test batch with loss 1.7061617374420166.\n",
      "Test batch with loss 1.8315660953521729.\n",
      "Test batch with loss 1.744788646697998.\n",
      "Test batch with loss 1.8831334114074707.\n",
      "Test batch with loss 1.7870326042175293.\n",
      "Test batch with loss 1.8102033138275146.\n",
      "Test batch with loss 1.7726467847824097.\n",
      "Test batch with loss 1.747703194618225.\n",
      "Test batch with loss 1.8688548803329468.\n",
      "Test batch with loss 1.7671889066696167.\n",
      "Test batch with loss 1.8213226795196533.\n",
      "Test batch with loss 1.7445042133331299.\n",
      "Test batch with loss 1.7953437566757202.\n",
      "Test batch with loss 1.7983945608139038.\n",
      "Test batch with loss 1.7881139516830444.\n",
      "Test batch with loss 1.8961459398269653.\n",
      "Test batch with loss 1.8057830333709717.\n",
      "Test batch with loss 1.7868459224700928.\n",
      "Test batch with loss 1.793512225151062.\n",
      "Test batch with loss 1.9091582298278809.\n",
      "Test batch with loss 1.8369332551956177.\n",
      "Test batch with loss 1.80052649974823.\n",
      "Test batch with loss 1.8392863273620605.\n",
      "Test batch with loss 1.7805286645889282.\n",
      "Test batch with loss 1.8213342428207397.\n",
      "Test batch with loss 1.8257948160171509.\n",
      "Test batch with loss 1.844853162765503.\n",
      "Test batch with loss 1.8871169090270996.\n",
      "Test batch with loss 1.750553846359253.\n",
      "Test batch with loss 1.8403241634368896.\n",
      "Test batch with loss 1.8660157918930054.\n",
      "Test batch with loss 1.8644773960113525.\n",
      "Test batch with loss 1.8895230293273926.\n",
      "Test batch with loss 1.846510410308838.\n",
      "Test batch with loss 1.8041001558303833.\n",
      "Test batch with loss 1.823708176612854.\n",
      "Test batch with loss 1.8465930223464966.\n",
      "Test batch with loss 1.8139839172363281.\n",
      "Test batch with loss 1.8549193143844604.\n",
      "Test batch with loss 1.9062026739120483.\n",
      "Test batch with loss 1.8064301013946533.\n",
      "Test batch with loss 1.8166041374206543.\n",
      "Test batch with loss 1.8691749572753906.\n",
      "Test batch with loss 1.853782057762146.\n",
      "Test batch with loss 1.7682510614395142.\n",
      "Test batch with loss 1.7947421073913574.\n",
      "Test batch with loss 1.7602951526641846.\n",
      "Test batch with loss 1.8542640209197998.\n",
      "Test batch with loss 1.797073483467102.\n",
      "Test batch with loss 1.8634085655212402.\n",
      "Test batch with loss 1.7946563959121704.\n",
      "Test batch with loss 1.941799521446228.\n",
      "Test batch with loss 1.8018832206726074.\n",
      "Test batch with loss 1.9079877138137817.\n",
      "Test batch with loss 1.8246101140975952.\n",
      "Test batch with loss 1.7848286628723145.\n",
      "Test batch with loss 1.8650100231170654.\n",
      "Test batch with loss 1.8140549659729004.\n",
      "Test batch with loss 1.937975287437439.\n",
      "Test batch with loss 1.8629523515701294.\n",
      "Test batch with loss 1.8763582706451416.\n",
      "Test batch with loss 1.7922042608261108.\n",
      "Test batch with loss 1.8391010761260986.\n",
      "Test batch with loss 1.8172556161880493.\n",
      "Test batch with loss 1.7923295497894287.\n",
      "Test batch with loss 1.8635258674621582.\n",
      "Test batch with loss 1.8123223781585693.\n",
      "Test batch with loss 1.7880696058273315.\n",
      "Test batch with loss 1.8585647344589233.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loading_model_path = Path(\"saved_models\")/\"dumb_model\"\n",
    "model = torch.load(loading_model_path)\n",
    "\n",
    "loaded_model_loss = []\n",
    "for batch_x,batch_y in dataloader_valid:\n",
    "        \n",
    "        batch_x.to(device)\n",
    "        batch_y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_y_predicted = model(batch_x.permute(0, 2, 1))  \n",
    "            \n",
    "        # loggez la loss et les métriques sur le batch de validation\n",
    "        l = loss(batch_y_predicted, batch_y)\n",
    "        print(f\"Test batch with loss {l}.\")\n",
    "        loaded_model_loss.append(l)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loaded_model_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e164707e",
   "metadata": {},
   "source": [
    "## Entraînement de réseaux de neurones\n",
    "\n",
    "Dans cette partie vous définissez une ou plusieurs architecture de réseaux de neurones profonds et vous les réglez sur les données d'entrainement. \n",
    "Vous pouvez notamment utiliser des réseaux à base de convolutions et/ou de couches réurrentes. Vous pouvez vous inspirer de ce qui a été dit en cours sur la reconnaissance vocale.\n",
    "\n",
    "Vous pouvez si vous le souhaitez mettre en place des stratégies d'augmentation de données pour améliorer vos résultats. Pour mettre l'augmentation de données en pratique pouvez vous renseigner sur l'argument collate_fn du dataloader standard de Pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a914bb87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46810f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "878c3943",
   "metadata": {},
   "source": [
    "## Synthèse de résultats \n",
    "\n",
    "Une fois que votre ou vos réseaux sont entrainez vous comparez leurs performances selon les métriques définies en introduction sur l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbac3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dda3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
