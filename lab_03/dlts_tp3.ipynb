{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c235e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import h5py # pour gérer les formats de données utilisés ici \n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa84c0",
   "metadata": {},
   "source": [
    "# TP3: Reconnaissance de signaux de communication par apprentissage profond \n",
    "\n",
    "<div class=consignes> Listez les noms des étudiants (2 au maximum) ayant participé à ce notebook dans la cellule suivante (prénom, nom, affectation).<br/>\n",
    "Au moment du rendu, le notebook doit être nommé nom1_nom2_dlts_tp3.ipynb \n",
    "\n",
    "3 séances de TP sur ce sujet : le 15 novembre (1h00), le 22 novembre (3h) et le 29 novembre (3h).<br> \n",
    "Deadline : 6 décembre 2023, 11h59, par mail à deepetsignal.mva@gmail.com <br> \n",
    "\n",
    "Pour installer les paquets nécessaires à la réalisation de ce TP vous pouvez utiliser dans le notebook \n",
    "    \n",
    "```\n",
    "!pip install \\< nom_du_paquet \\>\n",
    "```\n",
    "merci de regrouper toutes les installations dans la première cellule du notebook. \n",
    "Essayez de faire en sorte que votre notebook puisse se lire comme un compte rendu, évitez de laisser du code mort et prennez le temps de commenter vos observations et résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747407a",
   "metadata": {},
   "source": [
    "## Problématique\n",
    "\n",
    "On cherche à identifier un type d'émetteur de communication à partir de l'observation d'un signal provenant de l'émetteur \n",
    "de 2048 échantillons IQ (In Phase / Quadrature) ie le signal prend des valeurs complexes. On représente la partie \n",
    "réelle et la partie imaginaire du signal par deux canaux réel d'un signal multivarié. \n",
    "\n",
    "L'émetteur peut provenir de 6 catégories différentes. \n",
    "Les paramètres différenciant les différentes catégories sont \n",
    "- le type de modulation \n",
    "- la présence ou non de séquences pilotes et le cas échéant la structure de trame pilotes / données \n",
    "- le débit de la transmission \n",
    "\n",
    "Les signaux se propagent en champs libre et sont enregistrés par une antenne. Le signal reçu est transposé en bande de base c'est à dire que si le signal est transmis autour d'une fréquence centrale f0, une première étape de traitement du signal à la réception recentre le signal autour de la fréquence 0. \n",
    "\n",
    "\n",
    "Les différents signaux observés dans ce TP sont entachés de différentes erreurs caractéristiques de la propagation \n",
    "électromagnétiques comme : \n",
    "- modification aléatoire de la phase du signal lors de la transmission\n",
    "- imperfection de la transposition en bande de base qui laisse le signal transposé à une fréquence df0 << f0\n",
    "- présence d'interférence entre les symboles transmis (dûes par exemple à plusieurs chemins de propagation)\n",
    "- présence d'un bruit blanc additif gaussien\n",
    "\n",
    "Le niveau du bruit relativement à celui du signal utile est décrit par le SNR (Signal to Noise Ratio) et exprimé en dB. On suppose que le SNR est connu lors de l'acquisition d'un signal. Lors de ce TP nous rencontrerons 4 niveaux de SNR: 30 dB (facile), 20 dB, 10 dB et 0 dB (en espérant qu'on puisse faire quelque chose de ces données). \n",
    "Un de nos objectifs sera de qualifier la performance des algorithmes mis en place en fonction du SNR.\n",
    "\n",
    "Les objectifs de ce TP sont: \n",
    "1/ Définir une ou plusieurs architectures de réseaux de neurones profonds et les implémenter en PyTorch\n",
    "2/ Entrainer ces architectures, la fonction de perte employée pourra être la log vraisemblance négative: https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html. \n",
    "3/ Qualifier les performances de votre réseau de neurones sur l'ensemble de test via: \n",
    "   - Le calcul de l'accuracy implémentée par exemple dans le package TorchMetrics (https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html)\n",
    "   - La réalisation d'un graphique accuracy vs SNR \n",
    "   - La réalisation des matrices de confusion entre les différentes classes pour les différents SNR (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix)\n",
    "\n",
    "Durant l'entraînement on observera l'évolution de la fonction de perte et de l'accuracy sur l'ensemble d'entraînement et sur l'ensemble de validation. \n",
    "\n",
    "\n",
    "Les 4 premières parties sont un échauffement sur lequel vous pouvez passer vite si vous êtes à l'aise avec le sujet. \n",
    "Le gros du travail est dans la partie 5 \"Entraînemenent d'un réseau de neurones\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4865e6",
   "metadata": {},
   "source": [
    "## Chargement des données en numpy\n",
    "\n",
    "Le TP est composé de trois jeux de données : \n",
    "- train.hdf5 destiné à nourrir l'entrainement de réseaux de neurones \n",
    "- test.hdf5 destiné à évaluer les algorithmes après entrainement\n",
    "- samples.hdf5 qui est beaucoup plus petit que train.hdf5 et destiné à servir de modèle de données dans une phase de prototypage \n",
    "des algorithmes et de la pipeline d'entrainement\n",
    "\n",
    "Les trois jeux de données sont au format hdf5 qui peut être manipulé via l'API Python h5py https://docs.h5py.org/en/stable/quick.html.\n",
    "Un fichier hdf5 est consitué d'une arborescence de datasets et de groups. Un dataset hdf5 représente un tenseur n dimensionnel. Un dataset se convertit très facilement en numpy array.\n",
    "\n",
    "Par exemple vous pouvez charger les données samples selon la cellule suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "803ab62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path(\"data\")\n",
    "data_path = DATA/\"samples.hdf5\"\n",
    "assert data_path.exists()\n",
    "\n",
    "data = h5py.File(data_path , 'r')\n",
    "\n",
    "signals = np.array(data['signaux'])\n",
    "snr =  np.array(data['snr'])\n",
    "labels_id = np.array(data['labels'])\n",
    "\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66630a9e",
   "metadata": {},
   "source": [
    "Vous pouvez récupérer le nom de la correspondance entre un label et le nom du standard d'émetteur correspondant via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecab0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(open_h5_file): \n",
    "    return {\n",
    "        open_h5_file['label_name'].attrs[k] : k\n",
    "        for k in open_h5_file['label_name'].attrs.keys()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c61acbe",
   "metadata": {},
   "source": [
    "### Visualisation des données \n",
    "\n",
    "Commencez par étudier les données: \n",
    "\n",
    "    - observez leur taille \n",
    "    - la distribution des différentes classes et des différents SNR dans l'ensemble d'entrainement \n",
    "    - visualisez quelques signaux bien choisis selon une ou des représentations que vous choisirez "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f0a2f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des signaux : (200, 2048, 2)\n",
      "Shape SNR :  (200,)\n",
      "Shape SNR :  (200,)\n",
      "(200,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGyCAYAAABEN6Z8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuOklEQVR4nO3dfVxUZf7/8feoOEIh3qQzkIhUeFNoN1omWVAtFJlbkd1oN5jVWmj7JdtIdMuxLTB2Y+kRm1bb12iL9LvfzXLTSrqRdiMLS8vMLfuKSr+cZTUFUgOV6/eHD6bGQXMULhh7PR+P83g017nOOZ+5uIp315zDOIwxRgAAAJZ0au8CAADAzwvhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QOw4NNPP1VERIQef/zx9i4FANod4QM4TM8++6wcDodv69Kli6Kjo3X99ddr/fr1Bz2uvr5e48aN01133aW77rrLYsWBli5dKo/H0+K+AQMGaOLEib7X33zzjTwej1avXh3Q1+PxyOFwtE2RR8jhcBz0vbWV6upqZWVlaeDAgQoPD1evXr00dOhQ3X777aqurvb1ax6vvn37qr6+PuA8AwYM0OWXX+7X9uO55nA41L17dyUlJenFF19s8/cFtDXCBxCk+fPn6/3339ebb76pqVOnavHixRo9erS2b9/eYv9bb71V55xzjubMmWO50kBLly7V7NmzW9y3aNEi3X///b7X33zzjWbPnt1i+Ljtttv0/vvvt1WZIeHrr7/WWWedpbKyMk2bNk1Lly7Vf//3f2v8+PGqrKzUhg0bAo75z3/+o4KCgsO+xrhx4/T++++roqJC8+bNU11dnSZMmKDS0tLWfCuAdV3auwAg1CQmJmrEiBGSpJSUFO3bt0+zZs3Syy+/rFtuuSWg///8z//YLjHArl27FBERccg+Z5555mGfr1+/furXr9/RlhXSnn76aW3dulUffvih4uPjfe1XXnmlZsyYoaampoBjLr30Uv3xj3/UlClT5Ha7f/IaLpdL5557riRp1KhROu+88zRgwAA9+eSTmjBhQuu9GcAyVj6Ao9QcRP7973/7ta9cuVK//OUv1atXL3Xr1k1nnnlmQBBp/iinrKxMt9xyi3r16qXjjjtOY8eODfg/57KyMl1xxRXq16+funXrplNOOUWTJ0/W1q1b/fo1L/F//PHHGjdunHr27KmTTz5ZEydO1J/+9CdJ/kv6GzdulOT/scvy5ct19tlnS5JuueUWX9/mjzVa+tilqalJBQUFGjx4sJxOp/r27aubb75ZX3/9tV+/lJQUJSYmqrKyUueff74iIiJ00kknac6cOS3+wj5QXV2dbr/9dvXu3VvHH3+8Lr30Un355Zct9l2/fr0mTJigvn37yul0asiQIb4x+HHdDz30kAYNGqTw8HD16NFDw4YN02OPPXbIOrZt26ZOnTqpb9++Le7v1CnwP68PPfSQ9u7de8QfD8XFxalPnz4Bcw0INYQP4ChVVVVJkgYOHOhre+edd3Teeedpx44dmjdvnl555RWdccYZuu666/Tss88GnOPWW29Vp06dVFpaqqKiIn344YdKSUnRjh07fH3+7//+T6NGjdLcuXO1bNkyPfDAA/rggw80evRo7dmzJ+CcGRkZOuWUU/TXv/5V8+bN0/33369x48ZJkt5//33fFh0dHXDsWWedpfnz50uSfvvb3/r63nbbbQcdhzvvvFP33XefUlNTtXjxYv3ud7/T66+/rqSkpICA5PV6dcMNN+jGG2/U4sWLlZ6ertzcXD3//PMHH2hJxhhdeeWV+stf/qJ77rlHixYt0rnnnqv09PSAvp9//rnOPvtsffbZZ3r00Uf16quvasyYMfr1r3/t99FTQUGBPB6Pxo8fryVLlmjhwoW69dZb/ca+JaNGjVJTU5MyMjL0xhtvqK6u7pD9pf3hISsrS88888xBA9Oh1NbW6ttvv/Wba0BIMgAOy/z5840ks2LFCrNnzx5TX19vXn/9deN2u80FF1xg9uzZ4+s7ePBgc+aZZ/q1GWPM5ZdfbqKjo82+ffv8znnVVVf59XvvvfeMJPPQQw+1WEtTU5PZs2eP2bRpk5FkXnnlFd++WbNmGUnmgQceCDhuypQp5mD/2sfFxZnMzEzf68rKSiPJzJ8/P6Bv8zWarVu3zkgyWVlZfv0++OADI8nMmDHD15acnGwkmQ8++MCv76mnnmouueSSFmtr9tprrxlJ5rHHHvNrf/jhh40kM2vWLF/bJZdcYvr162dqa2v9+k6dOtV069bNfPvtt8aY/T+TM84445DXbUlTU5OZPHmy6dSpk5FkHA6HGTJkiLn77rtNVVWVX9/m8frPf/5jtm7daqKioszVV1/t2x8XF2fGjBnjd0zzeO7Zs8c0NjaaL7/80vzyl780kZGRZuXKlUHXC3QkrHwAQTr33HMVFhamyMhIXXrpperZs6deeeUVdemy/xaqr776Sv/61790ww03SJL27t3r2y677DJt2bJFX3zxhd85m/s2S0pKUlxcnN555x1fW01Nje644w7FxsaqS5cuCgsLU1xcnCRp3bp1AXVeffXVrfq+D6W5zh8/LSNJ55xzjoYMGaK33nrLr93tduucc87xaxs2bJg2bdp0WNc5cLwOvP/h+++/11tvvaWrrrpKERERAT+D77//XitWrPDV+MknnygrK+uwVzCk/R9dzZs3Txs2bNATTzyhW265RXv27NEf//hHnXbaaSovL2/xuN69e+u+++7T3/72N33wwQeHvMYTTzyhsLAwde3aVQMHDtRrr72mF198UcOHDz+sGoGOivABBOm5555TZWWl3n77bU2ePFnr1q3T+PHjffubP4//zW9+o7CwML8tKytLkgI+hmjp5kO3261t27ZJ2n9fQlpaml566SXl5OTorbfe0ocffuj7Bbp79+6A41v6OKWtNNfZ0jVjYmJ8+5v17t07oJ/T6WzxfRx4nS5dugQcf+D4bdu2TXv37tXjjz8e8DO47LLLJP3wM8jNzdUf/vAHrVixQunp6erdu7cuvvhirVy58ife9X5xcXG688479cwzz2j9+vVauHChvv/+e917770HPSY7O1sxMTHKyck55LmvvfZaVVZWqqKiQk8++aQiIyN/8tFuIBTwtAsQpCFDhvhuMr3wwgu1b98+/fnPf9b//u//aty4cTrhhBMk7f+llpGR0eI5Bg0a5Pfa6/UG9PF6vTrllFMkSZ999pk++eQTPfvss8rMzPT1+eqrrw5ap82/w9EcBrZs2RLwFMw333zjG5PWuM7evXu1bds2vwBy4Pj17NlTnTt31k033aQpU6a0eK7mJ1S6dOmiadOmadq0adqxY4fefPNNzZgxQ5dccomqq6t/8imhA1177bXKz8/XZ599dtA+4eHh8ng8+tWvfqUlS5YctF+fPn18c23UqFEaMmSIkpOTdffdd+vVV18Nqi6gI2HlAzhKBQUF6tmzpx544AE1NTVp0KBBSkhI0CeffKIRI0a0uEVGRvqd44UXXvB7XVFRoU2bNiklJUXSD0HC6XT69XvyySeDqrX5+J9aYQi270UXXSRJATeMVlZWat26dbr44ouDqvNgLrzwQkmB43Xg372IiIjQhRdeqFWrVmnYsGEt/gxaWn3p0aOHxo0bpylTpujbb7/1PQnUki1btrTY/t1336m6uloxMTGHfC+TJk3SkCFDNH369MN6ykeSzj//fN18881asmTJz/7vrCC0sfIBHKWePXsqNzdXOTk5Ki0t1Y033qgnn3xS6enpuuSSSzRx4kSdeOKJ+vbbb7Vu3Tp9/PHH+utf/+p3jpUrV+q2227TNddco+rqas2cOVMnnnii72OawYMH6+STT9b06dNljFGvXr3097//XWVlZUHVOnToUEnSI488ovT0dHXu3FnDhg1T165dA/qefPLJCg8P1wsvvKAhQ4bo+OOPV0xMTIu/VAcNGqRf/epXevzxx9WpUyelp6dr48aNuv/++xUbG6u77747qDoPJi0tTRdccIFycnK0c+dOjRgxQu+9957+8pe/BPR97LHHNHr0aJ1//vm68847NWDAANXX1+urr77S3//+d7399tuSpLFjx/r+dkufPn20adMmFRUVKS4uTgkJCQet5eGHH9Z7772n6667TmeccYbCw8NVVVWl4uJibdu2Tb///e8P+V46d+6svLw8XXXVVZL23/NyOH73u99p4cKFuv/++/Xmm28e1jFAh9Ped7wCoaL5yZTKysqAfbt37zb9+/c3CQkJZu/evcYYYz755BNz7bXXmr59+5qwsDDjdrvNRRddZObNmxdwzmXLlpmbbrrJ9OjRw4SHh5vLLrvMrF+/3u8an3/+uUlNTTWRkZGmZ8+e5pprrjGbN28OeMrjx09WHKihocHcdtttpk+fPsbhcBhJviczDnzaxRhjXnzxRTN48GATFhbmd50Dn3Yxxph9+/aZRx55xAwcONCEhYWZE044wdx4442murrar19ycrI57bTTAmrLzMw0cXFxAe0H2rFjh5k0aZLp0aOHiYiIMKmpqeZf//pXwDgYY0xVVZWZNGmSOfHEE01YWJjp06ePSUpK8nuK6NFHHzVJSUnmhBNOMF27djX9+/c3t956q9m4ceMh61ixYoWZMmWKOf30002vXr1M586dTZ8+fcyll15qli5d6tf3UD+TpKQkI6nFp12mTJnS4rXvvfdeI8mUl5cfskago3IYY0z7xB4Azz77rG655RZVVlb6PtsHgGMd93wAAACrCB8AAMAqPnYBAABWsfIBAACsInwAAACrCB8AAMCqDvdHxpqamvTNN98oMjLS6p+HBgAAR84Yo/r6esXExKhTp0OvbXS48PHNN98oNja2vcsAAABHoLq6OuA7ng7U4cJH83deVFdXq3v37u1cDQAAOBx1dXWKjY0N+O6qlnS48NH8UUv37t0JHwAAhJjDuWWCG04BAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgVVDhY+/evfrtb3+r+Ph4hYeH66STTtKDDz6opqYmXx9jjDwej2JiYhQeHq6UlBStXbu21QsHAAChKajw8cgjj2jevHkqLi7WunXrVFBQoN///vd6/PHHfX0KCgpUWFio4uJiVVZWyu12KzU1VfX19a1ePAAACD1BhY/3339fV1xxhcaMGaMBAwZo3LhxSktL08qVKyXtX/UoKirSzJkzlZGRocTERJWUlGjXrl0qLS1tkzcAAABCS1DhY/To0Xrrrbf05ZdfSpI++eQT/fOf/9Rll10mSaqqqpLX61VaWprvGKfTqeTkZFVUVLR4zoaGBtXV1fltAADg2NUlmM733XefamtrNXjwYHXu3Fn79u3Tww8/rPHjx0uSvF6vJMnlcvkd53K5tGnTphbPmZ+fr9mzZx9J7UCrGjB9SXuXELSNc8a0dwkAELSgVj4WLlyo559/XqWlpfr4449VUlKiP/zhDyopKfHr53A4/F4bYwLamuXm5qq2tta3VVdXB/kWAABAKAlq5ePee+/V9OnTdf3110uShg4dqk2bNik/P1+ZmZlyu92S9q+AREdH+46rqakJWA1p5nQ65XQ6j7R+AAAQYoJa+di1a5c6dfI/pHPnzr5HbePj4+V2u1VWVubb39jYqPLyciUlJbVCuQAAINQFtfIxduxYPfzww+rfv79OO+00rVq1SoWFhZo0aZKk/R+3ZGdnKy8vTwkJCUpISFBeXp4iIiI0YcKENnkDAAAgtAQVPh5//HHdf//9ysrKUk1NjWJiYjR58mQ98MADvj45OTnavXu3srKytH37do0cOVLLli1TZGRkqxcPAABCj8MYY9q7iB+rq6tTVFSUamtr1b179/YuBz8jPO0CAEcumN/ffLcLAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsCqo8DFgwAA5HI6AbcqUKZIkY4w8Ho9iYmIUHh6ulJQUrV27tk0KBwAAoSmo8FFZWaktW7b4trKyMknSNddcI0kqKChQYWGhiouLVVlZKbfbrdTUVNXX17d+5QAAICQFFT769Okjt9vt21599VWdfPLJSk5OljFGRUVFmjlzpjIyMpSYmKiSkhLt2rVLpaWlbVU/AAAIMUd8z0djY6Oef/55TZo0SQ6HQ1VVVfJ6vUpLS/P1cTqdSk5OVkVFxUHP09DQoLq6Or8NAAAcu444fLz88svasWOHJk6cKEnyer2SJJfL5dfP5XL59rUkPz9fUVFRvi02NvZISwIAACHgiMPHM888o/T0dMXExPi1OxwOv9fGmIC2H8vNzVVtba1vq66uPtKSAABACOhyJAdt2rRJb775pl566SVfm9vtlrR/BSQ6OtrXXlNTE7Aa8mNOp1NOp/NIygAAACHoiFY+5s+fr759+2rMmDG+tvj4eLndbt8TMNL++0LKy8uVlJR09JUCAIBjQtArH01NTZo/f74yMzPVpcsPhzscDmVnZysvL08JCQlKSEhQXl6eIiIiNGHChFYtGgAAhK6gw8ebb76pzZs3a9KkSQH7cnJytHv3bmVlZWn79u0aOXKkli1bpsjIyFYpFgAAhD6HMca0dxE/VldXp6ioKNXW1qp79+7tXQ5+RgZMX9LeJQRt45wxP90JACwI5vc33+0CAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArAo6fPy///f/dOONN6p3796KiIjQGWecoY8++si33xgjj8ejmJgYhYeHKyUlRWvXrm3VogEAQOgKKnxs375d5513nsLCwvTaa6/p888/16OPPqoePXr4+hQUFKiwsFDFxcWqrKyU2+1Wamqq6uvrW7t2AAAQgroE0/mRRx5RbGys5s+f72sbMGCA75+NMSoqKtLMmTOVkZEhSSopKZHL5VJpaakmT57cOlUDAICQFdTKx+LFizVixAhdc8016tu3r84880w9/fTTvv1VVVXyer1KS0vztTmdTiUnJ6uioqLFczY0NKiurs5vAwAAx66gVj42bNiguXPnatq0aZoxY4Y+/PBD/frXv5bT6dTNN98sr9crSXK5XH7HuVwubdq0qcVz5ufna/bs2UdYPgAA7WvA9CXtXULQNs4Z067XD2rlo6mpSWeddZby8vJ05plnavLkybr99ts1d+5cv34Oh8PvtTEmoK1Zbm6uamtrfVt1dXWQbwEAAISSoMJHdHS0Tj31VL+2IUOGaPPmzZIkt9stSb4VkGY1NTUBqyHNnE6nunfv7rcBAIBjV1Dh47zzztMXX3zh1/bll18qLi5OkhQfHy+3262ysjLf/sbGRpWXlyspKakVygUAAKEuqHs+7r77biUlJSkvL0/XXnutPvzwQz311FN66qmnJO3/uCU7O1t5eXlKSEhQQkKC8vLyFBERoQkTJrTJGwAAAKElqPBx9tlna9GiRcrNzdWDDz6o+Ph4FRUV6YYbbvD1ycnJ0e7du5WVlaXt27dr5MiRWrZsmSIjI1u9eAAAEHqCCh+SdPnll+vyyy8/6H6HwyGPxyOPx3M0dQEAgGMU3+0CAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArAoqfHg8HjkcDr/N7Xb79htj5PF4FBMTo/DwcKWkpGjt2rWtXjQAAAhdQa98nHbaadqyZYtvW7NmjW9fQUGBCgsLVVxcrMrKSrndbqWmpqq+vr5ViwYAAKEr6PDRpUsXud1u39anTx9J+1c9ioqKNHPmTGVkZCgxMVElJSXatWuXSktLW71wAAAQmoIOH+vXr1dMTIzi4+N1/fXXa8OGDZKkqqoqeb1epaWl+fo6nU4lJyeroqLioOdraGhQXV2d3wYAAI5dQYWPkSNH6rnnntMbb7yhp59+Wl6vV0lJSdq2bZu8Xq8kyeVy+R3jcrl8+1qSn5+vqKgo3xYbG3sEbwMAAISKoMJHenq6rr76ag0dOlS/+MUvtGTJEklSSUmJr4/D4fA7xhgT0PZjubm5qq2t9W3V1dXBlAQAAELMUT1qe9xxx2no0KFav36976mXA1c5ampqAlZDfszpdKp79+5+GwAAOHYdVfhoaGjQunXrFB0drfj4eLndbpWVlfn2NzY2qry8XElJSUddKAAAODZ0Cabzb37zG40dO1b9+/dXTU2NHnroIdXV1SkzM1MOh0PZ2dnKy8tTQkKCEhISlJeXp4iICE2YMKGt6gcAACEmqPDx9ddfa/z48dq6dav69Omjc889VytWrFBcXJwkKScnR7t371ZWVpa2b9+ukSNHatmyZYqMjGyT4gEAQOgJKnwsWLDgkPsdDoc8Ho88Hs/R1AQAAI5hfLcLAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsOqowkd+fr4cDoeys7N9bcYYeTwexcTEKDw8XCkpKVq7du3R1gkAAI4RRxw+Kisr9dRTT2nYsGF+7QUFBSosLFRxcbEqKyvldruVmpqq+vr6oy4WAACEviMKH999951uuOEGPf300+rZs6ev3RijoqIizZw5UxkZGUpMTFRJSYl27dql0tLSVisaAACEriMKH1OmTNGYMWP0i1/8wq+9qqpKXq9XaWlpvjan06nk5GRVVFS0eK6GhgbV1dX5bQAA4NjVJdgDFixYoI8//liVlZUB+7xeryTJ5XL5tbtcLm3atKnF8+Xn52v27NnBlgEAVg2YvqS9Swjaxjlj2rsEoEVBrXxUV1frv/7rv/T888+rW7duB+3ncDj8XhtjAtqa5ebmqra21rdVV1cHUxIAAAgxQa18fPTRR6qpqdHw4cN9bfv27dO7776r4uJiffHFF5L2r4BER0f7+tTU1ASshjRzOp1yOp1HUjsAAAhBQa18XHzxxVqzZo1Wr17t20aMGKEbbrhBq1ev1kknnSS3262ysjLfMY2NjSovL1dSUlKrFw8AAEJPUCsfkZGRSkxM9Gs77rjj1Lt3b197dna28vLylJCQoISEBOXl5SkiIkITJkxovaoBAEDICvqG05+Sk5Oj3bt3KysrS9u3b9fIkSO1bNkyRUZGtvalAABACDrq8LF8+XK/1w6HQx6PRx6P52hPDQAAjkF8twsAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwKqjwMXfuXA0bNkzdu3dX9+7dNWrUKL322mu+/cYYeTwexcTEKDw8XCkpKVq7dm2rFw0AAEJXUOGjX79+mjNnjlauXKmVK1fqoosu0hVXXOELGAUFBSosLFRxcbEqKyvldruVmpqq+vr6NikeAACEnqDCx9ixY3XZZZdp4MCBGjhwoB5++GEdf/zxWrFihYwxKioq0syZM5WRkaHExESVlJRo165dKi0tbav6AQBAiDniez727dunBQsWaOfOnRo1apSqqqrk9XqVlpbm6+N0OpWcnKyKioqDnqehoUF1dXV+GwAAOHYFHT7WrFmj448/Xk6nU3fccYcWLVqkU089VV6vV5Lkcrn8+rtcLt++luTn5ysqKsq3xcbGBlsSAAAIIUGHj0GDBmn16tVasWKF7rzzTmVmZurzzz/37Xc4HH79jTEBbT+Wm5ur2tpa31ZdXR1sSQAAIIR0CfaArl276pRTTpEkjRgxQpWVlXrsscd03333SZK8Xq+io6N9/WtqagJWQ37M6XTK6XQGWwYAAAhRR/13PowxamhoUHx8vNxut8rKynz7GhsbVV5erqSkpKO9DAAAOEYEtfIxY8YMpaenKzY2VvX19VqwYIGWL1+u119/XQ6HQ9nZ2crLy1NCQoISEhKUl5eniIgITZgwoa3qBwAAISao8PHvf/9bN910k7Zs2aKoqCgNGzZMr7/+ulJTUyVJOTk52r17t7KysrR9+3aNHDlSy5YtU2RkZJsUDwAAQk9Q4eOZZ5455H6HwyGPxyOPx3M0NQEAgGMY3+0CAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArAoqfOTn5+vss89WZGSk+vbtqyuvvFJffPGFXx9jjDwej2JiYhQeHq6UlBStXbu2VYsGAAChK6jwUV5erilTpmjFihUqKyvT3r17lZaWpp07d/r6FBQUqLCwUMXFxaqsrJTb7VZqaqrq6+tbvXgAABB6ugTT+fXXX/d7PX/+fPXt21cfffSRLrjgAhljVFRUpJkzZyojI0OSVFJSIpfLpdLSUk2ePLn1KgcAACHpqO75qK2tlST16tVLklRVVSWv16u0tDRfH6fTqeTkZFVUVLR4joaGBtXV1fltAADg2BXUysePGWM0bdo0jR49WomJiZIkr9crSXK5XH59XS6XNm3a1OJ58vPzNXv27CMtI2gDpi+xdq3WsnHOmPYuAQCAVnPEKx9Tp07Vp59+qhdffDFgn8Ph8HttjAloa5abm6va2lrfVl1dfaQlAQCAEHBEKx933XWXFi9erHfffVf9+vXztbvdbkn7V0Cio6N97TU1NQGrIc2cTqecTueRlAEAAEJQUCsfxhhNnTpVL730kt5++23Fx8f77Y+Pj5fb7VZZWZmvrbGxUeXl5UpKSmqdigEAQEgLauVjypQpKi0t1SuvvKLIyEjfPR5RUVEKDw+Xw+FQdna28vLylJCQoISEBOXl5SkiIkITJkxokzcAAABCS1DhY+7cuZKklJQUv/b58+dr4sSJkqScnBzt3r1bWVlZ2r59u0aOHKlly5YpMjKyVQoGAAChLajwYYz5yT4Oh0Mej0cej+dIawIAAMcwvtsFAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWBV0+Hj33Xc1duxYxcTEyOFw6OWXX/bbb4yRx+NRTEyMwsPDlZKSorVr17ZWvQAAIMQFHT527typ008/XcXFxS3uLygoUGFhoYqLi1VZWSm3263U1FTV19cfdbEAACD0dQn2gPT0dKWnp7e4zxijoqIizZw5UxkZGZKkkpISuVwulZaWavLkyUdXLQAACHmtes9HVVWVvF6v0tLSfG1Op1PJycmqqKho8ZiGhgbV1dX5bQAA4NjVquHD6/VKklwul1+7y+Xy7TtQfn6+oqKifFtsbGxrlgQAADqYNnnaxeFw+L02xgS0NcvNzVVtba1vq66ubouSAABABxH0PR+H4na7Je1fAYmOjva119TUBKyGNHM6nXI6na1ZBgAA6MBadeUjPj5ebrdbZWVlvrbGxkaVl5crKSmpNS8FAABCVNArH999952++uor3+uqqiqtXr1avXr1Uv/+/ZWdna28vDwlJCQoISFBeXl5ioiI0IQJE1q1cAAAEJqCDh8rV67UhRde6Hs9bdo0SVJmZqaeffZZ5eTkaPfu3crKytL27ds1cuRILVu2TJGRka1XNQAACFlBh4+UlBQZYw663+FwyOPxyOPxHE1dAADgGMV3uwAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACr2ix8PPHEE4qPj1e3bt00fPhw/eMf/2irSwEAgBDSJuFj4cKFys7O1syZM7Vq1Sqdf/75Sk9P1+bNm9vicgAAIIS0SfgoLCzUrbfeqttuu01DhgxRUVGRYmNjNXfu3La4HAAACCFdWvuEjY2N+uijjzR9+nS/9rS0NFVUVAT0b2hoUENDg+91bW2tJKmurq61S5MkNTXsapPztqW2Ggv4Y27gUJgfOBjmhv85jTE/2bfVw8fWrVu1b98+uVwuv3aXyyWv1xvQPz8/X7Nnzw5oj42Nbe3SQlZUUXtXgI6KuYFDYX7gYNpybtTX1ysqKuqQfVo9fDRzOBx+r40xAW2SlJubq2nTpvleNzU16dtvv1Xv3r1b7H806urqFBsbq+rqanXv3r1Vz32sYawOH2N1+Bir4DBeh4+xOnxtNVbGGNXX1ysmJuYn+7Z6+DjhhBPUuXPngFWOmpqagNUQSXI6nXI6nX5tPXr0aO2y/HTv3p3JeZgYq8PHWB0+xio4jNfhY6wOX1uM1U+teDRr9RtOu3btquHDh6usrMyvvaysTElJSa19OQAAEGLa5GOXadOm6aabbtKIESM0atQoPfXUU9q8ebPuuOOOtrgcAAAIIW0SPq677jpt27ZNDz74oLZs2aLExEQtXbpUcXFxbXG5w+Z0OjVr1qyAj3kQiLE6fIzV4WOsgsN4HT7G6vB1hLFymMN5JgYAAKCV8N0uAADAKsIHAACwivABAACsInwAAACrCB8AAMCqn034eOKJJxQfH69u3bpp+PDh+sc//tHeJXVIHo9HDofDb3O73e1dVofw7rvvauzYsYqJiZHD4dDLL7/st98YI4/Ho5iYGIWHhyslJUVr165tn2Lb2U+N1cSJEwPm2bnnnts+xbaz/Px8nX322YqMjFTfvn115ZVX6osvvvDrw9za73DGirm139y5czVs2DDfXzEdNWqUXnvtNd/+9p5TP4vwsXDhQmVnZ2vmzJlatWqVzj//fKWnp2vz5s3tXVqHdNppp2nLli2+bc2aNe1dUoewc+dOnX766SouLm5xf0FBgQoLC1VcXKzKykq53W6lpqaqvr7ecqXt76fGSpIuvfRSv3m2dOlSixV2HOXl5ZoyZYpWrFihsrIy7d27V2lpadq5c6evD3Nrv8MZK4m5JUn9+vXTnDlztHLlSq1cuVIXXXSRrrjiCl/AaPc5ZX4GzjnnHHPHHXf4tQ0ePNhMnz69nSrquGbNmmVOP/309i6jw5NkFi1a5Hvd1NRk3G63mTNnjq/t+++/N1FRUWbevHntUGHHceBYGWNMZmamueKKK9qlno6upqbGSDLl5eXGGObWoRw4VsYwtw6lZ8+e5s9//nOHmFPH/MpHY2OjPvroI6Wlpfm1p6WlqaKiop2q6tjWr1+vmJgYxcfH6/rrr9eGDRvau6QOr6qqSl6v12+eOZ1OJScnM88OYvny5erbt68GDhyo22+/XTU1Ne1dUodQW1srSerVq5ck5tahHDhWzZhb/vbt26cFCxZo586dGjVqVIeYU8d8+Ni6dav27dsX8I26Lpcr4Jt3IY0cOVLPPfec3njjDT399NPyer1KSkrStm3b2ru0Dq15LjHPDk96erpeeOEFvf3223r00UdVWVmpiy66SA0NDe1dWrsyxmjatGkaPXq0EhMTJTG3DqalsZKYWz+2Zs0aHX/88XI6nbrjjju0aNEinXrqqR1iTrXJd7t0RA6Hw++1MSagDfv/xW02dOhQjRo1SieffLJKSko0bdq0dqwsNDDPDs91113n++fExESNGDFCcXFxWrJkiTIyMtqxsvY1depUffrpp/rnP/8ZsI+55e9gY8Xc+sGgQYO0evVq7dixQ3/729+UmZmp8vJy3/72nFPH/MrHCSecoM6dOwekuZqamoDUh0DHHXechg4dqvXr17d3KR1a8xNBzLMjEx0drbi4uJ/1PLvrrru0ePFivfPOO+rXr5+vnbkV6GBj1ZKf89zq2rWrTjnlFI0YMUL5+fk6/fTT9dhjj3WIOXXMh4+uXbtq+PDhKisr82svKytTUlJSO1UVOhoaGrRu3TpFR0e3dykdWnx8vNxut988a2xsVHl5OfPsMGzbtk3V1dU/y3lmjNHUqVP10ksv6e2331Z8fLzffubWD35qrFryc55bBzLGqKGhoWPMKSu3tbazBQsWmLCwMPPMM8+Yzz//3GRnZ5vjjjvObNy4sb1L63Duueces3z5crNhwwazYsUKc/nll5vIyEjGyhhTX19vVq1aZVatWmUkmcLCQrNq1SqzadMmY4wxc+bMMVFRUeall14ya9asMePHjzfR0dGmrq6unSu371BjVV9fb+655x5TUVFhqqqqzDvvvGNGjRplTjzxxJ/lWN15550mKirKLF++3GzZssW37dq1y9eHubXfT40Vc+sHubm55t133zVVVVXm008/NTNmzDCdOnUyy5YtM8a0/5z6WYQPY4z505/+ZOLi4kzXrl3NWWed5fdoFn5w3XXXmejoaBMWFmZiYmJMRkaGWbt2bXuX1SG88847RlLAlpmZaYzZ/0jkrFmzjNvtNk6n01xwwQVmzZo17Vt0OznUWO3atcukpaWZPn36mLCwMNO/f3+TmZlpNm/e3N5lt4uWxkmSmT9/vq8Pc2u/nxor5tYPJk2a5Pud16dPH3PxxRf7gocx7T+nHMYYY2eNBQAA4GdwzwcAAOhYCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACw6v8DVtwqk+OGtaQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7150fb052d6e4eb99b02ffe1cb7cad9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='idx', max=199), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize(idx)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensions\n",
    "print(f\"Taille des signaux : {signals.shape}\")\n",
    "print(\"Shape SNR : \", snr.shape)\n",
    "print(\"Shape SNR : \", snr.shape)\n",
    "print(labels_id.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(labels_id)\n",
    "plt.title(\"Répartition des classes\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(snr)\n",
    "plt.title(\"Répartition des SNR\")\n",
    "plt.show()\n",
    "\n",
    "# Visualisation des données dans le repère IQ\n",
    "def visualize(idx):\n",
    "    plt.figure()\n",
    "    plt.plot(signals[idx, :, 0], signals[idx, :, 1], '.')\n",
    "    modulation = get_labels(h5py.File(data_path, 'r'))[labels_id[idx]]\n",
    "    plt.title(f'Signal {idx} de snr {snr[idx]} (mod {modulation})')\n",
    "    plt.grid()\n",
    "    plt.xlabel('I')\n",
    "    plt.ylabel('Q')\n",
    "    plt.show()\n",
    "\n",
    "interact(visualize, idx=IntSlider(min=0, max=signals.shape[0] - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque : \n",
    "Dans l'ensemble samples, il y a 20 signaux de 2048 échantillons sur 2 canaux (I et Q)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9fa901",
   "metadata": {},
   "source": [
    "## Chargement des données en Pytorch\n",
    "\n",
    "Pour entrainer des réseaux de neurones profond sur nos données nous allons utiliser le framework Pytorch. \n",
    "Une première étape va consister à transférer les données de numpy à PyTorch, cela passe par deux objets : \n",
    "    - un Dataset qui modélise le dataset à haut niveau dans la mémoire de l'ordinateur\n",
    "    - un Dataloader qui permet d'échantillonner le Dataset Pytorch dans les itérations de l'optimisation du réseau de neurones \n",
    "    \n",
    "Un dataset prend la forme \n",
    "```python\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path_to_data):\n",
    "        ...\n",
    "    def __len__(self): #retourne le nombre de données dans le dataset\n",
    "        ...\n",
    "    def __getitem__(self,i): #retourne pour chaque indice i un couple (data_i, lablel_i), data_i étant un signal et label_i le label associé au signal\n",
    "        ...\n",
    "```\n",
    "\n",
    "Implémentez une classe Dataset pour le dataset considéré ici "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "53406fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(torch.utils.data.Dataset) : \n",
    "    def __init__(self, path_to_data : Path) :\n",
    "        torch.utils.data.Dataset.__init__(self)\n",
    "        assert path_to_data.exists()\n",
    "        data = h5py.File(path_to_data , 'r')\n",
    "\n",
    "        self.signals = np.array(data['signaux'])\n",
    "        self.snr =  np.array(data['snr'])\n",
    "        self.labels_id = np.array(data['labels'])\n",
    "\n",
    "        data.close()\n",
    "    \n",
    "    def __len__(self) :\n",
    "        \"\"\"Retourne la taille du dataset\"\"\"\n",
    "        return self.signals.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i : int) :\n",
    "        \"\"\"retourne pour chaque indice i un couple (data_i, lablel_i),\n",
    "        data_i étant un signal et label_i le label associé au signal\"\"\"\n",
    "        return((self.signals[i], int(self.labels_id[i])))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69069805",
   "metadata": {},
   "source": [
    "Instanciez un objet dataset et testez le sur les données samples\n",
    "```python\n",
    "dataset = MyDataset(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "144c1e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dataset : 200\n",
      "Element 3 du dataset : \n",
      "signal [[ 0.01738485  0.4679469 ]\n",
      " [-0.02483279  0.3787216 ]\n",
      " [-0.22873749  0.1594786 ]\n",
      " ...\n",
      " [-0.33821508  1.5790684 ]\n",
      " [-0.50299037  1.0259963 ]\n",
      " [-0.6004157  -0.8126683 ]],\n",
      " label 3\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataSet(data_path)\n",
    "print(f\"Taille du dataset : {len(dataset)}\")\n",
    "print(f\"Element 3 du dataset : \\nsignal {dataset[3][0]},\\n label {dataset[3][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6532b5",
   "metadata": {},
   "source": [
    "Pytorch propose une classe Dataloader qui permet d'échantillonner des batchs de taille fixe à partir d'un dataset. \n",
    "La cellule suivante donne un exemple d'utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3044a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=10, \n",
    "                        shuffle=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f1b6e",
   "metadata": {},
   "source": [
    "Testez le dataloader pour différentes valeurs de batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "965210fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch de 10 : torch.Size([10, 2048, 2]) torch.Size([10])\n",
      "1er élément :  tensor([[ 0.2033,  0.6199],\n",
      "        [-0.1835,  0.4715],\n",
      "        [-0.7318,  0.5121],\n",
      "        ...,\n",
      "        [ 0.6348,  0.8279],\n",
      "        [ 0.4756,  0.4679],\n",
      "        [-0.0154, -0.6884]])\n",
      "batch de 30 : torch.Size([30, 2048, 2]) torch.Size([30])\n",
      "1er élément :  tensor([[-0.9284,  0.0633],\n",
      "        [-0.7979,  0.0506],\n",
      "        [ 0.4080,  0.0400],\n",
      "        ...,\n",
      "        [-0.6053, -1.8346],\n",
      "        [ 0.1402, -1.0426],\n",
      "        [-0.6492, -0.6717]])\n",
      "<class 'torch.utils.data.dataloader.DataLoader'> <class 'torch.Tensor'> <class 'torch.Tensor'> tensor([1, 0, 3, 3, 0, 3, 4, 5, 0, 5, 1, 0, 3, 4, 0, 5, 4, 1, 4, 2, 0, 1, 0, 4,\n",
      "        1, 2, 3, 2, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=10, \n",
    "                        shuffle=True\n",
    "                       )\n",
    "signals_set, labels_set = next(iter(dataloader))\n",
    "print(\"batch de 10 :\", signals_set.size(), labels_set.size())\n",
    "print(\"1er élément : \", signals_set[0])\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=30, \n",
    "                        shuffle=True\n",
    "                       )\n",
    "signals_set, labels_set = next(iter(dataloader))\n",
    "print(\"batch de 30 :\", signals_set.size(), labels_set.size())\n",
    "print(\"1er élément : \", signals_set[0])\n",
    "\n",
    "print(type(dataloader), type(signals_set), type(labels_set), labels_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La taille du batch_size permet de choisir le compromis entre un batch_size faible qui donne un gradient très bruité, et un batch_size élevé pour lequel le gradient est peu bruité mais qui est alors plus facilement \"bloqué\" sur un minima local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le DataLoader crée un tenseur avec une taille de batch donnée, les éléments étant tirés aléatoirement dans la base de données initiale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7a7c17",
   "metadata": {},
   "source": [
    "## Mise en place d'un réseau \"dumb\" pour tester la pipeline d'entrainement\n",
    "\n",
    "Définissez un premier modèle Pytorch qui prend en entrée un batch de données (tenseur de dimensions [B , C, T] avec B la taille du batch, C le nombre de canaux des signaux et T le nombre d'échantillons dans les signaux) et renvoie un batch de vecteur de probabilités (ou de log probabilités si vous préférez) (tenseur de dimensions [B,N] où N est le nombre de classe à identifier). \n",
    "\n",
    "Ce Modèle doit être très simple, il doit être rapide à exécuter, il servira à tester et éventuellement débugger la pipeline d'entrainement que vous mettrez en place un peu plus loin. Un template d'implémentation d'une classe Model se trouve dans les diapositives du cours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8710cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DumbModel(torch.nn.Module) :\n",
    "    def __init__(self, chan_size, time_size, nb_class : int) :\n",
    "        torch.nn.Module.__init__(self)\n",
    "\n",
    "        self.nb_class = nb_class\n",
    "        self.chan_size = chan_size\n",
    "        self.time_size = time_size\n",
    "        self.fc = torch.nn.Linear(chan_size * time_size, self.nb_class)\n",
    "        \n",
    "    \n",
    "    def forward(self, x : torch.Tensor) :\n",
    "        \"\"\" \n",
    "            Input :\n",
    "                x a batch of size [B, C, T]\n",
    "            Output : \n",
    "                a batch of size [B, self.nb_class]\n",
    "        \"\"\"\n",
    "        assert x.size(1) == self.chan_size\n",
    "        assert x.size(2) == self.time_size\n",
    "        \n",
    "        x = x.reshape(x.size(0), x.size(1) * x.size(2)) #[B, C * T] (flatten)\n",
    "        logits = self.fc(x)\n",
    "        # return torch.nn.functional.softmax(logits, dim = 1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Si le dumb model est trop long à éxécuter car ici on a CxTxnb_class = 2x2048x6 poids, on peut décimer le signal d'entrée (par ex. 1points sur 10) avant la couche fully connected). On renvoie les logits sans softmax par praticité pour les fonctions de loss utilisées ensuite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8767db4",
   "metadata": {},
   "source": [
    "Instanciez votre modèle et testez la consistence de ses entrées / sorties vis à vis des données étudiées "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "48d3766f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrée :  torch.Size([30, 2, 2048])\n",
      "Sortie :  torch.Size([30, 6]) de 1er elem :  tensor([ 0.4737, -0.3595,  0.3528,  0.6488,  0.0379, -0.2458],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(-1.1722, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True\n",
    "                       )\n",
    "signals_set, labels_set = next(iter(dataloader)) #dim [B, T, C] and [B, T]\n",
    "# Swap dimensions to have a tensor of dim [B, C, T] rather than [B, T, C]\n",
    "signals_set_bct = signals_set.permute(0, 2, 1) # dim [B, C, T]\n",
    "print(\"Entrée : \", signals_set_bct.size())\n",
    "\n",
    "dumbModel = DumbModel(signals_set_bct.size(1), signals_set_bct.size(2), 6)\n",
    "y = dumbModel(signals_set_bct) # forward\n",
    "\n",
    "print(\"Sortie : \", y.size(), \"de 1er elem : \", y[0])\n",
    "print(y[20].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617dcb82",
   "metadata": {},
   "source": [
    "## Mise en place de la pipeline d'entraînement\n",
    "\n",
    "La pipeline d'entrainement consiste à \n",
    "- charger les données \n",
    "- les batcher \n",
    "- réaliser des itération (epochs) de descente de gradient pour optimiser les paramètres d'un algorithme selon une fonction de perte (loss)\n",
    "- logger l'évolution au fil des epochs  de la loss sur l'ensemble train et l'ensemble de validation et éventuellement de métriques complémentaires \n",
    "\n",
    "Un cavnevas d'implémentation pourrait être:\n",
    "\n",
    "```python\n",
    "device = 'cpu' # set so 'cuda:xx' if you have a GPU, xx is GPU index. L'entraînement des réseaux de neurones est grandement accéléré par l'utilisation d'un GPU \n",
    "\n",
    "model = ...  # vous instanciez ici votre modèle\n",
    "\n",
    "loss = .... # définissez la fonction de perte selon laquelle le modèle sera optimisé\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters()) # en pratique on utilise pas une simple descente de gradient mais une procédure d'optimisation plus sophistiquée qui est implémentée sous la forme d'un objet Optimizer. Il en existe beaucoup d'optimizers différents, vous pouvez en tester différents, je vous propose d'utiliser en premier lieu l'algorithme Adam\n",
    "\n",
    "n_epochs = ... # le nombre d'itérations dans l'entrainement \n",
    "\n",
    "chemin_vers_sauvegarde_model = # chemin vers un fichier où vous sauvegarderez votre modèle après optimisation pour le réutiliser plus tard. \n",
    "\n",
    "model.to(device) # on place le modèle dans le GPU si nécessaire\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for batch_x,batch_y in dataloader_train:\n",
    "        \n",
    "        batch_x.to(device)\n",
    "        batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_y_predicted = model(batch_x)\n",
    "        \n",
    "        l = loss(batch_y_predicted, batch_y)\n",
    "        # loggez la loss sur le batch d'entraînement\n",
    "        \n",
    "        l.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    for batch_x,batch_y in dataloader_valid:\n",
    "        \n",
    "        batch_x.to(device)\n",
    "        batch_y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_y_predicted = model(batch_x)  \n",
    "            \n",
    "        # loggez la loss et les métriques sur le batch de validation\n",
    "\n",
    "torch.save(model, chemin_vers_sauvegarde_model)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c67127",
   "metadata": {},
   "source": [
    "Mettez en place votre pipeline et testez là sur votre modèle dumb. Faites en sorte que votre façon de logger les loss et les métriques vous permette de visualiser l'évolution de ces différents indicateurs sur l'ensemble d'entrainement et de validation au fil des epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parametres and datasets\n",
    "batch_size = 30\n",
    "DATA = Path(\"data\")\n",
    "dataset_valid = MyDataSet(DATA/\"samples.hdf5\")\n",
    "dataset_train = MyDataSet(DATA/\"train.hdf5\")\n",
    "dataset_validation = MyDataSet(DATA/\"validation.hdf5\")\n",
    "\n",
    "\n",
    "# Dataloaders\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True\n",
    "                       )\n",
    "dataloader_valid = DataLoader(dataset_validation, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True\n",
    "                       )\n",
    "print(len(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6d8ffa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0 batch 0 with loss 2.1457130908966064.\n",
      "Training epoch 0 batch 1 with loss 1.8323067426681519.\n",
      "Training epoch 0 batch 2 with loss 1.936645746231079.\n",
      "Training epoch 0 batch 3 with loss 1.7791804075241089.\n",
      "Training epoch 0 batch 4 with loss 1.9357151985168457.\n",
      "Training epoch 0 batch 5 with loss 1.974602222442627.\n",
      "Training epoch 0 batch 6 with loss 1.8369914293289185.\n",
      "Training epoch 0 batch 7 with loss 1.8568817377090454.\n",
      "Training epoch 0 batch 8 with loss 1.8576396703720093.\n",
      "Training epoch 0 batch 9 with loss 1.8350938558578491.\n",
      "Training epoch 0 batch 10 with loss 1.866126298904419.\n",
      "Training epoch 0 batch 11 with loss 1.7792227268218994.\n",
      "Training epoch 0 batch 12 with loss 1.7301290035247803.\n",
      "Training epoch 0 batch 13 with loss 1.9935067892074585.\n",
      "Training epoch 0 batch 14 with loss 1.7951204776763916.\n",
      "Training epoch 0 batch 15 with loss 1.7633171081542969.\n",
      "Training epoch 0 batch 16 with loss 1.740614652633667.\n",
      "Training epoch 0 batch 17 with loss 2.0332350730895996.\n",
      "Training epoch 0 batch 18 with loss 2.0372772216796875.\n",
      "Training epoch 0 batch 19 with loss 1.816082239151001.\n",
      "Training epoch 0 batch 20 with loss 1.9405133724212646.\n",
      "Training epoch 0 batch 21 with loss 2.2322349548339844.\n",
      "Training epoch 0 batch 22 with loss 1.721315860748291.\n",
      "Training epoch 0 batch 23 with loss 1.8940856456756592.\n",
      "Training epoch 0 batch 24 with loss 2.1693978309631348.\n",
      "Training epoch 0 batch 25 with loss 2.0417678356170654.\n",
      "Training epoch 0 batch 26 with loss 1.8259724378585815.\n",
      "Training epoch 0 batch 27 with loss 1.8572049140930176.\n",
      "Training epoch 0 batch 28 with loss 1.9708430767059326.\n",
      "Training epoch 0 batch 29 with loss 1.900052785873413.\n",
      "Training epoch 0 batch 30 with loss 1.8501250743865967.\n",
      "Training epoch 0 batch 31 with loss 1.8457729816436768.\n",
      "Training epoch 0 batch 32 with loss 1.8033536672592163.\n",
      "Training epoch 0 batch 33 with loss 1.8535115718841553.\n",
      "Training epoch 0 batch 34 with loss 2.157025098800659.\n",
      "Training epoch 0 batch 35 with loss 2.0560102462768555.\n",
      "Training epoch 0 batch 36 with loss 1.882412075996399.\n",
      "Training epoch 0 batch 37 with loss 1.8954920768737793.\n",
      "Training epoch 0 batch 38 with loss 1.9331581592559814.\n",
      "Training epoch 0 batch 39 with loss 1.8633387088775635.\n",
      "Training epoch 0 batch 40 with loss 2.094919443130493.\n",
      "Training epoch 0 batch 41 with loss 2.0792758464813232.\n",
      "Training epoch 0 batch 42 with loss 1.886134386062622.\n",
      "Training epoch 0 batch 43 with loss 1.9558137655258179.\n",
      "Training epoch 0 batch 44 with loss 1.9416307210922241.\n",
      "Training epoch 0 batch 45 with loss 1.9364688396453857.\n",
      "Training epoch 0 batch 46 with loss 1.9773832559585571.\n",
      "Training epoch 0 batch 47 with loss 2.0451741218566895.\n",
      "Training epoch 0 batch 48 with loss 2.1844820976257324.\n",
      "Training epoch 0 batch 49 with loss 1.9120713472366333.\n",
      "Training epoch 0 batch 50 with loss 1.9057525396347046.\n",
      "Training epoch 0 batch 51 with loss 2.0905869007110596.\n",
      "Training epoch 0 batch 52 with loss 2.0404937267303467.\n",
      "Training epoch 0 batch 53 with loss 2.15045428276062.\n",
      "Training epoch 0 batch 54 with loss 1.893792748451233.\n",
      "Training epoch 0 batch 55 with loss 1.8946741819381714.\n",
      "Training epoch 0 batch 56 with loss 1.8403822183609009.\n",
      "Training epoch 0 batch 57 with loss 1.8560166358947754.\n",
      "Training epoch 0 batch 58 with loss 1.804100751876831.\n",
      "Training epoch 0 batch 59 with loss 1.9259147644042969.\n",
      "Training epoch 0 batch 60 with loss 1.984399437904358.\n",
      "Training epoch 0 batch 61 with loss 2.1433370113372803.\n",
      "Training epoch 0 batch 62 with loss 2.2248713970184326.\n",
      "Training epoch 0 batch 63 with loss 1.8178004026412964.\n",
      "Training epoch 0 batch 64 with loss 1.7859796285629272.\n",
      "Training epoch 0 batch 65 with loss 2.0614559650421143.\n",
      "Training epoch 0 batch 66 with loss 1.9413704872131348.\n",
      "Training epoch 0 batch 67 with loss 1.967733383178711.\n",
      "Training epoch 0 batch 68 with loss 2.0605454444885254.\n",
      "Training epoch 0 batch 69 with loss 1.98956298828125.\n",
      "Training epoch 0 batch 70 with loss 1.946991205215454.\n",
      "Training epoch 0 batch 71 with loss 1.835899829864502.\n",
      "Training epoch 0 batch 72 with loss 1.8434040546417236.\n",
      "Training epoch 0 batch 73 with loss 2.052196502685547.\n",
      "Training epoch 0 batch 74 with loss 1.9954664707183838.\n",
      "Training epoch 0 batch 75 with loss 2.0456089973449707.\n",
      "Training epoch 0 batch 76 with loss 1.9172022342681885.\n",
      "Training epoch 0 batch 77 with loss 1.8530746698379517.\n",
      "Training epoch 0 batch 78 with loss 2.1239583492279053.\n",
      "Training epoch 0 batch 79 with loss 2.143418788909912.\n",
      "Training epoch 0 batch 80 with loss 2.202155113220215.\n",
      "Training epoch 0 batch 81 with loss 2.043705701828003.\n",
      "Training epoch 0 batch 82 with loss 1.8808364868164062.\n",
      "Training epoch 0 batch 83 with loss 2.1317660808563232.\n",
      "Training epoch 0 batch 84 with loss 1.9554654359817505.\n",
      "Training epoch 0 batch 85 with loss 1.9762791395187378.\n",
      "Training epoch 0 batch 86 with loss 2.250584840774536.\n",
      "Training epoch 0 batch 87 with loss 2.1526906490325928.\n",
      "Training epoch 0 batch 88 with loss 2.225032091140747.\n",
      "Training epoch 0 batch 89 with loss 1.9643007516860962.\n",
      "Training epoch 0 batch 90 with loss 1.8335222005844116.\n",
      "Training epoch 0 batch 91 with loss 2.0391435623168945.\n",
      "Training epoch 0 batch 92 with loss 2.1084632873535156.\n",
      "Training epoch 0 batch 93 with loss 2.1447863578796387.\n",
      "Training epoch 0 batch 94 with loss 1.931214451789856.\n",
      "Training epoch 0 batch 95 with loss 1.958977460861206.\n",
      "Training epoch 0 batch 96 with loss 1.9862911701202393.\n",
      "Training epoch 0 batch 97 with loss 2.0350496768951416.\n",
      "Training epoch 0 batch 98 with loss 2.207406520843506.\n",
      "Training epoch 0 batch 99 with loss 2.2725770473480225.\n",
      "Training epoch 0 batch 100 with loss 1.9139591455459595.\n",
      "Training epoch 0 batch 101 with loss 1.8542243242263794.\n",
      "Training epoch 0 batch 102 with loss 2.081716299057007.\n",
      "Training epoch 0 batch 103 with loss 2.162917375564575.\n",
      "Training epoch 0 batch 104 with loss 1.9657284021377563.\n",
      "Training epoch 0 batch 105 with loss 1.8987170457839966.\n",
      "Training epoch 0 batch 106 with loss 1.7552337646484375.\n",
      "Training epoch 0 batch 107 with loss 1.956405758857727.\n",
      "Training epoch 0 batch 108 with loss 1.9345225095748901.\n",
      "Training epoch 0 batch 109 with loss 2.01528000831604.\n",
      "Training epoch 0 batch 110 with loss 2.3006374835968018.\n",
      "Training epoch 0 batch 111 with loss 1.9452742338180542.\n",
      "Training epoch 0 batch 112 with loss 2.106440782546997.\n",
      "Training epoch 0 batch 113 with loss 2.0938127040863037.\n",
      "Training epoch 0 batch 114 with loss 2.0811288356781006.\n",
      "Training epoch 0 batch 115 with loss 1.9298261404037476.\n",
      "Training epoch 0 batch 116 with loss 1.8666958808898926.\n",
      "Training epoch 0 batch 117 with loss 2.0879299640655518.\n",
      "Training epoch 0 batch 118 with loss 2.2108712196350098.\n",
      "Training epoch 0 batch 119 with loss 2.29771089553833.\n",
      "Training epoch 0 batch 120 with loss 1.9834915399551392.\n",
      "Training epoch 0 batch 121 with loss 2.234588623046875.\n",
      "Training epoch 0 batch 122 with loss 2.019118070602417.\n",
      "Training epoch 0 batch 123 with loss 2.0860848426818848.\n",
      "Training epoch 0 batch 124 with loss 1.9571799039840698.\n",
      "Training epoch 0 batch 125 with loss 1.933771014213562.\n",
      "Training epoch 0 batch 126 with loss 2.1130239963531494.\n",
      "Training epoch 0 batch 127 with loss 1.9866992235183716.\n",
      "Training epoch 0 batch 128 with loss 1.976029872894287.\n",
      "Training epoch 0 batch 129 with loss 2.254826307296753.\n",
      "Training epoch 0 batch 130 with loss 2.356325626373291.\n",
      "Training epoch 0 batch 131 with loss 1.926720142364502.\n",
      "Training epoch 0 batch 132 with loss 1.76312255859375.\n",
      "Training epoch 0 batch 133 with loss 1.75574791431427.\n",
      "Training epoch 0 batch 134 with loss 2.09958815574646.\n",
      "Training epoch 0 batch 135 with loss 2.207585096359253.\n",
      "Training epoch 0 batch 136 with loss 2.237607717514038.\n",
      "Training epoch 0 batch 137 with loss 2.1876254081726074.\n",
      "Training epoch 0 batch 138 with loss 1.9278123378753662.\n",
      "Training epoch 0 batch 139 with loss 2.2690999507904053.\n",
      "Training epoch 0 batch 140 with loss 2.0960867404937744.\n",
      "Training epoch 0 batch 141 with loss 2.133615732192993.\n",
      "Training epoch 0 batch 142 with loss 2.1920723915100098.\n",
      "Training epoch 0 batch 143 with loss 1.9561330080032349.\n",
      "Training epoch 0 batch 144 with loss 2.086764097213745.\n",
      "Training epoch 0 batch 145 with loss 2.1178364753723145.\n",
      "Training epoch 0 batch 146 with loss 2.0799636840820312.\n",
      "Training epoch 0 batch 147 with loss 2.290720224380493.\n",
      "Training epoch 0 batch 148 with loss 1.9693067073822021.\n",
      "Training epoch 0 batch 149 with loss 2.0615313053131104.\n",
      "Training epoch 0 batch 150 with loss 1.9421707391738892.\n",
      "Training epoch 0 batch 151 with loss 2.107569456100464.\n",
      "Training epoch 0 batch 152 with loss 2.2541635036468506.\n",
      "Training epoch 0 batch 153 with loss 1.9329121112823486.\n",
      "Training epoch 0 batch 154 with loss 2.2981817722320557.\n",
      "Training epoch 0 batch 155 with loss 2.2338528633117676.\n",
      "Training epoch 0 batch 156 with loss 2.337804079055786.\n",
      "Training epoch 0 batch 157 with loss 2.126997470855713.\n",
      "Training epoch 0 batch 158 with loss 2.1346733570098877.\n",
      "Training epoch 0 batch 159 with loss 2.106459617614746.\n",
      "Training epoch 0 batch 160 with loss 2.051325559616089.\n",
      "Training epoch 0 batch 161 with loss 1.8888388872146606.\n",
      "Training epoch 0 batch 162 with loss 2.0447468757629395.\n",
      "Training epoch 0 batch 163 with loss 2.3167948722839355.\n",
      "Training epoch 0 batch 164 with loss 1.9682244062423706.\n",
      "Training epoch 0 batch 165 with loss 1.9431350231170654.\n",
      "Training epoch 0 batch 166 with loss 2.2626914978027344.\n",
      "Training epoch 0 batch 167 with loss 2.0897555351257324.\n",
      "Training epoch 0 batch 168 with loss 2.0235910415649414.\n",
      "Training epoch 0 batch 169 with loss 2.106238603591919.\n",
      "Training epoch 0 batch 170 with loss 2.1209986209869385.\n",
      "Training epoch 0 batch 171 with loss 1.8724483251571655.\n",
      "Training epoch 0 batch 172 with loss 1.9663615226745605.\n",
      "Training epoch 0 batch 173 with loss 2.0727667808532715.\n",
      "Training epoch 0 batch 174 with loss 2.061121702194214.\n",
      "Training epoch 0 batch 175 with loss 1.83432137966156.\n",
      "Training epoch 0 batch 176 with loss 2.2388875484466553.\n",
      "Training epoch 0 batch 177 with loss 2.032649517059326.\n",
      "Training epoch 0 batch 178 with loss 2.1110193729400635.\n",
      "Training epoch 0 batch 179 with loss 2.299981117248535.\n",
      "Training epoch 0 batch 180 with loss 2.107740879058838.\n",
      "Training epoch 0 batch 181 with loss 2.146158456802368.\n",
      "Training epoch 0 batch 182 with loss 2.3654332160949707.\n",
      "Training epoch 0 batch 183 with loss 2.1638967990875244.\n",
      "Training epoch 0 batch 184 with loss 2.321376323699951.\n",
      "Training epoch 0 batch 185 with loss 2.1444942951202393.\n",
      "Training epoch 0 batch 186 with loss 1.9409749507904053.\n",
      "Training epoch 0 batch 187 with loss 1.9664452075958252.\n",
      "Training epoch 0 batch 188 with loss 2.023451328277588.\n",
      "Training epoch 0 batch 189 with loss 2.0081546306610107.\n",
      "Training epoch 0 batch 190 with loss 1.8570268154144287.\n",
      "Training epoch 0 batch 191 with loss 1.9125438928604126.\n",
      "Training epoch 0 batch 192 with loss 1.9369319677352905.\n",
      "Training epoch 0 batch 193 with loss 2.156041145324707.\n",
      "Training epoch 0 batch 194 with loss 2.0779712200164795.\n",
      "Training epoch 0 batch 195 with loss 1.9257844686508179.\n",
      "Training epoch 0 batch 196 with loss 2.206256628036499.\n",
      "Training epoch 0 batch 197 with loss 2.1583900451660156.\n",
      "Training epoch 0 batch 198 with loss 1.9867329597473145.\n",
      "Training epoch 0 batch 199 with loss 1.8828943967819214.\n",
      "Training epoch 0 batch 200 with loss 2.1117031574249268.\n",
      "Training epoch 0 batch 201 with loss 1.8001986742019653.\n",
      "Training epoch 0 batch 202 with loss 1.8585008382797241.\n",
      "Training epoch 0 batch 203 with loss 1.8463093042373657.\n",
      "Training epoch 0 batch 204 with loss 2.1581501960754395.\n",
      "Training epoch 0 batch 205 with loss 2.0166547298431396.\n",
      "Training epoch 0 batch 206 with loss 1.9362205266952515.\n",
      "Training epoch 0 batch 207 with loss 2.2269253730773926.\n",
      "Training epoch 0 batch 208 with loss 1.8851983547210693.\n",
      "Training epoch 0 batch 209 with loss 2.0995357036590576.\n",
      "Training epoch 0 batch 210 with loss 2.1679601669311523.\n",
      "Training epoch 0 batch 211 with loss 1.9529900550842285.\n",
      "Training epoch 0 batch 212 with loss 1.9101141691207886.\n",
      "Training epoch 0 batch 213 with loss 1.9886904954910278.\n",
      "Training epoch 0 batch 214 with loss 2.1848342418670654.\n",
      "Training epoch 0 batch 215 with loss 1.907005786895752.\n",
      "Training epoch 0 batch 216 with loss 1.937659502029419.\n",
      "Training epoch 0 batch 217 with loss 2.0158071517944336.\n",
      "Training epoch 0 batch 218 with loss 2.487593173980713.\n",
      "Training epoch 0 batch 219 with loss 1.9885694980621338.\n",
      "Training epoch 0 batch 220 with loss 2.0705831050872803.\n",
      "Training epoch 0 batch 221 with loss 1.819728136062622.\n",
      "Training epoch 0 batch 222 with loss 2.1477866172790527.\n",
      "Training epoch 0 batch 223 with loss 1.916723608970642.\n",
      "Training epoch 0 batch 224 with loss 2.008768320083618.\n",
      "Training epoch 0 batch 225 with loss 2.1486523151397705.\n",
      "Training epoch 0 batch 226 with loss 1.9944114685058594.\n",
      "Training epoch 0 batch 227 with loss 2.318570613861084.\n",
      "Training epoch 0 batch 228 with loss 2.3839423656463623.\n",
      "Training epoch 0 batch 229 with loss 2.2037346363067627.\n",
      "Training epoch 0 batch 230 with loss 2.0656425952911377.\n",
      "Training epoch 0 batch 231 with loss 1.8619370460510254.\n",
      "Training epoch 0 batch 232 with loss 2.2929844856262207.\n",
      "Training epoch 0 batch 233 with loss 2.026803970336914.\n",
      "Training epoch 0 batch 234 with loss 1.9507745504379272.\n",
      "Training epoch 0 batch 235 with loss 2.174119234085083.\n",
      "Training epoch 0 batch 236 with loss 1.949475884437561.\n",
      "Training epoch 0 batch 237 with loss 2.1116585731506348.\n",
      "Training epoch 0 batch 238 with loss 1.9400789737701416.\n",
      "Training epoch 0 batch 239 with loss 1.8548662662506104.\n",
      "Training epoch 0 batch 240 with loss 1.8296387195587158.\n",
      "Training epoch 0 batch 241 with loss 1.978933572769165.\n",
      "Training epoch 0 batch 242 with loss 1.9874613285064697.\n",
      "Training epoch 0 batch 243 with loss 1.8777847290039062.\n",
      "Training epoch 0 batch 244 with loss 2.3414194583892822.\n",
      "Training epoch 0 batch 245 with loss 1.8777252435684204.\n",
      "Training epoch 0 batch 246 with loss 1.9839670658111572.\n",
      "Training epoch 0 batch 247 with loss 2.2365024089813232.\n",
      "Training epoch 0 batch 248 with loss 2.0806055068969727.\n",
      "Training epoch 0 batch 249 with loss 1.9767345190048218.\n",
      "Training epoch 0 batch 250 with loss 2.028276205062866.\n",
      "Training epoch 0 batch 251 with loss 2.2060465812683105.\n",
      "Training epoch 0 batch 252 with loss 1.986226201057434.\n",
      "Training epoch 0 batch 253 with loss 1.9704711437225342.\n",
      "Training epoch 0 batch 254 with loss 1.9296175241470337.\n",
      "Training epoch 0 batch 255 with loss 1.9773247241973877.\n",
      "Training epoch 0 batch 256 with loss 2.383960247039795.\n",
      "Training epoch 0 batch 257 with loss 2.188197374343872.\n",
      "Training epoch 0 batch 258 with loss 2.082575798034668.\n",
      "Training epoch 0 batch 259 with loss 1.8738495111465454.\n",
      "Training epoch 0 batch 260 with loss 2.2968451976776123.\n",
      "Training epoch 0 batch 261 with loss 1.9990668296813965.\n",
      "Training epoch 0 batch 262 with loss 2.0238168239593506.\n",
      "Training epoch 0 batch 263 with loss 2.3420331478118896.\n",
      "Training epoch 0 batch 264 with loss 2.0524322986602783.\n",
      "Training epoch 0 batch 265 with loss 2.021263837814331.\n",
      "Training epoch 0 batch 266 with loss 1.893384337425232.\n",
      "Training epoch 0 batch 267 with loss 2.388573408126831.\n",
      "Training epoch 0 batch 268 with loss 1.9128834009170532.\n",
      "Training epoch 0 batch 269 with loss 2.083221673965454.\n",
      "Training epoch 0 batch 270 with loss 2.222426176071167.\n",
      "Training epoch 0 batch 271 with loss 2.0373241901397705.\n",
      "Training epoch 0 batch 272 with loss 2.0303173065185547.\n",
      "Training epoch 0 batch 273 with loss 1.970862627029419.\n",
      "Training epoch 0 batch 274 with loss 2.075618028640747.\n",
      "Training epoch 0 batch 275 with loss 2.269902229309082.\n",
      "Training epoch 0 batch 276 with loss 1.8788437843322754.\n",
      "Training epoch 0 batch 277 with loss 2.075878381729126.\n",
      "Training epoch 0 batch 278 with loss 1.9639908075332642.\n",
      "Training epoch 0 batch 279 with loss 2.125025510787964.\n",
      "Training epoch 0 batch 280 with loss 2.070352077484131.\n",
      "Training epoch 0 batch 281 with loss 2.12595534324646.\n",
      "Training epoch 0 batch 282 with loss 2.5233471393585205.\n",
      "Training epoch 0 batch 283 with loss 2.0116212368011475.\n",
      "Training epoch 0 batch 284 with loss 1.7333948612213135.\n",
      "Training epoch 0 batch 285 with loss 1.9147311449050903.\n",
      "Training epoch 0 batch 286 with loss 2.4597251415252686.\n",
      "Training epoch 0 batch 287 with loss 2.00578236579895.\n",
      "Training epoch 0 batch 288 with loss 2.011051893234253.\n",
      "Training epoch 0 batch 289 with loss 2.1499013900756836.\n",
      "Training epoch 0 batch 290 with loss 2.1266157627105713.\n",
      "Training epoch 0 batch 291 with loss 2.2205517292022705.\n",
      "Training epoch 0 batch 292 with loss 1.9645220041275024.\n",
      "Training epoch 0 batch 293 with loss 2.387622594833374.\n",
      "Training epoch 0 batch 294 with loss 1.9304959774017334.\n",
      "Training epoch 0 batch 295 with loss 2.0727035999298096.\n",
      "Training epoch 0 batch 296 with loss 1.9481693506240845.\n",
      "Training epoch 0 batch 297 with loss 2.081718683242798.\n",
      "Training epoch 0 batch 298 with loss 2.2710864543914795.\n",
      "Training epoch 0 batch 299 with loss 2.370631217956543.\n",
      "Training epoch 0 batch 300 with loss 1.9514251947402954.\n",
      "Training epoch 0 batch 301 with loss 2.0106120109558105.\n",
      "Training epoch 0 batch 302 with loss 2.4048261642456055.\n",
      "Training epoch 0 batch 303 with loss 1.9331419467926025.\n",
      "Training epoch 0 batch 304 with loss 2.163813591003418.\n",
      "Training epoch 0 batch 305 with loss 2.3595378398895264.\n",
      "Training epoch 0 batch 306 with loss 2.0921196937561035.\n",
      "Training epoch 0 batch 307 with loss 2.1207809448242188.\n",
      "Training epoch 0 batch 308 with loss 2.162294626235962.\n",
      "Training epoch 0 batch 309 with loss 1.8851617574691772.\n",
      "Training epoch 0 batch 310 with loss 2.0840673446655273.\n",
      "Training epoch 0 batch 311 with loss 2.2872157096862793.\n",
      "Training epoch 0 batch 312 with loss 1.8570631742477417.\n",
      "Training epoch 0 batch 313 with loss 1.7862709760665894.\n",
      "Training epoch 0 batch 314 with loss 2.1694610118865967.\n",
      "Training epoch 0 batch 315 with loss 2.257103204727173.\n",
      "Training epoch 0 batch 316 with loss 2.3393123149871826.\n",
      "Training epoch 0 batch 317 with loss 2.3736326694488525.\n",
      "Training epoch 0 batch 318 with loss 2.281085729598999.\n",
      "Training epoch 0 batch 319 with loss 1.9719566106796265.\n",
      "Training epoch 0 batch 320 with loss 2.0828592777252197.\n",
      "Training epoch 0 batch 321 with loss 2.29544997215271.\n",
      "Training epoch 0 batch 322 with loss 2.2714219093322754.\n",
      "Training epoch 0 batch 323 with loss 2.2607192993164062.\n",
      "Training epoch 0 batch 324 with loss 2.2311511039733887.\n",
      "Training epoch 0 batch 325 with loss 2.3374814987182617.\n",
      "Training epoch 0 batch 326 with loss 2.012089490890503.\n",
      "Training epoch 0 batch 327 with loss 1.864031434059143.\n",
      "Training epoch 0 batch 328 with loss 2.0846612453460693.\n",
      "Training epoch 0 batch 329 with loss 2.2614312171936035.\n",
      "Training epoch 0 batch 330 with loss 2.087669849395752.\n",
      "Training epoch 0 batch 331 with loss 2.172790765762329.\n",
      "Training epoch 0 batch 332 with loss 2.0731279850006104.\n",
      "Training epoch 0 batch 333 with loss 2.1060588359832764.\n",
      "Training epoch 0 batch 334 with loss 2.1493706703186035.\n",
      "Training epoch 0 batch 335 with loss 2.352142810821533.\n",
      "Training epoch 0 batch 336 with loss 2.2871007919311523.\n",
      "Training epoch 0 batch 337 with loss 2.124748468399048.\n",
      "Training epoch 0 batch 338 with loss 2.238724708557129.\n",
      "Training epoch 0 batch 339 with loss 2.122612953186035.\n",
      "Training epoch 0 batch 340 with loss 1.9801993370056152.\n",
      "Training epoch 0 batch 341 with loss 2.246792793273926.\n",
      "Training epoch 0 batch 342 with loss 2.102926015853882.\n",
      "Training epoch 0 batch 343 with loss 2.3043622970581055.\n",
      "Training epoch 0 batch 344 with loss 2.436431884765625.\n",
      "Training epoch 0 batch 345 with loss 1.9024074077606201.\n",
      "Training epoch 0 batch 346 with loss 1.745310664176941.\n",
      "Training epoch 0 batch 347 with loss 1.8815792798995972.\n",
      "Training epoch 0 batch 348 with loss 2.003211736679077.\n",
      "Training epoch 0 batch 349 with loss 2.0233497619628906.\n",
      "Training epoch 0 batch 350 with loss 2.1641857624053955.\n",
      "Training epoch 0 batch 351 with loss 2.216033935546875.\n",
      "Training epoch 0 batch 352 with loss 2.2066609859466553.\n",
      "Training epoch 0 batch 353 with loss 1.8083398342132568.\n",
      "Training epoch 0 batch 354 with loss 2.060941457748413.\n",
      "Training epoch 0 batch 355 with loss 2.1462149620056152.\n",
      "Training epoch 0 batch 356 with loss 2.2054755687713623.\n",
      "Training epoch 0 batch 357 with loss 1.8947473764419556.\n",
      "Training epoch 0 batch 358 with loss 1.9139282703399658.\n",
      "Training epoch 0 batch 359 with loss 1.8976218700408936.\n",
      "Training epoch 0 batch 360 with loss 2.187513828277588.\n",
      "Training epoch 0 batch 361 with loss 2.25697660446167.\n",
      "Training epoch 0 batch 362 with loss 2.03450345993042.\n",
      "Training epoch 0 batch 363 with loss 2.368833303451538.\n",
      "Training epoch 0 batch 364 with loss 2.122371196746826.\n",
      "Training epoch 0 batch 365 with loss 2.023355007171631.\n",
      "Training epoch 0 batch 366 with loss 2.141395092010498.\n",
      "Training epoch 0 batch 367 with loss 1.8394923210144043.\n",
      "Training epoch 0 batch 368 with loss 2.398524284362793.\n",
      "Training epoch 0 batch 369 with loss 2.175076961517334.\n",
      "Training epoch 0 batch 370 with loss 2.3237850666046143.\n",
      "Training epoch 0 batch 371 with loss 2.149686098098755.\n",
      "Training epoch 0 batch 372 with loss 2.3538131713867188.\n",
      "Training epoch 0 batch 373 with loss 2.191990613937378.\n",
      "Training epoch 0 batch 374 with loss 2.253124952316284.\n",
      "Training epoch 0 batch 375 with loss 1.931718349456787.\n",
      "Training epoch 0 batch 376 with loss 2.083991765975952.\n",
      "Training epoch 0 batch 377 with loss 2.379305124282837.\n",
      "Training epoch 0 batch 378 with loss 2.3336021900177.\n",
      "Training epoch 0 batch 379 with loss 2.5229060649871826.\n",
      "Training epoch 0 batch 380 with loss 2.0820672512054443.\n",
      "Training epoch 0 batch 381 with loss 2.1164772510528564.\n",
      "Training epoch 0 batch 382 with loss 2.0364396572113037.\n",
      "Training epoch 0 batch 383 with loss 2.185276985168457.\n",
      "Training epoch 0 batch 384 with loss 2.237921953201294.\n",
      "Training epoch 0 batch 385 with loss 2.229674816131592.\n",
      "Training epoch 0 batch 386 with loss 2.5523180961608887.\n",
      "Training epoch 0 batch 387 with loss 1.8363980054855347.\n",
      "Training epoch 0 batch 388 with loss 1.930105209350586.\n",
      "Training epoch 0 batch 389 with loss 2.3757660388946533.\n",
      "Training epoch 0 batch 390 with loss 1.9171065092086792.\n",
      "Training epoch 0 batch 391 with loss 2.1937413215637207.\n",
      "Training epoch 0 batch 392 with loss 1.8327679634094238.\n",
      "Training epoch 0 batch 393 with loss 2.3904013633728027.\n",
      "Training epoch 0 batch 394 with loss 2.6713309288024902.\n",
      "Training epoch 0 batch 395 with loss 2.2371413707733154.\n",
      "Training epoch 0 batch 396 with loss 2.1249985694885254.\n",
      "Training epoch 0 batch 397 with loss 2.0793161392211914.\n",
      "Training epoch 0 batch 398 with loss 2.3450355529785156.\n",
      "Training epoch 0 batch 399 with loss 2.3126206398010254.\n",
      "Training epoch 0 batch 400 with loss 2.2922449111938477.\n",
      "Training epoch 0 batch 401 with loss 2.1872401237487793.\n",
      "Training epoch 0 batch 402 with loss 2.3921256065368652.\n",
      "Training epoch 0 batch 403 with loss 2.287850856781006.\n",
      "Training epoch 0 batch 404 with loss 2.303086996078491.\n",
      "Training epoch 0 batch 405 with loss 2.020775079727173.\n",
      "Training epoch 0 batch 406 with loss 2.293720006942749.\n",
      "Training epoch 0 batch 407 with loss 1.9932113885879517.\n",
      "Training epoch 0 batch 408 with loss 2.182558536529541.\n",
      "Training epoch 0 batch 409 with loss 2.0350818634033203.\n",
      "Training epoch 0 batch 410 with loss 2.2543771266937256.\n",
      "Training epoch 0 batch 411 with loss 2.342839002609253.\n",
      "Training epoch 0 batch 412 with loss 1.8502147197723389.\n",
      "Training epoch 0 batch 413 with loss 2.0599122047424316.\n",
      "Training epoch 0 batch 414 with loss 2.3377692699432373.\n",
      "Training epoch 0 batch 415 with loss 1.975484848022461.\n",
      "Training epoch 0 batch 416 with loss 2.1202359199523926.\n",
      "Training epoch 0 batch 417 with loss 2.102543354034424.\n",
      "Training epoch 0 batch 418 with loss 2.2643353939056396.\n",
      "Training epoch 0 batch 419 with loss 2.3025317192077637.\n",
      "Training epoch 0 batch 420 with loss 1.9894930124282837.\n",
      "Training epoch 0 batch 421 with loss 2.2139089107513428.\n",
      "Training epoch 0 batch 422 with loss 2.0473902225494385.\n",
      "Training epoch 0 batch 423 with loss 2.166092872619629.\n",
      "Training epoch 0 batch 424 with loss 2.1058454513549805.\n",
      "Training epoch 0 batch 425 with loss 1.8657450675964355.\n",
      "Training epoch 0 batch 426 with loss 1.8624751567840576.\n",
      "Training epoch 0 batch 427 with loss 2.2016170024871826.\n",
      "Training epoch 0 batch 428 with loss 2.1628916263580322.\n",
      "Training epoch 0 batch 429 with loss 1.980396032333374.\n",
      "Training epoch 0 batch 430 with loss 2.135344982147217.\n",
      "Training epoch 0 batch 431 with loss 2.404949188232422.\n",
      "Training epoch 0 batch 432 with loss 1.9209829568862915.\n",
      "Training epoch 0 batch 433 with loss 2.0667691230773926.\n",
      "Training epoch 0 batch 434 with loss 1.9288893938064575.\n",
      "Training epoch 0 batch 435 with loss 2.3776960372924805.\n",
      "Training epoch 0 batch 436 with loss 2.131760835647583.\n",
      "Training epoch 0 batch 437 with loss 2.1766700744628906.\n",
      "Training epoch 0 batch 438 with loss 2.119028329849243.\n",
      "Training epoch 0 batch 439 with loss 2.164379596710205.\n",
      "Training epoch 0 batch 440 with loss 1.9769723415374756.\n",
      "Training epoch 0 batch 441 with loss 1.7193349599838257.\n",
      "Training epoch 0 batch 442 with loss 2.3839423656463623.\n",
      "Training epoch 0 batch 443 with loss 2.0792746543884277.\n",
      "Training epoch 0 batch 444 with loss 2.2484757900238037.\n",
      "Training epoch 0 batch 445 with loss 1.8954106569290161.\n",
      "Training epoch 0 batch 446 with loss 1.9988996982574463.\n",
      "Training epoch 0 batch 447 with loss 1.8822863101959229.\n",
      "Training epoch 0 batch 448 with loss 2.237750768661499.\n",
      "Training epoch 0 batch 449 with loss 2.278385639190674.\n",
      "Training epoch 0 batch 450 with loss 1.9153416156768799.\n",
      "Training epoch 0 batch 451 with loss 2.0669970512390137.\n",
      "Training epoch 0 batch 452 with loss 1.9605085849761963.\n",
      "Training epoch 0 batch 453 with loss 1.9094078540802002.\n",
      "Training epoch 0 batch 454 with loss 2.1560466289520264.\n",
      "Training epoch 0 batch 455 with loss 2.0095627307891846.\n",
      "Training epoch 0 batch 456 with loss 2.330203056335449.\n",
      "Training epoch 0 batch 457 with loss 2.21160626411438.\n",
      "Training epoch 0 batch 458 with loss 2.4051902294158936.\n",
      "Training epoch 0 batch 459 with loss 2.2237441539764404.\n",
      "Training epoch 0 batch 460 with loss 2.240609645843506.\n",
      "Training epoch 0 batch 461 with loss 2.394583225250244.\n",
      "Training epoch 0 batch 462 with loss 1.908862829208374.\n",
      "Training epoch 0 batch 463 with loss 2.5263631343841553.\n",
      "Training epoch 0 batch 464 with loss 1.9659210443496704.\n",
      "Training epoch 0 batch 465 with loss 1.8925890922546387.\n",
      "Training epoch 0 batch 466 with loss 2.0903539657592773.\n",
      "Training epoch 0 batch 467 with loss 2.2904815673828125.\n",
      "Training epoch 0 batch 468 with loss 1.909420371055603.\n",
      "Training epoch 0 batch 469 with loss 2.4278299808502197.\n",
      "Training epoch 0 batch 470 with loss 2.3355462551116943.\n",
      "Training epoch 0 batch 471 with loss 1.8358752727508545.\n",
      "Training epoch 0 batch 472 with loss 2.379214286804199.\n",
      "Training epoch 0 batch 473 with loss 2.236868143081665.\n",
      "Training epoch 0 batch 474 with loss 2.0325021743774414.\n",
      "Training epoch 0 batch 475 with loss 2.056766986846924.\n",
      "Training epoch 0 batch 476 with loss 2.3485236167907715.\n",
      "Training epoch 0 batch 477 with loss 2.135606527328491.\n",
      "Training epoch 0 batch 478 with loss 1.9050705432891846.\n",
      "Training epoch 0 batch 479 with loss 2.116853952407837.\n",
      "Training epoch 0 batch 480 with loss 2.037217378616333.\n",
      "Training epoch 0 batch 481 with loss 2.0000131130218506.\n",
      "Training epoch 0 batch 482 with loss 2.3329696655273438.\n",
      "Training epoch 0 batch 483 with loss 1.993936538696289.\n",
      "Training epoch 0 batch 484 with loss 2.3299267292022705.\n",
      "Training epoch 0 batch 485 with loss 2.11354923248291.\n",
      "Training epoch 0 batch 486 with loss 2.329374074935913.\n",
      "Training epoch 0 batch 487 with loss 2.2161331176757812.\n",
      "Training epoch 0 batch 488 with loss 2.182567596435547.\n",
      "Training epoch 0 batch 489 with loss 2.525866985321045.\n",
      "Training epoch 0 batch 490 with loss 2.039046287536621.\n",
      "Training epoch 0 batch 491 with loss 2.3301546573638916.\n",
      "Training epoch 0 batch 492 with loss 2.0968308448791504.\n",
      "Training epoch 0 batch 493 with loss 2.133751153945923.\n",
      "Training epoch 0 batch 494 with loss 2.054694890975952.\n",
      "Training epoch 0 batch 495 with loss 2.0663270950317383.\n",
      "Training epoch 0 batch 496 with loss 2.357839822769165.\n",
      "Training epoch 0 batch 497 with loss 2.2765212059020996.\n",
      "Training epoch 0 batch 498 with loss 2.067871570587158.\n",
      "Training epoch 0 batch 499 with loss 2.117180109024048.\n",
      "Training epoch 0 batch 500 with loss 1.9730795621871948.\n",
      "Training epoch 0 batch 501 with loss 2.396923065185547.\n",
      "Training epoch 0 batch 502 with loss 2.2680697441101074.\n",
      "Training epoch 0 batch 503 with loss 2.118946075439453.\n",
      "Training epoch 0 batch 504 with loss 2.0097134113311768.\n",
      "Training epoch 0 batch 505 with loss 2.047895908355713.\n",
      "Training epoch 0 batch 506 with loss 2.3332040309906006.\n",
      "Training epoch 0 batch 507 with loss 2.07076096534729.\n",
      "Training epoch 0 batch 508 with loss 2.1851303577423096.\n",
      "Training epoch 0 batch 509 with loss 2.35209584236145.\n",
      "Training epoch 0 batch 510 with loss 2.3879764080047607.\n",
      "Training epoch 0 batch 511 with loss 2.0187766551971436.\n",
      "Training epoch 0 batch 512 with loss 2.2543020248413086.\n",
      "Training epoch 0 batch 513 with loss 2.1892929077148438.\n",
      "Training epoch 0 batch 514 with loss 2.7105324268341064.\n",
      "Training epoch 0 batch 515 with loss 2.3047258853912354.\n",
      "Training epoch 0 batch 516 with loss 2.086308717727661.\n",
      "Training epoch 0 batch 517 with loss 2.3304049968719482.\n",
      "Training epoch 0 batch 518 with loss 2.1157798767089844.\n",
      "Training epoch 0 batch 519 with loss 2.092392921447754.\n",
      "Training epoch 0 batch 520 with loss 2.188485860824585.\n",
      "Training epoch 0 batch 521 with loss 2.221332311630249.\n",
      "Training epoch 0 batch 522 with loss 2.5318679809570312.\n",
      "Training epoch 0 batch 523 with loss 1.845973253250122.\n",
      "Training epoch 0 batch 524 with loss 2.3114304542541504.\n",
      "Training epoch 0 batch 525 with loss 2.162205457687378.\n",
      "Training epoch 0 batch 526 with loss 1.9593182802200317.\n",
      "Training epoch 0 batch 527 with loss 2.1746985912323.\n",
      "Training epoch 0 batch 528 with loss 1.9659391641616821.\n",
      "Training epoch 0 batch 529 with loss 2.2683820724487305.\n",
      "Training epoch 0 batch 530 with loss 1.9883038997650146.\n",
      "Training epoch 0 batch 531 with loss 2.3842976093292236.\n",
      "Training epoch 0 batch 532 with loss 2.392402172088623.\n",
      "Training epoch 0 batch 533 with loss 1.9042315483093262.\n",
      "Training epoch 0 batch 534 with loss 2.423793315887451.\n",
      "Training epoch 0 batch 535 with loss 2.0531537532806396.\n",
      "Training epoch 0 batch 536 with loss 2.257643461227417.\n",
      "Training epoch 0 batch 537 with loss 2.14081072807312.\n",
      "Training epoch 0 batch 538 with loss 2.1017229557037354.\n",
      "Training epoch 0 batch 539 with loss 2.3761403560638428.\n",
      "Training epoch 0 batch 540 with loss 2.381985902786255.\n",
      "Training epoch 0 batch 541 with loss 2.29105806350708.\n",
      "Training epoch 0 batch 542 with loss 2.173938035964966.\n",
      "Training epoch 0 batch 543 with loss 2.086949586868286.\n",
      "Training epoch 0 batch 544 with loss 2.111751079559326.\n",
      "Training epoch 0 batch 545 with loss 2.3066554069519043.\n",
      "Training epoch 0 batch 546 with loss 2.4030542373657227.\n",
      "Training epoch 0 batch 547 with loss 2.372533082962036.\n",
      "Training epoch 0 batch 548 with loss 2.055396556854248.\n",
      "Training epoch 0 batch 549 with loss 1.8746240139007568.\n",
      "Training epoch 0 batch 550 with loss 1.9753241539001465.\n",
      "Training epoch 0 batch 551 with loss 1.8788303136825562.\n",
      "Training epoch 0 batch 552 with loss 1.8672820329666138.\n",
      "Training epoch 0 batch 553 with loss 2.18356990814209.\n",
      "Training epoch 0 batch 554 with loss 2.138925790786743.\n",
      "Training epoch 0 batch 555 with loss 2.160290002822876.\n",
      "Training epoch 0 batch 556 with loss 2.384989023208618.\n",
      "Training epoch 0 batch 557 with loss 2.412811279296875.\n",
      "Training epoch 0 batch 558 with loss 2.027862310409546.\n",
      "Training epoch 0 batch 559 with loss 2.532477378845215.\n",
      "Training epoch 0 batch 560 with loss 2.014371156692505.\n",
      "Training epoch 0 batch 561 with loss 2.003218173980713.\n",
      "Training epoch 0 batch 562 with loss 2.2564377784729004.\n",
      "Training epoch 0 batch 563 with loss 2.1849358081817627.\n",
      "Training epoch 0 batch 564 with loss 2.085078716278076.\n",
      "Training epoch 0 batch 565 with loss 2.219250202178955.\n",
      "Training epoch 0 batch 566 with loss 1.9763094186782837.\n",
      "Training epoch 0 batch 567 with loss 2.2754883766174316.\n",
      "Training epoch 0 batch 568 with loss 2.043808698654175.\n",
      "Training epoch 0 batch 569 with loss 2.3128836154937744.\n",
      "Training epoch 0 batch 570 with loss 2.0966014862060547.\n",
      "Training epoch 0 batch 571 with loss 2.2373154163360596.\n",
      "Training epoch 0 batch 572 with loss 2.159334659576416.\n",
      "Training epoch 0 batch 573 with loss 2.28696608543396.\n",
      "Training epoch 0 batch 574 with loss 1.9328269958496094.\n",
      "Training epoch 0 batch 575 with loss 2.265737295150757.\n",
      "Training epoch 0 batch 576 with loss 1.9943729639053345.\n",
      "Training epoch 0 batch 577 with loss 2.3430063724517822.\n",
      "Training epoch 0 batch 578 with loss 2.094270706176758.\n",
      "Training epoch 0 batch 579 with loss 2.382476568222046.\n",
      "Training epoch 0 batch 580 with loss 2.2207424640655518.\n",
      "Training epoch 0 batch 581 with loss 2.202023506164551.\n",
      "Training epoch 0 batch 582 with loss 1.8278385400772095.\n",
      "Training epoch 0 batch 583 with loss 1.9451831579208374.\n",
      "Training epoch 0 batch 584 with loss 2.3477065563201904.\n",
      "Training epoch 0 batch 585 with loss 2.3587934970855713.\n",
      "Training epoch 0 batch 586 with loss 2.0723235607147217.\n",
      "Training epoch 0 batch 587 with loss 1.9164657592773438.\n",
      "Training epoch 0 batch 588 with loss 1.7865378856658936.\n",
      "Training epoch 0 batch 589 with loss 2.3549792766571045.\n",
      "Training epoch 0 batch 590 with loss 2.296534299850464.\n",
      "Training epoch 0 batch 591 with loss 2.130411148071289.\n",
      "Training epoch 0 batch 592 with loss 2.0384230613708496.\n",
      "Training epoch 0 batch 593 with loss 2.154010772705078.\n",
      "Training epoch 0 batch 594 with loss 2.0380074977874756.\n",
      "Training epoch 0 batch 595 with loss 2.150907039642334.\n",
      "Training epoch 0 batch 596 with loss 2.2699484825134277.\n",
      "Training epoch 0 batch 597 with loss 1.9578644037246704.\n",
      "Training epoch 0 batch 598 with loss 2.1445934772491455.\n",
      "Training epoch 0 batch 599 with loss 2.194066286087036.\n",
      "Training epoch 0 batch 600 with loss 1.886737585067749.\n",
      "Training epoch 0 batch 601 with loss 1.9043498039245605.\n",
      "Training epoch 0 batch 602 with loss 2.162966251373291.\n",
      "Training epoch 0 batch 603 with loss 2.3938777446746826.\n",
      "Training epoch 0 batch 604 with loss 2.613844871520996.\n",
      "Training epoch 0 batch 605 with loss 2.1596696376800537.\n",
      "Training epoch 0 batch 606 with loss 2.2354190349578857.\n",
      "Training epoch 0 batch 607 with loss 2.315155029296875.\n",
      "Training epoch 0 batch 608 with loss 2.151092529296875.\n",
      "Training epoch 0 batch 609 with loss 2.395399570465088.\n",
      "Training epoch 0 batch 610 with loss 2.2724192142486572.\n",
      "Training epoch 0 batch 611 with loss 1.9495491981506348.\n",
      "Training epoch 0 batch 612 with loss 2.2301483154296875.\n",
      "Training epoch 0 batch 613 with loss 2.1967408657073975.\n",
      "Training epoch 0 batch 614 with loss 1.955085039138794.\n",
      "Training epoch 0 batch 615 with loss 2.2365622520446777.\n",
      "Training epoch 0 batch 616 with loss 2.1378750801086426.\n",
      "Training epoch 0 batch 617 with loss 2.3052029609680176.\n",
      "Training epoch 0 batch 618 with loss 2.5707523822784424.\n",
      "Training epoch 0 batch 619 with loss 2.1902172565460205.\n",
      "Training epoch 0 batch 620 with loss 1.996161699295044.\n",
      "Training epoch 0 batch 621 with loss 2.265080213546753.\n",
      "Training epoch 0 batch 622 with loss 2.3897056579589844.\n",
      "Training epoch 0 batch 623 with loss 2.0685720443725586.\n",
      "Training epoch 0 batch 624 with loss 2.136392831802368.\n",
      "Training epoch 0 batch 625 with loss 1.9744701385498047.\n",
      "Training epoch 0 batch 626 with loss 2.283432722091675.\n",
      "Training epoch 0 batch 627 with loss 2.194365978240967.\n",
      "Training epoch 0 batch 628 with loss 2.0774614810943604.\n",
      "Training epoch 0 batch 629 with loss 1.8950093984603882.\n",
      "Training epoch 0 batch 630 with loss 2.020920753479004.\n",
      "Training epoch 0 batch 631 with loss 2.333069324493408.\n",
      "Training epoch 0 batch 632 with loss 2.396139621734619.\n",
      "Training epoch 0 batch 633 with loss 2.273451328277588.\n",
      "Training epoch 0 batch 634 with loss 2.063999891281128.\n",
      "Training epoch 0 batch 635 with loss 2.1570498943328857.\n",
      "Training epoch 0 batch 636 with loss 2.334202527999878.\n",
      "Training epoch 0 batch 637 with loss 2.1627700328826904.\n",
      "Training epoch 0 batch 638 with loss 2.051469326019287.\n",
      "Training epoch 0 batch 639 with loss 2.4356753826141357.\n",
      "Training epoch 0 batch 640 with loss 2.0617635250091553.\n",
      "Training epoch 0 batch 641 with loss 2.297877073287964.\n",
      "Training epoch 0 batch 642 with loss 1.8361284732818604.\n",
      "Training epoch 0 batch 643 with loss 1.7594754695892334.\n",
      "Training epoch 0 batch 644 with loss 2.0161218643188477.\n",
      "Training epoch 0 batch 645 with loss 2.243821620941162.\n",
      "Training epoch 0 batch 646 with loss 2.4448776245117188.\n",
      "Training epoch 0 batch 647 with loss 2.387073040008545.\n",
      "Training epoch 0 batch 648 with loss 2.357974052429199.\n",
      "Training epoch 0 batch 649 with loss 2.316941976547241.\n",
      "Training epoch 0 batch 650 with loss 2.1605796813964844.\n",
      "Training epoch 0 batch 651 with loss 1.991613507270813.\n",
      "Training epoch 0 batch 652 with loss 2.5632758140563965.\n",
      "Training epoch 0 batch 653 with loss 2.312584400177002.\n",
      "Training epoch 0 batch 654 with loss 2.198542833328247.\n",
      "Training epoch 0 batch 655 with loss 2.2587409019470215.\n",
      "Training epoch 0 batch 656 with loss 2.328085422515869.\n",
      "Training epoch 0 batch 657 with loss 2.30243182182312.\n",
      "Training epoch 0 batch 658 with loss 2.498577117919922.\n",
      "Training epoch 0 batch 659 with loss 1.9935392141342163.\n",
      "Training epoch 0 batch 660 with loss 2.1799426078796387.\n",
      "Training epoch 0 batch 661 with loss 1.8598765134811401.\n",
      "Training epoch 0 batch 662 with loss 2.0588126182556152.\n",
      "Training epoch 0 batch 663 with loss 2.09574818611145.\n",
      "Training epoch 0 batch 664 with loss 2.3770859241485596.\n",
      "Training epoch 0 batch 665 with loss 2.159283399581909.\n",
      "Training epoch 0 batch 666 with loss 2.4026122093200684.\n",
      "Training epoch 0 batch 667 with loss 2.3706350326538086.\n",
      "Training epoch 0 batch 668 with loss 2.3679046630859375.\n",
      "Training epoch 0 batch 669 with loss 1.9394677877426147.\n",
      "Training epoch 0 batch 670 with loss 2.429036855697632.\n",
      "Training epoch 0 batch 671 with loss 1.9527000188827515.\n",
      "Training epoch 0 batch 672 with loss 2.1028730869293213.\n",
      "Training epoch 0 batch 673 with loss 1.9372577667236328.\n",
      "Training epoch 0 batch 674 with loss 2.633366823196411.\n",
      "Training epoch 0 batch 675 with loss 1.9846354722976685.\n",
      "Training epoch 0 batch 676 with loss 2.4459381103515625.\n",
      "Training epoch 0 batch 677 with loss 2.225660800933838.\n",
      "Training epoch 0 batch 678 with loss 2.3097195625305176.\n",
      "Training epoch 0 batch 679 with loss 2.134634256362915.\n",
      "Training epoch 0 batch 680 with loss 2.2129008769989014.\n",
      "Training epoch 0 batch 681 with loss 2.0494887828826904.\n",
      "Training epoch 0 batch 682 with loss 2.1399734020233154.\n",
      "Training epoch 0 batch 683 with loss 2.39487361907959.\n",
      "Training epoch 0 batch 684 with loss 2.2588894367218018.\n",
      "Training epoch 0 batch 685 with loss 2.0557265281677246.\n",
      "Training epoch 0 batch 686 with loss 2.449723958969116.\n",
      "Training epoch 0 batch 687 with loss 2.1204826831817627.\n",
      "Training epoch 0 batch 688 with loss 1.8077466487884521.\n",
      "Training epoch 0 batch 689 with loss 2.0430586338043213.\n",
      "Training epoch 0 batch 690 with loss 2.5012142658233643.\n",
      "Training epoch 0 batch 691 with loss 2.4345972537994385.\n",
      "Training epoch 0 batch 692 with loss 2.009126901626587.\n",
      "Training epoch 0 batch 693 with loss 2.2728822231292725.\n",
      "Training epoch 0 batch 694 with loss 2.3160972595214844.\n",
      "Training epoch 0 batch 695 with loss 2.06270432472229.\n",
      "Training epoch 0 batch 696 with loss 2.5980265140533447.\n",
      "Training epoch 0 batch 697 with loss 2.118211030960083.\n",
      "Training epoch 0 batch 698 with loss 2.0939126014709473.\n",
      "Training epoch 0 batch 699 with loss 2.03735613822937.\n",
      "Training epoch 0 batch 700 with loss 2.168889045715332.\n",
      "Training epoch 0 batch 701 with loss 1.9777932167053223.\n",
      "Training epoch 0 batch 702 with loss 2.4328372478485107.\n",
      "Training epoch 0 batch 703 with loss 1.9857317209243774.\n",
      "Training epoch 0 batch 704 with loss 2.0600335597991943.\n",
      "Training epoch 0 batch 705 with loss 1.9153960943222046.\n",
      "Training epoch 0 batch 706 with loss 2.2633628845214844.\n",
      "Training epoch 0 batch 707 with loss 2.3204641342163086.\n",
      "Training epoch 0 batch 708 with loss 2.3572709560394287.\n",
      "Training epoch 0 batch 709 with loss 2.0784223079681396.\n",
      "Training epoch 0 batch 710 with loss 2.1876611709594727.\n",
      "Training epoch 0 batch 711 with loss 2.546830177307129.\n",
      "Training epoch 0 batch 712 with loss 2.1277689933776855.\n",
      "Training epoch 0 batch 713 with loss 1.9302327632904053.\n",
      "Training epoch 0 batch 714 with loss 2.5664122104644775.\n",
      "Training epoch 0 batch 715 with loss 2.213883399963379.\n",
      "Training epoch 0 batch 716 with loss 2.3719096183776855.\n",
      "Training epoch 0 batch 717 with loss 2.391524314880371.\n",
      "Training epoch 0 batch 718 with loss 2.3044700622558594.\n",
      "Training epoch 0 batch 719 with loss 2.2614548206329346.\n",
      "Training epoch 0 batch 720 with loss 2.10984468460083.\n",
      "Training epoch 0 batch 721 with loss 2.3254497051239014.\n",
      "Training epoch 0 batch 722 with loss 2.102496862411499.\n",
      "Training epoch 0 batch 723 with loss 2.198807716369629.\n",
      "Training epoch 0 batch 724 with loss 1.9543910026550293.\n",
      "Training epoch 0 batch 725 with loss 1.9002068042755127.\n",
      "Training epoch 0 batch 726 with loss 2.4446065425872803.\n",
      "Training epoch 0 batch 727 with loss 2.2029740810394287.\n",
      "Training epoch 0 batch 728 with loss 2.45913028717041.\n",
      "Training epoch 0 batch 729 with loss 2.234771966934204.\n",
      "Training epoch 0 batch 730 with loss 1.8606690168380737.\n",
      "Training epoch 0 batch 731 with loss 2.225681781768799.\n",
      "Training epoch 0 batch 732 with loss 2.449185848236084.\n",
      "Training epoch 0 batch 733 with loss 2.257173776626587.\n",
      "Training epoch 0 batch 734 with loss 2.139907121658325.\n",
      "Training epoch 0 batch 735 with loss 2.1507062911987305.\n",
      "Training epoch 0 batch 736 with loss 2.208434820175171.\n",
      "Training epoch 0 batch 737 with loss 2.6029953956604004.\n",
      "Training epoch 0 batch 738 with loss 2.1821484565734863.\n",
      "Training epoch 0 batch 739 with loss 2.3125569820404053.\n",
      "Training epoch 0 batch 740 with loss 2.306140899658203.\n",
      "Training epoch 0 batch 741 with loss 2.2640156745910645.\n",
      "Training epoch 0 batch 742 with loss 2.2280304431915283.\n",
      "Training epoch 0 batch 743 with loss 2.1799206733703613.\n",
      "Training epoch 0 batch 744 with loss 2.087134838104248.\n",
      "Training epoch 0 batch 745 with loss 2.2271623611450195.\n",
      "Training epoch 0 batch 746 with loss 2.1354639530181885.\n",
      "Training epoch 0 batch 747 with loss 2.1382663249969482.\n",
      "Training epoch 0 batch 748 with loss 2.314997911453247.\n",
      "Training epoch 0 batch 749 with loss 2.421344757080078.\n",
      "Training epoch 0 batch 750 with loss 2.3962976932525635.\n",
      "Training epoch 0 batch 751 with loss 2.3319859504699707.\n",
      "Training epoch 0 batch 752 with loss 2.6341702938079834.\n",
      "Training epoch 0 batch 753 with loss 2.0818915367126465.\n",
      "Training epoch 0 batch 754 with loss 2.276334285736084.\n",
      "Training epoch 0 batch 755 with loss 2.0868000984191895.\n",
      "Training epoch 0 batch 756 with loss 2.093933582305908.\n",
      "Training epoch 0 batch 757 with loss 2.3661887645721436.\n",
      "Training epoch 0 batch 758 with loss 1.9573115110397339.\n",
      "Training epoch 0 batch 759 with loss 2.1603829860687256.\n",
      "Training epoch 0 batch 760 with loss 2.396460771560669.\n",
      "Training epoch 0 batch 761 with loss 2.117903709411621.\n",
      "Training epoch 0 batch 762 with loss 2.099505662918091.\n",
      "Training epoch 0 batch 763 with loss 2.0802996158599854.\n",
      "Training epoch 0 batch 764 with loss 2.3735837936401367.\n",
      "Training epoch 0 batch 765 with loss 2.0300755500793457.\n",
      "Training epoch 0 batch 766 with loss 2.1329922676086426.\n",
      "Training epoch 0 batch 767 with loss 1.8643239736557007.\n",
      "Training epoch 0 batch 768 with loss 2.2684483528137207.\n",
      "Training epoch 0 batch 769 with loss 1.8576362133026123.\n",
      "Training epoch 0 batch 770 with loss 2.4947080612182617.\n",
      "Training epoch 0 batch 771 with loss 2.2547898292541504.\n",
      "Training epoch 0 batch 772 with loss 2.2205610275268555.\n",
      "Training epoch 0 batch 773 with loss 2.2885711193084717.\n",
      "Training epoch 0 batch 774 with loss 2.0975050926208496.\n",
      "Training epoch 0 batch 775 with loss 2.344172477722168.\n",
      "Training epoch 0 batch 776 with loss 2.287384033203125.\n",
      "Training epoch 0 batch 777 with loss 2.4847216606140137.\n",
      "Training epoch 0 batch 778 with loss 2.3362958431243896.\n",
      "Training epoch 0 batch 779 with loss 2.289044141769409.\n",
      "Training epoch 0 batch 780 with loss 2.0518031120300293.\n",
      "Training epoch 0 batch 781 with loss 2.177062511444092.\n",
      "Training epoch 0 batch 782 with loss 2.3277478218078613.\n",
      "Training epoch 0 batch 783 with loss 2.144829750061035.\n",
      "Training epoch 0 batch 784 with loss 2.0862183570861816.\n",
      "Training epoch 0 batch 785 with loss 2.391249895095825.\n",
      "Training epoch 0 batch 786 with loss 2.285996198654175.\n",
      "Training epoch 0 batch 787 with loss 2.417701482772827.\n",
      "Training epoch 0 batch 788 with loss 2.0263192653656006.\n",
      "Training epoch 0 batch 789 with loss 1.8854560852050781.\n",
      "Training epoch 0 batch 790 with loss 2.0202670097351074.\n",
      "Training epoch 0 batch 791 with loss 2.0496716499328613.\n",
      "Training epoch 0 batch 792 with loss 2.1749379634857178.\n",
      "Training epoch 0 batch 793 with loss 2.2156574726104736.\n",
      "Training epoch 0 batch 794 with loss 2.624495267868042.\n",
      "Training epoch 0 batch 795 with loss 2.0499777793884277.\n",
      "Training epoch 0 batch 796 with loss 2.236501455307007.\n",
      "Training epoch 0 batch 797 with loss 2.003436803817749.\n",
      "Training epoch 0 batch 798 with loss 2.481243133544922.\n",
      "Training epoch 0 batch 799 with loss 2.370677947998047.\n",
      "Training epoch 0 batch 800 with loss 2.3044862747192383.\n",
      "Training epoch 0 batch 801 with loss 2.040720224380493.\n",
      "Training epoch 0 batch 802 with loss 2.064615488052368.\n",
      "Training epoch 0 batch 803 with loss 2.2288947105407715.\n",
      "Training epoch 0 batch 804 with loss 1.7095634937286377.\n",
      "Training epoch 0 batch 805 with loss 2.0998873710632324.\n",
      "Training epoch 0 batch 806 with loss 2.559784173965454.\n",
      "Training epoch 0 batch 807 with loss 2.2442750930786133.\n",
      "Training epoch 0 batch 808 with loss 2.565460681915283.\n",
      "Training epoch 0 batch 809 with loss 2.6287038326263428.\n",
      "Training epoch 0 batch 810 with loss 2.3637044429779053.\n",
      "Training epoch 0 batch 811 with loss 2.065901279449463.\n",
      "Training epoch 0 batch 812 with loss 2.598538398742676.\n",
      "Training epoch 0 batch 813 with loss 2.0242817401885986.\n",
      "Training epoch 0 batch 814 with loss 2.281979560852051.\n",
      "Training epoch 0 batch 815 with loss 2.26218318939209.\n",
      "Training epoch 0 batch 816 with loss 2.035290002822876.\n",
      "Training epoch 0 batch 817 with loss 2.3842480182647705.\n",
      "Training epoch 0 batch 818 with loss 2.301048517227173.\n",
      "Training epoch 0 batch 819 with loss 2.144798994064331.\n",
      "Training epoch 0 batch 820 with loss 2.1188950538635254.\n",
      "Training epoch 0 batch 821 with loss 2.2257001399993896.\n",
      "Training epoch 0 batch 822 with loss 2.564326047897339.\n",
      "Training epoch 0 batch 823 with loss 2.2879557609558105.\n",
      "Training epoch 0 batch 824 with loss 2.0198042392730713.\n",
      "Training epoch 0 batch 825 with loss 2.457805871963501.\n",
      "Training epoch 0 batch 826 with loss 2.0915398597717285.\n",
      "Training epoch 0 batch 827 with loss 1.9896138906478882.\n",
      "Training epoch 0 batch 828 with loss 2.208266019821167.\n",
      "Training epoch 0 batch 829 with loss 1.9640543460845947.\n",
      "Training epoch 0 batch 830 with loss 2.480774402618408.\n",
      "Training epoch 0 batch 831 with loss 2.4534568786621094.\n",
      "Training epoch 0 batch 832 with loss 1.868550419807434.\n",
      "Training epoch 0 batch 833 with loss 2.1036770343780518.\n",
      "Training epoch 0 batch 834 with loss 2.2580158710479736.\n",
      "Training epoch 0 batch 835 with loss 2.1012418270111084.\n",
      "Training epoch 0 batch 836 with loss 2.247765302658081.\n",
      "Training epoch 0 batch 837 with loss 2.6475327014923096.\n",
      "Training epoch 0 batch 838 with loss 2.023070812225342.\n",
      "Training epoch 0 batch 839 with loss 2.3692591190338135.\n",
      "Training epoch 0 batch 840 with loss 2.3745627403259277.\n",
      "Training epoch 0 batch 841 with loss 2.1538612842559814.\n",
      "Training epoch 0 batch 842 with loss 2.361786365509033.\n",
      "Training epoch 0 batch 843 with loss 2.200080394744873.\n",
      "Training epoch 0 batch 844 with loss 2.4172353744506836.\n",
      "Training epoch 0 batch 845 with loss 2.253093719482422.\n",
      "Training epoch 0 batch 846 with loss 2.5714235305786133.\n",
      "Training epoch 0 batch 847 with loss 2.3347065448760986.\n",
      "Training epoch 0 batch 848 with loss 2.1440353393554688.\n",
      "Training epoch 0 batch 849 with loss 2.173290491104126.\n",
      "Training epoch 0 batch 850 with loss 2.158690929412842.\n",
      "Training epoch 0 batch 851 with loss 1.9796142578125.\n",
      "Training epoch 0 batch 852 with loss 2.0902822017669678.\n",
      "Training epoch 0 batch 853 with loss 2.244828701019287.\n",
      "Training epoch 0 batch 854 with loss 2.202732563018799.\n",
      "Training epoch 0 batch 855 with loss 2.0973188877105713.\n",
      "Training epoch 0 batch 856 with loss 2.2303459644317627.\n",
      "Training epoch 0 batch 857 with loss 2.4491426944732666.\n",
      "Training epoch 0 batch 858 with loss 2.0231151580810547.\n",
      "Training epoch 0 batch 859 with loss 2.0492849349975586.\n",
      "Training epoch 0 batch 860 with loss 2.243032217025757.\n",
      "Training epoch 0 batch 861 with loss 2.340013027191162.\n",
      "Training epoch 0 batch 862 with loss 2.05477237701416.\n",
      "Training epoch 0 batch 863 with loss 2.6680290699005127.\n",
      "Training epoch 0 batch 864 with loss 2.0443708896636963.\n",
      "Training epoch 0 batch 865 with loss 2.1408193111419678.\n",
      "Training epoch 0 batch 866 with loss 2.001971960067749.\n",
      "Training epoch 0 batch 867 with loss 2.3280367851257324.\n",
      "Training epoch 0 batch 868 with loss 2.0876595973968506.\n",
      "Training epoch 0 batch 869 with loss 1.9142659902572632.\n",
      "Training epoch 0 batch 870 with loss 1.992607831954956.\n",
      "Training epoch 0 batch 871 with loss 2.298541307449341.\n",
      "Training epoch 0 batch 872 with loss 2.1486239433288574.\n",
      "Training epoch 0 batch 873 with loss 2.4258687496185303.\n",
      "Training epoch 0 batch 874 with loss 2.479536294937134.\n",
      "Training epoch 0 batch 875 with loss 2.473801612854004.\n",
      "Training epoch 0 batch 876 with loss 2.272144317626953.\n",
      "Training epoch 0 batch 877 with loss 2.1361303329467773.\n",
      "Training epoch 0 batch 878 with loss 2.2219934463500977.\n",
      "Training epoch 0 batch 879 with loss 2.3638622760772705.\n",
      "Training epoch 0 batch 880 with loss 1.8748947381973267.\n",
      "Training epoch 0 batch 881 with loss 1.9693034887313843.\n",
      "Training epoch 0 batch 882 with loss 2.2715272903442383.\n",
      "Training epoch 0 batch 883 with loss 2.1829888820648193.\n",
      "Training epoch 0 batch 884 with loss 1.6682133674621582.\n",
      "Training epoch 0 batch 885 with loss 2.1619787216186523.\n",
      "Training epoch 0 batch 886 with loss 2.254148244857788.\n",
      "Training epoch 0 batch 887 with loss 2.047245979309082.\n",
      "Training epoch 0 batch 888 with loss 2.003571033477783.\n",
      "Training epoch 0 batch 889 with loss 2.0199201107025146.\n",
      "Training epoch 0 batch 890 with loss 1.9996874332427979.\n",
      "Training epoch 0 batch 891 with loss 2.4946906566619873.\n",
      "Training epoch 0 batch 892 with loss 2.102815628051758.\n",
      "Training epoch 0 batch 893 with loss 2.2797152996063232.\n",
      "Training epoch 0 batch 894 with loss 2.0865328311920166.\n",
      "Training epoch 0 batch 895 with loss 2.2237167358398438.\n",
      "Training epoch 0 batch 896 with loss 2.2064192295074463.\n",
      "Training epoch 0 batch 897 with loss 2.484128475189209.\n",
      "Training epoch 0 batch 898 with loss 2.3635802268981934.\n",
      "Training epoch 0 batch 899 with loss 2.498103141784668.\n",
      "Training epoch 0 batch 900 with loss 2.1400671005249023.\n",
      "Training epoch 0 batch 901 with loss 2.194805145263672.\n",
      "Training epoch 0 batch 902 with loss 2.7058916091918945.\n",
      "Training epoch 0 batch 903 with loss 2.4519031047821045.\n",
      "Training epoch 0 batch 904 with loss 1.8246835470199585.\n",
      "Training epoch 0 batch 905 with loss 2.332423686981201.\n",
      "Training epoch 0 batch 906 with loss 2.1600751876831055.\n",
      "Training epoch 0 batch 907 with loss 2.147538661956787.\n",
      "Training epoch 0 batch 908 with loss 2.115100383758545.\n",
      "Training epoch 0 batch 909 with loss 2.111353874206543.\n",
      "Training epoch 0 batch 910 with loss 2.465167999267578.\n",
      "Training epoch 0 batch 911 with loss 2.3087522983551025.\n",
      "Training epoch 0 batch 912 with loss 2.326815128326416.\n",
      "Training epoch 0 batch 913 with loss 2.1771364212036133.\n",
      "Training epoch 0 batch 914 with loss 2.114271879196167.\n",
      "Training epoch 0 batch 915 with loss 2.242415428161621.\n",
      "Training epoch 0 batch 916 with loss 1.8706154823303223.\n",
      "Training epoch 0 batch 917 with loss 1.9561611413955688.\n",
      "Training epoch 0 batch 918 with loss 2.5580854415893555.\n",
      "Training epoch 0 batch 919 with loss 2.1590003967285156.\n",
      "Training epoch 0 batch 920 with loss 2.1365411281585693.\n",
      "Training epoch 0 batch 921 with loss 2.3520102500915527.\n",
      "Training epoch 0 batch 922 with loss 2.119866132736206.\n",
      "Training epoch 0 batch 923 with loss 2.067800283432007.\n",
      "Training epoch 0 batch 924 with loss 2.109855890274048.\n",
      "Training epoch 0 batch 925 with loss 2.0029072761535645.\n",
      "Training epoch 0 batch 926 with loss 1.9509223699569702.\n",
      "Training epoch 0 batch 927 with loss 1.9898775815963745.\n",
      "Training epoch 0 batch 928 with loss 1.929733395576477.\n",
      "Training epoch 0 batch 929 with loss 1.9854530096054077.\n",
      "Training epoch 0 batch 930 with loss 2.1601054668426514.\n",
      "Training epoch 0 batch 931 with loss 2.308627128601074.\n",
      "Training epoch 0 batch 932 with loss 2.5471878051757812.\n",
      "Training epoch 0 batch 933 with loss 2.154534339904785.\n",
      "Training epoch 0 batch 934 with loss 2.5943143367767334.\n",
      "Training epoch 0 batch 935 with loss 1.9333783388137817.\n",
      "Training epoch 0 batch 936 with loss 2.284402370452881.\n",
      "Training epoch 0 batch 937 with loss 2.1156022548675537.\n",
      "Training epoch 0 batch 938 with loss 2.1270503997802734.\n",
      "Training epoch 0 batch 939 with loss 2.2168264389038086.\n",
      "Training epoch 0 batch 940 with loss 2.2671399116516113.\n",
      "Training epoch 0 batch 941 with loss 2.00395131111145.\n",
      "Training epoch 0 batch 942 with loss 2.4001410007476807.\n",
      "Training epoch 0 batch 943 with loss 2.1600542068481445.\n",
      "Training epoch 0 batch 944 with loss 2.2324326038360596.\n",
      "Training epoch 0 batch 945 with loss 2.3350636959075928.\n",
      "Training epoch 0 batch 946 with loss 2.492680788040161.\n",
      "Training epoch 0 batch 947 with loss 2.419750452041626.\n",
      "Training epoch 0 batch 948 with loss 2.4626708030700684.\n",
      "Training epoch 0 batch 949 with loss 2.008784532546997.\n",
      "Training epoch 0 batch 950 with loss 2.2287380695343018.\n",
      "Training epoch 0 batch 951 with loss 2.258869171142578.\n",
      "Training epoch 0 batch 952 with loss 2.2522354125976562.\n",
      "Training epoch 0 batch 953 with loss 2.3101394176483154.\n",
      "Training epoch 0 batch 954 with loss 2.1881191730499268.\n",
      "Training epoch 0 batch 955 with loss 2.2018930912017822.\n",
      "Training epoch 0 batch 956 with loss 2.349780797958374.\n",
      "Training epoch 0 batch 957 with loss 2.1638290882110596.\n",
      "Training epoch 0 batch 958 with loss 2.362905979156494.\n",
      "Training epoch 0 batch 959 with loss 2.124439001083374.\n",
      "Training epoch 0 batch 960 with loss 2.5203917026519775.\n",
      "Training epoch 0 batch 961 with loss 2.3309597969055176.\n",
      "Training epoch 0 batch 962 with loss 2.2114334106445312.\n",
      "Training epoch 0 batch 963 with loss 2.016720771789551.\n",
      "Training epoch 0 batch 964 with loss 2.29345703125.\n",
      "Training epoch 0 batch 965 with loss 1.9116790294647217.\n",
      "Training epoch 0 batch 966 with loss 2.2511792182922363.\n",
      "Training epoch 0 batch 967 with loss 2.065290927886963.\n",
      "Training epoch 0 batch 968 with loss 2.620875358581543.\n",
      "Training epoch 0 batch 969 with loss 2.4168894290924072.\n",
      "Training epoch 0 batch 970 with loss 2.3486692905426025.\n",
      "Training epoch 0 batch 971 with loss 2.1033520698547363.\n",
      "Training epoch 0 batch 972 with loss 1.9699971675872803.\n",
      "Training epoch 0 batch 973 with loss 2.2198691368103027.\n",
      "Training epoch 0 batch 974 with loss 2.119441032409668.\n",
      "Training epoch 0 batch 975 with loss 2.3303985595703125.\n",
      "Training epoch 0 batch 976 with loss 2.218750476837158.\n",
      "Training epoch 0 batch 977 with loss 2.299525260925293.\n",
      "Training epoch 0 batch 978 with loss 2.015106439590454.\n",
      "Training epoch 0 batch 979 with loss 2.305105447769165.\n",
      "Training epoch 0 batch 980 with loss 2.062699317932129.\n",
      "Training epoch 0 batch 981 with loss 2.6157076358795166.\n",
      "Training epoch 0 batch 982 with loss 2.112330436706543.\n",
      "Training epoch 0 batch 983 with loss 2.217088222503662.\n",
      "Training epoch 0 batch 984 with loss 2.3791074752807617.\n",
      "Training epoch 0 batch 985 with loss 2.395857810974121.\n",
      "Training epoch 0 batch 986 with loss 2.1363723278045654.\n",
      "Training epoch 0 batch 987 with loss 2.1311450004577637.\n",
      "Training epoch 0 batch 988 with loss 2.068175792694092.\n",
      "Training epoch 0 batch 989 with loss 2.27761173248291.\n",
      "Training epoch 0 batch 990 with loss 2.0782954692840576.\n",
      "Training epoch 0 batch 991 with loss 2.371079444885254.\n",
      "Training epoch 0 batch 992 with loss 2.4178590774536133.\n",
      "Training epoch 0 batch 993 with loss 2.4160032272338867.\n",
      "Training epoch 0 batch 994 with loss 2.50079345703125.\n",
      "Training epoch 0 batch 995 with loss 2.4691319465637207.\n",
      "Training epoch 0 batch 996 with loss 1.9406476020812988.\n",
      "Training epoch 0 batch 997 with loss 2.288996934890747.\n",
      "Training epoch 0 batch 998 with loss 2.264275074005127.\n",
      "Training epoch 0 batch 999 with loss 2.3195037841796875.\n",
      "Test batch 0 with loss 1.9765228033065796.\n",
      "Test batch 1 with loss 2.0821661949157715.\n",
      "Test batch 2 with loss 2.0442795753479004.\n",
      "Test batch 3 with loss 1.8817908763885498.\n",
      "Test batch 4 with loss 2.1928868293762207.\n",
      "Test batch 5 with loss 1.824495792388916.\n",
      "Test batch 6 with loss 2.3901255130767822.\n",
      "Test batch 7 with loss 2.0353472232818604.\n",
      "Test batch 8 with loss 2.2562081813812256.\n",
      "Test batch 9 with loss 2.0475027561187744.\n",
      "Test batch 10 with loss 2.1703402996063232.\n",
      "Test batch 11 with loss 1.9049566984176636.\n",
      "Test batch 12 with loss 2.1951704025268555.\n",
      "Test batch 13 with loss 2.104344367980957.\n",
      "Test batch 14 with loss 2.0062577724456787.\n",
      "Test batch 15 with loss 2.209608316421509.\n",
      "Test batch 16 with loss 2.3436405658721924.\n",
      "Test batch 17 with loss 1.9232800006866455.\n",
      "Test batch 18 with loss 2.1149396896362305.\n",
      "Test batch 19 with loss 2.128809928894043.\n",
      "Test batch 20 with loss 2.1251368522644043.\n",
      "Test batch 21 with loss 2.3941118717193604.\n",
      "Test batch 22 with loss 2.3618431091308594.\n",
      "Test batch 23 with loss 2.1032166481018066.\n",
      "Test batch 24 with loss 1.9386982917785645.\n",
      "Test batch 25 with loss 2.2663629055023193.\n",
      "Test batch 26 with loss 2.1238811016082764.\n",
      "Test batch 27 with loss 2.13086199760437.\n",
      "Test batch 28 with loss 2.1520793437957764.\n",
      "Test batch 29 with loss 2.2476370334625244.\n",
      "Test batch 30 with loss 2.4005777835845947.\n",
      "Test batch 31 with loss 2.2177369594573975.\n",
      "Test batch 32 with loss 2.3387451171875.\n",
      "Test batch 33 with loss 2.1907236576080322.\n",
      "Test batch 34 with loss 2.1547906398773193.\n",
      "Test batch 35 with loss 2.193927049636841.\n",
      "Test batch 36 with loss 1.9460490942001343.\n",
      "Test batch 37 with loss 1.765092134475708.\n",
      "Test batch 38 with loss 2.3801140785217285.\n",
      "Test batch 39 with loss 2.320089817047119.\n",
      "Test batch 40 with loss 2.5620405673980713.\n",
      "Test batch 41 with loss 1.959216594696045.\n",
      "Test batch 42 with loss 2.3992269039154053.\n",
      "Test batch 43 with loss 1.7242588996887207.\n",
      "Test batch 44 with loss 2.4409918785095215.\n",
      "Test batch 45 with loss 2.129150152206421.\n",
      "Test batch 46 with loss 2.1662750244140625.\n",
      "Test batch 47 with loss 2.121947765350342.\n",
      "Test batch 48 with loss 2.464454412460327.\n",
      "Test batch 49 with loss 1.9238545894622803.\n",
      "Test batch 50 with loss 2.461838483810425.\n",
      "Test batch 51 with loss 2.1563327312469482.\n",
      "Test batch 52 with loss 2.1960434913635254.\n",
      "Test batch 53 with loss 2.1471924781799316.\n",
      "Test batch 54 with loss 2.558647632598877.\n",
      "Test batch 55 with loss 2.1192030906677246.\n",
      "Test batch 56 with loss 2.0181453227996826.\n",
      "Test batch 57 with loss 2.388828754425049.\n",
      "Test batch 58 with loss 2.0568509101867676.\n",
      "Test batch 59 with loss 2.279709815979004.\n",
      "Test batch 60 with loss 2.1633405685424805.\n",
      "Test batch 61 with loss 2.2617433071136475.\n",
      "Test batch 62 with loss 2.3717525005340576.\n",
      "Test batch 63 with loss 2.4560327529907227.\n",
      "Test batch 64 with loss 2.1173219680786133.\n",
      "Test batch 65 with loss 2.0680088996887207.\n",
      "Test batch 66 with loss 1.8465193510055542.\n",
      "Test batch 67 with loss 2.3323721885681152.\n",
      "Test batch 68 with loss 2.091724157333374.\n",
      "Test batch 69 with loss 2.3624658584594727.\n",
      "Test batch 70 with loss 2.1638457775115967.\n",
      "Test batch 71 with loss 2.23557710647583.\n",
      "Test batch 72 with loss 2.17533278465271.\n",
      "Test batch 73 with loss 2.209656000137329.\n",
      "Test batch 74 with loss 2.2207884788513184.\n",
      "Test batch 75 with loss 2.187633991241455.\n",
      "Test batch 76 with loss 2.02945613861084.\n",
      "Test batch 77 with loss 2.466216802597046.\n",
      "Test batch 78 with loss 2.249852180480957.\n",
      "Test batch 79 with loss 2.0821759700775146.\n",
      "Test batch 80 with loss 1.7880380153656006.\n",
      "Test batch 81 with loss 2.200134754180908.\n",
      "Test batch 82 with loss 2.14508318901062.\n",
      "Test batch 83 with loss 1.9486967325210571.\n",
      "Test batch 84 with loss 2.5530850887298584.\n",
      "Test batch 85 with loss 2.2066969871520996.\n",
      "Test batch 86 with loss 2.104924440383911.\n",
      "Test batch 87 with loss 2.658330202102661.\n",
      "Test batch 88 with loss 2.1726491451263428.\n",
      "Test batch 89 with loss 2.1410422325134277.\n",
      "Test batch 90 with loss 2.541815757751465.\n",
      "Test batch 91 with loss 2.232501268386841.\n",
      "Test batch 92 with loss 2.5565640926361084.\n",
      "Test batch 93 with loss 2.208066463470459.\n",
      "Test batch 94 with loss 2.1367623805999756.\n",
      "Test batch 95 with loss 2.162628173828125.\n",
      "Test batch 96 with loss 2.2265264987945557.\n",
      "Test batch 97 with loss 2.0519814491271973.\n",
      "Test batch 98 with loss 2.101762533187866.\n",
      "Test batch 99 with loss 2.173198938369751.\n",
      "Test batch 100 with loss 2.5468947887420654.\n",
      "Test batch 101 with loss 2.142624855041504.\n",
      "Test batch 102 with loss 1.8610540628433228.\n",
      "Test batch 103 with loss 2.32491397857666.\n",
      "Test batch 104 with loss 2.493380069732666.\n",
      "Test batch 105 with loss 1.8639343976974487.\n",
      "Test batch 106 with loss 2.283952236175537.\n",
      "Test batch 107 with loss 2.122058391571045.\n",
      "Test batch 108 with loss 2.0982167720794678.\n",
      "Test batch 109 with loss 2.4023542404174805.\n",
      "Test batch 110 with loss 2.341475009918213.\n",
      "Test batch 111 with loss 2.2937474250793457.\n",
      "Test batch 112 with loss 2.374878406524658.\n",
      "Test batch 113 with loss 1.8718395233154297.\n",
      "Test batch 114 with loss 2.2174019813537598.\n",
      "Test batch 115 with loss 2.166501760482788.\n",
      "Test batch 116 with loss 2.574997663497925.\n",
      "Test batch 117 with loss 2.6388907432556152.\n",
      "Test batch 118 with loss 2.0134096145629883.\n",
      "Test batch 119 with loss 2.5566012859344482.\n",
      "Test batch 120 with loss 2.1840717792510986.\n",
      "Test batch 121 with loss 2.050856590270996.\n",
      "Test batch 122 with loss 2.224635362625122.\n",
      "Test batch 123 with loss 1.866421103477478.\n",
      "Test batch 124 with loss 2.278254270553589.\n",
      "Test batch 125 with loss 2.0413413047790527.\n",
      "Test batch 126 with loss 2.3369157314300537.\n",
      "Test batch 127 with loss 2.0706777572631836.\n",
      "Test batch 128 with loss 2.108203649520874.\n",
      "Test batch 129 with loss 1.967421054840088.\n",
      "Test batch 130 with loss 2.2804808616638184.\n",
      "Test batch 131 with loss 2.1030805110931396.\n",
      "Test batch 132 with loss 2.328948974609375.\n",
      "Test batch 133 with loss 1.84406578540802.\n",
      "Test batch 134 with loss 2.1386795043945312.\n",
      "Test batch 135 with loss 2.117567777633667.\n",
      "Test batch 136 with loss 2.3801214694976807.\n",
      "Test batch 137 with loss 2.013878107070923.\n",
      "Test batch 138 with loss 2.421525001525879.\n",
      "Test batch 139 with loss 2.3365352153778076.\n",
      "Test batch 140 with loss 2.0240352153778076.\n",
      "Test batch 141 with loss 2.155496835708618.\n",
      "Test batch 142 with loss 2.1153109073638916.\n",
      "Test batch 143 with loss 2.3293120861053467.\n",
      "Test batch 144 with loss 2.112668514251709.\n",
      "Test batch 145 with loss 2.6943602561950684.\n",
      "Test batch 146 with loss 2.4561209678649902.\n",
      "Test batch 147 with loss 2.4709761142730713.\n",
      "Test batch 148 with loss 2.1587867736816406.\n",
      "Test batch 149 with loss 2.4005730152130127.\n",
      "Test batch 150 with loss 2.2491302490234375.\n",
      "Test batch 151 with loss 2.0686378479003906.\n",
      "Test batch 152 with loss 1.9462958574295044.\n",
      "Test batch 153 with loss 2.2596547603607178.\n",
      "Test batch 154 with loss 2.0831732749938965.\n",
      "Test batch 155 with loss 2.2374274730682373.\n",
      "Test batch 156 with loss 2.2875781059265137.\n",
      "Test batch 157 with loss 2.289128065109253.\n",
      "Test batch 158 with loss 2.0352044105529785.\n",
      "Test batch 159 with loss 2.3460919857025146.\n",
      "Test batch 160 with loss 2.2206268310546875.\n",
      "Test batch 161 with loss 1.8631640672683716.\n",
      "Test batch 162 with loss 2.212002992630005.\n",
      "Test batch 163 with loss 2.336742401123047.\n",
      "Test batch 164 with loss 2.0267817974090576.\n",
      "Test batch 165 with loss 1.9562808275222778.\n",
      "Test batch 166 with loss 2.239842176437378.\n",
      "Test batch 167 with loss 2.115933418273926.\n",
      "Test batch 168 with loss 2.1825602054595947.\n",
      "Test batch 169 with loss 2.2447333335876465.\n",
      "Test batch 170 with loss 2.175171375274658.\n",
      "Test batch 171 with loss 2.2424046993255615.\n",
      "Test batch 172 with loss 2.247542381286621.\n",
      "Test batch 173 with loss 1.971702218055725.\n",
      "Test batch 174 with loss 2.0122013092041016.\n",
      "Test batch 175 with loss 2.1863982677459717.\n",
      "Test batch 176 with loss 2.0472235679626465.\n",
      "Test batch 177 with loss 2.1806790828704834.\n",
      "Test batch 178 with loss 1.8171082735061646.\n",
      "Test batch 179 with loss 2.536661386489868.\n",
      "Test batch 180 with loss 2.4498801231384277.\n",
      "Test batch 181 with loss 2.1526174545288086.\n",
      "Test batch 182 with loss 2.433802366256714.\n",
      "Test batch 183 with loss 2.284271240234375.\n",
      "Test batch 184 with loss 2.3526642322540283.\n",
      "Test batch 185 with loss 2.140676259994507.\n",
      "Test batch 186 with loss 2.251807689666748.\n",
      "Test batch 187 with loss 2.113090753555298.\n",
      "Test batch 188 with loss 2.4735848903656006.\n",
      "Test batch 189 with loss 2.1515302658081055.\n",
      "Test batch 190 with loss 2.119293451309204.\n",
      "Test batch 191 with loss 1.940714716911316.\n",
      "Test batch 192 with loss 2.297525644302368.\n",
      "Test batch 193 with loss 2.329834222793579.\n",
      "Test batch 194 with loss 1.8525655269622803.\n",
      "Test batch 195 with loss 2.1601266860961914.\n",
      "Test batch 196 with loss 2.0381765365600586.\n",
      "Test batch 197 with loss 2.0621559619903564.\n",
      "Test batch 198 with loss 2.1261932849884033.\n",
      "Test batch 199 with loss 2.0909783840179443.\n",
      "Test batch 200 with loss 2.3007805347442627.\n",
      "Test batch 201 with loss 1.879172444343567.\n",
      "Test batch 202 with loss 2.2221436500549316.\n",
      "Test batch 203 with loss 2.096168041229248.\n",
      "Test batch 204 with loss 2.0337512493133545.\n",
      "Test batch 205 with loss 2.1531214714050293.\n",
      "Test batch 206 with loss 2.0161490440368652.\n",
      "Test batch 207 with loss 2.2105517387390137.\n",
      "Test batch 208 with loss 2.285574197769165.\n",
      "Test batch 209 with loss 2.5530083179473877.\n",
      "Test batch 210 with loss 2.175574779510498.\n",
      "Test batch 211 with loss 2.304227113723755.\n",
      "Test batch 212 with loss 2.4913930892944336.\n",
      "Test batch 213 with loss 2.087733268737793.\n",
      "Test batch 214 with loss 2.3433914184570312.\n",
      "Test batch 215 with loss 2.224491834640503.\n",
      "Test batch 216 with loss 2.2595486640930176.\n",
      "Test batch 217 with loss 1.5049419403076172.\n",
      "Test batch 218 with loss 2.17942476272583.\n",
      "Test batch 219 with loss 2.0367345809936523.\n",
      "Test batch 220 with loss 1.7583104372024536.\n",
      "Test batch 221 with loss 2.288743257522583.\n",
      "Test batch 222 with loss 2.299931526184082.\n",
      "Test batch 223 with loss 1.9144539833068848.\n",
      "Test batch 224 with loss 2.15938663482666.\n",
      "Test batch 225 with loss 2.1886656284332275.\n",
      "Test batch 226 with loss 2.0786967277526855.\n",
      "Test batch 227 with loss 2.1518449783325195.\n",
      "Test batch 228 with loss 2.290339469909668.\n",
      "Test batch 229 with loss 2.140564203262329.\n",
      "Test batch 230 with loss 2.1633121967315674.\n",
      "Test batch 231 with loss 1.9039603471755981.\n",
      "Test batch 232 with loss 2.1531479358673096.\n",
      "Test batch 233 with loss 2.166249990463257.\n",
      "Test batch 234 with loss 2.2968499660491943.\n",
      "Test batch 235 with loss 2.205939292907715.\n",
      "Test batch 236 with loss 2.080895185470581.\n",
      "Test batch 237 with loss 2.307373046875.\n",
      "Test batch 238 with loss 2.5220553874969482.\n",
      "Test batch 239 with loss 2.1824233531951904.\n",
      "Test batch 240 with loss 1.9780889749526978.\n",
      "Test batch 241 with loss 2.178452968597412.\n",
      "Test batch 242 with loss 2.046457529067993.\n",
      "Test batch 243 with loss 2.2408969402313232.\n",
      "Test batch 244 with loss 2.217043876647949.\n",
      "Test batch 245 with loss 1.9741817712783813.\n",
      "Test batch 246 with loss 1.9102420806884766.\n",
      "Test batch 247 with loss 2.1987457275390625.\n",
      "Test batch 248 with loss 2.1103906631469727.\n",
      "Test batch 249 with loss 1.901138186454773.\n",
      "Test batch 250 with loss 2.1780753135681152.\n",
      "Test batch 251 with loss 2.058377742767334.\n",
      "Test batch 252 with loss 2.1188101768493652.\n",
      "Test batch 253 with loss 2.060606002807617.\n",
      "Test batch 254 with loss 2.1428675651550293.\n",
      "Test batch 255 with loss 2.2997241020202637.\n",
      "Test batch 256 with loss 2.1452977657318115.\n",
      "Test batch 257 with loss 2.098248243331909.\n",
      "Test batch 258 with loss 2.4379894733428955.\n",
      "Test batch 259 with loss 2.2502987384796143.\n",
      "Test batch 260 with loss 2.2020394802093506.\n",
      "Test batch 261 with loss 2.1060516834259033.\n",
      "Test batch 262 with loss 2.1148288249969482.\n",
      "Test batch 263 with loss 2.4688174724578857.\n",
      "Test batch 264 with loss 2.006666421890259.\n",
      "Test batch 265 with loss 2.2405271530151367.\n",
      "Test batch 266 with loss 2.3349013328552246.\n",
      "Test batch 267 with loss 2.3323888778686523.\n",
      "Test batch 268 with loss 2.3245344161987305.\n",
      "Test batch 269 with loss 2.3448092937469482.\n",
      "Test batch 270 with loss 1.92458176612854.\n",
      "Test batch 271 with loss 2.5560073852539062.\n",
      "Test batch 272 with loss 2.422877311706543.\n",
      "Test batch 273 with loss 2.1466126441955566.\n",
      "Test batch 274 with loss 1.928269624710083.\n",
      "Test batch 275 with loss 2.3327090740203857.\n",
      "Test batch 276 with loss 2.11181902885437.\n",
      "Test batch 277 with loss 2.306241273880005.\n",
      "Test batch 278 with loss 2.1496269702911377.\n",
      "Test batch 279 with loss 2.090104341506958.\n",
      "Test batch 280 with loss 2.1567862033843994.\n",
      "Test batch 281 with loss 2.336331367492676.\n",
      "Test batch 282 with loss 2.422041177749634.\n",
      "Test batch 283 with loss 1.7732889652252197.\n",
      "Test batch 284 with loss 2.4368767738342285.\n",
      "Test batch 285 with loss 2.2483723163604736.\n",
      "Test batch 286 with loss 2.0580527782440186.\n",
      "Test batch 287 with loss 2.3432857990264893.\n",
      "Test batch 288 with loss 2.30692982673645.\n",
      "Test batch 289 with loss 2.4264166355133057.\n",
      "Test batch 290 with loss 2.0327162742614746.\n",
      "Test batch 291 with loss 2.380110502243042.\n",
      "Test batch 292 with loss 2.222480297088623.\n",
      "Test batch 293 with loss 2.3246676921844482.\n",
      "Test batch 294 with loss 2.219871997833252.\n",
      "Test batch 295 with loss 2.1649484634399414.\n",
      "Test batch 296 with loss 2.4039955139160156.\n",
      "Test batch 297 with loss 2.0778391361236572.\n",
      "Test batch 298 with loss 2.1935219764709473.\n",
      "Test batch 299 with loss 2.0978188514709473.\n",
      "Test batch 300 with loss 2.442706346511841.\n",
      "Test batch 301 with loss 2.189216375350952.\n",
      "Test batch 302 with loss 2.2016799449920654.\n",
      "Test batch 303 with loss 2.205838441848755.\n",
      "Test batch 304 with loss 1.8729338645935059.\n",
      "Test batch 305 with loss 1.9355121850967407.\n",
      "Test batch 306 with loss 2.1818161010742188.\n",
      "Test batch 307 with loss 2.4936814308166504.\n",
      "Test batch 308 with loss 2.3678250312805176.\n",
      "Test batch 309 with loss 1.8517556190490723.\n",
      "Test batch 310 with loss 1.8599135875701904.\n",
      "Test batch 311 with loss 2.4352636337280273.\n",
      "Test batch 312 with loss 2.227811098098755.\n",
      "Test batch 313 with loss 1.9421483278274536.\n",
      "Test batch 314 with loss 2.807892322540283.\n",
      "Test batch 315 with loss 2.1534392833709717.\n",
      "Test batch 316 with loss 2.4821035861968994.\n",
      "Test batch 317 with loss 2.3390378952026367.\n",
      "Test batch 318 with loss 2.0408437252044678.\n",
      "Test batch 319 with loss 2.080033302307129.\n",
      "Test batch 320 with loss 2.192927122116089.\n",
      "Test batch 321 with loss 2.2679054737091064.\n",
      "Test batch 322 with loss 1.8800610303878784.\n",
      "Test batch 323 with loss 2.0138771533966064.\n",
      "Test batch 324 with loss 2.280717372894287.\n",
      "Test batch 325 with loss 2.1945838928222656.\n",
      "Test batch 326 with loss 2.047877788543701.\n",
      "Test batch 327 with loss 2.1445858478546143.\n",
      "Test batch 328 with loss 2.335078477859497.\n",
      "Test batch 329 with loss 2.2219786643981934.\n",
      "Test batch 330 with loss 1.9960041046142578.\n",
      "Test batch 331 with loss 2.1593217849731445.\n",
      "Test batch 332 with loss 2.276411771774292.\n",
      "Test batch 333 with loss 1.9866693019866943.\n",
      "Training epoch 1 batch 0 with loss 1.7699261903762817.\n",
      "Training epoch 1 batch 1 with loss 1.5935626029968262.\n",
      "Training epoch 1 batch 2 with loss 1.6886582374572754.\n",
      "Training epoch 1 batch 3 with loss 1.5668529272079468.\n",
      "Training epoch 1 batch 4 with loss 1.5710135698318481.\n",
      "Training epoch 1 batch 5 with loss 1.575909972190857.\n",
      "Training epoch 1 batch 6 with loss 1.6741478443145752.\n",
      "Training epoch 1 batch 7 with loss 1.5597656965255737.\n",
      "Training epoch 1 batch 8 with loss 1.8174761533737183.\n",
      "Training epoch 1 batch 9 with loss 1.8346797227859497.\n",
      "Training epoch 1 batch 10 with loss 1.416806697845459.\n",
      "Training epoch 1 batch 11 with loss 1.6641011238098145.\n",
      "Training epoch 1 batch 12 with loss 1.6890761852264404.\n",
      "Training epoch 1 batch 13 with loss 1.5149857997894287.\n",
      "Training epoch 1 batch 14 with loss 1.6070348024368286.\n",
      "Training epoch 1 batch 15 with loss 1.1865060329437256.\n",
      "Training epoch 1 batch 16 with loss 1.28128182888031.\n",
      "Training epoch 1 batch 17 with loss 1.7230108976364136.\n",
      "Training epoch 1 batch 18 with loss 1.6194041967391968.\n",
      "Training epoch 1 batch 19 with loss 1.6244794130325317.\n",
      "Training epoch 1 batch 20 with loss 1.7391955852508545.\n",
      "Training epoch 1 batch 21 with loss 1.7587989568710327.\n",
      "Training epoch 1 batch 22 with loss 1.5377260446548462.\n",
      "Training epoch 1 batch 23 with loss 1.8760300874710083.\n",
      "Training epoch 1 batch 24 with loss 1.4475260972976685.\n",
      "Training epoch 1 batch 25 with loss 1.416656255722046.\n",
      "Training epoch 1 batch 26 with loss 1.7290714979171753.\n",
      "Training epoch 1 batch 27 with loss 1.622834324836731.\n",
      "Training epoch 1 batch 28 with loss 1.3671811819076538.\n",
      "Training epoch 1 batch 29 with loss 1.6720432043075562.\n",
      "Training epoch 1 batch 30 with loss 1.6016840934753418.\n",
      "Training epoch 1 batch 31 with loss 1.466247797012329.\n",
      "Training epoch 1 batch 32 with loss 1.3443819284439087.\n",
      "Training epoch 1 batch 33 with loss 1.8118656873703003.\n",
      "Training epoch 1 batch 34 with loss 1.7018611431121826.\n",
      "Training epoch 1 batch 35 with loss 1.4515241384506226.\n",
      "Training epoch 1 batch 36 with loss 1.549360990524292.\n",
      "Training epoch 1 batch 37 with loss 1.5878287553787231.\n",
      "Training epoch 1 batch 38 with loss 1.8241615295410156.\n",
      "Training epoch 1 batch 39 with loss 1.708345651626587.\n",
      "Training epoch 1 batch 40 with loss 1.7701785564422607.\n",
      "Training epoch 1 batch 41 with loss 1.520298719406128.\n",
      "Training epoch 1 batch 42 with loss 1.9702702760696411.\n",
      "Training epoch 1 batch 43 with loss 1.750000238418579.\n",
      "Training epoch 1 batch 44 with loss 1.6753580570220947.\n",
      "Training epoch 1 batch 45 with loss 1.618688702583313.\n",
      "Training epoch 1 batch 46 with loss 1.6578428745269775.\n",
      "Training epoch 1 batch 47 with loss 1.6228543519973755.\n",
      "Training epoch 1 batch 48 with loss 1.9790114164352417.\n",
      "Training epoch 1 batch 49 with loss 1.5703694820404053.\n",
      "Training epoch 1 batch 50 with loss 1.841360330581665.\n",
      "Training epoch 1 batch 51 with loss 1.5968570709228516.\n",
      "Training epoch 1 batch 52 with loss 1.6802036762237549.\n",
      "Training epoch 1 batch 53 with loss 1.6618428230285645.\n",
      "Training epoch 1 batch 54 with loss 1.5628360509872437.\n",
      "Training epoch 1 batch 55 with loss 1.8511197566986084.\n",
      "Training epoch 1 batch 56 with loss 1.6230095624923706.\n",
      "Training epoch 1 batch 57 with loss 1.629314661026001.\n",
      "Training epoch 1 batch 58 with loss 1.7689895629882812.\n",
      "Training epoch 1 batch 59 with loss 1.6733702421188354.\n",
      "Training epoch 1 batch 60 with loss 1.6223605871200562.\n",
      "Training epoch 1 batch 61 with loss 1.9278777837753296.\n",
      "Training epoch 1 batch 62 with loss 1.8479409217834473.\n",
      "Training epoch 1 batch 63 with loss 1.6563440561294556.\n",
      "Training epoch 1 batch 64 with loss 2.0082013607025146.\n",
      "Training epoch 1 batch 65 with loss 1.7147102355957031.\n",
      "Training epoch 1 batch 66 with loss 1.4980757236480713.\n",
      "Training epoch 1 batch 67 with loss 1.6013790369033813.\n",
      "Training epoch 1 batch 68 with loss 1.8456966876983643.\n",
      "Training epoch 1 batch 69 with loss 1.974717378616333.\n",
      "Training epoch 1 batch 70 with loss 1.7576900720596313.\n",
      "Training epoch 1 batch 71 with loss 1.8613804578781128.\n",
      "Training epoch 1 batch 72 with loss 1.9108132123947144.\n",
      "Training epoch 1 batch 73 with loss 1.5886805057525635.\n",
      "Training epoch 1 batch 74 with loss 1.3622814416885376.\n",
      "Training epoch 1 batch 75 with loss 1.6418025493621826.\n",
      "Training epoch 1 batch 76 with loss 1.8946385383605957.\n",
      "Training epoch 1 batch 77 with loss 1.5018550157546997.\n",
      "Training epoch 1 batch 78 with loss 1.5963338613510132.\n",
      "Training epoch 1 batch 79 with loss 2.007357358932495.\n",
      "Training epoch 1 batch 80 with loss 1.7306244373321533.\n",
      "Training epoch 1 batch 81 with loss 1.761281967163086.\n",
      "Training epoch 1 batch 82 with loss 1.6656157970428467.\n",
      "Training epoch 1 batch 83 with loss 1.891548991203308.\n",
      "Training epoch 1 batch 84 with loss 1.8457019329071045.\n",
      "Training epoch 1 batch 85 with loss 1.6840020418167114.\n",
      "Training epoch 1 batch 86 with loss 1.6595990657806396.\n",
      "Training epoch 1 batch 87 with loss 1.933604121208191.\n",
      "Training epoch 1 batch 88 with loss 1.5664130449295044.\n",
      "Training epoch 1 batch 89 with loss 1.6937968730926514.\n",
      "Training epoch 1 batch 90 with loss 1.4913609027862549.\n",
      "Training epoch 1 batch 91 with loss 1.782356858253479.\n",
      "Training epoch 1 batch 92 with loss 1.6461520195007324.\n",
      "Training epoch 1 batch 93 with loss 1.6448919773101807.\n",
      "Training epoch 1 batch 94 with loss 1.6215802431106567.\n",
      "Training epoch 1 batch 95 with loss 1.7480251789093018.\n",
      "Training epoch 1 batch 96 with loss 1.8584392070770264.\n",
      "Training epoch 1 batch 97 with loss 1.5933420658111572.\n",
      "Training epoch 1 batch 98 with loss 1.8077794313430786.\n",
      "Training epoch 1 batch 99 with loss 1.4788224697113037.\n",
      "Training epoch 1 batch 100 with loss 1.8282020092010498.\n",
      "Training epoch 1 batch 101 with loss 1.7770253419876099.\n",
      "Training epoch 1 batch 102 with loss 1.6668894290924072.\n",
      "Training epoch 1 batch 103 with loss 1.9652653932571411.\n",
      "Training epoch 1 batch 104 with loss 1.7975965738296509.\n",
      "Training epoch 1 batch 105 with loss 1.9680908918380737.\n",
      "Training epoch 1 batch 106 with loss 1.6179240942001343.\n",
      "Training epoch 1 batch 107 with loss 1.7628140449523926.\n",
      "Training epoch 1 batch 108 with loss 1.5684318542480469.\n",
      "Training epoch 1 batch 109 with loss 1.8536555767059326.\n",
      "Training epoch 1 batch 110 with loss 1.161205530166626.\n",
      "Training epoch 1 batch 111 with loss 1.703106164932251.\n",
      "Training epoch 1 batch 112 with loss 1.9749664068222046.\n",
      "Training epoch 1 batch 113 with loss 1.6168619394302368.\n",
      "Training epoch 1 batch 114 with loss 1.7788571119308472.\n",
      "Training epoch 1 batch 115 with loss 1.938463807106018.\n",
      "Training epoch 1 batch 116 with loss 1.8438513278961182.\n",
      "Training epoch 1 batch 117 with loss 1.8702692985534668.\n",
      "Training epoch 1 batch 118 with loss 1.6530723571777344.\n",
      "Training epoch 1 batch 119 with loss 2.0387017726898193.\n",
      "Training epoch 1 batch 120 with loss 1.712231159210205.\n",
      "Training epoch 1 batch 121 with loss 1.8992424011230469.\n",
      "Training epoch 1 batch 122 with loss 1.939433217048645.\n",
      "Training epoch 1 batch 123 with loss 1.7460235357284546.\n",
      "Training epoch 1 batch 124 with loss 1.5061087608337402.\n",
      "Training epoch 1 batch 125 with loss 1.7109732627868652.\n",
      "Training epoch 1 batch 126 with loss 2.0026583671569824.\n",
      "Training epoch 1 batch 127 with loss 1.8658331632614136.\n",
      "Training epoch 1 batch 128 with loss 1.9274786710739136.\n",
      "Training epoch 1 batch 129 with loss 1.824747085571289.\n",
      "Training epoch 1 batch 130 with loss 1.7323111295700073.\n",
      "Training epoch 1 batch 131 with loss 2.0023458003997803.\n",
      "Training epoch 1 batch 132 with loss 1.7102285623550415.\n",
      "Training epoch 1 batch 133 with loss 1.4703623056411743.\n",
      "Training epoch 1 batch 134 with loss 1.8114209175109863.\n",
      "Training epoch 1 batch 135 with loss 1.9956718683242798.\n",
      "Training epoch 1 batch 136 with loss 1.3881293535232544.\n",
      "Training epoch 1 batch 137 with loss 1.6284065246582031.\n",
      "Training epoch 1 batch 138 with loss 1.9052342176437378.\n",
      "Training epoch 1 batch 139 with loss 1.6871751546859741.\n",
      "Training epoch 1 batch 140 with loss 1.6461949348449707.\n",
      "Training epoch 1 batch 141 with loss 1.9658395051956177.\n",
      "Training epoch 1 batch 142 with loss 1.9687285423278809.\n",
      "Training epoch 1 batch 143 with loss 2.162543773651123.\n",
      "Training epoch 1 batch 144 with loss 1.4097570180892944.\n",
      "Training epoch 1 batch 145 with loss 1.901342511177063.\n",
      "Training epoch 1 batch 146 with loss 1.968529462814331.\n",
      "Training epoch 1 batch 147 with loss 2.2327253818511963.\n",
      "Training epoch 1 batch 148 with loss 1.8262892961502075.\n",
      "Training epoch 1 batch 149 with loss 1.5502779483795166.\n",
      "Training epoch 1 batch 150 with loss 2.0214407444000244.\n",
      "Training epoch 1 batch 151 with loss 1.7810300588607788.\n",
      "Training epoch 1 batch 152 with loss 1.741540551185608.\n",
      "Training epoch 1 batch 153 with loss 2.121431589126587.\n",
      "Training epoch 1 batch 154 with loss 1.8751391172409058.\n",
      "Training epoch 1 batch 155 with loss 1.8963030576705933.\n",
      "Training epoch 1 batch 156 with loss 1.560210108757019.\n",
      "Training epoch 1 batch 157 with loss 1.6779040098190308.\n",
      "Training epoch 1 batch 158 with loss 1.9459571838378906.\n",
      "Training epoch 1 batch 159 with loss 1.789864420890808.\n",
      "Training epoch 1 batch 160 with loss 2.038400411605835.\n",
      "Training epoch 1 batch 161 with loss 1.6794676780700684.\n",
      "Training epoch 1 batch 162 with loss 1.8372855186462402.\n",
      "Training epoch 1 batch 163 with loss 1.3905264139175415.\n",
      "Training epoch 1 batch 164 with loss 1.7082992792129517.\n",
      "Training epoch 1 batch 165 with loss 1.67658531665802.\n",
      "Training epoch 1 batch 166 with loss 1.9443233013153076.\n",
      "Training epoch 1 batch 167 with loss 1.8365212678909302.\n",
      "Training epoch 1 batch 168 with loss 1.6131720542907715.\n",
      "Training epoch 1 batch 169 with loss 1.8337175846099854.\n",
      "Training epoch 1 batch 170 with loss 1.8577626943588257.\n",
      "Training epoch 1 batch 171 with loss 1.7305099964141846.\n",
      "Training epoch 1 batch 172 with loss 1.6022945642471313.\n",
      "Training epoch 1 batch 173 with loss 1.962677001953125.\n",
      "Training epoch 1 batch 174 with loss 1.9960116147994995.\n",
      "Training epoch 1 batch 175 with loss 2.026494264602661.\n",
      "Training epoch 1 batch 176 with loss 1.926002025604248.\n",
      "Training epoch 1 batch 177 with loss 1.71552574634552.\n",
      "Training epoch 1 batch 178 with loss 1.6289067268371582.\n",
      "Training epoch 1 batch 179 with loss 1.8781051635742188.\n",
      "Training epoch 1 batch 180 with loss 1.9238885641098022.\n",
      "Training epoch 1 batch 181 with loss 1.7412643432617188.\n",
      "Training epoch 1 batch 182 with loss 1.647676944732666.\n",
      "Training epoch 1 batch 183 with loss 1.9785889387130737.\n",
      "Training epoch 1 batch 184 with loss 1.5639081001281738.\n",
      "Training epoch 1 batch 185 with loss 1.8780349493026733.\n",
      "Training epoch 1 batch 186 with loss 1.8601692914962769.\n",
      "Training epoch 1 batch 187 with loss 1.9060324430465698.\n",
      "Training epoch 1 batch 188 with loss 2.114839553833008.\n",
      "Training epoch 1 batch 189 with loss 2.0181729793548584.\n",
      "Training epoch 1 batch 190 with loss 1.6211975812911987.\n",
      "Training epoch 1 batch 191 with loss 1.8582402467727661.\n",
      "Training epoch 1 batch 192 with loss 1.8755898475646973.\n",
      "Training epoch 1 batch 193 with loss 1.7960916757583618.\n",
      "Training epoch 1 batch 194 with loss 2.3266794681549072.\n",
      "Training epoch 1 batch 195 with loss 1.8247205018997192.\n",
      "Training epoch 1 batch 196 with loss 1.7003649473190308.\n",
      "Training epoch 1 batch 197 with loss 1.7348583936691284.\n",
      "Training epoch 1 batch 198 with loss 1.830533742904663.\n",
      "Training epoch 1 batch 199 with loss 1.6326658725738525.\n",
      "Training epoch 1 batch 200 with loss 1.762510895729065.\n",
      "Training epoch 1 batch 201 with loss 2.090095043182373.\n",
      "Training epoch 1 batch 202 with loss 1.5913982391357422.\n",
      "Training epoch 1 batch 203 with loss 2.1902377605438232.\n",
      "Training epoch 1 batch 204 with loss 1.782087802886963.\n",
      "Training epoch 1 batch 205 with loss 1.720589518547058.\n",
      "Training epoch 1 batch 206 with loss 1.5546131134033203.\n",
      "Training epoch 1 batch 207 with loss 1.6148136854171753.\n",
      "Training epoch 1 batch 208 with loss 1.8675401210784912.\n",
      "Training epoch 1 batch 209 with loss 1.805908203125.\n",
      "Training epoch 1 batch 210 with loss 1.7840673923492432.\n",
      "Training epoch 1 batch 211 with loss 2.0444676876068115.\n",
      "Training epoch 1 batch 212 with loss 1.8542107343673706.\n",
      "Training epoch 1 batch 213 with loss 2.152989625930786.\n",
      "Training epoch 1 batch 214 with loss 1.653810977935791.\n",
      "Training epoch 1 batch 215 with loss 2.0362305641174316.\n",
      "Training epoch 1 batch 216 with loss 1.9335891008377075.\n",
      "Training epoch 1 batch 217 with loss 1.858239769935608.\n",
      "Training epoch 1 batch 218 with loss 1.8999072313308716.\n",
      "Training epoch 1 batch 219 with loss 1.8582990169525146.\n",
      "Training epoch 1 batch 220 with loss 2.0483434200286865.\n",
      "Training epoch 1 batch 221 with loss 2.1148312091827393.\n",
      "Training epoch 1 batch 222 with loss 1.5780760049819946.\n",
      "Training epoch 1 batch 223 with loss 1.43903386592865.\n",
      "Training epoch 1 batch 224 with loss 1.7931782007217407.\n",
      "Training epoch 1 batch 225 with loss 2.0744073390960693.\n",
      "Training epoch 1 batch 226 with loss 2.0633761882781982.\n",
      "Training epoch 1 batch 227 with loss 1.790569543838501.\n",
      "Training epoch 1 batch 228 with loss 1.9441245794296265.\n",
      "Training epoch 1 batch 229 with loss 1.8412140607833862.\n",
      "Training epoch 1 batch 230 with loss 2.099548101425171.\n",
      "Training epoch 1 batch 231 with loss 2.1447081565856934.\n",
      "Training epoch 1 batch 232 with loss 1.6651804447174072.\n",
      "Training epoch 1 batch 233 with loss 1.9421465396881104.\n",
      "Training epoch 1 batch 234 with loss 1.8001185655593872.\n",
      "Training epoch 1 batch 235 with loss 1.910496473312378.\n",
      "Training epoch 1 batch 236 with loss 1.7622781991958618.\n",
      "Training epoch 1 batch 237 with loss 1.8451884984970093.\n",
      "Training epoch 1 batch 238 with loss 1.8432505130767822.\n",
      "Training epoch 1 batch 239 with loss 1.9510422945022583.\n",
      "Training epoch 1 batch 240 with loss 2.0013091564178467.\n",
      "Training epoch 1 batch 241 with loss 1.6215707063674927.\n",
      "Training epoch 1 batch 242 with loss 1.7639234066009521.\n",
      "Training epoch 1 batch 243 with loss 1.6994612216949463.\n",
      "Training epoch 1 batch 244 with loss 1.8140815496444702.\n",
      "Training epoch 1 batch 245 with loss 1.7834471464157104.\n",
      "Training epoch 1 batch 246 with loss 1.6650314331054688.\n",
      "Training epoch 1 batch 247 with loss 1.604323148727417.\n",
      "Training epoch 1 batch 248 with loss 1.830806016921997.\n",
      "Training epoch 1 batch 249 with loss 2.2337114810943604.\n",
      "Training epoch 1 batch 250 with loss 2.16727614402771.\n",
      "Training epoch 1 batch 251 with loss 1.5901750326156616.\n",
      "Training epoch 1 batch 252 with loss 1.7188663482666016.\n",
      "Training epoch 1 batch 253 with loss 2.0633318424224854.\n",
      "Training epoch 1 batch 254 with loss 1.8341004848480225.\n",
      "Training epoch 1 batch 255 with loss 2.0326967239379883.\n",
      "Training epoch 1 batch 256 with loss 1.8339203596115112.\n",
      "Training epoch 1 batch 257 with loss 2.106671094894409.\n",
      "Training epoch 1 batch 258 with loss 1.8881855010986328.\n",
      "Training epoch 1 batch 259 with loss 1.897920846939087.\n",
      "Training epoch 1 batch 260 with loss 2.026125907897949.\n",
      "Training epoch 1 batch 261 with loss 1.7677017450332642.\n",
      "Training epoch 1 batch 262 with loss 1.5660374164581299.\n",
      "Training epoch 1 batch 263 with loss 2.123666763305664.\n",
      "Training epoch 1 batch 264 with loss 1.9284183979034424.\n",
      "Training epoch 1 batch 265 with loss 2.0791659355163574.\n",
      "Training epoch 1 batch 266 with loss 1.5644030570983887.\n",
      "Training epoch 1 batch 267 with loss 1.5942388772964478.\n",
      "Training epoch 1 batch 268 with loss 2.2468414306640625.\n",
      "Training epoch 1 batch 269 with loss 1.7877169847488403.\n",
      "Training epoch 1 batch 270 with loss 1.9267603158950806.\n",
      "Training epoch 1 batch 271 with loss 2.546231508255005.\n",
      "Training epoch 1 batch 272 with loss 1.775984287261963.\n",
      "Training epoch 1 batch 273 with loss 2.2365195751190186.\n",
      "Training epoch 1 batch 274 with loss 2.2282731533050537.\n",
      "Training epoch 1 batch 275 with loss 2.1350367069244385.\n",
      "Training epoch 1 batch 276 with loss 2.106839179992676.\n",
      "Training epoch 1 batch 277 with loss 1.7995556592941284.\n",
      "Training epoch 1 batch 278 with loss 1.9869614839553833.\n",
      "Training epoch 1 batch 279 with loss 1.7143670320510864.\n",
      "Training epoch 1 batch 280 with loss 1.7365461587905884.\n",
      "Training epoch 1 batch 281 with loss 1.7729445695877075.\n",
      "Training epoch 1 batch 282 with loss 1.834241509437561.\n",
      "Training epoch 1 batch 283 with loss 2.01657772064209.\n",
      "Training epoch 1 batch 284 with loss 1.6702920198440552.\n",
      "Training epoch 1 batch 285 with loss 1.9694175720214844.\n",
      "Training epoch 1 batch 286 with loss 2.138159990310669.\n",
      "Training epoch 1 batch 287 with loss 2.070786237716675.\n",
      "Training epoch 1 batch 288 with loss 1.6592440605163574.\n",
      "Training epoch 1 batch 289 with loss 1.859328031539917.\n",
      "Training epoch 1 batch 290 with loss 1.9114586114883423.\n",
      "Training epoch 1 batch 291 with loss 2.2804224491119385.\n",
      "Training epoch 1 batch 292 with loss 2.080533504486084.\n",
      "Training epoch 1 batch 293 with loss 1.6483724117279053.\n",
      "Training epoch 1 batch 294 with loss 2.12105655670166.\n",
      "Training epoch 1 batch 295 with loss 1.8532394170761108.\n",
      "Training epoch 1 batch 296 with loss 1.9494991302490234.\n",
      "Training epoch 1 batch 297 with loss 1.9597570896148682.\n",
      "Training epoch 1 batch 298 with loss 1.9184606075286865.\n",
      "Training epoch 1 batch 299 with loss 1.8531540632247925.\n",
      "Training epoch 1 batch 300 with loss 2.2038683891296387.\n",
      "Training epoch 1 batch 301 with loss 2.1517913341522217.\n",
      "Training epoch 1 batch 302 with loss 1.5154550075531006.\n",
      "Training epoch 1 batch 303 with loss 1.9524489641189575.\n",
      "Training epoch 1 batch 304 with loss 2.078489303588867.\n",
      "Training epoch 1 batch 305 with loss 1.952391505241394.\n",
      "Training epoch 1 batch 306 with loss 2.102494239807129.\n",
      "Training epoch 1 batch 307 with loss 1.9985121488571167.\n",
      "Training epoch 1 batch 308 with loss 1.8725831508636475.\n",
      "Training epoch 1 batch 309 with loss 1.9501080513000488.\n",
      "Training epoch 1 batch 310 with loss 2.263812303543091.\n",
      "Training epoch 1 batch 311 with loss 2.030287981033325.\n",
      "Training epoch 1 batch 312 with loss 1.8958457708358765.\n",
      "Training epoch 1 batch 313 with loss 1.7184098958969116.\n",
      "Training epoch 1 batch 314 with loss 1.938360571861267.\n",
      "Training epoch 1 batch 315 with loss 1.8636553287506104.\n",
      "Training epoch 1 batch 316 with loss 2.0719008445739746.\n",
      "Training epoch 1 batch 317 with loss 1.9363938570022583.\n",
      "Training epoch 1 batch 318 with loss 1.676154375076294.\n",
      "Training epoch 1 batch 319 with loss 1.8842443227767944.\n",
      "Training epoch 1 batch 320 with loss 1.8730239868164062.\n",
      "Training epoch 1 batch 321 with loss 1.820138931274414.\n",
      "Training epoch 1 batch 322 with loss 1.9458177089691162.\n",
      "Training epoch 1 batch 323 with loss 1.7713719606399536.\n",
      "Training epoch 1 batch 324 with loss 1.734161376953125.\n",
      "Training epoch 1 batch 325 with loss 1.8305718898773193.\n",
      "Training epoch 1 batch 326 with loss 1.8703010082244873.\n",
      "Training epoch 1 batch 327 with loss 1.6279854774475098.\n",
      "Training epoch 1 batch 328 with loss 2.221478223800659.\n",
      "Training epoch 1 batch 329 with loss 2.0407755374908447.\n",
      "Training epoch 1 batch 330 with loss 2.2741940021514893.\n",
      "Training epoch 1 batch 331 with loss 1.739956021308899.\n",
      "Training epoch 1 batch 332 with loss 1.8610107898712158.\n",
      "Training epoch 1 batch 333 with loss 1.9633715152740479.\n",
      "Training epoch 1 batch 334 with loss 1.7705786228179932.\n",
      "Training epoch 1 batch 335 with loss 1.8356775045394897.\n",
      "Training epoch 1 batch 336 with loss 1.943910837173462.\n",
      "Training epoch 1 batch 337 with loss 2.4342503547668457.\n",
      "Training epoch 1 batch 338 with loss 2.1958398818969727.\n",
      "Training epoch 1 batch 339 with loss 1.8925797939300537.\n",
      "Training epoch 1 batch 340 with loss 2.1029975414276123.\n",
      "Training epoch 1 batch 341 with loss 1.9879399538040161.\n",
      "Training epoch 1 batch 342 with loss 2.0223748683929443.\n",
      "Training epoch 1 batch 343 with loss 2.016857624053955.\n",
      "Training epoch 1 batch 344 with loss 1.8219174146652222.\n",
      "Training epoch 1 batch 345 with loss 2.115757465362549.\n",
      "Training epoch 1 batch 346 with loss 2.328723907470703.\n",
      "Training epoch 1 batch 347 with loss 1.9007222652435303.\n",
      "Training epoch 1 batch 348 with loss 2.0577034950256348.\n",
      "Training epoch 1 batch 349 with loss 2.219691514968872.\n",
      "Training epoch 1 batch 350 with loss 1.7537243366241455.\n",
      "Training epoch 1 batch 351 with loss 1.7565609216690063.\n",
      "Training epoch 1 batch 352 with loss 2.0235230922698975.\n",
      "Training epoch 1 batch 353 with loss 2.2372701168060303.\n",
      "Training epoch 1 batch 354 with loss 1.7998336553573608.\n",
      "Training epoch 1 batch 355 with loss 2.102710247039795.\n",
      "Training epoch 1 batch 356 with loss 1.8855385780334473.\n",
      "Training epoch 1 batch 357 with loss 2.305171251296997.\n",
      "Training epoch 1 batch 358 with loss 1.9110093116760254.\n",
      "Training epoch 1 batch 359 with loss 1.813314437866211.\n",
      "Training epoch 1 batch 360 with loss 1.8241078853607178.\n",
      "Training epoch 1 batch 361 with loss 2.094707727432251.\n",
      "Training epoch 1 batch 362 with loss 1.7659069299697876.\n",
      "Training epoch 1 batch 363 with loss 1.8236966133117676.\n",
      "Training epoch 1 batch 364 with loss 2.0457026958465576.\n",
      "Training epoch 1 batch 365 with loss 1.7383272647857666.\n",
      "Training epoch 1 batch 366 with loss 1.9462990760803223.\n",
      "Training epoch 1 batch 367 with loss 2.1317949295043945.\n",
      "Training epoch 1 batch 368 with loss 2.098881959915161.\n",
      "Training epoch 1 batch 369 with loss 1.814582347869873.\n",
      "Training epoch 1 batch 370 with loss 1.9956201314926147.\n",
      "Training epoch 1 batch 371 with loss 2.132864236831665.\n",
      "Training epoch 1 batch 372 with loss 2.040311336517334.\n",
      "Training epoch 1 batch 373 with loss 2.054898500442505.\n",
      "Training epoch 1 batch 374 with loss 1.8131579160690308.\n",
      "Training epoch 1 batch 375 with loss 2.0848734378814697.\n",
      "Training epoch 1 batch 376 with loss 1.8427598476409912.\n",
      "Training epoch 1 batch 377 with loss 2.097778558731079.\n",
      "Training epoch 1 batch 378 with loss 1.874306321144104.\n",
      "Training epoch 1 batch 379 with loss 2.0312869548797607.\n",
      "Training epoch 1 batch 380 with loss 1.9972858428955078.\n",
      "Training epoch 1 batch 381 with loss 1.7100342512130737.\n",
      "Training epoch 1 batch 382 with loss 1.9793411493301392.\n",
      "Training epoch 1 batch 383 with loss 2.448197841644287.\n",
      "Training epoch 1 batch 384 with loss 1.6061522960662842.\n",
      "Training epoch 1 batch 385 with loss 1.892277479171753.\n",
      "Training epoch 1 batch 386 with loss 2.0330190658569336.\n",
      "Training epoch 1 batch 387 with loss 1.8395922183990479.\n",
      "Training epoch 1 batch 388 with loss 2.0774266719818115.\n",
      "Training epoch 1 batch 389 with loss 2.02954363822937.\n",
      "Training epoch 1 batch 390 with loss 1.955223798751831.\n",
      "Training epoch 1 batch 391 with loss 2.0107131004333496.\n",
      "Training epoch 1 batch 392 with loss 1.8609869480133057.\n",
      "Training epoch 1 batch 393 with loss 1.843961238861084.\n",
      "Training epoch 1 batch 394 with loss 1.9563344717025757.\n",
      "Training epoch 1 batch 395 with loss 1.9227813482284546.\n",
      "Training epoch 1 batch 396 with loss 2.223991632461548.\n",
      "Training epoch 1 batch 397 with loss 2.1785149574279785.\n",
      "Training epoch 1 batch 398 with loss 1.9847240447998047.\n",
      "Training epoch 1 batch 399 with loss 2.0543935298919678.\n",
      "Training epoch 1 batch 400 with loss 1.7343716621398926.\n",
      "Training epoch 1 batch 401 with loss 2.1677398681640625.\n",
      "Training epoch 1 batch 402 with loss 1.8497521877288818.\n",
      "Training epoch 1 batch 403 with loss 1.5357903242111206.\n",
      "Training epoch 1 batch 404 with loss 1.810058832168579.\n",
      "Training epoch 1 batch 405 with loss 2.178478956222534.\n",
      "Training epoch 1 batch 406 with loss 1.8688724040985107.\n",
      "Training epoch 1 batch 407 with loss 1.971130132675171.\n",
      "Training epoch 1 batch 408 with loss 1.7839200496673584.\n",
      "Training epoch 1 batch 409 with loss 2.0204408168792725.\n",
      "Training epoch 1 batch 410 with loss 1.9933371543884277.\n",
      "Training epoch 1 batch 411 with loss 1.7276554107666016.\n",
      "Training epoch 1 batch 412 with loss 1.8595545291900635.\n",
      "Training epoch 1 batch 413 with loss 1.7803733348846436.\n",
      "Training epoch 1 batch 414 with loss 1.9663714170455933.\n",
      "Training epoch 1 batch 415 with loss 1.7216838598251343.\n",
      "Training epoch 1 batch 416 with loss 1.8845199346542358.\n",
      "Training epoch 1 batch 417 with loss 1.9962924718856812.\n",
      "Training epoch 1 batch 418 with loss 2.2410037517547607.\n",
      "Training epoch 1 batch 419 with loss 2.0437674522399902.\n",
      "Training epoch 1 batch 420 with loss 2.116889238357544.\n",
      "Training epoch 1 batch 421 with loss 1.9409154653549194.\n",
      "Training epoch 1 batch 422 with loss 2.442025661468506.\n",
      "Training epoch 1 batch 423 with loss 1.7290412187576294.\n",
      "Training epoch 1 batch 424 with loss 1.9454680681228638.\n",
      "Training epoch 1 batch 425 with loss 1.8455160856246948.\n",
      "Training epoch 1 batch 426 with loss 1.935662865638733.\n",
      "Training epoch 1 batch 427 with loss 1.9929293394088745.\n",
      "Training epoch 1 batch 428 with loss 1.9825626611709595.\n",
      "Training epoch 1 batch 429 with loss 2.051509380340576.\n",
      "Training epoch 1 batch 430 with loss 2.0347487926483154.\n",
      "Training epoch 1 batch 431 with loss 1.9289507865905762.\n",
      "Training epoch 1 batch 432 with loss 2.3466925621032715.\n",
      "Training epoch 1 batch 433 with loss 2.21220326423645.\n",
      "Training epoch 1 batch 434 with loss 1.9927903413772583.\n",
      "Training epoch 1 batch 435 with loss 1.8235268592834473.\n",
      "Training epoch 1 batch 436 with loss 2.0460550785064697.\n",
      "Training epoch 1 batch 437 with loss 1.9177582263946533.\n",
      "Training epoch 1 batch 438 with loss 1.9733502864837646.\n",
      "Training epoch 1 batch 439 with loss 2.0013527870178223.\n",
      "Training epoch 1 batch 440 with loss 1.9902610778808594.\n",
      "Training epoch 1 batch 441 with loss 2.2440810203552246.\n",
      "Training epoch 1 batch 442 with loss 2.0986227989196777.\n",
      "Training epoch 1 batch 443 with loss 2.1383590698242188.\n",
      "Training epoch 1 batch 444 with loss 1.9915019273757935.\n",
      "Training epoch 1 batch 445 with loss 2.039135694503784.\n",
      "Training epoch 1 batch 446 with loss 2.4502601623535156.\n",
      "Training epoch 1 batch 447 with loss 1.9297525882720947.\n",
      "Training epoch 1 batch 448 with loss 2.119596481323242.\n",
      "Training epoch 1 batch 449 with loss 1.9755892753601074.\n",
      "Training epoch 1 batch 450 with loss 1.472374439239502.\n",
      "Training epoch 1 batch 451 with loss 1.9054036140441895.\n",
      "Training epoch 1 batch 452 with loss 1.9018288850784302.\n",
      "Training epoch 1 batch 453 with loss 2.232111930847168.\n",
      "Training epoch 1 batch 454 with loss 1.6500164270401.\n",
      "Training epoch 1 batch 455 with loss 2.153177499771118.\n",
      "Training epoch 1 batch 456 with loss 2.2319376468658447.\n",
      "Training epoch 1 batch 457 with loss 2.115758180618286.\n",
      "Training epoch 1 batch 458 with loss 1.8958017826080322.\n",
      "Training epoch 1 batch 459 with loss 1.824963927268982.\n",
      "Training epoch 1 batch 460 with loss 1.8252087831497192.\n",
      "Training epoch 1 batch 461 with loss 1.7595614194869995.\n",
      "Training epoch 1 batch 462 with loss 1.516592264175415.\n",
      "Training epoch 1 batch 463 with loss 2.0798325538635254.\n",
      "Training epoch 1 batch 464 with loss 1.9926687479019165.\n",
      "Training epoch 1 batch 465 with loss 2.0124008655548096.\n",
      "Training epoch 1 batch 466 with loss 2.2243528366088867.\n",
      "Training epoch 1 batch 467 with loss 2.218977928161621.\n",
      "Training epoch 1 batch 468 with loss 1.7770575284957886.\n",
      "Training epoch 1 batch 469 with loss 1.714091420173645.\n",
      "Training epoch 1 batch 470 with loss 1.7926690578460693.\n",
      "Training epoch 1 batch 471 with loss 2.1780388355255127.\n",
      "Training epoch 1 batch 472 with loss 2.053676128387451.\n",
      "Training epoch 1 batch 473 with loss 2.1471846103668213.\n",
      "Training epoch 1 batch 474 with loss 1.4792430400848389.\n",
      "Training epoch 1 batch 475 with loss 2.2189836502075195.\n",
      "Training epoch 1 batch 476 with loss 2.1010360717773438.\n",
      "Training epoch 1 batch 477 with loss 1.9668246507644653.\n",
      "Training epoch 1 batch 478 with loss 1.628819227218628.\n",
      "Training epoch 1 batch 479 with loss 1.654151201248169.\n",
      "Training epoch 1 batch 480 with loss 2.464430570602417.\n",
      "Training epoch 1 batch 481 with loss 2.1888163089752197.\n",
      "Training epoch 1 batch 482 with loss 2.0393011569976807.\n",
      "Training epoch 1 batch 483 with loss 2.2071051597595215.\n",
      "Training epoch 1 batch 484 with loss 1.967283010482788.\n",
      "Training epoch 1 batch 485 with loss 2.034963607788086.\n",
      "Training epoch 1 batch 486 with loss 1.8308430910110474.\n",
      "Training epoch 1 batch 487 with loss 1.8608653545379639.\n",
      "Training epoch 1 batch 488 with loss 1.8831676244735718.\n",
      "Training epoch 1 batch 489 with loss 2.1448237895965576.\n",
      "Training epoch 1 batch 490 with loss 1.7189940214157104.\n",
      "Training epoch 1 batch 491 with loss 2.1543610095977783.\n",
      "Training epoch 1 batch 492 with loss 1.9461978673934937.\n",
      "Training epoch 1 batch 493 with loss 2.2833590507507324.\n",
      "Training epoch 1 batch 494 with loss 2.302121639251709.\n",
      "Training epoch 1 batch 495 with loss 2.23919677734375.\n",
      "Training epoch 1 batch 496 with loss 2.1359646320343018.\n",
      "Training epoch 1 batch 497 with loss 2.2020654678344727.\n",
      "Training epoch 1 batch 498 with loss 2.476383924484253.\n",
      "Training epoch 1 batch 499 with loss 2.407305955886841.\n",
      "Training epoch 1 batch 500 with loss 1.8079174757003784.\n",
      "Training epoch 1 batch 501 with loss 1.867767572402954.\n",
      "Training epoch 1 batch 502 with loss 2.0497043132781982.\n",
      "Training epoch 1 batch 503 with loss 1.910733938217163.\n",
      "Training epoch 1 batch 504 with loss 1.8090814352035522.\n",
      "Training epoch 1 batch 505 with loss 2.1788437366485596.\n",
      "Training epoch 1 batch 506 with loss 2.3536198139190674.\n",
      "Training epoch 1 batch 507 with loss 1.990996241569519.\n",
      "Training epoch 1 batch 508 with loss 1.89427649974823.\n",
      "Training epoch 1 batch 509 with loss 2.047175168991089.\n",
      "Training epoch 1 batch 510 with loss 2.3115508556365967.\n",
      "Training epoch 1 batch 511 with loss 2.0363523960113525.\n",
      "Training epoch 1 batch 512 with loss 2.2884740829467773.\n",
      "Training epoch 1 batch 513 with loss 2.082228422164917.\n",
      "Training epoch 1 batch 514 with loss 1.790910005569458.\n",
      "Training epoch 1 batch 515 with loss 2.1606087684631348.\n",
      "Training epoch 1 batch 516 with loss 1.7852174043655396.\n",
      "Training epoch 1 batch 517 with loss 1.8978211879730225.\n",
      "Training epoch 1 batch 518 with loss 2.1046156883239746.\n",
      "Training epoch 1 batch 519 with loss 1.9711644649505615.\n",
      "Training epoch 1 batch 520 with loss 2.118695020675659.\n",
      "Training epoch 1 batch 521 with loss 1.3912445306777954.\n",
      "Training epoch 1 batch 522 with loss 2.0613768100738525.\n",
      "Training epoch 1 batch 523 with loss 2.110809087753296.\n",
      "Training epoch 1 batch 524 with loss 1.8716145753860474.\n",
      "Training epoch 1 batch 525 with loss 1.9926774501800537.\n",
      "Training epoch 1 batch 526 with loss 2.228606939315796.\n",
      "Training epoch 1 batch 527 with loss 2.0013084411621094.\n",
      "Training epoch 1 batch 528 with loss 1.9852759838104248.\n",
      "Training epoch 1 batch 529 with loss 1.816063404083252.\n",
      "Training epoch 1 batch 530 with loss 2.4240329265594482.\n",
      "Training epoch 1 batch 531 with loss 2.4675800800323486.\n",
      "Training epoch 1 batch 532 with loss 2.1112165451049805.\n",
      "Training epoch 1 batch 533 with loss 2.0546822547912598.\n",
      "Training epoch 1 batch 534 with loss 1.7775723934173584.\n",
      "Training epoch 1 batch 535 with loss 2.2948825359344482.\n",
      "Training epoch 1 batch 536 with loss 1.9366148710250854.\n",
      "Training epoch 1 batch 537 with loss 2.1998229026794434.\n",
      "Training epoch 1 batch 538 with loss 2.069681406021118.\n",
      "Training epoch 1 batch 539 with loss 1.9606770277023315.\n",
      "Training epoch 1 batch 540 with loss 1.9312710762023926.\n",
      "Training epoch 1 batch 541 with loss 2.2454330921173096.\n",
      "Training epoch 1 batch 542 with loss 2.0736453533172607.\n",
      "Training epoch 1 batch 543 with loss 1.9215611219406128.\n",
      "Training epoch 1 batch 544 with loss 1.7626904249191284.\n",
      "Training epoch 1 batch 545 with loss 1.8630986213684082.\n",
      "Training epoch 1 batch 546 with loss 2.071812868118286.\n",
      "Training epoch 1 batch 547 with loss 2.0633199214935303.\n",
      "Training epoch 1 batch 548 with loss 1.968069314956665.\n",
      "Training epoch 1 batch 549 with loss 2.0378258228302.\n",
      "Training epoch 1 batch 550 with loss 2.0399794578552246.\n",
      "Training epoch 1 batch 551 with loss 2.0367369651794434.\n",
      "Training epoch 1 batch 552 with loss 1.9077478647232056.\n",
      "Training epoch 1 batch 553 with loss 1.9205880165100098.\n",
      "Training epoch 1 batch 554 with loss 2.049790620803833.\n",
      "Training epoch 1 batch 555 with loss 2.2113211154937744.\n",
      "Training epoch 1 batch 556 with loss 2.3324851989746094.\n",
      "Training epoch 1 batch 557 with loss 1.9372378587722778.\n",
      "Training epoch 1 batch 558 with loss 1.9326258897781372.\n",
      "Training epoch 1 batch 559 with loss 2.5007259845733643.\n",
      "Training epoch 1 batch 560 with loss 2.081866502761841.\n",
      "Training epoch 1 batch 561 with loss 2.105954170227051.\n",
      "Training epoch 1 batch 562 with loss 1.7394360303878784.\n",
      "Training epoch 1 batch 563 with loss 1.7056206464767456.\n",
      "Training epoch 1 batch 564 with loss 2.3136329650878906.\n",
      "Training epoch 1 batch 565 with loss 2.2481067180633545.\n",
      "Training epoch 1 batch 566 with loss 2.2433292865753174.\n",
      "Training epoch 1 batch 567 with loss 2.0406439304351807.\n",
      "Training epoch 1 batch 568 with loss 1.921926498413086.\n",
      "Training epoch 1 batch 569 with loss 2.0204246044158936.\n",
      "Training epoch 1 batch 570 with loss 1.8360133171081543.\n",
      "Training epoch 1 batch 571 with loss 1.9674111604690552.\n",
      "Training epoch 1 batch 572 with loss 1.9676438570022583.\n",
      "Training epoch 1 batch 573 with loss 2.0938239097595215.\n",
      "Training epoch 1 batch 574 with loss 1.7945314645767212.\n",
      "Training epoch 1 batch 575 with loss 2.0910425186157227.\n",
      "Training epoch 1 batch 576 with loss 1.8526053428649902.\n",
      "Training epoch 1 batch 577 with loss 2.0563125610351562.\n",
      "Training epoch 1 batch 578 with loss 2.1682777404785156.\n",
      "Training epoch 1 batch 579 with loss 1.8363673686981201.\n",
      "Training epoch 1 batch 580 with loss 1.7451711893081665.\n",
      "Training epoch 1 batch 581 with loss 1.9583585262298584.\n",
      "Training epoch 1 batch 582 with loss 2.3527963161468506.\n",
      "Training epoch 1 batch 583 with loss 2.202216386795044.\n",
      "Training epoch 1 batch 584 with loss 2.2292187213897705.\n",
      "Training epoch 1 batch 585 with loss 1.885177493095398.\n",
      "Training epoch 1 batch 586 with loss 2.0149412155151367.\n",
      "Training epoch 1 batch 587 with loss 2.1932356357574463.\n",
      "Training epoch 1 batch 588 with loss 2.20350980758667.\n",
      "Training epoch 1 batch 589 with loss 2.07873272895813.\n",
      "Training epoch 1 batch 590 with loss 2.0668578147888184.\n",
      "Training epoch 1 batch 591 with loss 1.7862343788146973.\n",
      "Training epoch 1 batch 592 with loss 2.3408944606781006.\n",
      "Training epoch 1 batch 593 with loss 1.9043869972229004.\n",
      "Training epoch 1 batch 594 with loss 1.9800153970718384.\n",
      "Training epoch 1 batch 595 with loss 1.9941895008087158.\n",
      "Training epoch 1 batch 596 with loss 2.418830633163452.\n",
      "Training epoch 1 batch 597 with loss 2.293055295944214.\n",
      "Training epoch 1 batch 598 with loss 2.4394681453704834.\n",
      "Training epoch 1 batch 599 with loss 1.927295446395874.\n",
      "Training epoch 1 batch 600 with loss 1.928030014038086.\n",
      "Training epoch 1 batch 601 with loss 2.527583599090576.\n",
      "Training epoch 1 batch 602 with loss 2.256742000579834.\n",
      "Training epoch 1 batch 603 with loss 2.1465582847595215.\n",
      "Training epoch 1 batch 604 with loss 2.2903175354003906.\n",
      "Training epoch 1 batch 605 with loss 2.0437676906585693.\n",
      "Training epoch 1 batch 606 with loss 2.418921947479248.\n",
      "Training epoch 1 batch 607 with loss 1.9855389595031738.\n",
      "Training epoch 1 batch 608 with loss 1.8556244373321533.\n",
      "Training epoch 1 batch 609 with loss 2.1556942462921143.\n",
      "Training epoch 1 batch 610 with loss 2.479130506515503.\n",
      "Training epoch 1 batch 611 with loss 1.922669529914856.\n",
      "Training epoch 1 batch 612 with loss 2.2233054637908936.\n",
      "Training epoch 1 batch 613 with loss 2.014979600906372.\n",
      "Training epoch 1 batch 614 with loss 2.1229143142700195.\n",
      "Training epoch 1 batch 615 with loss 2.3604891300201416.\n",
      "Training epoch 1 batch 616 with loss 2.2023725509643555.\n",
      "Training epoch 1 batch 617 with loss 2.4987494945526123.\n",
      "Training epoch 1 batch 618 with loss 1.9102129936218262.\n",
      "Training epoch 1 batch 619 with loss 1.8585582971572876.\n",
      "Training epoch 1 batch 620 with loss 1.9030911922454834.\n",
      "Training epoch 1 batch 621 with loss 2.221482276916504.\n",
      "Training epoch 1 batch 622 with loss 2.2195992469787598.\n",
      "Training epoch 1 batch 623 with loss 2.08433198928833.\n",
      "Training epoch 1 batch 624 with loss 2.1536028385162354.\n",
      "Training epoch 1 batch 625 with loss 2.124713182449341.\n",
      "Training epoch 1 batch 626 with loss 2.069730281829834.\n",
      "Training epoch 1 batch 627 with loss 1.76791250705719.\n",
      "Training epoch 1 batch 628 with loss 1.8662810325622559.\n",
      "Training epoch 1 batch 629 with loss 1.8664673566818237.\n",
      "Training epoch 1 batch 630 with loss 1.8022747039794922.\n",
      "Training epoch 1 batch 631 with loss 2.0960090160369873.\n",
      "Training epoch 1 batch 632 with loss 2.280492067337036.\n",
      "Training epoch 1 batch 633 with loss 2.195734739303589.\n",
      "Training epoch 1 batch 634 with loss 2.23726224899292.\n",
      "Training epoch 1 batch 635 with loss 1.6947156190872192.\n",
      "Training epoch 1 batch 636 with loss 1.9596514701843262.\n",
      "Training epoch 1 batch 637 with loss 1.9903305768966675.\n",
      "Training epoch 1 batch 638 with loss 1.9781475067138672.\n",
      "Training epoch 1 batch 639 with loss 1.9877160787582397.\n",
      "Training epoch 1 batch 640 with loss 1.7984212636947632.\n",
      "Training epoch 1 batch 641 with loss 1.8473576307296753.\n",
      "Training epoch 1 batch 642 with loss 1.8968799114227295.\n",
      "Training epoch 1 batch 643 with loss 1.9325157403945923.\n",
      "Training epoch 1 batch 644 with loss 2.521846294403076.\n",
      "Training epoch 1 batch 645 with loss 1.8934118747711182.\n",
      "Training epoch 1 batch 646 with loss 2.0312366485595703.\n",
      "Training epoch 1 batch 647 with loss 1.8157073259353638.\n",
      "Training epoch 1 batch 648 with loss 2.051888942718506.\n",
      "Training epoch 1 batch 649 with loss 1.7002447843551636.\n",
      "Training epoch 1 batch 650 with loss 1.949391484260559.\n",
      "Training epoch 1 batch 651 with loss 2.007326364517212.\n",
      "Training epoch 1 batch 652 with loss 1.6422232389450073.\n",
      "Training epoch 1 batch 653 with loss 1.8729839324951172.\n",
      "Training epoch 1 batch 654 with loss 2.385927677154541.\n",
      "Training epoch 1 batch 655 with loss 1.8404937982559204.\n",
      "Training epoch 1 batch 656 with loss 2.1818649768829346.\n",
      "Training epoch 1 batch 657 with loss 1.9647570848464966.\n",
      "Training epoch 1 batch 658 with loss 1.8690131902694702.\n",
      "Training epoch 1 batch 659 with loss 2.126339912414551.\n",
      "Training epoch 1 batch 660 with loss 2.088517904281616.\n",
      "Training epoch 1 batch 661 with loss 1.991312026977539.\n",
      "Training epoch 1 batch 662 with loss 2.2070984840393066.\n",
      "Training epoch 1 batch 663 with loss 1.9690134525299072.\n",
      "Training epoch 1 batch 664 with loss 1.7890183925628662.\n",
      "Training epoch 1 batch 665 with loss 2.2431046962738037.\n",
      "Training epoch 1 batch 666 with loss 1.878872275352478.\n",
      "Training epoch 1 batch 667 with loss 2.388665199279785.\n",
      "Training epoch 1 batch 668 with loss 2.056171417236328.\n",
      "Training epoch 1 batch 669 with loss 2.222393274307251.\n",
      "Training epoch 1 batch 670 with loss 2.2209882736206055.\n",
      "Training epoch 1 batch 671 with loss 2.1155436038970947.\n",
      "Training epoch 1 batch 672 with loss 1.7637876272201538.\n",
      "Training epoch 1 batch 673 with loss 2.123234510421753.\n",
      "Training epoch 1 batch 674 with loss 2.255798578262329.\n",
      "Training epoch 1 batch 675 with loss 2.3052573204040527.\n",
      "Training epoch 1 batch 676 with loss 2.363901376724243.\n",
      "Training epoch 1 batch 677 with loss 2.387092113494873.\n",
      "Training epoch 1 batch 678 with loss 1.7533552646636963.\n",
      "Training epoch 1 batch 679 with loss 2.378321409225464.\n",
      "Training epoch 1 batch 680 with loss 1.7898037433624268.\n",
      "Training epoch 1 batch 681 with loss 1.9422175884246826.\n",
      "Training epoch 1 batch 682 with loss 2.2659037113189697.\n",
      "Training epoch 1 batch 683 with loss 2.185875415802002.\n",
      "Training epoch 1 batch 684 with loss 2.201582193374634.\n",
      "Training epoch 1 batch 685 with loss 2.200514316558838.\n",
      "Training epoch 1 batch 686 with loss 2.2754173278808594.\n",
      "Training epoch 1 batch 687 with loss 2.3352766036987305.\n",
      "Training epoch 1 batch 688 with loss 2.244370460510254.\n",
      "Training epoch 1 batch 689 with loss 1.9276440143585205.\n",
      "Training epoch 1 batch 690 with loss 2.3659396171569824.\n",
      "Training epoch 1 batch 691 with loss 2.0826303958892822.\n",
      "Training epoch 1 batch 692 with loss 2.0010321140289307.\n",
      "Training epoch 1 batch 693 with loss 1.881359577178955.\n",
      "Training epoch 1 batch 694 with loss 1.99237859249115.\n",
      "Training epoch 1 batch 695 with loss 2.0838685035705566.\n",
      "Training epoch 1 batch 696 with loss 2.0526998043060303.\n",
      "Training epoch 1 batch 697 with loss 2.1066527366638184.\n",
      "Training epoch 1 batch 698 with loss 2.2980074882507324.\n",
      "Training epoch 1 batch 699 with loss 1.8981057405471802.\n",
      "Training epoch 1 batch 700 with loss 2.159562349319458.\n",
      "Training epoch 1 batch 701 with loss 1.8863487243652344.\n",
      "Training epoch 1 batch 702 with loss 2.4588136672973633.\n",
      "Training epoch 1 batch 703 with loss 1.785709261894226.\n",
      "Training epoch 1 batch 704 with loss 2.3909361362457275.\n",
      "Training epoch 1 batch 705 with loss 1.8487777709960938.\n",
      "Training epoch 1 batch 706 with loss 2.6715431213378906.\n",
      "Training epoch 1 batch 707 with loss 1.9389722347259521.\n",
      "Training epoch 1 batch 708 with loss 1.8434829711914062.\n",
      "Training epoch 1 batch 709 with loss 2.143038034439087.\n",
      "Training epoch 1 batch 710 with loss 2.316243886947632.\n",
      "Training epoch 1 batch 711 with loss 1.943880558013916.\n",
      "Training epoch 1 batch 712 with loss 2.4464123249053955.\n",
      "Training epoch 1 batch 713 with loss 2.3168296813964844.\n",
      "Training epoch 1 batch 714 with loss 2.2363085746765137.\n",
      "Training epoch 1 batch 715 with loss 2.1300771236419678.\n",
      "Training epoch 1 batch 716 with loss 2.0492491722106934.\n",
      "Training epoch 1 batch 717 with loss 2.190634250640869.\n",
      "Training epoch 1 batch 718 with loss 1.9581822156906128.\n",
      "Training epoch 1 batch 719 with loss 2.1325244903564453.\n",
      "Training epoch 1 batch 720 with loss 2.4312474727630615.\n",
      "Training epoch 1 batch 721 with loss 2.2599267959594727.\n",
      "Training epoch 1 batch 722 with loss 1.8950855731964111.\n",
      "Training epoch 1 batch 723 with loss 2.0962862968444824.\n",
      "Training epoch 1 batch 724 with loss 2.221545696258545.\n",
      "Training epoch 1 batch 725 with loss 2.0317790508270264.\n",
      "Training epoch 1 batch 726 with loss 1.7131417989730835.\n",
      "Training epoch 1 batch 727 with loss 2.208531618118286.\n",
      "Training epoch 1 batch 728 with loss 2.164787530899048.\n",
      "Training epoch 1 batch 729 with loss 2.0406055450439453.\n",
      "Training epoch 1 batch 730 with loss 2.1005454063415527.\n",
      "Training epoch 1 batch 731 with loss 1.7008056640625.\n",
      "Training epoch 1 batch 732 with loss 2.6116397380828857.\n",
      "Training epoch 1 batch 733 with loss 1.856507420539856.\n",
      "Training epoch 1 batch 734 with loss 2.05808424949646.\n",
      "Training epoch 1 batch 735 with loss 2.100507974624634.\n",
      "Training epoch 1 batch 736 with loss 2.0836243629455566.\n",
      "Training epoch 1 batch 737 with loss 2.015634775161743.\n",
      "Training epoch 1 batch 738 with loss 1.7521395683288574.\n",
      "Training epoch 1 batch 739 with loss 2.0830485820770264.\n",
      "Training epoch 1 batch 740 with loss 1.959352970123291.\n",
      "Training epoch 1 batch 741 with loss 2.0964958667755127.\n",
      "Training epoch 1 batch 742 with loss 1.9892021417617798.\n",
      "Training epoch 1 batch 743 with loss 2.4235360622406006.\n",
      "Training epoch 1 batch 744 with loss 2.019620418548584.\n",
      "Training epoch 1 batch 745 with loss 2.205657720565796.\n",
      "Training epoch 1 batch 746 with loss 2.0586841106414795.\n",
      "Training epoch 1 batch 747 with loss 1.7742087841033936.\n",
      "Training epoch 1 batch 748 with loss 1.8646776676177979.\n",
      "Training epoch 1 batch 749 with loss 2.2351112365722656.\n",
      "Training epoch 1 batch 750 with loss 1.8948649168014526.\n",
      "Training epoch 1 batch 751 with loss 2.137817144393921.\n",
      "Training epoch 1 batch 752 with loss 2.2337217330932617.\n",
      "Training epoch 1 batch 753 with loss 1.7040576934814453.\n",
      "Training epoch 1 batch 754 with loss 2.2948484420776367.\n",
      "Training epoch 1 batch 755 with loss 1.959629774093628.\n",
      "Training epoch 1 batch 756 with loss 2.089102029800415.\n",
      "Training epoch 1 batch 757 with loss 2.299210786819458.\n",
      "Training epoch 1 batch 758 with loss 1.9285081624984741.\n",
      "Training epoch 1 batch 759 with loss 2.225902795791626.\n",
      "Training epoch 1 batch 760 with loss 2.011798620223999.\n",
      "Training epoch 1 batch 761 with loss 1.8266419172286987.\n",
      "Training epoch 1 batch 762 with loss 1.8554517030715942.\n",
      "Training epoch 1 batch 763 with loss 2.0347390174865723.\n",
      "Training epoch 1 batch 764 with loss 1.9804768562316895.\n",
      "Training epoch 1 batch 765 with loss 2.093935251235962.\n",
      "Training epoch 1 batch 766 with loss 1.8276093006134033.\n",
      "Training epoch 1 batch 767 with loss 2.05879282951355.\n",
      "Training epoch 1 batch 768 with loss 2.195446014404297.\n",
      "Training epoch 1 batch 769 with loss 2.2523763179779053.\n",
      "Training epoch 1 batch 770 with loss 2.533961296081543.\n",
      "Training epoch 1 batch 771 with loss 2.0083296298980713.\n",
      "Training epoch 1 batch 772 with loss 2.161101818084717.\n",
      "Training epoch 1 batch 773 with loss 2.29323148727417.\n",
      "Training epoch 1 batch 774 with loss 2.1155776977539062.\n",
      "Training epoch 1 batch 775 with loss 1.9620755910873413.\n",
      "Training epoch 1 batch 776 with loss 1.9291750192642212.\n",
      "Training epoch 1 batch 777 with loss 2.3574728965759277.\n",
      "Training epoch 1 batch 778 with loss 2.0121195316314697.\n",
      "Training epoch 1 batch 779 with loss 2.21439528465271.\n",
      "Training epoch 1 batch 780 with loss 2.1912074089050293.\n",
      "Training epoch 1 batch 781 with loss 1.9913533926010132.\n",
      "Training epoch 1 batch 782 with loss 2.154541254043579.\n",
      "Training epoch 1 batch 783 with loss 2.1891584396362305.\n",
      "Training epoch 1 batch 784 with loss 2.0993151664733887.\n",
      "Training epoch 1 batch 785 with loss 2.088409900665283.\n",
      "Training epoch 1 batch 786 with loss 2.106644868850708.\n",
      "Training epoch 1 batch 787 with loss 2.3174383640289307.\n",
      "Training epoch 1 batch 788 with loss 2.440772533416748.\n",
      "Training epoch 1 batch 789 with loss 2.4529190063476562.\n",
      "Training epoch 1 batch 790 with loss 2.016305923461914.\n",
      "Training epoch 1 batch 791 with loss 1.8688122034072876.\n",
      "Training epoch 1 batch 792 with loss 2.0183138847351074.\n",
      "Training epoch 1 batch 793 with loss 2.123725175857544.\n",
      "Training epoch 1 batch 794 with loss 2.1081666946411133.\n",
      "Training epoch 1 batch 795 with loss 2.446251153945923.\n",
      "Training epoch 1 batch 796 with loss 2.4248147010803223.\n",
      "Training epoch 1 batch 797 with loss 2.0622620582580566.\n",
      "Training epoch 1 batch 798 with loss 1.9090468883514404.\n",
      "Training epoch 1 batch 799 with loss 2.286468267440796.\n",
      "Training epoch 1 batch 800 with loss 1.7212284803390503.\n",
      "Training epoch 1 batch 801 with loss 2.160579204559326.\n",
      "Training epoch 1 batch 802 with loss 2.3584611415863037.\n",
      "Training epoch 1 batch 803 with loss 2.2712180614471436.\n",
      "Training epoch 1 batch 804 with loss 2.2527823448181152.\n",
      "Training epoch 1 batch 805 with loss 2.060397148132324.\n",
      "Training epoch 1 batch 806 with loss 2.23380970954895.\n",
      "Training epoch 1 batch 807 with loss 1.9109162092208862.\n",
      "Training epoch 1 batch 808 with loss 2.485520124435425.\n",
      "Training epoch 1 batch 809 with loss 2.1015663146972656.\n",
      "Training epoch 1 batch 810 with loss 2.287686586380005.\n",
      "Training epoch 1 batch 811 with loss 2.1344194412231445.\n",
      "Training epoch 1 batch 812 with loss 2.3737752437591553.\n",
      "Training epoch 1 batch 813 with loss 2.4257707595825195.\n",
      "Training epoch 1 batch 814 with loss 2.007999897003174.\n",
      "Training epoch 1 batch 815 with loss 2.2653346061706543.\n",
      "Training epoch 1 batch 816 with loss 2.1425564289093018.\n",
      "Training epoch 1 batch 817 with loss 2.137305974960327.\n",
      "Training epoch 1 batch 818 with loss 1.8530464172363281.\n",
      "Training epoch 1 batch 819 with loss 1.9092622995376587.\n",
      "Training epoch 1 batch 820 with loss 1.9068317413330078.\n",
      "Training epoch 1 batch 821 with loss 2.3368964195251465.\n",
      "Training epoch 1 batch 822 with loss 2.332998037338257.\n",
      "Training epoch 1 batch 823 with loss 1.9334832429885864.\n",
      "Training epoch 1 batch 824 with loss 2.243691921234131.\n",
      "Training epoch 1 batch 825 with loss 2.3693594932556152.\n",
      "Training epoch 1 batch 826 with loss 1.8497560024261475.\n",
      "Training epoch 1 batch 827 with loss 1.8652626276016235.\n",
      "Training epoch 1 batch 828 with loss 1.9422757625579834.\n",
      "Training epoch 1 batch 829 with loss 1.7827848196029663.\n",
      "Training epoch 1 batch 830 with loss 2.093968629837036.\n",
      "Training epoch 1 batch 831 with loss 2.069232225418091.\n",
      "Training epoch 1 batch 832 with loss 2.1473283767700195.\n",
      "Training epoch 1 batch 833 with loss 1.7943121194839478.\n",
      "Training epoch 1 batch 834 with loss 2.479713201522827.\n",
      "Training epoch 1 batch 835 with loss 1.9678035974502563.\n",
      "Training epoch 1 batch 836 with loss 1.8460066318511963.\n",
      "Training epoch 1 batch 837 with loss 1.8795266151428223.\n",
      "Training epoch 1 batch 838 with loss 2.235349416732788.\n",
      "Training epoch 1 batch 839 with loss 2.16302227973938.\n",
      "Training epoch 1 batch 840 with loss 2.2701938152313232.\n",
      "Training epoch 1 batch 841 with loss 2.179034471511841.\n",
      "Training epoch 1 batch 842 with loss 1.9642047882080078.\n",
      "Training epoch 1 batch 843 with loss 1.9663944244384766.\n",
      "Training epoch 1 batch 844 with loss 2.2659192085266113.\n",
      "Training epoch 1 batch 845 with loss 2.036008358001709.\n",
      "Training epoch 1 batch 846 with loss 1.9864476919174194.\n",
      "Training epoch 1 batch 847 with loss 1.985486626625061.\n",
      "Training epoch 1 batch 848 with loss 1.7001922130584717.\n",
      "Training epoch 1 batch 849 with loss 2.0944700241088867.\n",
      "Training epoch 1 batch 850 with loss 2.219698667526245.\n",
      "Training epoch 1 batch 851 with loss 2.19856858253479.\n",
      "Training epoch 1 batch 852 with loss 2.0099525451660156.\n",
      "Training epoch 1 batch 853 with loss 2.463735342025757.\n",
      "Training epoch 1 batch 854 with loss 2.5586791038513184.\n",
      "Training epoch 1 batch 855 with loss 2.238530158996582.\n",
      "Training epoch 1 batch 856 with loss 2.173006534576416.\n",
      "Training epoch 1 batch 857 with loss 2.072244167327881.\n",
      "Training epoch 1 batch 858 with loss 2.361406087875366.\n",
      "Training epoch 1 batch 859 with loss 2.192237377166748.\n",
      "Training epoch 1 batch 860 with loss 2.1064491271972656.\n",
      "Training epoch 1 batch 861 with loss 1.996514916419983.\n",
      "Training epoch 1 batch 862 with loss 1.9414178133010864.\n",
      "Training epoch 1 batch 863 with loss 2.337104558944702.\n",
      "Training epoch 1 batch 864 with loss 2.0488741397857666.\n",
      "Training epoch 1 batch 865 with loss 2.0288119316101074.\n",
      "Training epoch 1 batch 866 with loss 2.2324178218841553.\n",
      "Training epoch 1 batch 867 with loss 2.434831142425537.\n",
      "Training epoch 1 batch 868 with loss 1.886164903640747.\n",
      "Training epoch 1 batch 869 with loss 2.1037700176239014.\n",
      "Training epoch 1 batch 870 with loss 2.3226468563079834.\n",
      "Training epoch 1 batch 871 with loss 2.1865320205688477.\n",
      "Training epoch 1 batch 872 with loss 1.817319631576538.\n",
      "Training epoch 1 batch 873 with loss 2.10830020904541.\n",
      "Training epoch 1 batch 874 with loss 2.2890329360961914.\n",
      "Training epoch 1 batch 875 with loss 1.908087968826294.\n",
      "Training epoch 1 batch 876 with loss 2.2311899662017822.\n",
      "Training epoch 1 batch 877 with loss 2.2061421871185303.\n",
      "Training epoch 1 batch 878 with loss 2.390925168991089.\n",
      "Training epoch 1 batch 879 with loss 2.4379491806030273.\n",
      "Training epoch 1 batch 880 with loss 2.077115058898926.\n",
      "Training epoch 1 batch 881 with loss 1.9567204713821411.\n",
      "Training epoch 1 batch 882 with loss 1.9692071676254272.\n",
      "Training epoch 1 batch 883 with loss 2.3100101947784424.\n",
      "Training epoch 1 batch 884 with loss 2.0004756450653076.\n",
      "Training epoch 1 batch 885 with loss 1.6857359409332275.\n",
      "Training epoch 1 batch 886 with loss 2.017505407333374.\n",
      "Training epoch 1 batch 887 with loss 2.098801612854004.\n",
      "Training epoch 1 batch 888 with loss 1.9946039915084839.\n",
      "Training epoch 1 batch 889 with loss 2.238051652908325.\n",
      "Training epoch 1 batch 890 with loss 2.1011617183685303.\n",
      "Training epoch 1 batch 891 with loss 2.045809030532837.\n",
      "Training epoch 1 batch 892 with loss 2.0457334518432617.\n",
      "Training epoch 1 batch 893 with loss 1.8616169691085815.\n",
      "Training epoch 1 batch 894 with loss 2.5018651485443115.\n",
      "Training epoch 1 batch 895 with loss 2.147374391555786.\n",
      "Training epoch 1 batch 896 with loss 2.2867207527160645.\n",
      "Training epoch 1 batch 897 with loss 2.285172939300537.\n",
      "Training epoch 1 batch 898 with loss 2.0356907844543457.\n",
      "Training epoch 1 batch 899 with loss 2.242009401321411.\n",
      "Training epoch 1 batch 900 with loss 2.332911968231201.\n",
      "Training epoch 1 batch 901 with loss 2.4317784309387207.\n",
      "Training epoch 1 batch 902 with loss 2.269357442855835.\n",
      "Training epoch 1 batch 903 with loss 1.9693200588226318.\n",
      "Training epoch 1 batch 904 with loss 2.336449146270752.\n",
      "Training epoch 1 batch 905 with loss 2.2528042793273926.\n",
      "Training epoch 1 batch 906 with loss 2.1162455081939697.\n",
      "Training epoch 1 batch 907 with loss 2.4053893089294434.\n",
      "Training epoch 1 batch 908 with loss 2.053356170654297.\n",
      "Training epoch 1 batch 909 with loss 1.9672266244888306.\n",
      "Training epoch 1 batch 910 with loss 1.9975717067718506.\n",
      "Training epoch 1 batch 911 with loss 2.035874605178833.\n",
      "Training epoch 1 batch 912 with loss 2.164821147918701.\n",
      "Training epoch 1 batch 913 with loss 2.1513421535491943.\n",
      "Training epoch 1 batch 914 with loss 2.2095892429351807.\n",
      "Training epoch 1 batch 915 with loss 1.9779794216156006.\n",
      "Training epoch 1 batch 916 with loss 2.121644973754883.\n",
      "Training epoch 1 batch 917 with loss 1.6722867488861084.\n",
      "Training epoch 1 batch 918 with loss 2.2695467472076416.\n",
      "Training epoch 1 batch 919 with loss 2.5547425746917725.\n",
      "Training epoch 1 batch 920 with loss 1.9166181087493896.\n",
      "Training epoch 1 batch 921 with loss 2.096410036087036.\n",
      "Training epoch 1 batch 922 with loss 2.2486088275909424.\n",
      "Training epoch 1 batch 923 with loss 2.3989498615264893.\n",
      "Training epoch 1 batch 924 with loss 1.6848770380020142.\n",
      "Training epoch 1 batch 925 with loss 2.3757123947143555.\n",
      "Training epoch 1 batch 926 with loss 2.241590738296509.\n",
      "Training epoch 1 batch 927 with loss 2.2817628383636475.\n",
      "Training epoch 1 batch 928 with loss 2.182924747467041.\n",
      "Training epoch 1 batch 929 with loss 2.423065662384033.\n",
      "Training epoch 1 batch 930 with loss 2.124472141265869.\n",
      "Training epoch 1 batch 931 with loss 1.9833557605743408.\n",
      "Training epoch 1 batch 932 with loss 2.0433146953582764.\n",
      "Training epoch 1 batch 933 with loss 2.2184762954711914.\n",
      "Training epoch 1 batch 934 with loss 2.186276912689209.\n",
      "Training epoch 1 batch 935 with loss 2.2546112537384033.\n",
      "Training epoch 1 batch 936 with loss 2.1714136600494385.\n",
      "Training epoch 1 batch 937 with loss 2.0434086322784424.\n",
      "Training epoch 1 batch 938 with loss 2.0308640003204346.\n",
      "Training epoch 1 batch 939 with loss 1.9336332082748413.\n",
      "Training epoch 1 batch 940 with loss 2.585080623626709.\n",
      "Training epoch 1 batch 941 with loss 2.4526207447052.\n",
      "Training epoch 1 batch 942 with loss 2.4581236839294434.\n",
      "Training epoch 1 batch 943 with loss 2.0744547843933105.\n",
      "Training epoch 1 batch 944 with loss 2.2833409309387207.\n",
      "Training epoch 1 batch 945 with loss 2.488276481628418.\n",
      "Training epoch 1 batch 946 with loss 2.1950807571411133.\n",
      "Training epoch 1 batch 947 with loss 1.8265845775604248.\n",
      "Training epoch 1 batch 948 with loss 2.018733263015747.\n",
      "Training epoch 1 batch 949 with loss 2.2439870834350586.\n",
      "Training epoch 1 batch 950 with loss 2.2303411960601807.\n",
      "Training epoch 1 batch 951 with loss 2.3872196674346924.\n",
      "Training epoch 1 batch 952 with loss 2.562957763671875.\n",
      "Training epoch 1 batch 953 with loss 2.285020112991333.\n",
      "Training epoch 1 batch 954 with loss 2.3611762523651123.\n",
      "Training epoch 1 batch 955 with loss 2.032783269882202.\n",
      "Training epoch 1 batch 956 with loss 2.3699400424957275.\n",
      "Training epoch 1 batch 957 with loss 1.9456409215927124.\n",
      "Training epoch 1 batch 958 with loss 2.049931287765503.\n",
      "Training epoch 1 batch 959 with loss 2.306952714920044.\n",
      "Training epoch 1 batch 960 with loss 1.9604170322418213.\n",
      "Training epoch 1 batch 961 with loss 1.8607555627822876.\n",
      "Training epoch 1 batch 962 with loss 2.107682704925537.\n",
      "Training epoch 1 batch 963 with loss 2.4442028999328613.\n",
      "Training epoch 1 batch 964 with loss 2.094738721847534.\n",
      "Training epoch 1 batch 965 with loss 2.078249216079712.\n",
      "Training epoch 1 batch 966 with loss 1.9624581336975098.\n",
      "Training epoch 1 batch 967 with loss 2.046480655670166.\n",
      "Training epoch 1 batch 968 with loss 2.0315210819244385.\n",
      "Training epoch 1 batch 969 with loss 2.0938596725463867.\n",
      "Training epoch 1 batch 970 with loss 2.072080135345459.\n",
      "Training epoch 1 batch 971 with loss 2.2386398315429688.\n",
      "Training epoch 1 batch 972 with loss 1.8793171644210815.\n",
      "Training epoch 1 batch 973 with loss 2.0179638862609863.\n",
      "Training epoch 1 batch 974 with loss 1.8765313625335693.\n",
      "Training epoch 1 batch 975 with loss 2.4631950855255127.\n",
      "Training epoch 1 batch 976 with loss 2.137993574142456.\n",
      "Training epoch 1 batch 977 with loss 1.9712938070297241.\n",
      "Training epoch 1 batch 978 with loss 1.8599696159362793.\n",
      "Training epoch 1 batch 979 with loss 2.2888741493225098.\n",
      "Training epoch 1 batch 980 with loss 2.5349414348602295.\n",
      "Training epoch 1 batch 981 with loss 2.158836603164673.\n",
      "Training epoch 1 batch 982 with loss 2.5026421546936035.\n",
      "Training epoch 1 batch 983 with loss 2.211920738220215.\n",
      "Training epoch 1 batch 984 with loss 2.313431739807129.\n",
      "Training epoch 1 batch 985 with loss 2.1168415546417236.\n",
      "Training epoch 1 batch 986 with loss 2.0570640563964844.\n",
      "Training epoch 1 batch 987 with loss 2.0555672645568848.\n",
      "Training epoch 1 batch 988 with loss 1.9953224658966064.\n",
      "Training epoch 1 batch 989 with loss 2.3736696243286133.\n",
      "Training epoch 1 batch 990 with loss 2.3287322521209717.\n",
      "Training epoch 1 batch 991 with loss 2.398766040802002.\n",
      "Training epoch 1 batch 992 with loss 2.165501594543457.\n",
      "Training epoch 1 batch 993 with loss 2.4571197032928467.\n",
      "Training epoch 1 batch 994 with loss 1.9435241222381592.\n",
      "Training epoch 1 batch 995 with loss 1.823173999786377.\n",
      "Training epoch 1 batch 996 with loss 1.795075535774231.\n",
      "Training epoch 1 batch 997 with loss 1.9292223453521729.\n",
      "Training epoch 1 batch 998 with loss 1.7979263067245483.\n",
      "Training epoch 1 batch 999 with loss 2.09668231010437.\n",
      "Test batch 0 with loss 2.245271921157837.\n",
      "Test batch 1 with loss 2.206310987472534.\n",
      "Test batch 2 with loss 2.7150816917419434.\n",
      "Test batch 3 with loss 2.4991891384124756.\n",
      "Test batch 4 with loss 2.6897010803222656.\n",
      "Test batch 5 with loss 2.2944276332855225.\n",
      "Test batch 6 with loss 2.650298595428467.\n",
      "Test batch 7 with loss 2.54087495803833.\n",
      "Test batch 8 with loss 2.1032209396362305.\n",
      "Test batch 9 with loss 2.4359116554260254.\n",
      "Test batch 10 with loss 2.663273811340332.\n",
      "Test batch 11 with loss 2.249965190887451.\n",
      "Test batch 12 with loss 2.2377021312713623.\n",
      "Test batch 13 with loss 2.7893831729888916.\n",
      "Test batch 14 with loss 2.317119598388672.\n",
      "Test batch 15 with loss 2.9369192123413086.\n",
      "Test batch 16 with loss 2.2077744007110596.\n",
      "Test batch 17 with loss 2.488044261932373.\n",
      "Test batch 18 with loss 2.0082552433013916.\n",
      "Test batch 19 with loss 2.704669237136841.\n",
      "Test batch 20 with loss 2.0852034091949463.\n",
      "Test batch 21 with loss 2.712768077850342.\n",
      "Test batch 22 with loss 2.589949607849121.\n",
      "Test batch 23 with loss 2.5547468662261963.\n",
      "Test batch 24 with loss 2.684462785720825.\n",
      "Test batch 25 with loss 2.010610580444336.\n",
      "Test batch 26 with loss 1.8166825771331787.\n",
      "Test batch 27 with loss 2.313368082046509.\n",
      "Test batch 28 with loss 2.465116024017334.\n",
      "Test batch 29 with loss 2.3045132160186768.\n",
      "Test batch 30 with loss 1.9239057302474976.\n",
      "Test batch 31 with loss 2.4057719707489014.\n",
      "Test batch 32 with loss 2.4802613258361816.\n",
      "Test batch 33 with loss 2.7739744186401367.\n",
      "Test batch 34 with loss 2.622952699661255.\n",
      "Test batch 35 with loss 2.21915340423584.\n",
      "Test batch 36 with loss 2.099705696105957.\n",
      "Test batch 37 with loss 2.4224114418029785.\n",
      "Test batch 38 with loss 2.1125032901763916.\n",
      "Test batch 39 with loss 2.189849615097046.\n",
      "Test batch 40 with loss 2.1252758502960205.\n",
      "Test batch 41 with loss 2.133378505706787.\n",
      "Test batch 42 with loss 2.5570521354675293.\n",
      "Test batch 43 with loss 2.0179522037506104.\n",
      "Test batch 44 with loss 2.6223418712615967.\n",
      "Test batch 45 with loss 2.1782546043395996.\n",
      "Test batch 46 with loss 2.409804105758667.\n",
      "Test batch 47 with loss 2.054893732070923.\n",
      "Test batch 48 with loss 2.575726270675659.\n",
      "Test batch 49 with loss 2.0885725021362305.\n",
      "Test batch 50 with loss 2.8044326305389404.\n",
      "Test batch 51 with loss 2.1248831748962402.\n",
      "Test batch 52 with loss 2.0294253826141357.\n",
      "Test batch 53 with loss 2.028015613555908.\n",
      "Test batch 54 with loss 2.445530652999878.\n",
      "Test batch 55 with loss 2.333609104156494.\n",
      "Test batch 56 with loss 2.32621169090271.\n",
      "Test batch 57 with loss 2.2489380836486816.\n",
      "Test batch 58 with loss 1.961681842803955.\n",
      "Test batch 59 with loss 2.162161350250244.\n",
      "Test batch 60 with loss 2.3109328746795654.\n",
      "Test batch 61 with loss 2.3483188152313232.\n",
      "Test batch 62 with loss 2.5445809364318848.\n",
      "Test batch 63 with loss 2.204899311065674.\n",
      "Test batch 64 with loss 1.8874880075454712.\n",
      "Test batch 65 with loss 2.4128224849700928.\n",
      "Test batch 66 with loss 2.1326563358306885.\n",
      "Test batch 67 with loss 2.506558418273926.\n",
      "Test batch 68 with loss 2.3193483352661133.\n",
      "Test batch 69 with loss 2.3663623332977295.\n",
      "Test batch 70 with loss 2.365421772003174.\n",
      "Test batch 71 with loss 1.8884224891662598.\n",
      "Test batch 72 with loss 2.009124279022217.\n",
      "Test batch 73 with loss 2.6935925483703613.\n",
      "Test batch 74 with loss 2.1529083251953125.\n",
      "Test batch 75 with loss 2.3478305339813232.\n",
      "Test batch 76 with loss 2.3191232681274414.\n",
      "Test batch 77 with loss 2.171002149581909.\n",
      "Test batch 78 with loss 2.1173768043518066.\n",
      "Test batch 79 with loss 2.582166910171509.\n",
      "Test batch 80 with loss 1.9426100254058838.\n",
      "Test batch 81 with loss 2.517624616622925.\n",
      "Test batch 82 with loss 2.2517714500427246.\n",
      "Test batch 83 with loss 1.9084616899490356.\n",
      "Test batch 84 with loss 1.945508599281311.\n",
      "Test batch 85 with loss 2.421058416366577.\n",
      "Test batch 86 with loss 1.9818719625473022.\n",
      "Test batch 87 with loss 2.4918107986450195.\n",
      "Test batch 88 with loss 2.091961145401001.\n",
      "Test batch 89 with loss 2.5524051189422607.\n",
      "Test batch 90 with loss 2.3203039169311523.\n",
      "Test batch 91 with loss 2.4615871906280518.\n",
      "Test batch 92 with loss 2.152564764022827.\n",
      "Test batch 93 with loss 2.352062702178955.\n",
      "Test batch 94 with loss 2.308486223220825.\n",
      "Test batch 95 with loss 2.3691823482513428.\n",
      "Test batch 96 with loss 1.7748101949691772.\n",
      "Test batch 97 with loss 2.5635170936584473.\n",
      "Test batch 98 with loss 2.194690465927124.\n",
      "Test batch 99 with loss 2.2523064613342285.\n",
      "Test batch 100 with loss 2.5520756244659424.\n",
      "Test batch 101 with loss 2.2740838527679443.\n",
      "Test batch 102 with loss 2.420427083969116.\n",
      "Test batch 103 with loss 2.1862075328826904.\n",
      "Test batch 104 with loss 2.3370604515075684.\n",
      "Test batch 105 with loss 1.9706556797027588.\n",
      "Test batch 106 with loss 2.511875867843628.\n",
      "Test batch 107 with loss 2.3825066089630127.\n",
      "Test batch 108 with loss 2.2467846870422363.\n",
      "Test batch 109 with loss 2.064225912094116.\n",
      "Test batch 110 with loss 2.0669503211975098.\n",
      "Test batch 111 with loss 2.5317606925964355.\n",
      "Test batch 112 with loss 2.3667068481445312.\n",
      "Test batch 113 with loss 2.1371071338653564.\n",
      "Test batch 114 with loss 2.4857852458953857.\n",
      "Test batch 115 with loss 2.610471248626709.\n",
      "Test batch 116 with loss 2.527611494064331.\n",
      "Test batch 117 with loss 2.2726213932037354.\n",
      "Test batch 118 with loss 2.069226026535034.\n",
      "Test batch 119 with loss 2.330566167831421.\n",
      "Test batch 120 with loss 2.095838785171509.\n",
      "Test batch 121 with loss 2.4077744483947754.\n",
      "Test batch 122 with loss 2.2403640747070312.\n",
      "Test batch 123 with loss 2.3303263187408447.\n",
      "Test batch 124 with loss 1.8978385925292969.\n",
      "Test batch 125 with loss 2.143012523651123.\n",
      "Test batch 126 with loss 2.256965160369873.\n",
      "Test batch 127 with loss 2.6772067546844482.\n",
      "Test batch 128 with loss 2.2993857860565186.\n",
      "Test batch 129 with loss 2.2683231830596924.\n",
      "Test batch 130 with loss 2.1253883838653564.\n",
      "Test batch 131 with loss 2.2273967266082764.\n",
      "Test batch 132 with loss 1.9434325695037842.\n",
      "Test batch 133 with loss 1.8787009716033936.\n",
      "Test batch 134 with loss 2.3845174312591553.\n",
      "Test batch 135 with loss 2.4449005126953125.\n",
      "Test batch 136 with loss 2.3839869499206543.\n",
      "Test batch 137 with loss 2.494713544845581.\n",
      "Test batch 138 with loss 2.153822183609009.\n",
      "Test batch 139 with loss 2.18198299407959.\n",
      "Test batch 140 with loss 2.484402656555176.\n",
      "Test batch 141 with loss 2.5598599910736084.\n",
      "Test batch 142 with loss 2.36344838142395.\n",
      "Test batch 143 with loss 2.193546772003174.\n",
      "Test batch 144 with loss 2.006012439727783.\n",
      "Test batch 145 with loss 2.3105156421661377.\n",
      "Test batch 146 with loss 2.4945924282073975.\n",
      "Test batch 147 with loss 2.3927392959594727.\n",
      "Test batch 148 with loss 2.345233201980591.\n",
      "Test batch 149 with loss 2.0644278526306152.\n",
      "Test batch 150 with loss 2.2340569496154785.\n",
      "Test batch 151 with loss 2.0539209842681885.\n",
      "Test batch 152 with loss 2.163170337677002.\n",
      "Test batch 153 with loss 1.959810495376587.\n",
      "Test batch 154 with loss 2.1691830158233643.\n",
      "Test batch 155 with loss 2.6293325424194336.\n",
      "Test batch 156 with loss 2.4390485286712646.\n",
      "Test batch 157 with loss 2.1465086936950684.\n",
      "Test batch 158 with loss 1.978996753692627.\n",
      "Test batch 159 with loss 2.5493509769439697.\n",
      "Test batch 160 with loss 2.025362014770508.\n",
      "Test batch 161 with loss 2.227020263671875.\n",
      "Test batch 162 with loss 1.9123528003692627.\n",
      "Test batch 163 with loss 2.53398060798645.\n",
      "Test batch 164 with loss 2.484020233154297.\n",
      "Test batch 165 with loss 2.647688388824463.\n",
      "Test batch 166 with loss 2.0497145652770996.\n",
      "Test batch 167 with loss 2.41825270652771.\n",
      "Test batch 168 with loss 2.327666997909546.\n",
      "Test batch 169 with loss 2.1781387329101562.\n",
      "Test batch 170 with loss 2.3547589778900146.\n",
      "Test batch 171 with loss 2.2442705631256104.\n",
      "Test batch 172 with loss 2.6530864238739014.\n",
      "Test batch 173 with loss 2.3163914680480957.\n",
      "Test batch 174 with loss 1.999950885772705.\n",
      "Test batch 175 with loss 2.270504951477051.\n",
      "Test batch 176 with loss 2.4075024127960205.\n",
      "Test batch 177 with loss 2.1279327869415283.\n",
      "Test batch 178 with loss 2.249022960662842.\n",
      "Test batch 179 with loss 2.4428834915161133.\n",
      "Test batch 180 with loss 2.2305643558502197.\n",
      "Test batch 181 with loss 2.123443365097046.\n",
      "Test batch 182 with loss 2.252027750015259.\n",
      "Test batch 183 with loss 2.6359810829162598.\n",
      "Test batch 184 with loss 2.125915050506592.\n",
      "Test batch 185 with loss 2.2356536388397217.\n",
      "Test batch 186 with loss 2.3273301124572754.\n",
      "Test batch 187 with loss 2.5798308849334717.\n",
      "Test batch 188 with loss 2.604736566543579.\n",
      "Test batch 189 with loss 2.1754658222198486.\n",
      "Test batch 190 with loss 2.424923896789551.\n",
      "Test batch 191 with loss 2.2461740970611572.\n",
      "Test batch 192 with loss 2.3468399047851562.\n",
      "Test batch 193 with loss 2.095820665359497.\n",
      "Test batch 194 with loss 1.9437909126281738.\n",
      "Test batch 195 with loss 2.342113733291626.\n",
      "Test batch 196 with loss 2.499199867248535.\n",
      "Test batch 197 with loss 2.3973171710968018.\n",
      "Test batch 198 with loss 2.436760902404785.\n",
      "Test batch 199 with loss 2.4669337272644043.\n",
      "Test batch 200 with loss 2.279884099960327.\n",
      "Test batch 201 with loss 2.1778647899627686.\n",
      "Test batch 202 with loss 2.1971373558044434.\n",
      "Test batch 203 with loss 2.215627908706665.\n",
      "Test batch 204 with loss 2.3634488582611084.\n",
      "Test batch 205 with loss 2.4184887409210205.\n",
      "Test batch 206 with loss 2.2712483406066895.\n",
      "Test batch 207 with loss 2.0259358882904053.\n",
      "Test batch 208 with loss 2.4969961643218994.\n",
      "Test batch 209 with loss 2.3190953731536865.\n",
      "Test batch 210 with loss 2.3293025493621826.\n",
      "Test batch 211 with loss 2.565788984298706.\n",
      "Test batch 212 with loss 2.6236698627471924.\n",
      "Test batch 213 with loss 2.4553592205047607.\n",
      "Test batch 214 with loss 2.272430419921875.\n",
      "Test batch 215 with loss 2.2773633003234863.\n",
      "Test batch 216 with loss 2.2068254947662354.\n",
      "Test batch 217 with loss 3.003516674041748.\n",
      "Test batch 218 with loss 2.242621421813965.\n",
      "Test batch 219 with loss 2.8263137340545654.\n",
      "Test batch 220 with loss 2.497877597808838.\n",
      "Test batch 221 with loss 2.229612112045288.\n",
      "Test batch 222 with loss 1.9941153526306152.\n",
      "Test batch 223 with loss 2.3551347255706787.\n",
      "Test batch 224 with loss 2.526979923248291.\n",
      "Test batch 225 with loss 2.0284295082092285.\n",
      "Test batch 226 with loss 2.59507417678833.\n",
      "Test batch 227 with loss 2.453727960586548.\n",
      "Test batch 228 with loss 2.1472153663635254.\n",
      "Test batch 229 with loss 2.4453301429748535.\n",
      "Test batch 230 with loss 2.1689679622650146.\n",
      "Test batch 231 with loss 2.484311103820801.\n",
      "Test batch 232 with loss 2.4413113594055176.\n",
      "Test batch 233 with loss 2.0935275554656982.\n",
      "Test batch 234 with loss 2.5773138999938965.\n",
      "Test batch 235 with loss 2.191189765930176.\n",
      "Test batch 236 with loss 2.479520559310913.\n",
      "Test batch 237 with loss 2.8002994060516357.\n",
      "Test batch 238 with loss 2.293177366256714.\n",
      "Test batch 239 with loss 2.2376441955566406.\n",
      "Test batch 240 with loss 2.437807559967041.\n",
      "Test batch 241 with loss 2.600022077560425.\n",
      "Test batch 242 with loss 1.9037401676177979.\n",
      "Test batch 243 with loss 2.46323561668396.\n",
      "Test batch 244 with loss 2.431384325027466.\n",
      "Test batch 245 with loss 2.560826063156128.\n",
      "Test batch 246 with loss 2.7924201488494873.\n",
      "Test batch 247 with loss 2.1425671577453613.\n",
      "Test batch 248 with loss 2.4189658164978027.\n",
      "Test batch 249 with loss 2.3563878536224365.\n",
      "Test batch 250 with loss 2.381117820739746.\n",
      "Test batch 251 with loss 2.111976146697998.\n",
      "Test batch 252 with loss 2.3848066329956055.\n",
      "Test batch 253 with loss 2.224102258682251.\n",
      "Test batch 254 with loss 2.2659037113189697.\n",
      "Test batch 255 with loss 2.1884868144989014.\n",
      "Test batch 256 with loss 2.1769089698791504.\n",
      "Test batch 257 with loss 1.9820599555969238.\n",
      "Test batch 258 with loss 2.3222031593322754.\n",
      "Test batch 259 with loss 2.517336845397949.\n",
      "Test batch 260 with loss 2.2322518825531006.\n",
      "Test batch 261 with loss 2.661503314971924.\n",
      "Test batch 262 with loss 2.1661574840545654.\n",
      "Test batch 263 with loss 2.2598519325256348.\n",
      "Test batch 264 with loss 2.3213789463043213.\n",
      "Test batch 265 with loss 2.2353157997131348.\n",
      "Test batch 266 with loss 2.5688681602478027.\n",
      "Test batch 267 with loss 2.2720882892608643.\n",
      "Test batch 268 with loss 2.570369005203247.\n",
      "Test batch 269 with loss 2.3242316246032715.\n",
      "Test batch 270 with loss 2.366147756576538.\n",
      "Test batch 271 with loss 2.471543073654175.\n",
      "Test batch 272 with loss 2.202871084213257.\n",
      "Test batch 273 with loss 2.3112685680389404.\n",
      "Test batch 274 with loss 2.0854101181030273.\n",
      "Test batch 275 with loss 1.9461638927459717.\n",
      "Test batch 276 with loss 2.415092945098877.\n",
      "Test batch 277 with loss 1.9795674085617065.\n",
      "Test batch 278 with loss 2.8277666568756104.\n",
      "Test batch 279 with loss 2.0018022060394287.\n",
      "Test batch 280 with loss 1.9095083475112915.\n",
      "Test batch 281 with loss 2.5390923023223877.\n",
      "Test batch 282 with loss 2.382683038711548.\n",
      "Test batch 283 with loss 2.4767391681671143.\n",
      "Test batch 284 with loss 2.2499074935913086.\n",
      "Test batch 285 with loss 2.4318771362304688.\n",
      "Test batch 286 with loss 2.807647466659546.\n",
      "Test batch 287 with loss 2.266951084136963.\n",
      "Test batch 288 with loss 2.614086866378784.\n",
      "Test batch 289 with loss 2.7680625915527344.\n",
      "Test batch 290 with loss 2.524190664291382.\n",
      "Test batch 291 with loss 2.0172300338745117.\n",
      "Test batch 292 with loss 2.1847012042999268.\n",
      "Test batch 293 with loss 2.2587525844573975.\n",
      "Test batch 294 with loss 2.441596508026123.\n",
      "Test batch 295 with loss 2.6263797283172607.\n",
      "Test batch 296 with loss 2.4950051307678223.\n",
      "Test batch 297 with loss 2.4518139362335205.\n",
      "Test batch 298 with loss 2.6168806552886963.\n",
      "Test batch 299 with loss 2.1935555934906006.\n",
      "Test batch 300 with loss 2.169917106628418.\n",
      "Test batch 301 with loss 2.4753787517547607.\n",
      "Test batch 302 with loss 2.164428472518921.\n",
      "Test batch 303 with loss 2.407184362411499.\n",
      "Test batch 304 with loss 2.4669389724731445.\n",
      "Test batch 305 with loss 2.4201629161834717.\n",
      "Test batch 306 with loss 2.6226437091827393.\n",
      "Test batch 307 with loss 2.4111974239349365.\n",
      "Test batch 308 with loss 2.7083325386047363.\n",
      "Test batch 309 with loss 2.2452118396759033.\n",
      "Test batch 310 with loss 2.086292028427124.\n",
      "Test batch 311 with loss 2.394495964050293.\n",
      "Test batch 312 with loss 1.7321888208389282.\n",
      "Test batch 313 with loss 2.5756146907806396.\n",
      "Test batch 314 with loss 2.2164130210876465.\n",
      "Test batch 315 with loss 2.334296464920044.\n",
      "Test batch 316 with loss 2.077008008956909.\n",
      "Test batch 317 with loss 2.1716396808624268.\n",
      "Test batch 318 with loss 2.4148669242858887.\n",
      "Test batch 319 with loss 2.528529167175293.\n",
      "Test batch 320 with loss 2.605867624282837.\n",
      "Test batch 321 with loss 2.5605392456054688.\n",
      "Test batch 322 with loss 2.3217344284057617.\n",
      "Test batch 323 with loss 2.8003318309783936.\n",
      "Test batch 324 with loss 2.5244598388671875.\n",
      "Test batch 325 with loss 2.0950796604156494.\n",
      "Test batch 326 with loss 2.2769880294799805.\n",
      "Test batch 327 with loss 2.3121109008789062.\n",
      "Test batch 328 with loss 2.2961981296539307.\n",
      "Test batch 329 with loss 2.3029074668884277.\n",
      "Test batch 330 with loss 2.2789840698242188.\n",
      "Test batch 331 with loss 2.3479299545288086.\n",
      "Test batch 332 with loss 2.4107065200805664.\n",
      "Test batch 333 with loss 2.5284743309020996.\n",
      "Training epoch 2 batch 0 with loss 1.443068027496338.\n",
      "Training epoch 2 batch 1 with loss 1.6258248090744019.\n",
      "Training epoch 2 batch 2 with loss 1.6413723230361938.\n",
      "Training epoch 2 batch 3 with loss 1.6681103706359863.\n",
      "Training epoch 2 batch 4 with loss 1.3668521642684937.\n",
      "Training epoch 2 batch 5 with loss 1.7816659212112427.\n",
      "Training epoch 2 batch 6 with loss 1.7307522296905518.\n",
      "Training epoch 2 batch 7 with loss 1.6197363138198853.\n",
      "Training epoch 2 batch 8 with loss 1.8337421417236328.\n",
      "Training epoch 2 batch 9 with loss 1.4671356678009033.\n",
      "Training epoch 2 batch 10 with loss 1.633354902267456.\n",
      "Training epoch 2 batch 11 with loss 1.4557000398635864.\n",
      "Training epoch 2 batch 12 with loss 1.42507803440094.\n",
      "Training epoch 2 batch 13 with loss 2.031813144683838.\n",
      "Training epoch 2 batch 14 with loss 1.500867247581482.\n",
      "Training epoch 2 batch 15 with loss 1.418567419052124.\n",
      "Training epoch 2 batch 16 with loss 1.55487859249115.\n",
      "Training epoch 2 batch 17 with loss 1.4361108541488647.\n",
      "Training epoch 2 batch 18 with loss 1.8086925745010376.\n",
      "Training epoch 2 batch 19 with loss 1.7441413402557373.\n",
      "Training epoch 2 batch 20 with loss 1.4448431730270386.\n",
      "Training epoch 2 batch 21 with loss 1.8788111209869385.\n",
      "Training epoch 2 batch 22 with loss 1.7987627983093262.\n",
      "Training epoch 2 batch 23 with loss 1.5126723051071167.\n",
      "Training epoch 2 batch 24 with loss 1.627062201499939.\n",
      "Training epoch 2 batch 25 with loss 2.0302345752716064.\n",
      "Training epoch 2 batch 26 with loss 1.862928867340088.\n",
      "Training epoch 2 batch 27 with loss 1.6044223308563232.\n",
      "Training epoch 2 batch 28 with loss 1.4751471281051636.\n",
      "Training epoch 2 batch 29 with loss 1.8668313026428223.\n",
      "Training epoch 2 batch 30 with loss 1.541263222694397.\n",
      "Training epoch 2 batch 31 with loss 1.522337555885315.\n",
      "Training epoch 2 batch 32 with loss 1.5505115985870361.\n",
      "Training epoch 2 batch 33 with loss 1.5714528560638428.\n",
      "Training epoch 2 batch 34 with loss 1.6777698993682861.\n",
      "Training epoch 2 batch 35 with loss 1.6465466022491455.\n",
      "Training epoch 2 batch 36 with loss 1.6196330785751343.\n",
      "Training epoch 2 batch 37 with loss 1.303965449333191.\n",
      "Training epoch 2 batch 38 with loss 1.734079122543335.\n",
      "Training epoch 2 batch 39 with loss 1.6451836824417114.\n",
      "Training epoch 2 batch 40 with loss 1.666388988494873.\n",
      "Training epoch 2 batch 41 with loss 1.6827294826507568.\n",
      "Training epoch 2 batch 42 with loss 1.8619840145111084.\n",
      "Training epoch 2 batch 43 with loss 1.5244189500808716.\n",
      "Training epoch 2 batch 44 with loss 1.8527902364730835.\n",
      "Training epoch 2 batch 45 with loss 1.7446125745773315.\n",
      "Training epoch 2 batch 46 with loss 1.7343865633010864.\n",
      "Training epoch 2 batch 47 with loss 1.6818288564682007.\n",
      "Training epoch 2 batch 48 with loss 1.667682409286499.\n",
      "Training epoch 2 batch 49 with loss 2.100419282913208.\n",
      "Training epoch 2 batch 50 with loss 2.013279914855957.\n",
      "Training epoch 2 batch 51 with loss 1.898282766342163.\n",
      "Training epoch 2 batch 52 with loss 1.6912261247634888.\n",
      "Training epoch 2 batch 53 with loss 1.6945315599441528.\n",
      "Training epoch 2 batch 54 with loss 1.575602412223816.\n",
      "Training epoch 2 batch 55 with loss 1.4657785892486572.\n",
      "Training epoch 2 batch 56 with loss 1.6679272651672363.\n",
      "Training epoch 2 batch 57 with loss 1.2990024089813232.\n",
      "Training epoch 2 batch 58 with loss 1.5838435888290405.\n",
      "Training epoch 2 batch 59 with loss 1.629494309425354.\n",
      "Training epoch 2 batch 60 with loss 1.6588386297225952.\n",
      "Training epoch 2 batch 61 with loss 1.9705791473388672.\n",
      "Training epoch 2 batch 62 with loss 1.505144715309143.\n",
      "Training epoch 2 batch 63 with loss 1.650679349899292.\n",
      "Training epoch 2 batch 64 with loss 1.5651676654815674.\n",
      "Training epoch 2 batch 65 with loss 1.5603480339050293.\n",
      "Training epoch 2 batch 66 with loss 1.590267300605774.\n",
      "Training epoch 2 batch 67 with loss 2.067486524581909.\n",
      "Training epoch 2 batch 68 with loss 1.4360556602478027.\n",
      "Training epoch 2 batch 69 with loss 1.785925269126892.\n",
      "Training epoch 2 batch 70 with loss 1.6764919757843018.\n",
      "Training epoch 2 batch 71 with loss 1.5800726413726807.\n",
      "Training epoch 2 batch 72 with loss 1.8426491022109985.\n",
      "Training epoch 2 batch 73 with loss 1.2402050495147705.\n",
      "Training epoch 2 batch 74 with loss 1.4976112842559814.\n",
      "Training epoch 2 batch 75 with loss 1.8583651781082153.\n",
      "Training epoch 2 batch 76 with loss 1.581156849861145.\n",
      "Training epoch 2 batch 77 with loss 1.7218211889266968.\n",
      "Training epoch 2 batch 78 with loss 1.9018319845199585.\n",
      "Training epoch 2 batch 79 with loss 1.8277384042739868.\n",
      "Training epoch 2 batch 80 with loss 1.643769383430481.\n",
      "Training epoch 2 batch 81 with loss 1.633840560913086.\n",
      "Training epoch 2 batch 82 with loss 1.4223519563674927.\n",
      "Training epoch 2 batch 83 with loss 1.5780270099639893.\n",
      "Training epoch 2 batch 84 with loss 1.9357341527938843.\n",
      "Training epoch 2 batch 85 with loss 1.5523051023483276.\n",
      "Training epoch 2 batch 86 with loss 1.6504853963851929.\n",
      "Training epoch 2 batch 87 with loss 2.0495638847351074.\n",
      "Training epoch 2 batch 88 with loss 1.6705840826034546.\n",
      "Training epoch 2 batch 89 with loss 1.9025001525878906.\n",
      "Training epoch 2 batch 90 with loss 1.5544817447662354.\n",
      "Training epoch 2 batch 91 with loss 1.7086461782455444.\n",
      "Training epoch 2 batch 92 with loss 1.7586896419525146.\n",
      "Training epoch 2 batch 93 with loss 1.654268503189087.\n",
      "Training epoch 2 batch 94 with loss 1.8301448822021484.\n",
      "Training epoch 2 batch 95 with loss 1.7435479164123535.\n",
      "Training epoch 2 batch 96 with loss 1.7884314060211182.\n",
      "Training epoch 2 batch 97 with loss 1.7284749746322632.\n",
      "Training epoch 2 batch 98 with loss 1.8295254707336426.\n",
      "Training epoch 2 batch 99 with loss 1.380834698677063.\n",
      "Training epoch 2 batch 100 with loss 2.230895757675171.\n",
      "Training epoch 2 batch 101 with loss 1.635093092918396.\n",
      "Training epoch 2 batch 102 with loss 1.9086917638778687.\n",
      "Training epoch 2 batch 103 with loss 1.8356316089630127.\n",
      "Training epoch 2 batch 104 with loss 1.8113540410995483.\n",
      "Training epoch 2 batch 105 with loss 1.8392897844314575.\n",
      "Training epoch 2 batch 106 with loss 1.6346255540847778.\n",
      "Training epoch 2 batch 107 with loss 1.4988195896148682.\n",
      "Training epoch 2 batch 108 with loss 1.6417677402496338.\n",
      "Training epoch 2 batch 109 with loss 1.684142827987671.\n",
      "Training epoch 2 batch 110 with loss 1.3720512390136719.\n",
      "Training epoch 2 batch 111 with loss 1.45663583278656.\n",
      "Training epoch 2 batch 112 with loss 1.4401729106903076.\n",
      "Training epoch 2 batch 113 with loss 1.7777289152145386.\n",
      "Training epoch 2 batch 114 with loss 1.6459906101226807.\n",
      "Training epoch 2 batch 115 with loss 1.7072522640228271.\n",
      "Training epoch 2 batch 116 with loss 1.7261106967926025.\n",
      "Training epoch 2 batch 117 with loss 1.8664716482162476.\n",
      "Training epoch 2 batch 118 with loss 1.7417513132095337.\n",
      "Training epoch 2 batch 119 with loss 1.7955126762390137.\n",
      "Training epoch 2 batch 120 with loss 1.8600741624832153.\n",
      "Training epoch 2 batch 121 with loss 1.8071614503860474.\n",
      "Training epoch 2 batch 122 with loss 1.5394444465637207.\n",
      "Training epoch 2 batch 123 with loss 1.9066184759140015.\n",
      "Training epoch 2 batch 124 with loss 1.5203641653060913.\n",
      "Training epoch 2 batch 125 with loss 1.8661943674087524.\n",
      "Training epoch 2 batch 126 with loss 1.9174784421920776.\n",
      "Training epoch 2 batch 127 with loss 1.457034945487976.\n",
      "Training epoch 2 batch 128 with loss 1.7158173322677612.\n",
      "Training epoch 2 batch 129 with loss 1.8489564657211304.\n",
      "Training epoch 2 batch 130 with loss 2.213239908218384.\n",
      "Training epoch 2 batch 131 with loss 1.835901141166687.\n",
      "Training epoch 2 batch 132 with loss 1.577210783958435.\n",
      "Training epoch 2 batch 133 with loss 1.624558925628662.\n",
      "Training epoch 2 batch 134 with loss 1.7088087797164917.\n",
      "Training epoch 2 batch 135 with loss 2.1470119953155518.\n",
      "Training epoch 2 batch 136 with loss 1.8803354501724243.\n",
      "Training epoch 2 batch 137 with loss 1.6139373779296875.\n",
      "Training epoch 2 batch 138 with loss 1.8350375890731812.\n",
      "Training epoch 2 batch 139 with loss 1.6919598579406738.\n",
      "Training epoch 2 batch 140 with loss 1.6839371919631958.\n",
      "Training epoch 2 batch 141 with loss 1.883044719696045.\n",
      "Training epoch 2 batch 142 with loss 1.759431004524231.\n",
      "Training epoch 2 batch 143 with loss 1.5508061647415161.\n",
      "Training epoch 2 batch 144 with loss 1.5601131916046143.\n",
      "Training epoch 2 batch 145 with loss 1.7441413402557373.\n",
      "Training epoch 2 batch 146 with loss 1.4634138345718384.\n",
      "Training epoch 2 batch 147 with loss 2.049708127975464.\n",
      "Training epoch 2 batch 148 with loss 2.141753911972046.\n",
      "Training epoch 2 batch 149 with loss 1.8420872688293457.\n",
      "Training epoch 2 batch 150 with loss 1.7346705198287964.\n",
      "Training epoch 2 batch 151 with loss 1.467839002609253.\n",
      "Training epoch 2 batch 152 with loss 1.814439296722412.\n",
      "Training epoch 2 batch 153 with loss 1.4033160209655762.\n",
      "Training epoch 2 batch 154 with loss 1.7413876056671143.\n",
      "Training epoch 2 batch 155 with loss 1.8649522066116333.\n",
      "Training epoch 2 batch 156 with loss 1.7880641222000122.\n",
      "Training epoch 2 batch 157 with loss 1.960296392440796.\n",
      "Training epoch 2 batch 158 with loss 2.0518202781677246.\n",
      "Training epoch 2 batch 159 with loss 1.7232508659362793.\n",
      "Training epoch 2 batch 160 with loss 1.9086557626724243.\n",
      "Training epoch 2 batch 161 with loss 1.4604504108428955.\n",
      "Training epoch 2 batch 162 with loss 1.4031685590744019.\n",
      "Training epoch 2 batch 163 with loss 1.6028556823730469.\n",
      "Training epoch 2 batch 164 with loss 1.7163881063461304.\n",
      "Training epoch 2 batch 165 with loss 1.8702266216278076.\n",
      "Training epoch 2 batch 166 with loss 1.673936128616333.\n",
      "Training epoch 2 batch 167 with loss 1.975929856300354.\n",
      "Training epoch 2 batch 168 with loss 1.7432537078857422.\n",
      "Training epoch 2 batch 169 with loss 2.1202392578125.\n",
      "Training epoch 2 batch 170 with loss 1.690334439277649.\n",
      "Training epoch 2 batch 171 with loss 1.3263607025146484.\n",
      "Training epoch 2 batch 172 with loss 1.6278139352798462.\n",
      "Training epoch 2 batch 173 with loss 1.4859492778778076.\n",
      "Training epoch 2 batch 174 with loss 1.448041558265686.\n",
      "Training epoch 2 batch 175 with loss 1.786620855331421.\n",
      "Training epoch 2 batch 176 with loss 2.0764310359954834.\n",
      "Training epoch 2 batch 177 with loss 2.1015617847442627.\n",
      "Training epoch 2 batch 178 with loss 2.0471396446228027.\n",
      "Training epoch 2 batch 179 with loss 1.6760573387145996.\n",
      "Training epoch 2 batch 180 with loss 1.5567477941513062.\n",
      "Training epoch 2 batch 181 with loss 1.5891281366348267.\n",
      "Training epoch 2 batch 182 with loss 2.009068727493286.\n",
      "Training epoch 2 batch 183 with loss 1.7660127878189087.\n",
      "Training epoch 2 batch 184 with loss 1.6450778245925903.\n",
      "Training epoch 2 batch 185 with loss 1.8251672983169556.\n",
      "Training epoch 2 batch 186 with loss 1.8656045198440552.\n",
      "Training epoch 2 batch 187 with loss 1.8272427320480347.\n",
      "Training epoch 2 batch 188 with loss 1.46132493019104.\n",
      "Training epoch 2 batch 189 with loss 2.0735933780670166.\n",
      "Training epoch 2 batch 190 with loss 1.7136142253875732.\n",
      "Training epoch 2 batch 191 with loss 1.5501677989959717.\n",
      "Training epoch 2 batch 192 with loss 1.7061134576797485.\n",
      "Training epoch 2 batch 193 with loss 1.6218934059143066.\n",
      "Training epoch 2 batch 194 with loss 1.7949750423431396.\n",
      "Training epoch 2 batch 195 with loss 1.767217993736267.\n",
      "Training epoch 2 batch 196 with loss 1.6403833627700806.\n",
      "Training epoch 2 batch 197 with loss 1.7680308818817139.\n",
      "Training epoch 2 batch 198 with loss 1.8740445375442505.\n",
      "Training epoch 2 batch 199 with loss 1.7815643548965454.\n",
      "Training epoch 2 batch 200 with loss 1.7893911600112915.\n",
      "Training epoch 2 batch 201 with loss 1.9088529348373413.\n",
      "Training epoch 2 batch 202 with loss 1.7388381958007812.\n",
      "Training epoch 2 batch 203 with loss 2.375701665878296.\n",
      "Training epoch 2 batch 204 with loss 1.7913682460784912.\n",
      "Training epoch 2 batch 205 with loss 1.7630436420440674.\n",
      "Training epoch 2 batch 206 with loss 2.3163154125213623.\n",
      "Training epoch 2 batch 207 with loss 1.6597936153411865.\n",
      "Training epoch 2 batch 208 with loss 1.880897045135498.\n",
      "Training epoch 2 batch 209 with loss 1.6757413148880005.\n",
      "Training epoch 2 batch 210 with loss 1.9452311992645264.\n",
      "Training epoch 2 batch 211 with loss 2.0092248916625977.\n",
      "Training epoch 2 batch 212 with loss 1.916596531867981.\n",
      "Training epoch 2 batch 213 with loss 1.7873222827911377.\n",
      "Training epoch 2 batch 214 with loss 1.7665560245513916.\n",
      "Training epoch 2 batch 215 with loss 1.9952598810195923.\n",
      "Training epoch 2 batch 216 with loss 1.9174960851669312.\n",
      "Training epoch 2 batch 217 with loss 1.9747477769851685.\n",
      "Training epoch 2 batch 218 with loss 1.5351589918136597.\n",
      "Training epoch 2 batch 219 with loss 1.766461730003357.\n",
      "Training epoch 2 batch 220 with loss 2.073068857192993.\n",
      "Training epoch 2 batch 221 with loss 2.055832862854004.\n",
      "Training epoch 2 batch 222 with loss 1.61485755443573.\n",
      "Training epoch 2 batch 223 with loss 2.0173468589782715.\n",
      "Training epoch 2 batch 224 with loss 2.0150818824768066.\n",
      "Training epoch 2 batch 225 with loss 1.8757981061935425.\n",
      "Training epoch 2 batch 226 with loss 1.6844955682754517.\n",
      "Training epoch 2 batch 227 with loss 2.030238628387451.\n",
      "Training epoch 2 batch 228 with loss 1.7275246381759644.\n",
      "Training epoch 2 batch 229 with loss 2.1946980953216553.\n",
      "Training epoch 2 batch 230 with loss 1.7812138795852661.\n",
      "Training epoch 2 batch 231 with loss 1.9660096168518066.\n",
      "Training epoch 2 batch 232 with loss 2.0692708492279053.\n",
      "Training epoch 2 batch 233 with loss 2.183161735534668.\n",
      "Training epoch 2 batch 234 with loss 1.9100582599639893.\n",
      "Training epoch 2 batch 235 with loss 1.9048107862472534.\n",
      "Training epoch 2 batch 236 with loss 1.6927893161773682.\n",
      "Training epoch 2 batch 237 with loss 1.546679139137268.\n",
      "Training epoch 2 batch 238 with loss 1.9207829236984253.\n",
      "Training epoch 2 batch 239 with loss 1.6455169916152954.\n",
      "Training epoch 2 batch 240 with loss 1.7548327445983887.\n",
      "Training epoch 2 batch 241 with loss 1.7005633115768433.\n",
      "Training epoch 2 batch 242 with loss 1.7805306911468506.\n",
      "Training epoch 2 batch 243 with loss 2.329157829284668.\n",
      "Training epoch 2 batch 244 with loss 1.6246713399887085.\n",
      "Training epoch 2 batch 245 with loss 1.583606243133545.\n",
      "Training epoch 2 batch 246 with loss 1.7495359182357788.\n",
      "Training epoch 2 batch 247 with loss 2.208209991455078.\n",
      "Training epoch 2 batch 248 with loss 1.7426109313964844.\n",
      "Training epoch 2 batch 249 with loss 1.83693265914917.\n",
      "Training epoch 2 batch 250 with loss 1.8730971813201904.\n",
      "Training epoch 2 batch 251 with loss 1.6753498315811157.\n",
      "Training epoch 2 batch 252 with loss 1.7932219505310059.\n",
      "Training epoch 2 batch 253 with loss 1.7514784336090088.\n",
      "Training epoch 2 batch 254 with loss 1.9821275472640991.\n",
      "Training epoch 2 batch 255 with loss 2.020352363586426.\n",
      "Training epoch 2 batch 256 with loss 1.7925044298171997.\n",
      "Training epoch 2 batch 257 with loss 1.9502958059310913.\n",
      "Training epoch 2 batch 258 with loss 2.01485276222229.\n",
      "Training epoch 2 batch 259 with loss 1.585131049156189.\n",
      "Training epoch 2 batch 260 with loss 1.887609839439392.\n",
      "Training epoch 2 batch 261 with loss 2.0016114711761475.\n",
      "Training epoch 2 batch 262 with loss 1.9903484582901.\n",
      "Training epoch 2 batch 263 with loss 1.6997809410095215.\n",
      "Training epoch 2 batch 264 with loss 1.828385591506958.\n",
      "Training epoch 2 batch 265 with loss 1.7777373790740967.\n",
      "Training epoch 2 batch 266 with loss 1.5269310474395752.\n",
      "Training epoch 2 batch 267 with loss 2.11173415184021.\n",
      "Training epoch 2 batch 268 with loss 2.0823934078216553.\n",
      "Training epoch 2 batch 269 with loss 2.009298801422119.\n",
      "Training epoch 2 batch 270 with loss 1.9958913326263428.\n",
      "Training epoch 2 batch 271 with loss 1.6913150548934937.\n",
      "Training epoch 2 batch 272 with loss 1.8305363655090332.\n",
      "Training epoch 2 batch 273 with loss 1.829818606376648.\n",
      "Training epoch 2 batch 274 with loss 1.8151997327804565.\n",
      "Training epoch 2 batch 275 with loss 2.0566766262054443.\n",
      "Training epoch 2 batch 276 with loss 1.8243852853775024.\n",
      "Training epoch 2 batch 277 with loss 1.7754079103469849.\n",
      "Training epoch 2 batch 278 with loss 1.9373379945755005.\n",
      "Training epoch 2 batch 279 with loss 1.4599876403808594.\n",
      "Training epoch 2 batch 280 with loss 1.7126119136810303.\n",
      "Training epoch 2 batch 281 with loss 1.9312782287597656.\n",
      "Training epoch 2 batch 282 with loss 1.6961613893508911.\n",
      "Training epoch 2 batch 283 with loss 1.9276875257492065.\n",
      "Training epoch 2 batch 284 with loss 1.9816728830337524.\n",
      "Training epoch 2 batch 285 with loss 1.995144009590149.\n",
      "Training epoch 2 batch 286 with loss 1.9776992797851562.\n",
      "Training epoch 2 batch 287 with loss 1.809425711631775.\n",
      "Training epoch 2 batch 288 with loss 2.1133439540863037.\n",
      "Training epoch 2 batch 289 with loss 1.8076608180999756.\n",
      "Training epoch 2 batch 290 with loss 1.8315143585205078.\n",
      "Training epoch 2 batch 291 with loss 1.706214189529419.\n",
      "Training epoch 2 batch 292 with loss 1.7886310815811157.\n",
      "Training epoch 2 batch 293 with loss 1.6996947526931763.\n",
      "Training epoch 2 batch 294 with loss 2.062716484069824.\n",
      "Training epoch 2 batch 295 with loss 2.0423381328582764.\n",
      "Training epoch 2 batch 296 with loss 2.044353723526001.\n",
      "Training epoch 2 batch 297 with loss 1.7924000024795532.\n",
      "Training epoch 2 batch 298 with loss 1.8235729932785034.\n",
      "Training epoch 2 batch 299 with loss 1.7378621101379395.\n",
      "Training epoch 2 batch 300 with loss 1.9132109880447388.\n",
      "Training epoch 2 batch 301 with loss 1.6710401773452759.\n",
      "Training epoch 2 batch 302 with loss 2.2562506198883057.\n",
      "Training epoch 2 batch 303 with loss 1.9179399013519287.\n",
      "Training epoch 2 batch 304 with loss 2.0938477516174316.\n",
      "Training epoch 2 batch 305 with loss 1.8786331415176392.\n",
      "Training epoch 2 batch 306 with loss 2.444373846054077.\n",
      "Training epoch 2 batch 307 with loss 1.9704586267471313.\n",
      "Training epoch 2 batch 308 with loss 1.7436274290084839.\n",
      "Training epoch 2 batch 309 with loss 1.8839869499206543.\n",
      "Training epoch 2 batch 310 with loss 2.2522873878479004.\n",
      "Training epoch 2 batch 311 with loss 1.9411909580230713.\n",
      "Training epoch 2 batch 312 with loss 2.1213388442993164.\n",
      "Training epoch 2 batch 313 with loss 1.8419667482376099.\n",
      "Training epoch 2 batch 314 with loss 2.1910910606384277.\n",
      "Training epoch 2 batch 315 with loss 2.003441333770752.\n",
      "Training epoch 2 batch 316 with loss 1.8020097017288208.\n",
      "Training epoch 2 batch 317 with loss 1.5822875499725342.\n",
      "Training epoch 2 batch 318 with loss 1.6806962490081787.\n",
      "Training epoch 2 batch 319 with loss 1.7886191606521606.\n",
      "Training epoch 2 batch 320 with loss 2.2368927001953125.\n",
      "Training epoch 2 batch 321 with loss 1.5395114421844482.\n",
      "Training epoch 2 batch 322 with loss 1.6462013721466064.\n",
      "Training epoch 2 batch 323 with loss 1.9515159130096436.\n",
      "Training epoch 2 batch 324 with loss 1.774387240409851.\n",
      "Training epoch 2 batch 325 with loss 2.2278025150299072.\n",
      "Training epoch 2 batch 326 with loss 1.9144395589828491.\n",
      "Training epoch 2 batch 327 with loss 1.7034167051315308.\n",
      "Training epoch 2 batch 328 with loss 2.0993361473083496.\n",
      "Training epoch 2 batch 329 with loss 1.8495737314224243.\n",
      "Training epoch 2 batch 330 with loss 2.034362554550171.\n",
      "Training epoch 2 batch 331 with loss 1.8983479738235474.\n",
      "Training epoch 2 batch 332 with loss 1.933968186378479.\n",
      "Training epoch 2 batch 333 with loss 1.69387686252594.\n",
      "Training epoch 2 batch 334 with loss 1.9196518659591675.\n",
      "Training epoch 2 batch 335 with loss 1.5311893224716187.\n",
      "Training epoch 2 batch 336 with loss 2.057406187057495.\n",
      "Training epoch 2 batch 337 with loss 1.5379714965820312.\n",
      "Training epoch 2 batch 338 with loss 1.9674702882766724.\n",
      "Training epoch 2 batch 339 with loss 1.7175977230072021.\n",
      "Training epoch 2 batch 340 with loss 1.9194711446762085.\n",
      "Training epoch 2 batch 341 with loss 2.155447483062744.\n",
      "Training epoch 2 batch 342 with loss 1.6342689990997314.\n",
      "Training epoch 2 batch 343 with loss 1.926526665687561.\n",
      "Training epoch 2 batch 344 with loss 2.012098789215088.\n",
      "Training epoch 2 batch 345 with loss 1.8646838665008545.\n",
      "Training epoch 2 batch 346 with loss 1.5592423677444458.\n",
      "Training epoch 2 batch 347 with loss 1.9276412725448608.\n",
      "Training epoch 2 batch 348 with loss 1.8987610340118408.\n",
      "Training epoch 2 batch 349 with loss 1.817121982574463.\n",
      "Training epoch 2 batch 350 with loss 1.949475646018982.\n",
      "Training epoch 2 batch 351 with loss 1.9919133186340332.\n",
      "Training epoch 2 batch 352 with loss 2.0512452125549316.\n",
      "Training epoch 2 batch 353 with loss 2.1240575313568115.\n",
      "Training epoch 2 batch 354 with loss 1.730934500694275.\n",
      "Training epoch 2 batch 355 with loss 2.026582956314087.\n",
      "Training epoch 2 batch 356 with loss 2.0025112628936768.\n",
      "Training epoch 2 batch 357 with loss 1.628564476966858.\n",
      "Training epoch 2 batch 358 with loss 1.8113385438919067.\n",
      "Training epoch 2 batch 359 with loss 2.214085817337036.\n",
      "Training epoch 2 batch 360 with loss 1.9837391376495361.\n",
      "Training epoch 2 batch 361 with loss 1.748360276222229.\n",
      "Training epoch 2 batch 362 with loss 1.8466851711273193.\n",
      "Training epoch 2 batch 363 with loss 1.8729454278945923.\n",
      "Training epoch 2 batch 364 with loss 2.144516706466675.\n",
      "Training epoch 2 batch 365 with loss 1.8392581939697266.\n",
      "Training epoch 2 batch 366 with loss 1.8445849418640137.\n",
      "Training epoch 2 batch 367 with loss 1.5260573625564575.\n",
      "Training epoch 2 batch 368 with loss 1.633037805557251.\n",
      "Training epoch 2 batch 369 with loss 2.3233299255371094.\n",
      "Training epoch 2 batch 370 with loss 1.6106477975845337.\n",
      "Training epoch 2 batch 371 with loss 1.7118892669677734.\n",
      "Training epoch 2 batch 372 with loss 1.9754825830459595.\n",
      "Training epoch 2 batch 373 with loss 1.5295580625534058.\n",
      "Training epoch 2 batch 374 with loss 1.7387703657150269.\n",
      "Training epoch 2 batch 375 with loss 1.9465173482894897.\n",
      "Training epoch 2 batch 376 with loss 1.8312116861343384.\n",
      "Training epoch 2 batch 377 with loss 2.0192573070526123.\n",
      "Training epoch 2 batch 378 with loss 1.5238735675811768.\n",
      "Training epoch 2 batch 379 with loss 2.171434164047241.\n",
      "Training epoch 2 batch 380 with loss 2.084601402282715.\n",
      "Training epoch 2 batch 381 with loss 2.0220420360565186.\n",
      "Training epoch 2 batch 382 with loss 2.0343995094299316.\n",
      "Training epoch 2 batch 383 with loss 2.0531890392303467.\n",
      "Training epoch 2 batch 384 with loss 1.9702098369598389.\n",
      "Training epoch 2 batch 385 with loss 2.055647850036621.\n",
      "Training epoch 2 batch 386 with loss 2.1482362747192383.\n",
      "Training epoch 2 batch 387 with loss 1.6885168552398682.\n",
      "Training epoch 2 batch 388 with loss 1.5445725917816162.\n",
      "Training epoch 2 batch 389 with loss 1.5813573598861694.\n",
      "Training epoch 2 batch 390 with loss 2.0740199089050293.\n",
      "Training epoch 2 batch 391 with loss 1.3427386283874512.\n",
      "Training epoch 2 batch 392 with loss 1.3895187377929688.\n",
      "Training epoch 2 batch 393 with loss 2.011807918548584.\n",
      "Training epoch 2 batch 394 with loss 2.046334743499756.\n",
      "Training epoch 2 batch 395 with loss 2.1323487758636475.\n",
      "Training epoch 2 batch 396 with loss 1.878529667854309.\n",
      "Training epoch 2 batch 397 with loss 2.0833499431610107.\n",
      "Training epoch 2 batch 398 with loss 1.8480925559997559.\n",
      "Training epoch 2 batch 399 with loss 2.095468521118164.\n",
      "Training epoch 2 batch 400 with loss 1.8324168920516968.\n",
      "Training epoch 2 batch 401 with loss 2.1285340785980225.\n",
      "Training epoch 2 batch 402 with loss 1.9036014080047607.\n",
      "Training epoch 2 batch 403 with loss 2.050417423248291.\n",
      "Training epoch 2 batch 404 with loss 1.9745539426803589.\n",
      "Training epoch 2 batch 405 with loss 2.1211049556732178.\n",
      "Training epoch 2 batch 406 with loss 2.094750165939331.\n",
      "Training epoch 2 batch 407 with loss 1.8299832344055176.\n",
      "Training epoch 2 batch 408 with loss 2.1807501316070557.\n",
      "Training epoch 2 batch 409 with loss 1.7218469381332397.\n",
      "Training epoch 2 batch 410 with loss 2.031956911087036.\n",
      "Training epoch 2 batch 411 with loss 2.1069109439849854.\n",
      "Training epoch 2 batch 412 with loss 1.838627576828003.\n",
      "Training epoch 2 batch 413 with loss 1.8146674633026123.\n",
      "Training epoch 2 batch 414 with loss 1.7841545343399048.\n",
      "Training epoch 2 batch 415 with loss 1.9231246709823608.\n",
      "Training epoch 2 batch 416 with loss 1.9626768827438354.\n",
      "Training epoch 2 batch 417 with loss 1.9681038856506348.\n",
      "Training epoch 2 batch 418 with loss 1.9723848104476929.\n",
      "Training epoch 2 batch 419 with loss 2.249368190765381.\n",
      "Training epoch 2 batch 420 with loss 1.7051054239273071.\n",
      "Training epoch 2 batch 421 with loss 1.684757947921753.\n",
      "Training epoch 2 batch 422 with loss 2.1903066635131836.\n",
      "Training epoch 2 batch 423 with loss 1.9625802040100098.\n",
      "Training epoch 2 batch 424 with loss 1.5402041673660278.\n",
      "Training epoch 2 batch 425 with loss 2.1414544582366943.\n",
      "Training epoch 2 batch 426 with loss 1.957963228225708.\n",
      "Training epoch 2 batch 427 with loss 1.497809886932373.\n",
      "Training epoch 2 batch 428 with loss 1.7345455884933472.\n",
      "Training epoch 2 batch 429 with loss 2.1787948608398438.\n",
      "Training epoch 2 batch 430 with loss 2.212385416030884.\n",
      "Training epoch 2 batch 431 with loss 2.206909418106079.\n",
      "Training epoch 2 batch 432 with loss 2.018822193145752.\n",
      "Training epoch 2 batch 433 with loss 1.786217212677002.\n",
      "Training epoch 2 batch 434 with loss 2.159085988998413.\n",
      "Training epoch 2 batch 435 with loss 1.9619815349578857.\n",
      "Training epoch 2 batch 436 with loss 2.010159492492676.\n",
      "Training epoch 2 batch 437 with loss 2.3569064140319824.\n",
      "Training epoch 2 batch 438 with loss 1.6422172784805298.\n",
      "Training epoch 2 batch 439 with loss 2.1311519145965576.\n",
      "Training epoch 2 batch 440 with loss 2.211852550506592.\n",
      "Training epoch 2 batch 441 with loss 1.7122207880020142.\n",
      "Training epoch 2 batch 442 with loss 1.7092678546905518.\n",
      "Training epoch 2 batch 443 with loss 1.8521790504455566.\n",
      "Training epoch 2 batch 444 with loss 2.0397963523864746.\n",
      "Training epoch 2 batch 445 with loss 1.8698067665100098.\n",
      "Training epoch 2 batch 446 with loss 1.8567532300949097.\n",
      "Training epoch 2 batch 447 with loss 2.2803726196289062.\n",
      "Training epoch 2 batch 448 with loss 1.9039745330810547.\n",
      "Training epoch 2 batch 449 with loss 2.065585136413574.\n",
      "Training epoch 2 batch 450 with loss 1.663654088973999.\n",
      "Training epoch 2 batch 451 with loss 2.2033867835998535.\n",
      "Training epoch 2 batch 452 with loss 1.9403626918792725.\n",
      "Training epoch 2 batch 453 with loss 2.0702736377716064.\n",
      "Training epoch 2 batch 454 with loss 1.746337652206421.\n",
      "Training epoch 2 batch 455 with loss 1.8154752254486084.\n",
      "Training epoch 2 batch 456 with loss 1.728083848953247.\n",
      "Training epoch 2 batch 457 with loss 2.2110776901245117.\n",
      "Training epoch 2 batch 458 with loss 2.076533794403076.\n",
      "Training epoch 2 batch 459 with loss 2.537867784500122.\n",
      "Training epoch 2 batch 460 with loss 1.9338799715042114.\n",
      "Training epoch 2 batch 461 with loss 1.892520785331726.\n",
      "Training epoch 2 batch 462 with loss 1.9025077819824219.\n",
      "Training epoch 2 batch 463 with loss 2.063121795654297.\n",
      "Training epoch 2 batch 464 with loss 2.1899516582489014.\n",
      "Training epoch 2 batch 465 with loss 1.8813618421554565.\n",
      "Training epoch 2 batch 466 with loss 2.48533034324646.\n",
      "Training epoch 2 batch 467 with loss 1.8791059255599976.\n",
      "Training epoch 2 batch 468 with loss 2.0458273887634277.\n",
      "Training epoch 2 batch 469 with loss 1.5817360877990723.\n",
      "Training epoch 2 batch 470 with loss 2.2813031673431396.\n",
      "Training epoch 2 batch 471 with loss 1.6646424531936646.\n",
      "Training epoch 2 batch 472 with loss 1.8419201374053955.\n",
      "Training epoch 2 batch 473 with loss 1.9976004362106323.\n",
      "Training epoch 2 batch 474 with loss 2.5418574810028076.\n",
      "Training epoch 2 batch 475 with loss 2.293072462081909.\n",
      "Training epoch 2 batch 476 with loss 2.064908266067505.\n",
      "Training epoch 2 batch 477 with loss 2.2177443504333496.\n",
      "Training epoch 2 batch 478 with loss 2.110232353210449.\n",
      "Training epoch 2 batch 479 with loss 2.035586357116699.\n",
      "Training epoch 2 batch 480 with loss 2.043943405151367.\n",
      "Training epoch 2 batch 481 with loss 1.765148639678955.\n",
      "Training epoch 2 batch 482 with loss 2.2876055240631104.\n",
      "Training epoch 2 batch 483 with loss 1.8537182807922363.\n",
      "Training epoch 2 batch 484 with loss 2.0232672691345215.\n",
      "Training epoch 2 batch 485 with loss 1.8755587339401245.\n",
      "Training epoch 2 batch 486 with loss 1.9242193698883057.\n",
      "Training epoch 2 batch 487 with loss 1.510347604751587.\n",
      "Training epoch 2 batch 488 with loss 1.6658707857131958.\n",
      "Training epoch 2 batch 489 with loss 1.8469722270965576.\n",
      "Training epoch 2 batch 490 with loss 1.907246470451355.\n",
      "Training epoch 2 batch 491 with loss 1.9016603231430054.\n",
      "Training epoch 2 batch 492 with loss 1.972230315208435.\n",
      "Training epoch 2 batch 493 with loss 2.1105031967163086.\n",
      "Training epoch 2 batch 494 with loss 2.3007097244262695.\n",
      "Training epoch 2 batch 495 with loss 1.8093990087509155.\n",
      "Training epoch 2 batch 496 with loss 2.061041831970215.\n",
      "Training epoch 2 batch 497 with loss 1.7781884670257568.\n",
      "Training epoch 2 batch 498 with loss 1.9536062479019165.\n",
      "Training epoch 2 batch 499 with loss 1.9179366827011108.\n",
      "Training epoch 2 batch 500 with loss 1.8927747011184692.\n",
      "Training epoch 2 batch 501 with loss 1.6037877798080444.\n",
      "Training epoch 2 batch 502 with loss 2.1332788467407227.\n",
      "Training epoch 2 batch 503 with loss 2.4631738662719727.\n",
      "Training epoch 2 batch 504 with loss 2.1121697425842285.\n",
      "Training epoch 2 batch 505 with loss 2.3360581398010254.\n",
      "Training epoch 2 batch 506 with loss 2.1015138626098633.\n",
      "Training epoch 2 batch 507 with loss 1.9691803455352783.\n",
      "Training epoch 2 batch 508 with loss 1.968614101409912.\n",
      "Training epoch 2 batch 509 with loss 1.8937442302703857.\n",
      "Training epoch 2 batch 510 with loss 2.2908053398132324.\n",
      "Training epoch 2 batch 511 with loss 2.000836133956909.\n",
      "Training epoch 2 batch 512 with loss 1.5907683372497559.\n",
      "Training epoch 2 batch 513 with loss 2.2446882724761963.\n",
      "Training epoch 2 batch 514 with loss 2.197917938232422.\n",
      "Training epoch 2 batch 515 with loss 2.1141557693481445.\n",
      "Training epoch 2 batch 516 with loss 2.027111291885376.\n",
      "Training epoch 2 batch 517 with loss 2.2760589122772217.\n",
      "Training epoch 2 batch 518 with loss 1.664573073387146.\n",
      "Training epoch 2 batch 519 with loss 2.2330243587493896.\n",
      "Training epoch 2 batch 520 with loss 2.04227352142334.\n",
      "Training epoch 2 batch 521 with loss 1.869775414466858.\n",
      "Training epoch 2 batch 522 with loss 1.7138327360153198.\n",
      "Training epoch 2 batch 523 with loss 1.9401092529296875.\n",
      "Training epoch 2 batch 524 with loss 2.063556432723999.\n",
      "Training epoch 2 batch 525 with loss 1.675898790359497.\n",
      "Training epoch 2 batch 526 with loss 1.9884898662567139.\n",
      "Training epoch 2 batch 527 with loss 1.7522286176681519.\n",
      "Training epoch 2 batch 528 with loss 1.9315184354782104.\n",
      "Training epoch 2 batch 529 with loss 1.7998313903808594.\n",
      "Training epoch 2 batch 530 with loss 1.9588178396224976.\n",
      "Training epoch 2 batch 531 with loss 2.047215461730957.\n",
      "Training epoch 2 batch 532 with loss 2.0270936489105225.\n",
      "Training epoch 2 batch 533 with loss 1.8520318269729614.\n",
      "Training epoch 2 batch 534 with loss 1.9808546304702759.\n",
      "Training epoch 2 batch 535 with loss 2.0508482456207275.\n",
      "Training epoch 2 batch 536 with loss 2.1493544578552246.\n",
      "Training epoch 2 batch 537 with loss 2.085869312286377.\n",
      "Training epoch 2 batch 538 with loss 1.5579791069030762.\n",
      "Training epoch 2 batch 539 with loss 1.961909532546997.\n",
      "Training epoch 2 batch 540 with loss 2.150235891342163.\n",
      "Training epoch 2 batch 541 with loss 2.1543078422546387.\n",
      "Training epoch 2 batch 542 with loss 1.8999274969100952.\n",
      "Training epoch 2 batch 543 with loss 1.7859803438186646.\n",
      "Training epoch 2 batch 544 with loss 1.8408610820770264.\n",
      "Training epoch 2 batch 545 with loss 1.9953066110610962.\n",
      "Training epoch 2 batch 546 with loss 1.7409008741378784.\n",
      "Training epoch 2 batch 547 with loss 2.3000905513763428.\n",
      "Training epoch 2 batch 548 with loss 2.04911208152771.\n",
      "Training epoch 2 batch 549 with loss 2.1508069038391113.\n",
      "Training epoch 2 batch 550 with loss 1.9029946327209473.\n",
      "Training epoch 2 batch 551 with loss 2.2634308338165283.\n",
      "Training epoch 2 batch 552 with loss 1.9668633937835693.\n",
      "Training epoch 2 batch 553 with loss 1.943138599395752.\n",
      "Training epoch 2 batch 554 with loss 2.2872209548950195.\n",
      "Training epoch 2 batch 555 with loss 2.1685080528259277.\n",
      "Training epoch 2 batch 556 with loss 2.3616459369659424.\n",
      "Training epoch 2 batch 557 with loss 2.619738817214966.\n",
      "Training epoch 2 batch 558 with loss 1.622294545173645.\n",
      "Training epoch 2 batch 559 with loss 1.9894505739212036.\n",
      "Training epoch 2 batch 560 with loss 1.5533207654953003.\n",
      "Training epoch 2 batch 561 with loss 2.275151252746582.\n",
      "Training epoch 2 batch 562 with loss 1.8694754838943481.\n",
      "Training epoch 2 batch 563 with loss 1.757079839706421.\n",
      "Training epoch 2 batch 564 with loss 1.8191773891448975.\n",
      "Training epoch 2 batch 565 with loss 1.6171989440917969.\n",
      "Training epoch 2 batch 566 with loss 2.0085110664367676.\n",
      "Training epoch 2 batch 567 with loss 1.781385898590088.\n",
      "Training epoch 2 batch 568 with loss 2.2216029167175293.\n",
      "Training epoch 2 batch 569 with loss 2.1997218132019043.\n",
      "Training epoch 2 batch 570 with loss 2.2097585201263428.\n",
      "Training epoch 2 batch 571 with loss 1.8753116130828857.\n",
      "Training epoch 2 batch 572 with loss 2.2714779376983643.\n",
      "Training epoch 2 batch 573 with loss 2.0331363677978516.\n",
      "Training epoch 2 batch 574 with loss 2.3019859790802.\n",
      "Training epoch 2 batch 575 with loss 1.9257378578186035.\n",
      "Training epoch 2 batch 576 with loss 2.2341978549957275.\n",
      "Training epoch 2 batch 577 with loss 2.022212028503418.\n",
      "Training epoch 2 batch 578 with loss 2.4559597969055176.\n",
      "Training epoch 2 batch 579 with loss 2.0704379081726074.\n",
      "Training epoch 2 batch 580 with loss 2.0294296741485596.\n",
      "Training epoch 2 batch 581 with loss 2.1246426105499268.\n",
      "Training epoch 2 batch 582 with loss 1.9081708192825317.\n",
      "Training epoch 2 batch 583 with loss 2.3154242038726807.\n",
      "Training epoch 2 batch 584 with loss 1.5368549823760986.\n",
      "Training epoch 2 batch 585 with loss 1.8476084470748901.\n",
      "Training epoch 2 batch 586 with loss 1.966552734375.\n",
      "Training epoch 2 batch 587 with loss 2.0700480937957764.\n",
      "Training epoch 2 batch 588 with loss 2.054811716079712.\n",
      "Training epoch 2 batch 589 with loss 1.9108563661575317.\n",
      "Training epoch 2 batch 590 with loss 1.6497035026550293.\n",
      "Training epoch 2 batch 591 with loss 2.191744565963745.\n",
      "Training epoch 2 batch 592 with loss 2.025956630706787.\n",
      "Training epoch 2 batch 593 with loss 2.005708694458008.\n",
      "Training epoch 2 batch 594 with loss 1.8804547786712646.\n",
      "Training epoch 2 batch 595 with loss 1.6895231008529663.\n",
      "Training epoch 2 batch 596 with loss 2.2293128967285156.\n",
      "Training epoch 2 batch 597 with loss 2.35564923286438.\n",
      "Training epoch 2 batch 598 with loss 1.7155638933181763.\n",
      "Training epoch 2 batch 599 with loss 2.0059351921081543.\n",
      "Training epoch 2 batch 600 with loss 1.6333975791931152.\n",
      "Training epoch 2 batch 601 with loss 1.93039071559906.\n",
      "Training epoch 2 batch 602 with loss 1.7571299076080322.\n",
      "Training epoch 2 batch 603 with loss 2.01533579826355.\n",
      "Training epoch 2 batch 604 with loss 1.727334976196289.\n",
      "Training epoch 2 batch 605 with loss 2.1177380084991455.\n",
      "Training epoch 2 batch 606 with loss 2.140312910079956.\n",
      "Training epoch 2 batch 607 with loss 2.041161298751831.\n",
      "Training epoch 2 batch 608 with loss 2.4472553730010986.\n",
      "Training epoch 2 batch 609 with loss 1.9256571531295776.\n",
      "Training epoch 2 batch 610 with loss 1.7383308410644531.\n",
      "Training epoch 2 batch 611 with loss 2.078383445739746.\n",
      "Training epoch 2 batch 612 with loss 2.0289814472198486.\n",
      "Training epoch 2 batch 613 with loss 1.8584825992584229.\n",
      "Training epoch 2 batch 614 with loss 1.93909752368927.\n",
      "Training epoch 2 batch 615 with loss 2.506525754928589.\n",
      "Training epoch 2 batch 616 with loss 2.3928940296173096.\n",
      "Training epoch 2 batch 617 with loss 2.065795660018921.\n",
      "Training epoch 2 batch 618 with loss 1.8651421070098877.\n",
      "Training epoch 2 batch 619 with loss 1.835763931274414.\n",
      "Training epoch 2 batch 620 with loss 1.727107286453247.\n",
      "Training epoch 2 batch 621 with loss 1.9143742322921753.\n",
      "Training epoch 2 batch 622 with loss 1.9164987802505493.\n",
      "Training epoch 2 batch 623 with loss 2.297044515609741.\n",
      "Training epoch 2 batch 624 with loss 2.1761631965637207.\n",
      "Training epoch 2 batch 625 with loss 2.3674850463867188.\n",
      "Training epoch 2 batch 626 with loss 1.8284341096878052.\n",
      "Training epoch 2 batch 627 with loss 1.782314658164978.\n",
      "Training epoch 2 batch 628 with loss 1.6835469007492065.\n",
      "Training epoch 2 batch 629 with loss 2.866865873336792.\n",
      "Training epoch 2 batch 630 with loss 2.0255134105682373.\n",
      "Training epoch 2 batch 631 with loss 2.131746530532837.\n",
      "Training epoch 2 batch 632 with loss 2.372985601425171.\n",
      "Training epoch 2 batch 633 with loss 2.1489624977111816.\n",
      "Training epoch 2 batch 634 with loss 2.567437171936035.\n",
      "Training epoch 2 batch 635 with loss 2.023434638977051.\n",
      "Training epoch 2 batch 636 with loss 2.373317003250122.\n",
      "Training epoch 2 batch 637 with loss 2.027090549468994.\n",
      "Training epoch 2 batch 638 with loss 2.326068878173828.\n",
      "Training epoch 2 batch 639 with loss 2.3947858810424805.\n",
      "Training epoch 2 batch 640 with loss 1.7847609519958496.\n",
      "Training epoch 2 batch 641 with loss 1.8553773164749146.\n",
      "Training epoch 2 batch 642 with loss 1.8837226629257202.\n",
      "Training epoch 2 batch 643 with loss 2.340583324432373.\n",
      "Training epoch 2 batch 644 with loss 2.317998170852661.\n",
      "Training epoch 2 batch 645 with loss 2.0327579975128174.\n",
      "Training epoch 2 batch 646 with loss 2.064518451690674.\n",
      "Training epoch 2 batch 647 with loss 2.176583766937256.\n",
      "Training epoch 2 batch 648 with loss 2.355860710144043.\n",
      "Training epoch 2 batch 649 with loss 1.9064669609069824.\n",
      "Training epoch 2 batch 650 with loss 2.150373935699463.\n",
      "Training epoch 2 batch 651 with loss 2.0958378314971924.\n",
      "Training epoch 2 batch 652 with loss 2.046572208404541.\n",
      "Training epoch 2 batch 653 with loss 1.6220718622207642.\n",
      "Training epoch 2 batch 654 with loss 2.054643154144287.\n",
      "Training epoch 2 batch 655 with loss 2.421511173248291.\n",
      "Training epoch 2 batch 656 with loss 1.590470790863037.\n",
      "Training epoch 2 batch 657 with loss 2.220275402069092.\n",
      "Training epoch 2 batch 658 with loss 2.073910713195801.\n",
      "Training epoch 2 batch 659 with loss 2.5060653686523438.\n",
      "Training epoch 2 batch 660 with loss 2.1929116249084473.\n",
      "Training epoch 2 batch 661 with loss 1.7881500720977783.\n",
      "Training epoch 2 batch 662 with loss 2.025869846343994.\n",
      "Training epoch 2 batch 663 with loss 1.6516356468200684.\n",
      "Training epoch 2 batch 664 with loss 1.5765042304992676.\n",
      "Training epoch 2 batch 665 with loss 2.2891948223114014.\n",
      "Training epoch 2 batch 666 with loss 1.9882789850234985.\n",
      "Training epoch 2 batch 667 with loss 1.7147789001464844.\n",
      "Training epoch 2 batch 668 with loss 2.245903968811035.\n",
      "Training epoch 2 batch 669 with loss 2.1973986625671387.\n",
      "Training epoch 2 batch 670 with loss 2.0482804775238037.\n",
      "Training epoch 2 batch 671 with loss 2.3486733436584473.\n",
      "Training epoch 2 batch 672 with loss 2.2828900814056396.\n",
      "Training epoch 2 batch 673 with loss 1.8707195520401.\n",
      "Training epoch 2 batch 674 with loss 2.430464267730713.\n",
      "Training epoch 2 batch 675 with loss 2.498082399368286.\n",
      "Training epoch 2 batch 676 with loss 2.0119807720184326.\n",
      "Training epoch 2 batch 677 with loss 2.2617344856262207.\n",
      "Training epoch 2 batch 678 with loss 2.1350393295288086.\n",
      "Training epoch 2 batch 679 with loss 2.1728479862213135.\n",
      "Training epoch 2 batch 680 with loss 2.317352294921875.\n",
      "Training epoch 2 batch 681 with loss 1.9034990072250366.\n",
      "Training epoch 2 batch 682 with loss 1.7854143381118774.\n",
      "Training epoch 2 batch 683 with loss 2.8454251289367676.\n",
      "Training epoch 2 batch 684 with loss 2.4270644187927246.\n",
      "Training epoch 2 batch 685 with loss 1.9579836130142212.\n",
      "Training epoch 2 batch 686 with loss 2.2297475337982178.\n",
      "Training epoch 2 batch 687 with loss 1.930334210395813.\n",
      "Training epoch 2 batch 688 with loss 1.8146835565567017.\n",
      "Training epoch 2 batch 689 with loss 2.0419771671295166.\n",
      "Training epoch 2 batch 690 with loss 2.4386813640594482.\n",
      "Training epoch 2 batch 691 with loss 2.1862645149230957.\n",
      "Training epoch 2 batch 692 with loss 2.4098095893859863.\n",
      "Training epoch 2 batch 693 with loss 2.187525987625122.\n",
      "Training epoch 2 batch 694 with loss 2.2345869541168213.\n",
      "Training epoch 2 batch 695 with loss 2.0250003337860107.\n",
      "Training epoch 2 batch 696 with loss 1.9650599956512451.\n",
      "Training epoch 2 batch 697 with loss 1.8523838520050049.\n",
      "Training epoch 2 batch 698 with loss 1.6903254985809326.\n",
      "Training epoch 2 batch 699 with loss 2.088245153427124.\n",
      "Training epoch 2 batch 700 with loss 2.008924961090088.\n",
      "Training epoch 2 batch 701 with loss 1.8040156364440918.\n",
      "Training epoch 2 batch 702 with loss 2.0401360988616943.\n",
      "Training epoch 2 batch 703 with loss 2.012232780456543.\n",
      "Training epoch 2 batch 704 with loss 2.2896647453308105.\n",
      "Training epoch 2 batch 705 with loss 2.4018940925598145.\n",
      "Training epoch 2 batch 706 with loss 1.969193935394287.\n",
      "Training epoch 2 batch 707 with loss 1.8380272388458252.\n",
      "Training epoch 2 batch 708 with loss 2.0812041759490967.\n",
      "Training epoch 2 batch 709 with loss 2.7473788261413574.\n",
      "Training epoch 2 batch 710 with loss 1.9780983924865723.\n",
      "Training epoch 2 batch 711 with loss 1.8861807584762573.\n",
      "Training epoch 2 batch 712 with loss 2.1129956245422363.\n",
      "Training epoch 2 batch 713 with loss 2.2723634243011475.\n",
      "Training epoch 2 batch 714 with loss 1.6724951267242432.\n",
      "Training epoch 2 batch 715 with loss 2.1547017097473145.\n",
      "Training epoch 2 batch 716 with loss 1.9227012395858765.\n",
      "Training epoch 2 batch 717 with loss 2.480881929397583.\n",
      "Training epoch 2 batch 718 with loss 1.9801743030548096.\n",
      "Training epoch 2 batch 719 with loss 2.056647777557373.\n",
      "Training epoch 2 batch 720 with loss 2.0781149864196777.\n",
      "Training epoch 2 batch 721 with loss 1.846631407737732.\n",
      "Training epoch 2 batch 722 with loss 1.779322862625122.\n",
      "Training epoch 2 batch 723 with loss 2.3971962928771973.\n",
      "Training epoch 2 batch 724 with loss 2.134260416030884.\n",
      "Training epoch 2 batch 725 with loss 1.9116817712783813.\n",
      "Training epoch 2 batch 726 with loss 1.9128985404968262.\n",
      "Training epoch 2 batch 727 with loss 2.2535276412963867.\n",
      "Training epoch 2 batch 728 with loss 1.8556816577911377.\n",
      "Training epoch 2 batch 729 with loss 2.1311750411987305.\n",
      "Training epoch 2 batch 730 with loss 1.8804882764816284.\n",
      "Training epoch 2 batch 731 with loss 2.1024179458618164.\n",
      "Training epoch 2 batch 732 with loss 2.123664379119873.\n",
      "Training epoch 2 batch 733 with loss 1.7747483253479004.\n",
      "Training epoch 2 batch 734 with loss 2.1761326789855957.\n",
      "Training epoch 2 batch 735 with loss 2.0574750900268555.\n",
      "Training epoch 2 batch 736 with loss 2.1881673336029053.\n",
      "Training epoch 2 batch 737 with loss 2.107154607772827.\n",
      "Training epoch 2 batch 738 with loss 2.248033285140991.\n",
      "Training epoch 2 batch 739 with loss 1.8628541231155396.\n",
      "Training epoch 2 batch 740 with loss 2.2885665893554688.\n",
      "Training epoch 2 batch 741 with loss 2.0783770084381104.\n",
      "Training epoch 2 batch 742 with loss 1.9650155305862427.\n",
      "Training epoch 2 batch 743 with loss 2.169407844543457.\n",
      "Training epoch 2 batch 744 with loss 1.8283019065856934.\n",
      "Training epoch 2 batch 745 with loss 2.202132225036621.\n",
      "Training epoch 2 batch 746 with loss 2.116359233856201.\n",
      "Training epoch 2 batch 747 with loss 2.4124250411987305.\n",
      "Training epoch 2 batch 748 with loss 2.245051145553589.\n",
      "Training epoch 2 batch 749 with loss 1.7008424997329712.\n",
      "Training epoch 2 batch 750 with loss 2.37009334564209.\n",
      "Training epoch 2 batch 751 with loss 2.1542065143585205.\n",
      "Training epoch 2 batch 752 with loss 2.1505138874053955.\n",
      "Training epoch 2 batch 753 with loss 2.0184245109558105.\n",
      "Training epoch 2 batch 754 with loss 2.0185999870300293.\n",
      "Training epoch 2 batch 755 with loss 2.3091423511505127.\n",
      "Training epoch 2 batch 756 with loss 2.161137580871582.\n",
      "Training epoch 2 batch 757 with loss 2.268768310546875.\n",
      "Training epoch 2 batch 758 with loss 2.250107526779175.\n",
      "Training epoch 2 batch 759 with loss 1.9803797006607056.\n",
      "Training epoch 2 batch 760 with loss 1.7960002422332764.\n",
      "Training epoch 2 batch 761 with loss 1.9944695234298706.\n",
      "Training epoch 2 batch 762 with loss 2.0125932693481445.\n",
      "Training epoch 2 batch 763 with loss 2.2169852256774902.\n",
      "Training epoch 2 batch 764 with loss 2.0139057636260986.\n",
      "Training epoch 2 batch 765 with loss 1.9956059455871582.\n",
      "Training epoch 2 batch 766 with loss 2.192735433578491.\n",
      "Training epoch 2 batch 767 with loss 2.2273595333099365.\n",
      "Training epoch 2 batch 768 with loss 1.8681366443634033.\n",
      "Training epoch 2 batch 769 with loss 1.9478685855865479.\n",
      "Training epoch 2 batch 770 with loss 2.054650068283081.\n",
      "Training epoch 2 batch 771 with loss 2.1140005588531494.\n",
      "Training epoch 2 batch 772 with loss 2.0166749954223633.\n",
      "Training epoch 2 batch 773 with loss 1.6956348419189453.\n",
      "Training epoch 2 batch 774 with loss 2.538041114807129.\n",
      "Training epoch 2 batch 775 with loss 2.0094237327575684.\n",
      "Training epoch 2 batch 776 with loss 1.5949783325195312.\n",
      "Training epoch 2 batch 777 with loss 1.5366261005401611.\n",
      "Training epoch 2 batch 778 with loss 1.7529315948486328.\n",
      "Training epoch 2 batch 779 with loss 2.087700605392456.\n",
      "Training epoch 2 batch 780 with loss 2.210059881210327.\n",
      "Training epoch 2 batch 781 with loss 2.3578298091888428.\n",
      "Training epoch 2 batch 782 with loss 1.9844669103622437.\n",
      "Training epoch 2 batch 783 with loss 1.8046705722808838.\n",
      "Training epoch 2 batch 784 with loss 2.2265048027038574.\n",
      "Training epoch 2 batch 785 with loss 2.57255220413208.\n",
      "Training epoch 2 batch 786 with loss 2.089846611022949.\n",
      "Training epoch 2 batch 787 with loss 2.047149896621704.\n",
      "Training epoch 2 batch 788 with loss 2.226889133453369.\n",
      "Training epoch 2 batch 789 with loss 2.061436414718628.\n",
      "Training epoch 2 batch 790 with loss 2.2724733352661133.\n",
      "Training epoch 2 batch 791 with loss 1.8263821601867676.\n",
      "Training epoch 2 batch 792 with loss 1.9721505641937256.\n",
      "Training epoch 2 batch 793 with loss 2.2886478900909424.\n",
      "Training epoch 2 batch 794 with loss 2.0371615886688232.\n",
      "Training epoch 2 batch 795 with loss 2.135786533355713.\n",
      "Training epoch 2 batch 796 with loss 2.677872657775879.\n",
      "Training epoch 2 batch 797 with loss 1.8309962749481201.\n",
      "Training epoch 2 batch 798 with loss 2.524564504623413.\n",
      "Training epoch 2 batch 799 with loss 1.7544101476669312.\n",
      "Training epoch 2 batch 800 with loss 2.236884832382202.\n",
      "Training epoch 2 batch 801 with loss 2.3089945316314697.\n",
      "Training epoch 2 batch 802 with loss 2.2898805141448975.\n",
      "Training epoch 2 batch 803 with loss 1.9475042819976807.\n",
      "Training epoch 2 batch 804 with loss 2.3783485889434814.\n",
      "Training epoch 2 batch 805 with loss 1.499475121498108.\n",
      "Training epoch 2 batch 806 with loss 1.8496525287628174.\n",
      "Training epoch 2 batch 807 with loss 2.1263649463653564.\n",
      "Training epoch 2 batch 808 with loss 1.9675042629241943.\n",
      "Training epoch 2 batch 809 with loss 2.0767440795898438.\n",
      "Training epoch 2 batch 810 with loss 2.4636311531066895.\n",
      "Training epoch 2 batch 811 with loss 2.21359920501709.\n",
      "Training epoch 2 batch 812 with loss 1.8605819940567017.\n",
      "Training epoch 2 batch 813 with loss 2.1056251525878906.\n",
      "Training epoch 2 batch 814 with loss 2.0871005058288574.\n",
      "Training epoch 2 batch 815 with loss 2.1346938610076904.\n",
      "Training epoch 2 batch 816 with loss 1.7836426496505737.\n",
      "Training epoch 2 batch 817 with loss 2.2409138679504395.\n",
      "Training epoch 2 batch 818 with loss 2.0709757804870605.\n",
      "Training epoch 2 batch 819 with loss 1.8603092432022095.\n",
      "Training epoch 2 batch 820 with loss 2.315173625946045.\n",
      "Training epoch 2 batch 821 with loss 1.836717963218689.\n",
      "Training epoch 2 batch 822 with loss 2.2960851192474365.\n",
      "Training epoch 2 batch 823 with loss 2.006098985671997.\n",
      "Training epoch 2 batch 824 with loss 2.1657121181488037.\n",
      "Training epoch 2 batch 825 with loss 1.658932089805603.\n",
      "Training epoch 2 batch 826 with loss 1.9765856266021729.\n",
      "Training epoch 2 batch 827 with loss 2.2098653316497803.\n",
      "Training epoch 2 batch 828 with loss 2.162494421005249.\n",
      "Training epoch 2 batch 829 with loss 1.948245882987976.\n",
      "Training epoch 2 batch 830 with loss 2.2153847217559814.\n",
      "Training epoch 2 batch 831 with loss 2.1076598167419434.\n",
      "Training epoch 2 batch 832 with loss 1.8293169736862183.\n",
      "Training epoch 2 batch 833 with loss 2.621276617050171.\n",
      "Training epoch 2 batch 834 with loss 1.8535062074661255.\n",
      "Training epoch 2 batch 835 with loss 2.1017825603485107.\n",
      "Training epoch 2 batch 836 with loss 1.885617733001709.\n",
      "Training epoch 2 batch 837 with loss 2.147317409515381.\n",
      "Training epoch 2 batch 838 with loss 2.131568193435669.\n",
      "Training epoch 2 batch 839 with loss 2.0568225383758545.\n",
      "Training epoch 2 batch 840 with loss 2.1543238162994385.\n",
      "Training epoch 2 batch 841 with loss 1.9998470544815063.\n",
      "Training epoch 2 batch 842 with loss 2.054621934890747.\n",
      "Training epoch 2 batch 843 with loss 2.161667823791504.\n",
      "Training epoch 2 batch 844 with loss 2.093918561935425.\n",
      "Training epoch 2 batch 845 with loss 2.000095844268799.\n",
      "Training epoch 2 batch 846 with loss 2.407496213912964.\n",
      "Training epoch 2 batch 847 with loss 2.0226359367370605.\n",
      "Training epoch 2 batch 848 with loss 2.0652081966400146.\n",
      "Training epoch 2 batch 849 with loss 1.8117506504058838.\n",
      "Training epoch 2 batch 850 with loss 2.3128654956817627.\n",
      "Training epoch 2 batch 851 with loss 1.9841409921646118.\n",
      "Training epoch 2 batch 852 with loss 2.005966901779175.\n",
      "Training epoch 2 batch 853 with loss 1.806637167930603.\n",
      "Training epoch 2 batch 854 with loss 2.0805468559265137.\n",
      "Training epoch 2 batch 855 with loss 2.011361598968506.\n",
      "Training epoch 2 batch 856 with loss 2.2925665378570557.\n",
      "Training epoch 2 batch 857 with loss 2.117304563522339.\n",
      "Training epoch 2 batch 858 with loss 1.5761687755584717.\n",
      "Training epoch 2 batch 859 with loss 1.992225170135498.\n",
      "Training epoch 2 batch 860 with loss 2.5659523010253906.\n",
      "Training epoch 2 batch 861 with loss 2.0265352725982666.\n",
      "Training epoch 2 batch 862 with loss 2.481998920440674.\n",
      "Training epoch 2 batch 863 with loss 1.881460189819336.\n",
      "Training epoch 2 batch 864 with loss 1.8702819347381592.\n",
      "Training epoch 2 batch 865 with loss 2.2452762126922607.\n",
      "Training epoch 2 batch 866 with loss 2.0313596725463867.\n",
      "Training epoch 2 batch 867 with loss 2.1136984825134277.\n",
      "Training epoch 2 batch 868 with loss 2.08306622505188.\n",
      "Training epoch 2 batch 869 with loss 2.0785505771636963.\n",
      "Training epoch 2 batch 870 with loss 1.611425757408142.\n",
      "Training epoch 2 batch 871 with loss 1.7407714128494263.\n",
      "Training epoch 2 batch 872 with loss 1.9009443521499634.\n",
      "Training epoch 2 batch 873 with loss 2.331573247909546.\n",
      "Training epoch 2 batch 874 with loss 1.8707526922225952.\n",
      "Training epoch 2 batch 875 with loss 2.207855701446533.\n",
      "Training epoch 2 batch 876 with loss 2.0180346965789795.\n",
      "Training epoch 2 batch 877 with loss 1.9736887216567993.\n",
      "Training epoch 2 batch 878 with loss 1.7272019386291504.\n",
      "Training epoch 2 batch 879 with loss 1.8274964094161987.\n",
      "Training epoch 2 batch 880 with loss 2.098950147628784.\n",
      "Training epoch 2 batch 881 with loss 2.3589234352111816.\n",
      "Training epoch 2 batch 882 with loss 2.3628926277160645.\n",
      "Training epoch 2 batch 883 with loss 2.203871965408325.\n",
      "Training epoch 2 batch 884 with loss 2.047269105911255.\n",
      "Training epoch 2 batch 885 with loss 2.503349542617798.\n",
      "Training epoch 2 batch 886 with loss 2.2037768363952637.\n",
      "Training epoch 2 batch 887 with loss 2.1893789768218994.\n",
      "Training epoch 2 batch 888 with loss 2.1345062255859375.\n",
      "Training epoch 2 batch 889 with loss 2.3619749546051025.\n",
      "Training epoch 2 batch 890 with loss 2.1024422645568848.\n",
      "Training epoch 2 batch 891 with loss 2.221712589263916.\n",
      "Training epoch 2 batch 892 with loss 1.8709619045257568.\n",
      "Training epoch 2 batch 893 with loss 2.01012921333313.\n",
      "Training epoch 2 batch 894 with loss 2.0161030292510986.\n",
      "Training epoch 2 batch 895 with loss 2.2173590660095215.\n",
      "Training epoch 2 batch 896 with loss 2.257944345474243.\n",
      "Training epoch 2 batch 897 with loss 1.9415709972381592.\n",
      "Training epoch 2 batch 898 with loss 1.8665183782577515.\n",
      "Training epoch 2 batch 899 with loss 2.376716136932373.\n",
      "Training epoch 2 batch 900 with loss 1.8690574169158936.\n",
      "Training epoch 2 batch 901 with loss 1.9045889377593994.\n",
      "Training epoch 2 batch 902 with loss 2.3309292793273926.\n",
      "Training epoch 2 batch 903 with loss 2.3139076232910156.\n",
      "Training epoch 2 batch 904 with loss 1.908637285232544.\n",
      "Training epoch 2 batch 905 with loss 2.022629737854004.\n",
      "Training epoch 2 batch 906 with loss 2.0781850814819336.\n",
      "Training epoch 2 batch 907 with loss 2.1232590675354004.\n",
      "Training epoch 2 batch 908 with loss 1.771485686302185.\n",
      "Training epoch 2 batch 909 with loss 2.5201807022094727.\n",
      "Training epoch 2 batch 910 with loss 2.3512113094329834.\n",
      "Training epoch 2 batch 911 with loss 2.1346898078918457.\n",
      "Training epoch 2 batch 912 with loss 2.173597812652588.\n",
      "Training epoch 2 batch 913 with loss 2.1728272438049316.\n",
      "Training epoch 2 batch 914 with loss 2.325178384780884.\n",
      "Training epoch 2 batch 915 with loss 2.43800950050354.\n",
      "Training epoch 2 batch 916 with loss 2.476973295211792.\n",
      "Training epoch 2 batch 917 with loss 1.9313615560531616.\n",
      "Training epoch 2 batch 918 with loss 2.004927158355713.\n",
      "Training epoch 2 batch 919 with loss 2.0694689750671387.\n",
      "Training epoch 2 batch 920 with loss 1.7739344835281372.\n",
      "Training epoch 2 batch 921 with loss 2.680272102355957.\n",
      "Training epoch 2 batch 922 with loss 2.0755984783172607.\n",
      "Training epoch 2 batch 923 with loss 2.035160541534424.\n",
      "Training epoch 2 batch 924 with loss 2.3697054386138916.\n",
      "Training epoch 2 batch 925 with loss 1.9167733192443848.\n",
      "Training epoch 2 batch 926 with loss 2.263545274734497.\n",
      "Training epoch 2 batch 927 with loss 2.2813708782196045.\n",
      "Training epoch 2 batch 928 with loss 2.4333674907684326.\n",
      "Training epoch 2 batch 929 with loss 2.382157564163208.\n",
      "Training epoch 2 batch 930 with loss 2.6167919635772705.\n",
      "Training epoch 2 batch 931 with loss 1.8248809576034546.\n",
      "Training epoch 2 batch 932 with loss 2.273867607116699.\n",
      "Training epoch 2 batch 933 with loss 2.1558048725128174.\n",
      "Training epoch 2 batch 934 with loss 1.9880446195602417.\n",
      "Training epoch 2 batch 935 with loss 1.8796929121017456.\n",
      "Training epoch 2 batch 936 with loss 2.0014703273773193.\n",
      "Training epoch 2 batch 937 with loss 1.8669594526290894.\n",
      "Training epoch 2 batch 938 with loss 1.8623052835464478.\n",
      "Training epoch 2 batch 939 with loss 2.4469783306121826.\n",
      "Training epoch 2 batch 940 with loss 2.4876861572265625.\n",
      "Training epoch 2 batch 941 with loss 2.47979474067688.\n",
      "Training epoch 2 batch 942 with loss 2.0598363876342773.\n",
      "Training epoch 2 batch 943 with loss 2.2407515048980713.\n",
      "Training epoch 2 batch 944 with loss 2.020364999771118.\n",
      "Training epoch 2 batch 945 with loss 2.219195604324341.\n",
      "Training epoch 2 batch 946 with loss 2.22981333732605.\n",
      "Training epoch 2 batch 947 with loss 2.00771164894104.\n",
      "Training epoch 2 batch 948 with loss 2.3557612895965576.\n",
      "Training epoch 2 batch 949 with loss 1.8297584056854248.\n",
      "Training epoch 2 batch 950 with loss 2.506056785583496.\n",
      "Training epoch 2 batch 951 with loss 2.2506821155548096.\n",
      "Training epoch 2 batch 952 with loss 2.1258764266967773.\n",
      "Training epoch 2 batch 953 with loss 2.0323281288146973.\n",
      "Training epoch 2 batch 954 with loss 1.9490550756454468.\n",
      "Training epoch 2 batch 955 with loss 2.2367546558380127.\n",
      "Training epoch 2 batch 956 with loss 2.0259673595428467.\n",
      "Training epoch 2 batch 957 with loss 2.29911208152771.\n",
      "Training epoch 2 batch 958 with loss 2.3981292247772217.\n",
      "Training epoch 2 batch 959 with loss 2.159874439239502.\n",
      "Training epoch 2 batch 960 with loss 2.5193393230438232.\n",
      "Training epoch 2 batch 961 with loss 2.3799400329589844.\n",
      "Training epoch 2 batch 962 with loss 2.4936513900756836.\n",
      "Training epoch 2 batch 963 with loss 1.8076280355453491.\n",
      "Training epoch 2 batch 964 with loss 2.109309673309326.\n",
      "Training epoch 2 batch 965 with loss 1.6392414569854736.\n",
      "Training epoch 2 batch 966 with loss 2.091993570327759.\n",
      "Training epoch 2 batch 967 with loss 2.1900014877319336.\n",
      "Training epoch 2 batch 968 with loss 3.050786018371582.\n",
      "Training epoch 2 batch 969 with loss 1.9113960266113281.\n",
      "Training epoch 2 batch 970 with loss 2.0620579719543457.\n",
      "Training epoch 2 batch 971 with loss 2.0159506797790527.\n",
      "Training epoch 2 batch 972 with loss 2.1333959102630615.\n",
      "Training epoch 2 batch 973 with loss 2.20815372467041.\n",
      "Training epoch 2 batch 974 with loss 2.240856885910034.\n",
      "Training epoch 2 batch 975 with loss 1.9075340032577515.\n",
      "Training epoch 2 batch 976 with loss 2.3803834915161133.\n",
      "Training epoch 2 batch 977 with loss 2.0129222869873047.\n",
      "Training epoch 2 batch 978 with loss 2.2748665809631348.\n",
      "Training epoch 2 batch 979 with loss 2.381094455718994.\n",
      "Training epoch 2 batch 980 with loss 2.664931297302246.\n",
      "Training epoch 2 batch 981 with loss 2.7198331356048584.\n",
      "Training epoch 2 batch 982 with loss 1.9686379432678223.\n",
      "Training epoch 2 batch 983 with loss 2.385032892227173.\n",
      "Training epoch 2 batch 984 with loss 2.177234411239624.\n",
      "Training epoch 2 batch 985 with loss 2.033898115158081.\n",
      "Training epoch 2 batch 986 with loss 1.970376968383789.\n",
      "Training epoch 2 batch 987 with loss 2.405935525894165.\n",
      "Training epoch 2 batch 988 with loss 1.971431851387024.\n",
      "Training epoch 2 batch 989 with loss 2.0720126628875732.\n",
      "Training epoch 2 batch 990 with loss 2.115025758743286.\n",
      "Training epoch 2 batch 991 with loss 2.266016721725464.\n",
      "Training epoch 2 batch 992 with loss 1.7610270977020264.\n",
      "Training epoch 2 batch 993 with loss 1.9939887523651123.\n",
      "Training epoch 2 batch 994 with loss 2.3902883529663086.\n",
      "Training epoch 2 batch 995 with loss 2.199247121810913.\n",
      "Training epoch 2 batch 996 with loss 2.222015142440796.\n",
      "Training epoch 2 batch 997 with loss 2.2496209144592285.\n",
      "Training epoch 2 batch 998 with loss 2.1379048824310303.\n",
      "Training epoch 2 batch 999 with loss 2.006324052810669.\n",
      "Test batch 0 with loss 2.7664260864257812.\n",
      "Test batch 1 with loss 2.0625646114349365.\n",
      "Test batch 2 with loss 2.391707181930542.\n",
      "Test batch 3 with loss 2.2834670543670654.\n",
      "Test batch 4 with loss 2.4493796825408936.\n",
      "Test batch 5 with loss 2.0476579666137695.\n",
      "Test batch 6 with loss 2.1147282123565674.\n",
      "Test batch 7 with loss 2.2760250568389893.\n",
      "Test batch 8 with loss 2.596698522567749.\n",
      "Test batch 9 with loss 2.233386993408203.\n",
      "Test batch 10 with loss 2.571589946746826.\n",
      "Test batch 11 with loss 2.0998504161834717.\n",
      "Test batch 12 with loss 2.444286823272705.\n",
      "Test batch 13 with loss 2.4456589221954346.\n",
      "Test batch 14 with loss 1.8176108598709106.\n",
      "Test batch 15 with loss 2.810734272003174.\n",
      "Test batch 16 with loss 2.2750210762023926.\n",
      "Test batch 17 with loss 2.6003494262695312.\n",
      "Test batch 18 with loss 2.2244231700897217.\n",
      "Test batch 19 with loss 2.080204963684082.\n",
      "Test batch 20 with loss 2.743802070617676.\n",
      "Test batch 21 with loss 2.6576812267303467.\n",
      "Test batch 22 with loss 2.201774835586548.\n",
      "Test batch 23 with loss 1.9877314567565918.\n",
      "Test batch 24 with loss 2.3141543865203857.\n",
      "Test batch 25 with loss 2.327101707458496.\n",
      "Test batch 26 with loss 2.2748286724090576.\n",
      "Test batch 27 with loss 2.493802309036255.\n",
      "Test batch 28 with loss 2.6895668506622314.\n",
      "Test batch 29 with loss 2.448028087615967.\n",
      "Test batch 30 with loss 2.374345064163208.\n",
      "Test batch 31 with loss 2.8015027046203613.\n",
      "Test batch 32 with loss 2.289536476135254.\n",
      "Test batch 33 with loss 2.521369218826294.\n",
      "Test batch 34 with loss 2.2232155799865723.\n",
      "Test batch 35 with loss 2.2293553352355957.\n",
      "Test batch 36 with loss 3.059368133544922.\n",
      "Test batch 37 with loss 2.614030599594116.\n",
      "Test batch 38 with loss 2.2810287475585938.\n",
      "Test batch 39 with loss 2.0695414543151855.\n",
      "Test batch 40 with loss 2.7735798358917236.\n",
      "Test batch 41 with loss 2.2151782512664795.\n",
      "Test batch 42 with loss 2.16540265083313.\n",
      "Test batch 43 with loss 2.086493492126465.\n",
      "Test batch 44 with loss 2.6308581829071045.\n",
      "Test batch 45 with loss 2.4221949577331543.\n",
      "Test batch 46 with loss 2.028390645980835.\n",
      "Test batch 47 with loss 2.043212890625.\n",
      "Test batch 48 with loss 2.443866729736328.\n",
      "Test batch 49 with loss 2.134042978286743.\n",
      "Test batch 50 with loss 1.8487850427627563.\n",
      "Test batch 51 with loss 2.3228371143341064.\n",
      "Test batch 52 with loss 2.235217809677124.\n",
      "Test batch 53 with loss 2.3787224292755127.\n",
      "Test batch 54 with loss 2.5381016731262207.\n",
      "Test batch 55 with loss 2.670917510986328.\n",
      "Test batch 56 with loss 3.112264394760132.\n",
      "Test batch 57 with loss 2.1826462745666504.\n",
      "Test batch 58 with loss 3.1898012161254883.\n",
      "Test batch 59 with loss 2.55247163772583.\n",
      "Test batch 60 with loss 2.0930230617523193.\n",
      "Test batch 61 with loss 2.8516435623168945.\n",
      "Test batch 62 with loss 2.4791743755340576.\n",
      "Test batch 63 with loss 2.631035566329956.\n",
      "Test batch 64 with loss 2.1080660820007324.\n",
      "Test batch 65 with loss 2.3754210472106934.\n",
      "Test batch 66 with loss 2.5647706985473633.\n",
      "Test batch 67 with loss 2.538358449935913.\n",
      "Test batch 68 with loss 2.271101474761963.\n",
      "Test batch 69 with loss 1.96326744556427.\n",
      "Test batch 70 with loss 2.1978142261505127.\n",
      "Test batch 71 with loss 2.3095104694366455.\n",
      "Test batch 72 with loss 2.498304605484009.\n",
      "Test batch 73 with loss 2.29382061958313.\n",
      "Test batch 74 with loss 1.9540375471115112.\n",
      "Test batch 75 with loss 2.1312787532806396.\n",
      "Test batch 76 with loss 2.6768100261688232.\n",
      "Test batch 77 with loss 2.547311305999756.\n",
      "Test batch 78 with loss 2.764045238494873.\n",
      "Test batch 79 with loss 2.5578110218048096.\n",
      "Test batch 80 with loss 2.84822678565979.\n",
      "Test batch 81 with loss 2.5399117469787598.\n",
      "Test batch 82 with loss 2.5504467487335205.\n",
      "Test batch 83 with loss 2.322890520095825.\n",
      "Test batch 84 with loss 2.2345006465911865.\n",
      "Test batch 85 with loss 2.102353811264038.\n",
      "Test batch 86 with loss 2.212433338165283.\n",
      "Test batch 87 with loss 2.4814250469207764.\n",
      "Test batch 88 with loss 2.6681501865386963.\n",
      "Test batch 89 with loss 2.265915632247925.\n",
      "Test batch 90 with loss 2.460236072540283.\n",
      "Test batch 91 with loss 2.174649477005005.\n",
      "Test batch 92 with loss 2.4296188354492188.\n",
      "Test batch 93 with loss 2.6112334728240967.\n",
      "Test batch 94 with loss 2.484164237976074.\n",
      "Test batch 95 with loss 2.8729147911071777.\n",
      "Test batch 96 with loss 2.4740805625915527.\n",
      "Test batch 97 with loss 2.5899603366851807.\n",
      "Test batch 98 with loss 2.4111742973327637.\n",
      "Test batch 99 with loss 2.2918851375579834.\n",
      "Test batch 100 with loss 3.119079113006592.\n",
      "Test batch 101 with loss 2.501927137374878.\n",
      "Test batch 102 with loss 2.613555908203125.\n",
      "Test batch 103 with loss 3.0481512546539307.\n",
      "Test batch 104 with loss 2.4354028701782227.\n",
      "Test batch 105 with loss 2.391594409942627.\n",
      "Test batch 106 with loss 2.7527341842651367.\n",
      "Test batch 107 with loss 2.0826330184936523.\n",
      "Test batch 108 with loss 2.8488032817840576.\n",
      "Test batch 109 with loss 3.0273003578186035.\n",
      "Test batch 110 with loss 2.456575632095337.\n",
      "Test batch 111 with loss 2.3681771755218506.\n",
      "Test batch 112 with loss 2.4425048828125.\n",
      "Test batch 113 with loss 2.424466371536255.\n",
      "Test batch 114 with loss 2.469661235809326.\n",
      "Test batch 115 with loss 2.0514445304870605.\n",
      "Test batch 116 with loss 1.647104263305664.\n",
      "Test batch 117 with loss 2.3698763847351074.\n",
      "Test batch 118 with loss 2.265850067138672.\n",
      "Test batch 119 with loss 2.6102607250213623.\n",
      "Test batch 120 with loss 2.525465965270996.\n",
      "Test batch 121 with loss 2.07430100440979.\n",
      "Test batch 122 with loss 2.3900601863861084.\n",
      "Test batch 123 with loss 2.2292494773864746.\n",
      "Test batch 124 with loss 2.7510056495666504.\n",
      "Test batch 125 with loss 2.57155179977417.\n",
      "Test batch 126 with loss 1.8952953815460205.\n",
      "Test batch 127 with loss 3.000518321990967.\n",
      "Test batch 128 with loss 3.188528537750244.\n",
      "Test batch 129 with loss 2.5083882808685303.\n",
      "Test batch 130 with loss 2.185772180557251.\n",
      "Test batch 131 with loss 2.1862952709198.\n",
      "Test batch 132 with loss 2.371784210205078.\n",
      "Test batch 133 with loss 2.270982027053833.\n",
      "Test batch 134 with loss 2.4255456924438477.\n",
      "Test batch 135 with loss 2.661362886428833.\n",
      "Test batch 136 with loss 2.897383689880371.\n",
      "Test batch 137 with loss 2.182525157928467.\n",
      "Test batch 138 with loss 2.543152332305908.\n",
      "Test batch 139 with loss 2.466998338699341.\n",
      "Test batch 140 with loss 2.3407464027404785.\n",
      "Test batch 141 with loss 2.4960107803344727.\n",
      "Test batch 142 with loss 2.5081496238708496.\n",
      "Test batch 143 with loss 2.832521677017212.\n",
      "Test batch 144 with loss 2.6041953563690186.\n",
      "Test batch 145 with loss 2.3008663654327393.\n",
      "Test batch 146 with loss 2.495816707611084.\n",
      "Test batch 147 with loss 2.4329817295074463.\n",
      "Test batch 148 with loss 2.5171759128570557.\n",
      "Test batch 149 with loss 2.0196659564971924.\n",
      "Test batch 150 with loss 2.012227773666382.\n",
      "Test batch 151 with loss 2.3857181072235107.\n",
      "Test batch 152 with loss 2.2530603408813477.\n",
      "Test batch 153 with loss 2.568664789199829.\n",
      "Test batch 154 with loss 2.6001484394073486.\n",
      "Test batch 155 with loss 2.4837803840637207.\n",
      "Test batch 156 with loss 2.5331218242645264.\n",
      "Test batch 157 with loss 2.5465879440307617.\n",
      "Test batch 158 with loss 2.433427333831787.\n",
      "Test batch 159 with loss 2.200758695602417.\n",
      "Test batch 160 with loss 2.9726898670196533.\n",
      "Test batch 161 with loss 2.441859245300293.\n",
      "Test batch 162 with loss 2.2699759006500244.\n",
      "Test batch 163 with loss 2.488630533218384.\n",
      "Test batch 164 with loss 2.2871181964874268.\n",
      "Test batch 165 with loss 2.070681571960449.\n",
      "Test batch 166 with loss 2.9592738151550293.\n",
      "Test batch 167 with loss 2.3726699352264404.\n",
      "Test batch 168 with loss 2.2287728786468506.\n",
      "Test batch 169 with loss 2.1296792030334473.\n",
      "Test batch 170 with loss 2.0981762409210205.\n",
      "Test batch 171 with loss 2.3195254802703857.\n",
      "Test batch 172 with loss 2.5592010021209717.\n",
      "Test batch 173 with loss 2.9023630619049072.\n",
      "Test batch 174 with loss 2.6199758052825928.\n",
      "Test batch 175 with loss 2.597203493118286.\n",
      "Test batch 176 with loss 2.5066795349121094.\n",
      "Test batch 177 with loss 2.287824869155884.\n",
      "Test batch 178 with loss 3.094592809677124.\n",
      "Test batch 179 with loss 2.1085190773010254.\n",
      "Test batch 180 with loss 2.05698561668396.\n",
      "Test batch 181 with loss 2.592439889907837.\n",
      "Test batch 182 with loss 2.176300287246704.\n",
      "Test batch 183 with loss 2.0775351524353027.\n",
      "Test batch 184 with loss 2.076420545578003.\n",
      "Test batch 185 with loss 2.3561787605285645.\n",
      "Test batch 186 with loss 2.436314821243286.\n",
      "Test batch 187 with loss 2.2120931148529053.\n",
      "Test batch 188 with loss 1.9198956489562988.\n",
      "Test batch 189 with loss 2.5647120475769043.\n",
      "Test batch 190 with loss 2.1320791244506836.\n",
      "Test batch 191 with loss 1.9011948108673096.\n",
      "Test batch 192 with loss 2.626739740371704.\n",
      "Test batch 193 with loss 2.322566270828247.\n",
      "Test batch 194 with loss 2.1069726943969727.\n",
      "Test batch 195 with loss 2.017759084701538.\n",
      "Test batch 196 with loss 2.779017686843872.\n",
      "Test batch 197 with loss 2.2143163681030273.\n",
      "Test batch 198 with loss 2.131383180618286.\n",
      "Test batch 199 with loss 2.838872194290161.\n",
      "Test batch 200 with loss 2.1071412563323975.\n",
      "Test batch 201 with loss 2.1144323348999023.\n",
      "Test batch 202 with loss 2.014014959335327.\n",
      "Test batch 203 with loss 2.152419090270996.\n",
      "Test batch 204 with loss 2.571859121322632.\n",
      "Test batch 205 with loss 2.158458709716797.\n",
      "Test batch 206 with loss 2.1620352268218994.\n",
      "Test batch 207 with loss 2.6972875595092773.\n",
      "Test batch 208 with loss 2.5548558235168457.\n",
      "Test batch 209 with loss 2.338456392288208.\n",
      "Test batch 210 with loss 2.2269928455352783.\n",
      "Test batch 211 with loss 2.36435866355896.\n",
      "Test batch 212 with loss 2.4191672801971436.\n",
      "Test batch 213 with loss 2.678903818130493.\n",
      "Test batch 214 with loss 2.285133123397827.\n",
      "Test batch 215 with loss 2.48575496673584.\n",
      "Test batch 216 with loss 2.4086339473724365.\n",
      "Test batch 217 with loss 2.331960916519165.\n",
      "Test batch 218 with loss 2.695976734161377.\n",
      "Test batch 219 with loss 2.423146963119507.\n",
      "Test batch 220 with loss 2.5139551162719727.\n",
      "Test batch 221 with loss 2.3170104026794434.\n",
      "Test batch 222 with loss 2.5845391750335693.\n",
      "Test batch 223 with loss 2.5212175846099854.\n",
      "Test batch 224 with loss 2.452540874481201.\n",
      "Test batch 225 with loss 2.033203363418579.\n",
      "Test batch 226 with loss 2.702927350997925.\n",
      "Test batch 227 with loss 2.203521251678467.\n",
      "Test batch 228 with loss 2.1912143230438232.\n",
      "Test batch 229 with loss 2.088224172592163.\n",
      "Test batch 230 with loss 1.7143923044204712.\n",
      "Test batch 231 with loss 2.6811532974243164.\n",
      "Test batch 232 with loss 2.3471672534942627.\n",
      "Test batch 233 with loss 2.437636613845825.\n",
      "Test batch 234 with loss 2.680203914642334.\n",
      "Test batch 235 with loss 2.380906105041504.\n",
      "Test batch 236 with loss 2.067169427871704.\n",
      "Test batch 237 with loss 2.327869415283203.\n",
      "Test batch 238 with loss 2.5206615924835205.\n",
      "Test batch 239 with loss 2.5208091735839844.\n",
      "Test batch 240 with loss 2.259300947189331.\n",
      "Test batch 241 with loss 2.583226442337036.\n",
      "Test batch 242 with loss 2.35888671875.\n",
      "Test batch 243 with loss 2.2845330238342285.\n",
      "Test batch 244 with loss 2.311452627182007.\n",
      "Test batch 245 with loss 2.7210206985473633.\n",
      "Test batch 246 with loss 2.273331642150879.\n",
      "Test batch 247 with loss 2.6747498512268066.\n",
      "Test batch 248 with loss 2.1160409450531006.\n",
      "Test batch 249 with loss 2.485870599746704.\n",
      "Test batch 250 with loss 2.400198221206665.\n",
      "Test batch 251 with loss 2.5783438682556152.\n",
      "Test batch 252 with loss 2.155879497528076.\n",
      "Test batch 253 with loss 2.2250587940216064.\n",
      "Test batch 254 with loss 2.9691321849823.\n",
      "Test batch 255 with loss 2.622689962387085.\n",
      "Test batch 256 with loss 2.205578565597534.\n",
      "Test batch 257 with loss 2.1588401794433594.\n",
      "Test batch 258 with loss 2.3594539165496826.\n",
      "Test batch 259 with loss 1.8568955659866333.\n",
      "Test batch 260 with loss 2.1848881244659424.\n",
      "Test batch 261 with loss 2.463531494140625.\n",
      "Test batch 262 with loss 2.626746892929077.\n",
      "Test batch 263 with loss 2.721349000930786.\n",
      "Test batch 264 with loss 2.7081446647644043.\n",
      "Test batch 265 with loss 2.3403584957122803.\n",
      "Test batch 266 with loss 2.7403457164764404.\n",
      "Test batch 267 with loss 2.174363136291504.\n",
      "Test batch 268 with loss 3.1356618404388428.\n",
      "Test batch 269 with loss 2.112607717514038.\n",
      "Test batch 270 with loss 2.25405216217041.\n",
      "Test batch 271 with loss 2.6928601264953613.\n",
      "Test batch 272 with loss 2.4063446521759033.\n",
      "Test batch 273 with loss 2.7450127601623535.\n",
      "Test batch 274 with loss 2.670987129211426.\n",
      "Test batch 275 with loss 3.0625998973846436.\n",
      "Test batch 276 with loss 2.8020031452178955.\n",
      "Test batch 277 with loss 2.7250680923461914.\n",
      "Test batch 278 with loss 2.1430325508117676.\n",
      "Test batch 279 with loss 2.373063802719116.\n",
      "Test batch 280 with loss 2.0628998279571533.\n",
      "Test batch 281 with loss 2.3357279300689697.\n",
      "Test batch 282 with loss 2.359151601791382.\n",
      "Test batch 283 with loss 2.50363826751709.\n",
      "Test batch 284 with loss 2.4965689182281494.\n",
      "Test batch 285 with loss 2.900916337966919.\n",
      "Test batch 286 with loss 2.6358423233032227.\n",
      "Test batch 287 with loss 2.075315237045288.\n",
      "Test batch 288 with loss 1.879233717918396.\n",
      "Test batch 289 with loss 2.3501064777374268.\n",
      "Test batch 290 with loss 2.41833758354187.\n",
      "Test batch 291 with loss 2.563385009765625.\n",
      "Test batch 292 with loss 2.1591131687164307.\n",
      "Test batch 293 with loss 2.3810830116271973.\n",
      "Test batch 294 with loss 2.9529645442962646.\n",
      "Test batch 295 with loss 2.047046184539795.\n",
      "Test batch 296 with loss 2.0784125328063965.\n",
      "Test batch 297 with loss 2.2448718547821045.\n",
      "Test batch 298 with loss 2.3092739582061768.\n",
      "Test batch 299 with loss 2.386582612991333.\n",
      "Test batch 300 with loss 2.534682273864746.\n",
      "Test batch 301 with loss 2.4016542434692383.\n",
      "Test batch 302 with loss 2.596972942352295.\n",
      "Test batch 303 with loss 2.742077589035034.\n",
      "Test batch 304 with loss 2.102241277694702.\n",
      "Test batch 305 with loss 1.9991832971572876.\n",
      "Test batch 306 with loss 2.7657790184020996.\n",
      "Test batch 307 with loss 2.472891092300415.\n",
      "Test batch 308 with loss 2.8751168251037598.\n",
      "Test batch 309 with loss 2.2517149448394775.\n",
      "Test batch 310 with loss 2.6213533878326416.\n",
      "Test batch 311 with loss 2.8098793029785156.\n",
      "Test batch 312 with loss 2.499633550643921.\n",
      "Test batch 313 with loss 2.565265655517578.\n",
      "Test batch 314 with loss 2.8936755657196045.\n",
      "Test batch 315 with loss 2.8558123111724854.\n",
      "Test batch 316 with loss 2.4529356956481934.\n",
      "Test batch 317 with loss 2.0926871299743652.\n",
      "Test batch 318 with loss 3.0032331943511963.\n",
      "Test batch 319 with loss 2.32450008392334.\n",
      "Test batch 320 with loss 2.4786863327026367.\n",
      "Test batch 321 with loss 2.581986904144287.\n",
      "Test batch 322 with loss 2.4098658561706543.\n",
      "Test batch 323 with loss 2.7646594047546387.\n",
      "Test batch 324 with loss 2.240189552307129.\n",
      "Test batch 325 with loss 2.9139745235443115.\n",
      "Test batch 326 with loss 2.4765121936798096.\n",
      "Test batch 327 with loss 3.0063955783843994.\n",
      "Test batch 328 with loss 2.280097723007202.\n",
      "Test batch 329 with loss 2.8625328540802.\n",
      "Test batch 330 with loss 2.721759796142578.\n",
      "Test batch 331 with loss 2.4927279949188232.\n",
      "Test batch 332 with loss 2.4758644104003906.\n",
      "Test batch 333 with loss 1.602579116821289.\n",
      "Training epoch 3 batch 0 with loss 1.5634257793426514.\n",
      "Training epoch 3 batch 1 with loss 1.4437695741653442.\n",
      "Training epoch 3 batch 2 with loss 1.5227736234664917.\n",
      "Training epoch 3 batch 3 with loss 1.663711667060852.\n",
      "Training epoch 3 batch 4 with loss 1.5218219757080078.\n",
      "Training epoch 3 batch 5 with loss 2.1133430004119873.\n",
      "Training epoch 3 batch 6 with loss 1.778818130493164.\n",
      "Training epoch 3 batch 7 with loss 1.8576581478118896.\n",
      "Training epoch 3 batch 8 with loss 1.6762956380844116.\n",
      "Training epoch 3 batch 9 with loss 1.8796099424362183.\n",
      "Training epoch 3 batch 10 with loss 1.689618468284607.\n",
      "Training epoch 3 batch 11 with loss 1.6314884424209595.\n",
      "Training epoch 3 batch 12 with loss 1.5557397603988647.\n",
      "Training epoch 3 batch 13 with loss 1.6925272941589355.\n",
      "Training epoch 3 batch 14 with loss 1.6724929809570312.\n",
      "Training epoch 3 batch 15 with loss 1.9845181703567505.\n",
      "Training epoch 3 batch 16 with loss 1.502739429473877.\n",
      "Training epoch 3 batch 17 with loss 1.5401028394699097.\n",
      "Training epoch 3 batch 18 with loss 1.5448169708251953.\n",
      "Training epoch 3 batch 19 with loss 1.490442156791687.\n",
      "Training epoch 3 batch 20 with loss 1.7754331827163696.\n",
      "Training epoch 3 batch 21 with loss 1.675389051437378.\n",
      "Training epoch 3 batch 22 with loss 1.4874613285064697.\n",
      "Training epoch 3 batch 23 with loss 1.9416885375976562.\n",
      "Training epoch 3 batch 24 with loss 1.6787859201431274.\n",
      "Training epoch 3 batch 25 with loss 1.8181076049804688.\n",
      "Training epoch 3 batch 26 with loss 1.9445576667785645.\n",
      "Training epoch 3 batch 27 with loss 1.7405052185058594.\n",
      "Training epoch 3 batch 28 with loss 1.4572633504867554.\n",
      "Training epoch 3 batch 29 with loss 1.7512441873550415.\n",
      "Training epoch 3 batch 30 with loss 1.7168081998825073.\n",
      "Training epoch 3 batch 31 with loss 1.7944815158843994.\n",
      "Training epoch 3 batch 32 with loss 1.7778772115707397.\n",
      "Training epoch 3 batch 33 with loss 1.568271517753601.\n",
      "Training epoch 3 batch 34 with loss 1.4701550006866455.\n",
      "Training epoch 3 batch 35 with loss 1.7735847234725952.\n",
      "Training epoch 3 batch 36 with loss 1.6064430475234985.\n",
      "Training epoch 3 batch 37 with loss 1.7263466119766235.\n",
      "Training epoch 3 batch 38 with loss 1.313064694404602.\n",
      "Training epoch 3 batch 39 with loss 1.8099867105484009.\n",
      "Training epoch 3 batch 40 with loss 1.9736592769622803.\n",
      "Training epoch 3 batch 41 with loss 1.7701027393341064.\n",
      "Training epoch 3 batch 42 with loss 1.6953853368759155.\n",
      "Training epoch 3 batch 43 with loss 1.5488508939743042.\n",
      "Training epoch 3 batch 44 with loss 1.7760947942733765.\n",
      "Training epoch 3 batch 45 with loss 1.823270559310913.\n",
      "Training epoch 3 batch 46 with loss 1.7168676853179932.\n",
      "Training epoch 3 batch 47 with loss 1.7978060245513916.\n",
      "Training epoch 3 batch 48 with loss 1.6075998544692993.\n",
      "Training epoch 3 batch 49 with loss 1.5998644828796387.\n",
      "Training epoch 3 batch 50 with loss 1.7806888818740845.\n",
      "Training epoch 3 batch 51 with loss 1.7577064037322998.\n",
      "Training epoch 3 batch 52 with loss 1.7754074335098267.\n",
      "Training epoch 3 batch 53 with loss 1.470613956451416.\n",
      "Training epoch 3 batch 54 with loss 1.8376762866973877.\n",
      "Training epoch 3 batch 55 with loss 1.8406938314437866.\n",
      "Training epoch 3 batch 56 with loss 1.5516624450683594.\n",
      "Training epoch 3 batch 57 with loss 1.7060266733169556.\n",
      "Training epoch 3 batch 58 with loss 1.743996500968933.\n",
      "Training epoch 3 batch 59 with loss 1.5121012926101685.\n",
      "Training epoch 3 batch 60 with loss 1.6319564580917358.\n",
      "Training epoch 3 batch 61 with loss 1.742733120918274.\n",
      "Training epoch 3 batch 62 with loss 1.8620617389678955.\n",
      "Training epoch 3 batch 63 with loss 2.00156307220459.\n",
      "Training epoch 3 batch 64 with loss 1.5339423418045044.\n",
      "Training epoch 3 batch 65 with loss 1.596907377243042.\n",
      "Training epoch 3 batch 66 with loss 1.3205209970474243.\n",
      "Training epoch 3 batch 67 with loss 1.4696333408355713.\n",
      "Training epoch 3 batch 68 with loss 1.7127305269241333.\n",
      "Training epoch 3 batch 69 with loss 1.6401299238204956.\n",
      "Training epoch 3 batch 70 with loss 1.577605962753296.\n",
      "Training epoch 3 batch 71 with loss 1.5910179615020752.\n",
      "Training epoch 3 batch 72 with loss 1.7480841875076294.\n",
      "Training epoch 3 batch 73 with loss 1.5431479215621948.\n",
      "Training epoch 3 batch 74 with loss 1.8856154680252075.\n",
      "Training epoch 3 batch 75 with loss 1.6495277881622314.\n",
      "Training epoch 3 batch 76 with loss 1.8572903871536255.\n",
      "Training epoch 3 batch 77 with loss 1.552947759628296.\n",
      "Training epoch 3 batch 78 with loss 1.6946531534194946.\n",
      "Training epoch 3 batch 79 with loss 1.3726484775543213.\n",
      "Training epoch 3 batch 80 with loss 1.7947068214416504.\n",
      "Training epoch 3 batch 81 with loss 1.4952986240386963.\n",
      "Training epoch 3 batch 82 with loss 1.5649892091751099.\n",
      "Training epoch 3 batch 83 with loss 1.8433438539505005.\n",
      "Training epoch 3 batch 84 with loss 1.6813240051269531.\n",
      "Training epoch 3 batch 85 with loss 1.7473636865615845.\n",
      "Training epoch 3 batch 86 with loss 1.4135019779205322.\n",
      "Training epoch 3 batch 87 with loss 1.9069361686706543.\n",
      "Training epoch 3 batch 88 with loss 1.8452613353729248.\n",
      "Training epoch 3 batch 89 with loss 1.5445706844329834.\n",
      "Training epoch 3 batch 90 with loss 1.6941088438034058.\n",
      "Training epoch 3 batch 91 with loss 1.6334519386291504.\n",
      "Training epoch 3 batch 92 with loss 1.879480004310608.\n",
      "Training epoch 3 batch 93 with loss 1.6320422887802124.\n",
      "Training epoch 3 batch 94 with loss 1.7212661504745483.\n",
      "Training epoch 3 batch 95 with loss 1.4511046409606934.\n",
      "Training epoch 3 batch 96 with loss 1.9300651550292969.\n",
      "Training epoch 3 batch 97 with loss 1.6713767051696777.\n",
      "Training epoch 3 batch 98 with loss 1.360950231552124.\n",
      "Training epoch 3 batch 99 with loss 1.5953847169876099.\n",
      "Training epoch 3 batch 100 with loss 1.5689741373062134.\n",
      "Training epoch 3 batch 101 with loss 1.974098563194275.\n",
      "Training epoch 3 batch 102 with loss 1.8695359230041504.\n",
      "Training epoch 3 batch 103 with loss 1.4418855905532837.\n",
      "Training epoch 3 batch 104 with loss 2.0620596408843994.\n",
      "Training epoch 3 batch 105 with loss 1.4695241451263428.\n",
      "Training epoch 3 batch 106 with loss 1.5664523839950562.\n",
      "Training epoch 3 batch 107 with loss 1.6868956089019775.\n",
      "Training epoch 3 batch 108 with loss 1.3820254802703857.\n",
      "Training epoch 3 batch 109 with loss 1.6731256246566772.\n",
      "Training epoch 3 batch 110 with loss 1.6484087705612183.\n",
      "Training epoch 3 batch 111 with loss 1.746389627456665.\n",
      "Training epoch 3 batch 112 with loss 1.8383945226669312.\n",
      "Training epoch 3 batch 113 with loss 1.8848532438278198.\n",
      "Training epoch 3 batch 114 with loss 2.3236167430877686.\n",
      "Training epoch 3 batch 115 with loss 1.4702454805374146.\n",
      "Training epoch 3 batch 116 with loss 1.7348865270614624.\n",
      "Training epoch 3 batch 117 with loss 1.6615078449249268.\n",
      "Training epoch 3 batch 118 with loss 1.5762860774993896.\n",
      "Training epoch 3 batch 119 with loss 1.5270136594772339.\n",
      "Training epoch 3 batch 120 with loss 1.9099565744400024.\n",
      "Training epoch 3 batch 121 with loss 1.7126872539520264.\n",
      "Training epoch 3 batch 122 with loss 1.6281694173812866.\n",
      "Training epoch 3 batch 123 with loss 1.7661200761795044.\n",
      "Training epoch 3 batch 124 with loss 1.5569878816604614.\n",
      "Training epoch 3 batch 125 with loss 1.454329013824463.\n",
      "Training epoch 3 batch 126 with loss 1.9544928073883057.\n",
      "Training epoch 3 batch 127 with loss 1.4339025020599365.\n",
      "Training epoch 3 batch 128 with loss 1.7281956672668457.\n",
      "Training epoch 3 batch 129 with loss 1.9710609912872314.\n",
      "Training epoch 3 batch 130 with loss 1.9822280406951904.\n",
      "Training epoch 3 batch 131 with loss 1.697965383529663.\n",
      "Training epoch 3 batch 132 with loss 1.6330517530441284.\n",
      "Training epoch 3 batch 133 with loss 1.6919177770614624.\n",
      "Training epoch 3 batch 134 with loss 2.1047136783599854.\n",
      "Training epoch 3 batch 135 with loss 1.7688418626785278.\n",
      "Training epoch 3 batch 136 with loss 1.4518905878067017.\n",
      "Training epoch 3 batch 137 with loss 1.9076809883117676.\n",
      "Training epoch 3 batch 138 with loss 1.708984375.\n",
      "Training epoch 3 batch 139 with loss 1.7352070808410645.\n",
      "Training epoch 3 batch 140 with loss 1.5682122707366943.\n",
      "Training epoch 3 batch 141 with loss 1.519399642944336.\n",
      "Training epoch 3 batch 142 with loss 1.808067798614502.\n",
      "Training epoch 3 batch 143 with loss 1.719205617904663.\n",
      "Training epoch 3 batch 144 with loss 1.4611163139343262.\n",
      "Training epoch 3 batch 145 with loss 1.7485768795013428.\n",
      "Training epoch 3 batch 146 with loss 1.5633492469787598.\n",
      "Training epoch 3 batch 147 with loss 1.502953290939331.\n",
      "Training epoch 3 batch 148 with loss 1.8218345642089844.\n",
      "Training epoch 3 batch 149 with loss 1.7408486604690552.\n",
      "Training epoch 3 batch 150 with loss 2.0179905891418457.\n",
      "Training epoch 3 batch 151 with loss 1.5255762338638306.\n",
      "Training epoch 3 batch 152 with loss 1.5374141931533813.\n",
      "Training epoch 3 batch 153 with loss 1.6329715251922607.\n",
      "Training epoch 3 batch 154 with loss 2.072371006011963.\n",
      "Training epoch 3 batch 155 with loss 1.6767826080322266.\n",
      "Training epoch 3 batch 156 with loss 1.814180612564087.\n",
      "Training epoch 3 batch 157 with loss 2.1516616344451904.\n",
      "Training epoch 3 batch 158 with loss 2.083744525909424.\n",
      "Training epoch 3 batch 159 with loss 1.798540711402893.\n",
      "Training epoch 3 batch 160 with loss 1.6929340362548828.\n",
      "Training epoch 3 batch 161 with loss 1.4898780584335327.\n",
      "Training epoch 3 batch 162 with loss 1.683519721031189.\n",
      "Training epoch 3 batch 163 with loss 1.8472429513931274.\n",
      "Training epoch 3 batch 164 with loss 1.6782900094985962.\n",
      "Training epoch 3 batch 165 with loss 1.4800150394439697.\n",
      "Training epoch 3 batch 166 with loss 1.8330762386322021.\n",
      "Training epoch 3 batch 167 with loss 1.6538747549057007.\n",
      "Training epoch 3 batch 168 with loss 2.309185266494751.\n",
      "Training epoch 3 batch 169 with loss 1.4809002876281738.\n",
      "Training epoch 3 batch 170 with loss 1.9367741346359253.\n",
      "Training epoch 3 batch 171 with loss 1.795267105102539.\n",
      "Training epoch 3 batch 172 with loss 1.9220789670944214.\n",
      "Training epoch 3 batch 173 with loss 1.5437425374984741.\n",
      "Training epoch 3 batch 174 with loss 1.6573978662490845.\n",
      "Training epoch 3 batch 175 with loss 1.6226857900619507.\n",
      "Training epoch 3 batch 176 with loss 2.0339555740356445.\n",
      "Training epoch 3 batch 177 with loss 1.7225433588027954.\n",
      "Training epoch 3 batch 178 with loss 1.7274519205093384.\n",
      "Training epoch 3 batch 179 with loss 1.9314059019088745.\n",
      "Training epoch 3 batch 180 with loss 1.8971270322799683.\n",
      "Training epoch 3 batch 181 with loss 1.6493366956710815.\n",
      "Training epoch 3 batch 182 with loss 1.7443044185638428.\n",
      "Training epoch 3 batch 183 with loss 1.7157618999481201.\n",
      "Training epoch 3 batch 184 with loss 1.6554958820343018.\n",
      "Training epoch 3 batch 185 with loss 1.6146576404571533.\n",
      "Training epoch 3 batch 186 with loss 1.6412547826766968.\n",
      "Training epoch 3 batch 187 with loss 1.5958116054534912.\n",
      "Training epoch 3 batch 188 with loss 1.8650023937225342.\n",
      "Training epoch 3 batch 189 with loss 1.8824571371078491.\n",
      "Training epoch 3 batch 190 with loss 1.431692123413086.\n",
      "Training epoch 3 batch 191 with loss 1.9493696689605713.\n",
      "Training epoch 3 batch 192 with loss 1.9391690492630005.\n",
      "Training epoch 3 batch 193 with loss 2.028489351272583.\n",
      "Training epoch 3 batch 194 with loss 1.82115638256073.\n",
      "Training epoch 3 batch 195 with loss 1.6267420053482056.\n",
      "Training epoch 3 batch 196 with loss 1.5416035652160645.\n",
      "Training epoch 3 batch 197 with loss 2.083869457244873.\n",
      "Training epoch 3 batch 198 with loss 1.4805107116699219.\n",
      "Training epoch 3 batch 199 with loss 1.6054975986480713.\n",
      "Training epoch 3 batch 200 with loss 1.5458632707595825.\n",
      "Training epoch 3 batch 201 with loss 1.7736058235168457.\n",
      "Training epoch 3 batch 202 with loss 1.7208008766174316.\n",
      "Training epoch 3 batch 203 with loss 1.6782313585281372.\n",
      "Training epoch 3 batch 204 with loss 1.5125902891159058.\n",
      "Training epoch 3 batch 205 with loss 1.5453184843063354.\n",
      "Training epoch 3 batch 206 with loss 1.952638030052185.\n",
      "Training epoch 3 batch 207 with loss 1.443032145500183.\n",
      "Training epoch 3 batch 208 with loss 1.9690582752227783.\n",
      "Training epoch 3 batch 209 with loss 1.7921326160430908.\n",
      "Training epoch 3 batch 210 with loss 1.8403126001358032.\n",
      "Training epoch 3 batch 211 with loss 1.5678304433822632.\n",
      "Training epoch 3 batch 212 with loss 1.9450136423110962.\n",
      "Training epoch 3 batch 213 with loss 1.5844686031341553.\n",
      "Training epoch 3 batch 214 with loss 1.9499335289001465.\n",
      "Training epoch 3 batch 215 with loss 1.6210434436798096.\n",
      "Training epoch 3 batch 216 with loss 1.947750449180603.\n",
      "Training epoch 3 batch 217 with loss 2.0587477684020996.\n",
      "Training epoch 3 batch 218 with loss 1.9598366022109985.\n",
      "Training epoch 3 batch 219 with loss 1.9869309663772583.\n",
      "Training epoch 3 batch 220 with loss 1.5516396760940552.\n",
      "Training epoch 3 batch 221 with loss 1.9574165344238281.\n",
      "Training epoch 3 batch 222 with loss 1.9793587923049927.\n",
      "Training epoch 3 batch 223 with loss 1.8722114562988281.\n",
      "Training epoch 3 batch 224 with loss 1.9670815467834473.\n",
      "Training epoch 3 batch 225 with loss 1.8129661083221436.\n",
      "Training epoch 3 batch 226 with loss 1.9994367361068726.\n",
      "Training epoch 3 batch 227 with loss 1.6472197771072388.\n",
      "Training epoch 3 batch 228 with loss 1.5890775918960571.\n",
      "Training epoch 3 batch 229 with loss 2.04879093170166.\n",
      "Training epoch 3 batch 230 with loss 2.006395101547241.\n",
      "Training epoch 3 batch 231 with loss 2.2827110290527344.\n",
      "Training epoch 3 batch 232 with loss 1.9478858709335327.\n",
      "Training epoch 3 batch 233 with loss 1.7798351049423218.\n",
      "Training epoch 3 batch 234 with loss 1.790238618850708.\n",
      "Training epoch 3 batch 235 with loss 1.739408016204834.\n",
      "Training epoch 3 batch 236 with loss 1.6336016654968262.\n",
      "Training epoch 3 batch 237 with loss 1.729396939277649.\n",
      "Training epoch 3 batch 238 with loss 1.9913794994354248.\n",
      "Training epoch 3 batch 239 with loss 1.7521063089370728.\n",
      "Training epoch 3 batch 240 with loss 2.0624642372131348.\n",
      "Training epoch 3 batch 241 with loss 2.066092014312744.\n",
      "Training epoch 3 batch 242 with loss 1.5728708505630493.\n",
      "Training epoch 3 batch 243 with loss 1.6029374599456787.\n",
      "Training epoch 3 batch 244 with loss 1.935928225517273.\n",
      "Training epoch 3 batch 245 with loss 1.1554558277130127.\n",
      "Training epoch 3 batch 246 with loss 1.5394841432571411.\n",
      "Training epoch 3 batch 247 with loss 1.8674834966659546.\n",
      "Training epoch 3 batch 248 with loss 1.8453863859176636.\n",
      "Training epoch 3 batch 249 with loss 1.6265628337860107.\n",
      "Training epoch 3 batch 250 with loss 1.5032727718353271.\n",
      "Training epoch 3 batch 251 with loss 1.9191431999206543.\n",
      "Training epoch 3 batch 252 with loss 1.4995543956756592.\n",
      "Training epoch 3 batch 253 with loss 2.101257562637329.\n",
      "Training epoch 3 batch 254 with loss 2.3058340549468994.\n",
      "Training epoch 3 batch 255 with loss 1.7000236511230469.\n",
      "Training epoch 3 batch 256 with loss 1.7441438436508179.\n",
      "Training epoch 3 batch 257 with loss 1.7445509433746338.\n",
      "Training epoch 3 batch 258 with loss 1.9624801874160767.\n",
      "Training epoch 3 batch 259 with loss 1.7165660858154297.\n",
      "Training epoch 3 batch 260 with loss 2.1523547172546387.\n",
      "Training epoch 3 batch 261 with loss 1.669559121131897.\n",
      "Training epoch 3 batch 262 with loss 1.7268173694610596.\n",
      "Training epoch 3 batch 263 with loss 1.9846616983413696.\n",
      "Training epoch 3 batch 264 with loss 1.8371628522872925.\n",
      "Training epoch 3 batch 265 with loss 1.9217396974563599.\n",
      "Training epoch 3 batch 266 with loss 1.8598586320877075.\n",
      "Training epoch 3 batch 267 with loss 2.1827685832977295.\n",
      "Training epoch 3 batch 268 with loss 1.5530076026916504.\n",
      "Training epoch 3 batch 269 with loss 2.141146659851074.\n",
      "Training epoch 3 batch 270 with loss 2.0483553409576416.\n",
      "Training epoch 3 batch 271 with loss 2.171114683151245.\n",
      "Training epoch 3 batch 272 with loss 2.0146639347076416.\n",
      "Training epoch 3 batch 273 with loss 1.9404925107955933.\n",
      "Training epoch 3 batch 274 with loss 1.711234211921692.\n",
      "Training epoch 3 batch 275 with loss 1.6400108337402344.\n",
      "Training epoch 3 batch 276 with loss 1.6780093908309937.\n",
      "Training epoch 3 batch 277 with loss 1.8999632596969604.\n",
      "Training epoch 3 batch 278 with loss 1.6856530904769897.\n",
      "Training epoch 3 batch 279 with loss 2.1457667350769043.\n",
      "Training epoch 3 batch 280 with loss 1.6681677103042603.\n",
      "Training epoch 3 batch 281 with loss 1.660090684890747.\n",
      "Training epoch 3 batch 282 with loss 2.017073631286621.\n",
      "Training epoch 3 batch 283 with loss 1.9325215816497803.\n",
      "Training epoch 3 batch 284 with loss 2.1343400478363037.\n",
      "Training epoch 3 batch 285 with loss 2.12399959564209.\n",
      "Training epoch 3 batch 286 with loss 1.8101693391799927.\n",
      "Training epoch 3 batch 287 with loss 1.3979235887527466.\n",
      "Training epoch 3 batch 288 with loss 2.0936365127563477.\n",
      "Training epoch 3 batch 289 with loss 1.766735553741455.\n",
      "Training epoch 3 batch 290 with loss 1.856070637702942.\n",
      "Training epoch 3 batch 291 with loss 1.6297547817230225.\n",
      "Training epoch 3 batch 292 with loss 1.5751209259033203.\n",
      "Training epoch 3 batch 293 with loss 1.78108811378479.\n",
      "Training epoch 3 batch 294 with loss 1.7145689725875854.\n",
      "Training epoch 3 batch 295 with loss 1.8543668985366821.\n",
      "Training epoch 3 batch 296 with loss 1.7727144956588745.\n",
      "Training epoch 3 batch 297 with loss 2.266704559326172.\n",
      "Training epoch 3 batch 298 with loss 1.8833808898925781.\n",
      "Training epoch 3 batch 299 with loss 1.3562655448913574.\n",
      "Training epoch 3 batch 300 with loss 1.8776754140853882.\n",
      "Training epoch 3 batch 301 with loss 1.73762845993042.\n",
      "Training epoch 3 batch 302 with loss 1.7250102758407593.\n",
      "Training epoch 3 batch 303 with loss 2.246032476425171.\n",
      "Training epoch 3 batch 304 with loss 2.0521185398101807.\n",
      "Training epoch 3 batch 305 with loss 1.9007068872451782.\n",
      "Training epoch 3 batch 306 with loss 1.7053358554840088.\n",
      "Training epoch 3 batch 307 with loss 1.6790823936462402.\n",
      "Training epoch 3 batch 308 with loss 1.7673661708831787.\n",
      "Training epoch 3 batch 309 with loss 1.9074212312698364.\n",
      "Training epoch 3 batch 310 with loss 1.8809943199157715.\n",
      "Training epoch 3 batch 311 with loss 2.0730292797088623.\n",
      "Training epoch 3 batch 312 with loss 1.842867136001587.\n",
      "Training epoch 3 batch 313 with loss 1.7570029497146606.\n",
      "Training epoch 3 batch 314 with loss 1.8836580514907837.\n",
      "Training epoch 3 batch 315 with loss 1.7501355409622192.\n",
      "Training epoch 3 batch 316 with loss 1.7611024379730225.\n",
      "Training epoch 3 batch 317 with loss 1.770463466644287.\n",
      "Training epoch 3 batch 318 with loss 1.8747278451919556.\n",
      "Training epoch 3 batch 319 with loss 1.8103139400482178.\n",
      "Training epoch 3 batch 320 with loss 1.5983641147613525.\n",
      "Training epoch 3 batch 321 with loss 1.658012866973877.\n",
      "Training epoch 3 batch 322 with loss 2.066209077835083.\n",
      "Training epoch 3 batch 323 with loss 2.087531566619873.\n",
      "Training epoch 3 batch 324 with loss 1.8416770696640015.\n",
      "Training epoch 3 batch 325 with loss 1.934571623802185.\n",
      "Training epoch 3 batch 326 with loss 1.8879727125167847.\n",
      "Training epoch 3 batch 327 with loss 1.7537672519683838.\n",
      "Training epoch 3 batch 328 with loss 2.086925983428955.\n",
      "Training epoch 3 batch 329 with loss 1.609454870223999.\n",
      "Training epoch 3 batch 330 with loss 2.1271958351135254.\n",
      "Training epoch 3 batch 331 with loss 2.1210877895355225.\n",
      "Training epoch 3 batch 332 with loss 1.85776948928833.\n",
      "Training epoch 3 batch 333 with loss 1.682946801185608.\n",
      "Training epoch 3 batch 334 with loss 2.001110792160034.\n",
      "Training epoch 3 batch 335 with loss 1.7490466833114624.\n",
      "Training epoch 3 batch 336 with loss 2.050616502761841.\n",
      "Training epoch 3 batch 337 with loss 1.8139458894729614.\n",
      "Training epoch 3 batch 338 with loss 2.1753439903259277.\n",
      "Training epoch 3 batch 339 with loss 1.730969786643982.\n",
      "Training epoch 3 batch 340 with loss 1.8667951822280884.\n",
      "Training epoch 3 batch 341 with loss 2.1319243907928467.\n",
      "Training epoch 3 batch 342 with loss 1.759031891822815.\n",
      "Training epoch 3 batch 343 with loss 1.8649311065673828.\n",
      "Training epoch 3 batch 344 with loss 1.8384884595870972.\n",
      "Training epoch 3 batch 345 with loss 1.7429983615875244.\n",
      "Training epoch 3 batch 346 with loss 1.9088642597198486.\n",
      "Training epoch 3 batch 347 with loss 1.5447120666503906.\n",
      "Training epoch 3 batch 348 with loss 1.9129157066345215.\n",
      "Training epoch 3 batch 349 with loss 1.641353726387024.\n",
      "Training epoch 3 batch 350 with loss 2.112217426300049.\n",
      "Training epoch 3 batch 351 with loss 1.6170023679733276.\n",
      "Training epoch 3 batch 352 with loss 1.9418237209320068.\n",
      "Training epoch 3 batch 353 with loss 2.0193116664886475.\n",
      "Training epoch 3 batch 354 with loss 1.5964158773422241.\n",
      "Training epoch 3 batch 355 with loss 1.6269562244415283.\n",
      "Training epoch 3 batch 356 with loss 1.9815794229507446.\n",
      "Training epoch 3 batch 357 with loss 1.5815556049346924.\n",
      "Training epoch 3 batch 358 with loss 2.0126211643218994.\n",
      "Training epoch 3 batch 359 with loss 1.8815444707870483.\n",
      "Training epoch 3 batch 360 with loss 1.9854568243026733.\n",
      "Training epoch 3 batch 361 with loss 1.653188705444336.\n",
      "Training epoch 3 batch 362 with loss 1.9045147895812988.\n",
      "Training epoch 3 batch 363 with loss 2.0548043251037598.\n",
      "Training epoch 3 batch 364 with loss 1.4432183504104614.\n",
      "Training epoch 3 batch 365 with loss 1.8654916286468506.\n",
      "Training epoch 3 batch 366 with loss 1.6927647590637207.\n",
      "Training epoch 3 batch 367 with loss 1.841245412826538.\n",
      "Training epoch 3 batch 368 with loss 2.232203960418701.\n",
      "Training epoch 3 batch 369 with loss 1.5174312591552734.\n",
      "Training epoch 3 batch 370 with loss 2.0063695907592773.\n",
      "Training epoch 3 batch 371 with loss 2.267192840576172.\n",
      "Training epoch 3 batch 372 with loss 2.2950446605682373.\n",
      "Training epoch 3 batch 373 with loss 1.9583306312561035.\n",
      "Training epoch 3 batch 374 with loss 2.154388427734375.\n",
      "Training epoch 3 batch 375 with loss 1.8635565042495728.\n",
      "Training epoch 3 batch 376 with loss 1.7406538724899292.\n",
      "Training epoch 3 batch 377 with loss 1.7332346439361572.\n",
      "Training epoch 3 batch 378 with loss 1.713042140007019.\n",
      "Training epoch 3 batch 379 with loss 1.9127930402755737.\n",
      "Training epoch 3 batch 380 with loss 1.7922836542129517.\n",
      "Training epoch 3 batch 381 with loss 2.3516435623168945.\n",
      "Training epoch 3 batch 382 with loss 2.212820529937744.\n",
      "Training epoch 3 batch 383 with loss 1.8914967775344849.\n",
      "Training epoch 3 batch 384 with loss 1.9133011102676392.\n",
      "Training epoch 3 batch 385 with loss 1.9335144758224487.\n",
      "Training epoch 3 batch 386 with loss 1.6053123474121094.\n",
      "Training epoch 3 batch 387 with loss 1.80275559425354.\n",
      "Training epoch 3 batch 388 with loss 1.8673210144042969.\n",
      "Training epoch 3 batch 389 with loss 2.1962039470672607.\n",
      "Training epoch 3 batch 390 with loss 2.0265543460845947.\n",
      "Training epoch 3 batch 391 with loss 2.125598669052124.\n",
      "Training epoch 3 batch 392 with loss 2.2615268230438232.\n",
      "Training epoch 3 batch 393 with loss 1.5022976398468018.\n",
      "Training epoch 3 batch 394 with loss 1.5975425243377686.\n",
      "Training epoch 3 batch 395 with loss 1.9249993562698364.\n",
      "Training epoch 3 batch 396 with loss 1.8284990787506104.\n",
      "Training epoch 3 batch 397 with loss 2.005521297454834.\n",
      "Training epoch 3 batch 398 with loss 2.204500675201416.\n",
      "Training epoch 3 batch 399 with loss 1.927590250968933.\n",
      "Training epoch 3 batch 400 with loss 1.8570971488952637.\n",
      "Training epoch 3 batch 401 with loss 2.0011117458343506.\n",
      "Training epoch 3 batch 402 with loss 2.135906219482422.\n",
      "Training epoch 3 batch 403 with loss 1.55556058883667.\n",
      "Training epoch 3 batch 404 with loss 2.1993837356567383.\n",
      "Training epoch 3 batch 405 with loss 1.7429547309875488.\n",
      "Training epoch 3 batch 406 with loss 2.161987066268921.\n",
      "Training epoch 3 batch 407 with loss 1.9734376668930054.\n",
      "Training epoch 3 batch 408 with loss 1.9697346687316895.\n",
      "Training epoch 3 batch 409 with loss 2.036278009414673.\n",
      "Training epoch 3 batch 410 with loss 1.7585618495941162.\n",
      "Training epoch 3 batch 411 with loss 2.072218656539917.\n",
      "Training epoch 3 batch 412 with loss 2.0181806087493896.\n",
      "Training epoch 3 batch 413 with loss 2.035951852798462.\n",
      "Training epoch 3 batch 414 with loss 1.8683916330337524.\n",
      "Training epoch 3 batch 415 with loss 2.1609482765197754.\n",
      "Training epoch 3 batch 416 with loss 1.8545714616775513.\n",
      "Training epoch 3 batch 417 with loss 1.954433560371399.\n",
      "Training epoch 3 batch 418 with loss 2.0164663791656494.\n",
      "Training epoch 3 batch 419 with loss 2.102477788925171.\n",
      "Training epoch 3 batch 420 with loss 2.3417258262634277.\n",
      "Training epoch 3 batch 421 with loss 1.9612854719161987.\n",
      "Training epoch 3 batch 422 with loss 2.093670606613159.\n",
      "Training epoch 3 batch 423 with loss 1.9517947435379028.\n",
      "Training epoch 3 batch 424 with loss 1.9912053346633911.\n",
      "Training epoch 3 batch 425 with loss 1.830014705657959.\n",
      "Training epoch 3 batch 426 with loss 1.9190919399261475.\n",
      "Training epoch 3 batch 427 with loss 1.8785005807876587.\n",
      "Training epoch 3 batch 428 with loss 1.7666099071502686.\n",
      "Training epoch 3 batch 429 with loss 1.6033936738967896.\n",
      "Training epoch 3 batch 430 with loss 2.182532787322998.\n",
      "Training epoch 3 batch 431 with loss 2.2269210815429688.\n",
      "Training epoch 3 batch 432 with loss 1.5198590755462646.\n",
      "Training epoch 3 batch 433 with loss 1.924254298210144.\n",
      "Training epoch 3 batch 434 with loss 2.1537349224090576.\n",
      "Training epoch 3 batch 435 with loss 2.1841228008270264.\n",
      "Training epoch 3 batch 436 with loss 1.8179088830947876.\n",
      "Training epoch 3 batch 437 with loss 1.8654592037200928.\n",
      "Training epoch 3 batch 438 with loss 1.813387155532837.\n",
      "Training epoch 3 batch 439 with loss 1.5960803031921387.\n",
      "Training epoch 3 batch 440 with loss 1.8090037107467651.\n",
      "Training epoch 3 batch 441 with loss 2.41630220413208.\n",
      "Training epoch 3 batch 442 with loss 2.014159917831421.\n",
      "Training epoch 3 batch 443 with loss 2.0241410732269287.\n",
      "Training epoch 3 batch 444 with loss 1.8836643695831299.\n",
      "Training epoch 3 batch 445 with loss 1.7143292427062988.\n",
      "Training epoch 3 batch 446 with loss 1.8727658987045288.\n",
      "Training epoch 3 batch 447 with loss 2.1773109436035156.\n",
      "Training epoch 3 batch 448 with loss 1.9938864707946777.\n",
      "Training epoch 3 batch 449 with loss 1.7957420349121094.\n",
      "Training epoch 3 batch 450 with loss 2.695854663848877.\n",
      "Training epoch 3 batch 451 with loss 2.09177565574646.\n",
      "Training epoch 3 batch 452 with loss 2.008864641189575.\n",
      "Training epoch 3 batch 453 with loss 1.6196541786193848.\n",
      "Training epoch 3 batch 454 with loss 1.8918876647949219.\n",
      "Training epoch 3 batch 455 with loss 1.7366600036621094.\n",
      "Training epoch 3 batch 456 with loss 2.0115983486175537.\n",
      "Training epoch 3 batch 457 with loss 2.344895124435425.\n",
      "Training epoch 3 batch 458 with loss 2.322622537612915.\n",
      "Training epoch 3 batch 459 with loss 1.9076588153839111.\n",
      "Training epoch 3 batch 460 with loss 1.9267574548721313.\n",
      "Training epoch 3 batch 461 with loss 2.1060714721679688.\n",
      "Training epoch 3 batch 462 with loss 2.0458669662475586.\n",
      "Training epoch 3 batch 463 with loss 1.8247770071029663.\n",
      "Training epoch 3 batch 464 with loss 1.7425193786621094.\n",
      "Training epoch 3 batch 465 with loss 1.8001854419708252.\n",
      "Training epoch 3 batch 466 with loss 1.8273530006408691.\n",
      "Training epoch 3 batch 467 with loss 1.9211385250091553.\n",
      "Training epoch 3 batch 468 with loss 1.897175908088684.\n",
      "Training epoch 3 batch 469 with loss 1.8143067359924316.\n",
      "Training epoch 3 batch 470 with loss 2.045224905014038.\n",
      "Training epoch 3 batch 471 with loss 1.555356740951538.\n",
      "Training epoch 3 batch 472 with loss 2.2851035594940186.\n",
      "Training epoch 3 batch 473 with loss 2.2194342613220215.\n",
      "Training epoch 3 batch 474 with loss 1.9250096082687378.\n",
      "Training epoch 3 batch 475 with loss 1.9479583501815796.\n",
      "Training epoch 3 batch 476 with loss 2.1832854747772217.\n",
      "Training epoch 3 batch 477 with loss 2.1551225185394287.\n",
      "Training epoch 3 batch 478 with loss 1.7871819734573364.\n",
      "Training epoch 3 batch 479 with loss 1.7845698595046997.\n",
      "Training epoch 3 batch 480 with loss 1.762621283531189.\n",
      "Training epoch 3 batch 481 with loss 2.3122222423553467.\n",
      "Training epoch 3 batch 482 with loss 2.194636106491089.\n",
      "Training epoch 3 batch 483 with loss 2.1444413661956787.\n",
      "Training epoch 3 batch 484 with loss 2.3542561531066895.\n",
      "Training epoch 3 batch 485 with loss 1.6430026292800903.\n",
      "Training epoch 3 batch 486 with loss 2.2453160285949707.\n",
      "Training epoch 3 batch 487 with loss 2.06954026222229.\n",
      "Training epoch 3 batch 488 with loss 2.2096612453460693.\n",
      "Training epoch 3 batch 489 with loss 1.9399851560592651.\n",
      "Training epoch 3 batch 490 with loss 1.6805437803268433.\n",
      "Training epoch 3 batch 491 with loss 1.8702492713928223.\n",
      "Training epoch 3 batch 492 with loss 1.8257802724838257.\n",
      "Training epoch 3 batch 493 with loss 2.0900321006774902.\n",
      "Training epoch 3 batch 494 with loss 1.9590423107147217.\n",
      "Training epoch 3 batch 495 with loss 1.8698840141296387.\n",
      "Training epoch 3 batch 496 with loss 2.376591682434082.\n",
      "Training epoch 3 batch 497 with loss 1.901450514793396.\n",
      "Training epoch 3 batch 498 with loss 1.7855921983718872.\n",
      "Training epoch 3 batch 499 with loss 2.1551566123962402.\n",
      "Training epoch 3 batch 500 with loss 2.3673298358917236.\n",
      "Training epoch 3 batch 501 with loss 2.1270511150360107.\n",
      "Training epoch 3 batch 502 with loss 2.196110963821411.\n",
      "Training epoch 3 batch 503 with loss 1.821404218673706.\n",
      "Training epoch 3 batch 504 with loss 2.0847361087799072.\n",
      "Training epoch 3 batch 505 with loss 1.8469523191452026.\n",
      "Training epoch 3 batch 506 with loss 2.0860846042633057.\n",
      "Training epoch 3 batch 507 with loss 1.7498548030853271.\n",
      "Training epoch 3 batch 508 with loss 1.9657189846038818.\n",
      "Training epoch 3 batch 509 with loss 2.1121296882629395.\n",
      "Training epoch 3 batch 510 with loss 2.1239242553710938.\n",
      "Training epoch 3 batch 511 with loss 1.8723002672195435.\n",
      "Training epoch 3 batch 512 with loss 1.8122142553329468.\n",
      "Training epoch 3 batch 513 with loss 2.2414677143096924.\n",
      "Training epoch 3 batch 514 with loss 2.1185781955718994.\n",
      "Training epoch 3 batch 515 with loss 2.3155386447906494.\n",
      "Training epoch 3 batch 516 with loss 2.0572214126586914.\n",
      "Training epoch 3 batch 517 with loss 2.1288421154022217.\n",
      "Training epoch 3 batch 518 with loss 1.8996578454971313.\n",
      "Training epoch 3 batch 519 with loss 1.5943081378936768.\n",
      "Training epoch 3 batch 520 with loss 1.9970732927322388.\n",
      "Training epoch 3 batch 521 with loss 1.79512357711792.\n",
      "Training epoch 3 batch 522 with loss 1.8886432647705078.\n",
      "Training epoch 3 batch 523 with loss 2.109687566757202.\n",
      "Training epoch 3 batch 524 with loss 2.125627279281616.\n",
      "Training epoch 3 batch 525 with loss 1.913356065750122.\n",
      "Training epoch 3 batch 526 with loss 2.1649322509765625.\n",
      "Training epoch 3 batch 527 with loss 1.9422675371170044.\n",
      "Training epoch 3 batch 528 with loss 2.2036614418029785.\n",
      "Training epoch 3 batch 529 with loss 1.881676197052002.\n",
      "Training epoch 3 batch 530 with loss 2.0542213916778564.\n",
      "Training epoch 3 batch 531 with loss 1.76334810256958.\n",
      "Training epoch 3 batch 532 with loss 2.2036712169647217.\n",
      "Training epoch 3 batch 533 with loss 1.6913340091705322.\n",
      "Training epoch 3 batch 534 with loss 2.168954372406006.\n",
      "Training epoch 3 batch 535 with loss 1.879412055015564.\n",
      "Training epoch 3 batch 536 with loss 2.3060977458953857.\n",
      "Training epoch 3 batch 537 with loss 2.0940167903900146.\n",
      "Training epoch 3 batch 538 with loss 2.176377296447754.\n",
      "Training epoch 3 batch 539 with loss 1.5804630517959595.\n",
      "Training epoch 3 batch 540 with loss 1.9252568483352661.\n",
      "Training epoch 3 batch 541 with loss 1.9879539012908936.\n",
      "Training epoch 3 batch 542 with loss 2.0050127506256104.\n",
      "Training epoch 3 batch 543 with loss 1.9466832876205444.\n",
      "Training epoch 3 batch 544 with loss 2.27557110786438.\n",
      "Training epoch 3 batch 545 with loss 2.314211130142212.\n",
      "Training epoch 3 batch 546 with loss 2.165217876434326.\n",
      "Training epoch 3 batch 547 with loss 2.033073663711548.\n",
      "Training epoch 3 batch 548 with loss 1.966371774673462.\n",
      "Training epoch 3 batch 549 with loss 1.4885921478271484.\n",
      "Training epoch 3 batch 550 with loss 2.218480348587036.\n",
      "Training epoch 3 batch 551 with loss 2.5294506549835205.\n",
      "Training epoch 3 batch 552 with loss 2.0976805686950684.\n",
      "Training epoch 3 batch 553 with loss 1.623665452003479.\n",
      "Training epoch 3 batch 554 with loss 1.6551297903060913.\n",
      "Training epoch 3 batch 555 with loss 1.985503077507019.\n",
      "Training epoch 3 batch 556 with loss 2.16330885887146.\n",
      "Training epoch 3 batch 557 with loss 2.020608425140381.\n",
      "Training epoch 3 batch 558 with loss 2.032813787460327.\n",
      "Training epoch 3 batch 559 with loss 1.6941901445388794.\n",
      "Training epoch 3 batch 560 with loss 2.0279481410980225.\n",
      "Training epoch 3 batch 561 with loss 2.2027106285095215.\n",
      "Training epoch 3 batch 562 with loss 2.2828710079193115.\n",
      "Training epoch 3 batch 563 with loss 1.750658392906189.\n",
      "Training epoch 3 batch 564 with loss 1.8146239519119263.\n",
      "Training epoch 3 batch 565 with loss 2.169863224029541.\n",
      "Training epoch 3 batch 566 with loss 1.8340626955032349.\n",
      "Training epoch 3 batch 567 with loss 1.9536651372909546.\n",
      "Training epoch 3 batch 568 with loss 2.231710195541382.\n",
      "Training epoch 3 batch 569 with loss 2.522963762283325.\n",
      "Training epoch 3 batch 570 with loss 1.9558018445968628.\n",
      "Training epoch 3 batch 571 with loss 2.2846643924713135.\n",
      "Training epoch 3 batch 572 with loss 1.7651677131652832.\n",
      "Training epoch 3 batch 573 with loss 2.215980291366577.\n",
      "Training epoch 3 batch 574 with loss 1.903298020362854.\n",
      "Training epoch 3 batch 575 with loss 2.065856456756592.\n",
      "Training epoch 3 batch 576 with loss 2.0488433837890625.\n",
      "Training epoch 3 batch 577 with loss 2.1202433109283447.\n",
      "Training epoch 3 batch 578 with loss 1.6835957765579224.\n",
      "Training epoch 3 batch 579 with loss 1.8396008014678955.\n",
      "Training epoch 3 batch 580 with loss 1.8551535606384277.\n",
      "Training epoch 3 batch 581 with loss 1.639198660850525.\n",
      "Training epoch 3 batch 582 with loss 2.0752861499786377.\n",
      "Training epoch 3 batch 583 with loss 1.7277488708496094.\n",
      "Training epoch 3 batch 584 with loss 1.8582462072372437.\n",
      "Training epoch 3 batch 585 with loss 2.0276894569396973.\n",
      "Training epoch 3 batch 586 with loss 1.9988129138946533.\n",
      "Training epoch 3 batch 587 with loss 1.7368583679199219.\n",
      "Training epoch 3 batch 588 with loss 2.268172025680542.\n",
      "Training epoch 3 batch 589 with loss 1.895574927330017.\n",
      "Training epoch 3 batch 590 with loss 1.9928821325302124.\n",
      "Training epoch 3 batch 591 with loss 1.9723156690597534.\n",
      "Training epoch 3 batch 592 with loss 1.9407024383544922.\n",
      "Training epoch 3 batch 593 with loss 1.8699437379837036.\n",
      "Training epoch 3 batch 594 with loss 1.860246181488037.\n",
      "Training epoch 3 batch 595 with loss 1.9605125188827515.\n",
      "Training epoch 3 batch 596 with loss 2.043949604034424.\n",
      "Training epoch 3 batch 597 with loss 1.9547022581100464.\n",
      "Training epoch 3 batch 598 with loss 1.899559497833252.\n",
      "Training epoch 3 batch 599 with loss 2.1556005477905273.\n",
      "Training epoch 3 batch 600 with loss 1.7611541748046875.\n",
      "Training epoch 3 batch 601 with loss 1.765209674835205.\n",
      "Training epoch 3 batch 602 with loss 1.973009705543518.\n",
      "Training epoch 3 batch 603 with loss 2.0312821865081787.\n",
      "Training epoch 3 batch 604 with loss 2.24820876121521.\n",
      "Training epoch 3 batch 605 with loss 1.9209582805633545.\n",
      "Training epoch 3 batch 606 with loss 2.004171848297119.\n",
      "Training epoch 3 batch 607 with loss 2.1629951000213623.\n",
      "Training epoch 3 batch 608 with loss 2.195671319961548.\n",
      "Training epoch 3 batch 609 with loss 2.0879733562469482.\n",
      "Training epoch 3 batch 610 with loss 1.9284071922302246.\n",
      "Training epoch 3 batch 611 with loss 1.626952052116394.\n",
      "Training epoch 3 batch 612 with loss 2.165691614151001.\n",
      "Training epoch 3 batch 613 with loss 2.1266751289367676.\n",
      "Training epoch 3 batch 614 with loss 2.146481513977051.\n",
      "Training epoch 3 batch 615 with loss 1.927263617515564.\n",
      "Training epoch 3 batch 616 with loss 2.2019381523132324.\n",
      "Training epoch 3 batch 617 with loss 1.9986748695373535.\n",
      "Training epoch 3 batch 618 with loss 2.015580415725708.\n",
      "Training epoch 3 batch 619 with loss 1.8842719793319702.\n",
      "Training epoch 3 batch 620 with loss 1.9550639390945435.\n",
      "Training epoch 3 batch 621 with loss 2.0440666675567627.\n",
      "Training epoch 3 batch 622 with loss 2.2659995555877686.\n",
      "Training epoch 3 batch 623 with loss 2.342784881591797.\n",
      "Training epoch 3 batch 624 with loss 2.441586494445801.\n",
      "Training epoch 3 batch 625 with loss 2.3030402660369873.\n",
      "Training epoch 3 batch 626 with loss 2.3748066425323486.\n",
      "Training epoch 3 batch 627 with loss 1.882270097732544.\n",
      "Training epoch 3 batch 628 with loss 2.2077927589416504.\n",
      "Training epoch 3 batch 629 with loss 1.93659508228302.\n",
      "Training epoch 3 batch 630 with loss 2.3278396129608154.\n",
      "Training epoch 3 batch 631 with loss 2.0064358711242676.\n",
      "Training epoch 3 batch 632 with loss 1.8838293552398682.\n",
      "Training epoch 3 batch 633 with loss 2.3092846870422363.\n",
      "Training epoch 3 batch 634 with loss 2.2615597248077393.\n",
      "Training epoch 3 batch 635 with loss 1.8366369009017944.\n",
      "Training epoch 3 batch 636 with loss 2.0805537700653076.\n",
      "Training epoch 3 batch 637 with loss 2.0106513500213623.\n",
      "Training epoch 3 batch 638 with loss 1.8129794597625732.\n",
      "Training epoch 3 batch 639 with loss 2.199819326400757.\n",
      "Training epoch 3 batch 640 with loss 1.9961501359939575.\n",
      "Training epoch 3 batch 641 with loss 1.8379641771316528.\n",
      "Training epoch 3 batch 642 with loss 2.3833720684051514.\n",
      "Training epoch 3 batch 643 with loss 2.0892205238342285.\n",
      "Training epoch 3 batch 644 with loss 2.5519957542419434.\n",
      "Training epoch 3 batch 645 with loss 2.007073163986206.\n",
      "Training epoch 3 batch 646 with loss 2.2935876846313477.\n",
      "Training epoch 3 batch 647 with loss 1.8923174142837524.\n",
      "Training epoch 3 batch 648 with loss 1.8623830080032349.\n",
      "Training epoch 3 batch 649 with loss 2.223254919052124.\n",
      "Training epoch 3 batch 650 with loss 1.564233422279358.\n",
      "Training epoch 3 batch 651 with loss 1.9168760776519775.\n",
      "Training epoch 3 batch 652 with loss 2.3318161964416504.\n",
      "Training epoch 3 batch 653 with loss 1.9778012037277222.\n",
      "Training epoch 3 batch 654 with loss 2.2897117137908936.\n",
      "Training epoch 3 batch 655 with loss 1.7995495796203613.\n",
      "Training epoch 3 batch 656 with loss 2.4066827297210693.\n",
      "Training epoch 3 batch 657 with loss 2.2434325218200684.\n",
      "Training epoch 3 batch 658 with loss 1.7976324558258057.\n",
      "Training epoch 3 batch 659 with loss 2.349655866622925.\n",
      "Training epoch 3 batch 660 with loss 2.4244070053100586.\n",
      "Training epoch 3 batch 661 with loss 2.1038997173309326.\n",
      "Training epoch 3 batch 662 with loss 1.9622598886489868.\n",
      "Training epoch 3 batch 663 with loss 1.88356351852417.\n",
      "Training epoch 3 batch 664 with loss 1.8726383447647095.\n",
      "Training epoch 3 batch 665 with loss 1.9445347785949707.\n",
      "Training epoch 3 batch 666 with loss 2.3558030128479004.\n",
      "Training epoch 3 batch 667 with loss 2.0394487380981445.\n",
      "Training epoch 3 batch 668 with loss 2.3737759590148926.\n",
      "Training epoch 3 batch 669 with loss 2.136080741882324.\n",
      "Training epoch 3 batch 670 with loss 2.2579848766326904.\n",
      "Training epoch 3 batch 671 with loss 1.619931697845459.\n",
      "Training epoch 3 batch 672 with loss 2.4035425186157227.\n",
      "Training epoch 3 batch 673 with loss 1.6598018407821655.\n",
      "Training epoch 3 batch 674 with loss 1.988258957862854.\n",
      "Training epoch 3 batch 675 with loss 2.166207790374756.\n",
      "Training epoch 3 batch 676 with loss 2.3110835552215576.\n",
      "Training epoch 3 batch 677 with loss 2.2306716442108154.\n",
      "Training epoch 3 batch 678 with loss 2.1821441650390625.\n",
      "Training epoch 3 batch 679 with loss 1.8307381868362427.\n",
      "Training epoch 3 batch 680 with loss 1.948517084121704.\n",
      "Training epoch 3 batch 681 with loss 2.2077341079711914.\n",
      "Training epoch 3 batch 682 with loss 2.117953062057495.\n",
      "Training epoch 3 batch 683 with loss 2.0018506050109863.\n",
      "Training epoch 3 batch 684 with loss 1.8951396942138672.\n",
      "Training epoch 3 batch 685 with loss 2.18503475189209.\n",
      "Training epoch 3 batch 686 with loss 1.9016592502593994.\n",
      "Training epoch 3 batch 687 with loss 1.7643884420394897.\n",
      "Training epoch 3 batch 688 with loss 1.877461314201355.\n",
      "Training epoch 3 batch 689 with loss 2.601395606994629.\n",
      "Training epoch 3 batch 690 with loss 1.5228623151779175.\n",
      "Training epoch 3 batch 691 with loss 2.2782247066497803.\n",
      "Training epoch 3 batch 692 with loss 2.156402587890625.\n",
      "Training epoch 3 batch 693 with loss 1.8953136205673218.\n",
      "Training epoch 3 batch 694 with loss 2.664019823074341.\n",
      "Training epoch 3 batch 695 with loss 1.8520698547363281.\n",
      "Training epoch 3 batch 696 with loss 1.45963454246521.\n",
      "Training epoch 3 batch 697 with loss 1.87553071975708.\n",
      "Training epoch 3 batch 698 with loss 1.9696505069732666.\n",
      "Training epoch 3 batch 699 with loss 2.4713926315307617.\n",
      "Training epoch 3 batch 700 with loss 1.7849714756011963.\n",
      "Training epoch 3 batch 701 with loss 1.885933518409729.\n",
      "Training epoch 3 batch 702 with loss 2.195950508117676.\n",
      "Training epoch 3 batch 703 with loss 2.1549761295318604.\n",
      "Training epoch 3 batch 704 with loss 1.9387831687927246.\n",
      "Training epoch 3 batch 705 with loss 2.175513982772827.\n",
      "Training epoch 3 batch 706 with loss 2.3980705738067627.\n",
      "Training epoch 3 batch 707 with loss 2.2757503986358643.\n",
      "Training epoch 3 batch 708 with loss 2.1568849086761475.\n",
      "Training epoch 3 batch 709 with loss 1.824396014213562.\n",
      "Training epoch 3 batch 710 with loss 1.897179126739502.\n",
      "Training epoch 3 batch 711 with loss 2.0495829582214355.\n",
      "Training epoch 3 batch 712 with loss 2.1123385429382324.\n",
      "Training epoch 3 batch 713 with loss 2.1601617336273193.\n",
      "Training epoch 3 batch 714 with loss 1.9387242794036865.\n",
      "Training epoch 3 batch 715 with loss 2.008584499359131.\n",
      "Training epoch 3 batch 716 with loss 2.3271803855895996.\n",
      "Training epoch 3 batch 717 with loss 2.179607629776001.\n",
      "Training epoch 3 batch 718 with loss 1.7429890632629395.\n",
      "Training epoch 3 batch 719 with loss 1.7847992181777954.\n",
      "Training epoch 3 batch 720 with loss 2.2263712882995605.\n",
      "Training epoch 3 batch 721 with loss 2.0436971187591553.\n",
      "Training epoch 3 batch 722 with loss 2.1857361793518066.\n",
      "Training epoch 3 batch 723 with loss 1.8631373643875122.\n",
      "Training epoch 3 batch 724 with loss 2.246546506881714.\n",
      "Training epoch 3 batch 725 with loss 2.1143994331359863.\n",
      "Training epoch 3 batch 726 with loss 2.0485522747039795.\n",
      "Training epoch 3 batch 727 with loss 1.7883379459381104.\n",
      "Training epoch 3 batch 728 with loss 2.1611738204956055.\n",
      "Training epoch 3 batch 729 with loss 2.48343825340271.\n",
      "Training epoch 3 batch 730 with loss 2.298870325088501.\n",
      "Training epoch 3 batch 731 with loss 2.078732490539551.\n",
      "Training epoch 3 batch 732 with loss 1.9604824781417847.\n",
      "Training epoch 3 batch 733 with loss 2.2847018241882324.\n",
      "Training epoch 3 batch 734 with loss 2.0003983974456787.\n",
      "Training epoch 3 batch 735 with loss 2.0462183952331543.\n",
      "Training epoch 3 batch 736 with loss 2.2436840534210205.\n",
      "Training epoch 3 batch 737 with loss 1.9129520654678345.\n",
      "Training epoch 3 batch 738 with loss 2.3563969135284424.\n",
      "Training epoch 3 batch 739 with loss 2.1548683643341064.\n",
      "Training epoch 3 batch 740 with loss 1.833254098892212.\n",
      "Training epoch 3 batch 741 with loss 1.955443024635315.\n",
      "Training epoch 3 batch 742 with loss 2.2377569675445557.\n",
      "Training epoch 3 batch 743 with loss 1.6902161836624146.\n",
      "Training epoch 3 batch 744 with loss 2.0561323165893555.\n",
      "Training epoch 3 batch 745 with loss 2.3029303550720215.\n",
      "Training epoch 3 batch 746 with loss 1.6428618431091309.\n",
      "Training epoch 3 batch 747 with loss 1.963138461112976.\n",
      "Training epoch 3 batch 748 with loss 2.532515525817871.\n",
      "Training epoch 3 batch 749 with loss 2.122579336166382.\n",
      "Training epoch 3 batch 750 with loss 2.1728453636169434.\n",
      "Training epoch 3 batch 751 with loss 2.071735143661499.\n",
      "Training epoch 3 batch 752 with loss 1.9089964628219604.\n",
      "Training epoch 3 batch 753 with loss 2.0426881313323975.\n",
      "Training epoch 3 batch 754 with loss 2.3475122451782227.\n",
      "Training epoch 3 batch 755 with loss 2.199251890182495.\n",
      "Training epoch 3 batch 756 with loss 1.9996825456619263.\n",
      "Training epoch 3 batch 757 with loss 1.6435455083847046.\n",
      "Training epoch 3 batch 758 with loss 1.910353183746338.\n",
      "Training epoch 3 batch 759 with loss 1.8719348907470703.\n",
      "Training epoch 3 batch 760 with loss 2.0914082527160645.\n",
      "Training epoch 3 batch 761 with loss 2.086995840072632.\n",
      "Training epoch 3 batch 762 with loss 2.0759692192077637.\n",
      "Training epoch 3 batch 763 with loss 2.358421564102173.\n",
      "Training epoch 3 batch 764 with loss 2.0577006340026855.\n",
      "Training epoch 3 batch 765 with loss 1.9274983406066895.\n",
      "Training epoch 3 batch 766 with loss 2.014129877090454.\n",
      "Training epoch 3 batch 767 with loss 2.0982871055603027.\n",
      "Training epoch 3 batch 768 with loss 2.1729156970977783.\n",
      "Training epoch 3 batch 769 with loss 2.0973517894744873.\n",
      "Training epoch 3 batch 770 with loss 2.4154884815216064.\n",
      "Training epoch 3 batch 771 with loss 2.069643974304199.\n",
      "Training epoch 3 batch 772 with loss 1.8997960090637207.\n",
      "Training epoch 3 batch 773 with loss 1.763906478881836.\n",
      "Training epoch 3 batch 774 with loss 2.035639524459839.\n",
      "Training epoch 3 batch 775 with loss 2.2400879859924316.\n",
      "Training epoch 3 batch 776 with loss 1.883895993232727.\n",
      "Training epoch 3 batch 777 with loss 1.7979638576507568.\n",
      "Training epoch 3 batch 778 with loss 2.2671401500701904.\n",
      "Training epoch 3 batch 779 with loss 1.7599682807922363.\n",
      "Training epoch 3 batch 780 with loss 1.8010669946670532.\n",
      "Training epoch 3 batch 781 with loss 2.2567436695098877.\n",
      "Training epoch 3 batch 782 with loss 2.1505110263824463.\n",
      "Training epoch 3 batch 783 with loss 2.2116148471832275.\n",
      "Training epoch 3 batch 784 with loss 1.6632071733474731.\n",
      "Training epoch 3 batch 785 with loss 1.9738048315048218.\n",
      "Training epoch 3 batch 786 with loss 2.2390475273132324.\n",
      "Training epoch 3 batch 787 with loss 2.183892250061035.\n",
      "Training epoch 3 batch 788 with loss 2.3027658462524414.\n",
      "Training epoch 3 batch 789 with loss 2.0989673137664795.\n",
      "Training epoch 3 batch 790 with loss 2.1708288192749023.\n",
      "Training epoch 3 batch 791 with loss 1.927629828453064.\n",
      "Training epoch 3 batch 792 with loss 2.081885576248169.\n",
      "Training epoch 3 batch 793 with loss 2.2463018894195557.\n",
      "Training epoch 3 batch 794 with loss 2.4779269695281982.\n",
      "Training epoch 3 batch 795 with loss 2.0765349864959717.\n",
      "Training epoch 3 batch 796 with loss 1.9081249237060547.\n",
      "Training epoch 3 batch 797 with loss 2.103444814682007.\n",
      "Training epoch 3 batch 798 with loss 2.4553704261779785.\n",
      "Training epoch 3 batch 799 with loss 1.6328054666519165.\n",
      "Training epoch 3 batch 800 with loss 1.7465159893035889.\n",
      "Training epoch 3 batch 801 with loss 2.397869348526001.\n",
      "Training epoch 3 batch 802 with loss 1.852169156074524.\n",
      "Training epoch 3 batch 803 with loss 1.8648197650909424.\n",
      "Training epoch 3 batch 804 with loss 1.9070178270339966.\n",
      "Training epoch 3 batch 805 with loss 2.252664089202881.\n",
      "Training epoch 3 batch 806 with loss 1.832418441772461.\n",
      "Training epoch 3 batch 807 with loss 2.347081184387207.\n",
      "Training epoch 3 batch 808 with loss 2.3167502880096436.\n",
      "Training epoch 3 batch 809 with loss 2.5411264896392822.\n",
      "Training epoch 3 batch 810 with loss 1.7739758491516113.\n",
      "Training epoch 3 batch 811 with loss 1.8857554197311401.\n",
      "Training epoch 3 batch 812 with loss 1.995566964149475.\n",
      "Training epoch 3 batch 813 with loss 2.200319766998291.\n",
      "Training epoch 3 batch 814 with loss 2.0650599002838135.\n",
      "Training epoch 3 batch 815 with loss 1.9249380826950073.\n",
      "Training epoch 3 batch 816 with loss 2.2991843223571777.\n",
      "Training epoch 3 batch 817 with loss 2.318084955215454.\n",
      "Training epoch 3 batch 818 with loss 2.0028481483459473.\n",
      "Training epoch 3 batch 819 with loss 2.296412229537964.\n",
      "Training epoch 3 batch 820 with loss 1.9130586385726929.\n",
      "Training epoch 3 batch 821 with loss 2.0537357330322266.\n",
      "Training epoch 3 batch 822 with loss 1.736477017402649.\n",
      "Training epoch 3 batch 823 with loss 2.2986855506896973.\n",
      "Training epoch 3 batch 824 with loss 2.3406949043273926.\n",
      "Training epoch 3 batch 825 with loss 2.0463709831237793.\n",
      "Training epoch 3 batch 826 with loss 1.853253960609436.\n",
      "Training epoch 3 batch 827 with loss 1.9301629066467285.\n",
      "Training epoch 3 batch 828 with loss 2.11269211769104.\n",
      "Training epoch 3 batch 829 with loss 2.207911729812622.\n",
      "Training epoch 3 batch 830 with loss 2.2048017978668213.\n",
      "Training epoch 3 batch 831 with loss 2.143433094024658.\n",
      "Training epoch 3 batch 832 with loss 2.1364259719848633.\n",
      "Training epoch 3 batch 833 with loss 1.9123376607894897.\n",
      "Training epoch 3 batch 834 with loss 1.8824251890182495.\n",
      "Training epoch 3 batch 835 with loss 1.8459880352020264.\n",
      "Training epoch 3 batch 836 with loss 2.0146329402923584.\n",
      "Training epoch 3 batch 837 with loss 1.8481643199920654.\n",
      "Training epoch 3 batch 838 with loss 1.9000399112701416.\n",
      "Training epoch 3 batch 839 with loss 1.9670194387435913.\n",
      "Training epoch 3 batch 840 with loss 1.9194679260253906.\n",
      "Training epoch 3 batch 841 with loss 2.0158913135528564.\n",
      "Training epoch 3 batch 842 with loss 2.090566635131836.\n",
      "Training epoch 3 batch 843 with loss 2.223907232284546.\n",
      "Training epoch 3 batch 844 with loss 2.06915545463562.\n",
      "Training epoch 3 batch 845 with loss 2.027308225631714.\n",
      "Training epoch 3 batch 846 with loss 1.8924771547317505.\n",
      "Training epoch 3 batch 847 with loss 2.009045124053955.\n",
      "Training epoch 3 batch 848 with loss 1.9925868511199951.\n",
      "Training epoch 3 batch 849 with loss 2.160159111022949.\n",
      "Training epoch 3 batch 850 with loss 2.2594947814941406.\n",
      "Training epoch 3 batch 851 with loss 2.0716865062713623.\n",
      "Training epoch 3 batch 852 with loss 1.6518399715423584.\n",
      "Training epoch 3 batch 853 with loss 2.272880792617798.\n",
      "Training epoch 3 batch 854 with loss 2.2604832649230957.\n",
      "Training epoch 3 batch 855 with loss 2.2091004848480225.\n",
      "Training epoch 3 batch 856 with loss 1.828033208847046.\n",
      "Training epoch 3 batch 857 with loss 1.8383554220199585.\n",
      "Training epoch 3 batch 858 with loss 1.978533387184143.\n",
      "Training epoch 3 batch 859 with loss 2.0759222507476807.\n",
      "Training epoch 3 batch 860 with loss 2.1472480297088623.\n",
      "Training epoch 3 batch 861 with loss 2.305556058883667.\n",
      "Training epoch 3 batch 862 with loss 2.107698440551758.\n",
      "Training epoch 3 batch 863 with loss 2.0021884441375732.\n",
      "Training epoch 3 batch 864 with loss 2.1100709438323975.\n",
      "Training epoch 3 batch 865 with loss 2.0030629634857178.\n",
      "Training epoch 3 batch 866 with loss 2.1814029216766357.\n",
      "Training epoch 3 batch 867 with loss 1.9346121549606323.\n",
      "Training epoch 3 batch 868 with loss 2.4970672130584717.\n",
      "Training epoch 3 batch 869 with loss 2.1954703330993652.\n",
      "Training epoch 3 batch 870 with loss 2.3492777347564697.\n",
      "Training epoch 3 batch 871 with loss 2.388068199157715.\n",
      "Training epoch 3 batch 872 with loss 2.0959651470184326.\n",
      "Training epoch 3 batch 873 with loss 2.1447396278381348.\n",
      "Training epoch 3 batch 874 with loss 1.4282244443893433.\n",
      "Training epoch 3 batch 875 with loss 1.9699883460998535.\n",
      "Training epoch 3 batch 876 with loss 2.081331968307495.\n",
      "Training epoch 3 batch 877 with loss 2.315887212753296.\n",
      "Training epoch 3 batch 878 with loss 1.9557673931121826.\n",
      "Training epoch 3 batch 879 with loss 2.121788263320923.\n",
      "Training epoch 3 batch 880 with loss 1.9006983041763306.\n",
      "Training epoch 3 batch 881 with loss 1.9105937480926514.\n",
      "Training epoch 3 batch 882 with loss 2.346518039703369.\n",
      "Training epoch 3 batch 883 with loss 2.259617567062378.\n",
      "Training epoch 3 batch 884 with loss 2.039785146713257.\n",
      "Training epoch 3 batch 885 with loss 1.890096664428711.\n",
      "Training epoch 3 batch 886 with loss 2.4183244705200195.\n",
      "Training epoch 3 batch 887 with loss 1.9082621335983276.\n",
      "Training epoch 3 batch 888 with loss 1.683899998664856.\n",
      "Training epoch 3 batch 889 with loss 2.269090414047241.\n",
      "Training epoch 3 batch 890 with loss 1.9043784141540527.\n",
      "Training epoch 3 batch 891 with loss 2.2342007160186768.\n",
      "Training epoch 3 batch 892 with loss 2.0397121906280518.\n",
      "Training epoch 3 batch 893 with loss 2.0111191272735596.\n",
      "Training epoch 3 batch 894 with loss 2.2215940952301025.\n",
      "Training epoch 3 batch 895 with loss 2.4194576740264893.\n",
      "Training epoch 3 batch 896 with loss 2.0911381244659424.\n",
      "Training epoch 3 batch 897 with loss 2.1205098628997803.\n",
      "Training epoch 3 batch 898 with loss 2.2764201164245605.\n",
      "Training epoch 3 batch 899 with loss 1.8892265558242798.\n",
      "Training epoch 3 batch 900 with loss 2.0383877754211426.\n",
      "Training epoch 3 batch 901 with loss 1.955126166343689.\n",
      "Training epoch 3 batch 902 with loss 1.936185598373413.\n",
      "Training epoch 3 batch 903 with loss 2.296640634536743.\n",
      "Training epoch 3 batch 904 with loss 2.1013264656066895.\n",
      "Training epoch 3 batch 905 with loss 2.415355920791626.\n",
      "Training epoch 3 batch 906 with loss 2.2208950519561768.\n",
      "Training epoch 3 batch 907 with loss 2.201101779937744.\n",
      "Training epoch 3 batch 908 with loss 2.1725876331329346.\n",
      "Training epoch 3 batch 909 with loss 2.281862735748291.\n",
      "Training epoch 3 batch 910 with loss 2.0803494453430176.\n",
      "Training epoch 3 batch 911 with loss 2.281184434890747.\n",
      "Training epoch 3 batch 912 with loss 1.768468976020813.\n",
      "Training epoch 3 batch 913 with loss 2.1368408203125.\n",
      "Training epoch 3 batch 914 with loss 1.779462218284607.\n",
      "Training epoch 3 batch 915 with loss 2.414544105529785.\n",
      "Training epoch 3 batch 916 with loss 2.2407960891723633.\n",
      "Training epoch 3 batch 917 with loss 1.8949981927871704.\n",
      "Training epoch 3 batch 918 with loss 2.132628917694092.\n",
      "Training epoch 3 batch 919 with loss 2.1733181476593018.\n",
      "Training epoch 3 batch 920 with loss 2.267493486404419.\n",
      "Training epoch 3 batch 921 with loss 2.2585973739624023.\n",
      "Training epoch 3 batch 922 with loss 1.4754201173782349.\n",
      "Training epoch 3 batch 923 with loss 1.7654826641082764.\n",
      "Training epoch 3 batch 924 with loss 2.0211853981018066.\n",
      "Training epoch 3 batch 925 with loss 2.2513067722320557.\n",
      "Training epoch 3 batch 926 with loss 2.1215927600860596.\n",
      "Training epoch 3 batch 927 with loss 1.9999719858169556.\n",
      "Training epoch 3 batch 928 with loss 2.2139201164245605.\n",
      "Training epoch 3 batch 929 with loss 2.084630012512207.\n",
      "Training epoch 3 batch 930 with loss 2.047851800918579.\n",
      "Training epoch 3 batch 931 with loss 2.0514378547668457.\n",
      "Training epoch 3 batch 932 with loss 2.374232530593872.\n",
      "Training epoch 3 batch 933 with loss 2.0361366271972656.\n",
      "Training epoch 3 batch 934 with loss 2.030461549758911.\n",
      "Training epoch 3 batch 935 with loss 1.5980600118637085.\n",
      "Training epoch 3 batch 936 with loss 1.8180891275405884.\n",
      "Training epoch 3 batch 937 with loss 2.3396201133728027.\n",
      "Training epoch 3 batch 938 with loss 1.8200695514678955.\n",
      "Training epoch 3 batch 939 with loss 1.7544552087783813.\n",
      "Training epoch 3 batch 940 with loss 2.1115920543670654.\n",
      "Training epoch 3 batch 941 with loss 2.1963820457458496.\n",
      "Training epoch 3 batch 942 with loss 2.0029637813568115.\n",
      "Training epoch 3 batch 943 with loss 2.29595947265625.\n",
      "Training epoch 3 batch 944 with loss 2.387744665145874.\n",
      "Training epoch 3 batch 945 with loss 1.7000006437301636.\n",
      "Training epoch 3 batch 946 with loss 2.1862566471099854.\n",
      "Training epoch 3 batch 947 with loss 2.3755927085876465.\n",
      "Training epoch 3 batch 948 with loss 2.104506254196167.\n",
      "Training epoch 3 batch 949 with loss 2.2389230728149414.\n",
      "Training epoch 3 batch 950 with loss 2.1305999755859375.\n",
      "Training epoch 3 batch 951 with loss 1.9958256483078003.\n",
      "Training epoch 3 batch 952 with loss 1.6911182403564453.\n",
      "Training epoch 3 batch 953 with loss 1.8993797302246094.\n",
      "Training epoch 3 batch 954 with loss 2.3632705211639404.\n",
      "Training epoch 3 batch 955 with loss 2.318577289581299.\n",
      "Training epoch 3 batch 956 with loss 1.9645841121673584.\n",
      "Training epoch 3 batch 957 with loss 1.8647823333740234.\n",
      "Training epoch 3 batch 958 with loss 2.1712677478790283.\n",
      "Training epoch 3 batch 959 with loss 1.9118379354476929.\n",
      "Training epoch 3 batch 960 with loss 2.116525411605835.\n",
      "Training epoch 3 batch 961 with loss 2.742236375808716.\n",
      "Training epoch 3 batch 962 with loss 2.130906343460083.\n",
      "Training epoch 3 batch 963 with loss 2.303832530975342.\n",
      "Training epoch 3 batch 964 with loss 1.8301537036895752.\n",
      "Training epoch 3 batch 965 with loss 1.9968512058258057.\n",
      "Training epoch 3 batch 966 with loss 2.2411952018737793.\n",
      "Training epoch 3 batch 967 with loss 1.834106683731079.\n",
      "Training epoch 3 batch 968 with loss 1.6945630311965942.\n",
      "Training epoch 3 batch 969 with loss 1.9704698324203491.\n",
      "Training epoch 3 batch 970 with loss 2.0743470191955566.\n",
      "Training epoch 3 batch 971 with loss 1.912662148475647.\n",
      "Training epoch 3 batch 972 with loss 2.3327925205230713.\n",
      "Training epoch 3 batch 973 with loss 2.465322732925415.\n",
      "Training epoch 3 batch 974 with loss 2.2370102405548096.\n",
      "Training epoch 3 batch 975 with loss 2.125678062438965.\n",
      "Training epoch 3 batch 976 with loss 2.164463758468628.\n",
      "Training epoch 3 batch 977 with loss 2.379089117050171.\n",
      "Training epoch 3 batch 978 with loss 1.9514714479446411.\n",
      "Training epoch 3 batch 979 with loss 2.2001194953918457.\n",
      "Training epoch 3 batch 980 with loss 2.307007312774658.\n",
      "Training epoch 3 batch 981 with loss 2.372013807296753.\n",
      "Training epoch 3 batch 982 with loss 2.095059871673584.\n",
      "Training epoch 3 batch 983 with loss 2.264941930770874.\n",
      "Training epoch 3 batch 984 with loss 2.003650188446045.\n",
      "Training epoch 3 batch 985 with loss 1.9059451818466187.\n",
      "Training epoch 3 batch 986 with loss 2.004873514175415.\n",
      "Training epoch 3 batch 987 with loss 1.7601755857467651.\n",
      "Training epoch 3 batch 988 with loss 1.9741361141204834.\n",
      "Training epoch 3 batch 989 with loss 2.501530408859253.\n",
      "Training epoch 3 batch 990 with loss 1.8892414569854736.\n",
      "Training epoch 3 batch 991 with loss 2.1683332920074463.\n",
      "Training epoch 3 batch 992 with loss 2.20858097076416.\n",
      "Training epoch 3 batch 993 with loss 2.406590461730957.\n",
      "Training epoch 3 batch 994 with loss 2.064735174179077.\n",
      "Training epoch 3 batch 995 with loss 1.9965237379074097.\n",
      "Training epoch 3 batch 996 with loss 1.8031402826309204.\n",
      "Training epoch 3 batch 997 with loss 1.8052157163619995.\n",
      "Training epoch 3 batch 998 with loss 2.079239845275879.\n",
      "Training epoch 3 batch 999 with loss 2.0435807704925537.\n",
      "Test batch 0 with loss 2.690319061279297.\n",
      "Test batch 1 with loss 2.1712048053741455.\n",
      "Test batch 2 with loss 2.3795793056488037.\n",
      "Test batch 3 with loss 2.9752426147460938.\n",
      "Test batch 4 with loss 2.558474063873291.\n",
      "Test batch 5 with loss 3.018521785736084.\n",
      "Test batch 6 with loss 3.0640311241149902.\n",
      "Test batch 7 with loss 2.4985873699188232.\n",
      "Test batch 8 with loss 2.4555888175964355.\n",
      "Test batch 9 with loss 2.2241129875183105.\n",
      "Test batch 10 with loss 3.0293023586273193.\n",
      "Test batch 11 with loss 2.9862265586853027.\n",
      "Test batch 12 with loss 2.5885777473449707.\n",
      "Test batch 13 with loss 2.7892518043518066.\n",
      "Test batch 14 with loss 1.9373047351837158.\n",
      "Test batch 15 with loss 2.0774216651916504.\n",
      "Test batch 16 with loss 2.597125768661499.\n",
      "Test batch 17 with loss 2.6923108100891113.\n",
      "Test batch 18 with loss 2.8199541568756104.\n",
      "Test batch 19 with loss 2.941617488861084.\n",
      "Test batch 20 with loss 1.9479858875274658.\n",
      "Test batch 21 with loss 2.4498450756073.\n",
      "Test batch 22 with loss 2.2264151573181152.\n",
      "Test batch 23 with loss 2.6976964473724365.\n",
      "Test batch 24 with loss 2.2279164791107178.\n",
      "Test batch 25 with loss 3.0526278018951416.\n",
      "Test batch 26 with loss 2.4923160076141357.\n",
      "Test batch 27 with loss 2.4340591430664062.\n",
      "Test batch 28 with loss 2.41908860206604.\n",
      "Test batch 29 with loss 3.2816522121429443.\n",
      "Test batch 30 with loss 2.392747640609741.\n",
      "Test batch 31 with loss 2.3945839405059814.\n",
      "Test batch 32 with loss 2.7979917526245117.\n",
      "Test batch 33 with loss 2.432765245437622.\n",
      "Test batch 34 with loss 2.0657012462615967.\n",
      "Test batch 35 with loss 2.807124376296997.\n",
      "Test batch 36 with loss 1.9633610248565674.\n",
      "Test batch 37 with loss 2.1704163551330566.\n",
      "Test batch 38 with loss 2.406489133834839.\n",
      "Test batch 39 with loss 2.387800931930542.\n",
      "Test batch 40 with loss 2.6667256355285645.\n",
      "Test batch 41 with loss 2.638099431991577.\n",
      "Test batch 42 with loss 2.5593841075897217.\n",
      "Test batch 43 with loss 2.589961290359497.\n",
      "Test batch 44 with loss 2.4423820972442627.\n",
      "Test batch 45 with loss 2.6549558639526367.\n",
      "Test batch 46 with loss 2.5199687480926514.\n",
      "Test batch 47 with loss 2.2984983921051025.\n",
      "Test batch 48 with loss 2.2586019039154053.\n",
      "Test batch 49 with loss 2.7953858375549316.\n",
      "Test batch 50 with loss 2.2552876472473145.\n",
      "Test batch 51 with loss 2.7039036750793457.\n",
      "Test batch 52 with loss 2.5642004013061523.\n",
      "Test batch 53 with loss 2.1348447799682617.\n",
      "Test batch 54 with loss 2.507141590118408.\n",
      "Test batch 55 with loss 2.7487494945526123.\n",
      "Test batch 56 with loss 2.8154056072235107.\n",
      "Test batch 57 with loss 2.8139257431030273.\n",
      "Test batch 58 with loss 2.1603596210479736.\n",
      "Test batch 59 with loss 2.7125892639160156.\n",
      "Test batch 60 with loss 2.2459871768951416.\n",
      "Test batch 61 with loss 2.8161401748657227.\n",
      "Test batch 62 with loss 2.3249688148498535.\n",
      "Test batch 63 with loss 2.224482536315918.\n",
      "Test batch 64 with loss 2.41544771194458.\n",
      "Test batch 65 with loss 2.3393218517303467.\n",
      "Test batch 66 with loss 2.306464195251465.\n",
      "Test batch 67 with loss 2.944732427597046.\n",
      "Test batch 68 with loss 2.894273042678833.\n",
      "Test batch 69 with loss 2.7453131675720215.\n",
      "Test batch 70 with loss 2.7382116317749023.\n",
      "Test batch 71 with loss 2.642443895339966.\n",
      "Test batch 72 with loss 2.34370493888855.\n",
      "Test batch 73 with loss 2.8414387702941895.\n",
      "Test batch 74 with loss 2.2591629028320312.\n",
      "Test batch 75 with loss 2.8597915172576904.\n",
      "Test batch 76 with loss 2.3197264671325684.\n",
      "Test batch 77 with loss 2.422360897064209.\n",
      "Test batch 78 with loss 2.3460490703582764.\n",
      "Test batch 79 with loss 2.5075857639312744.\n",
      "Test batch 80 with loss 2.2380783557891846.\n",
      "Test batch 81 with loss 2.555431842803955.\n",
      "Test batch 82 with loss 2.34562611579895.\n",
      "Test batch 83 with loss 2.5702226161956787.\n",
      "Test batch 84 with loss 2.722123622894287.\n",
      "Test batch 85 with loss 2.3559465408325195.\n",
      "Test batch 86 with loss 2.967146396636963.\n",
      "Test batch 87 with loss 3.2397050857543945.\n",
      "Test batch 88 with loss 2.2596936225891113.\n",
      "Test batch 89 with loss 2.8369717597961426.\n",
      "Test batch 90 with loss 2.542742967605591.\n",
      "Test batch 91 with loss 2.451606035232544.\n",
      "Test batch 92 with loss 2.252932548522949.\n",
      "Test batch 93 with loss 2.2945683002471924.\n",
      "Test batch 94 with loss 2.037583112716675.\n",
      "Test batch 95 with loss 2.2016563415527344.\n",
      "Test batch 96 with loss 2.8319449424743652.\n",
      "Test batch 97 with loss 2.165013551712036.\n",
      "Test batch 98 with loss 2.425814628601074.\n",
      "Test batch 99 with loss 2.5195534229278564.\n",
      "Test batch 100 with loss 2.5652740001678467.\n",
      "Test batch 101 with loss 2.6117560863494873.\n",
      "Test batch 102 with loss 2.325077533721924.\n",
      "Test batch 103 with loss 2.9971694946289062.\n",
      "Test batch 104 with loss 2.8057961463928223.\n",
      "Test batch 105 with loss 2.410078287124634.\n",
      "Test batch 106 with loss 2.806968927383423.\n",
      "Test batch 107 with loss 2.9423439502716064.\n",
      "Test batch 108 with loss 2.5968737602233887.\n",
      "Test batch 109 with loss 2.101679801940918.\n",
      "Test batch 110 with loss 2.5207817554473877.\n",
      "Test batch 111 with loss 2.42936635017395.\n",
      "Test batch 112 with loss 2.8737802505493164.\n",
      "Test batch 113 with loss 2.3725345134735107.\n",
      "Test batch 114 with loss 2.621614694595337.\n",
      "Test batch 115 with loss 2.7186195850372314.\n",
      "Test batch 116 with loss 2.8193352222442627.\n",
      "Test batch 117 with loss 2.876802921295166.\n",
      "Test batch 118 with loss 2.6686322689056396.\n",
      "Test batch 119 with loss 2.4698598384857178.\n",
      "Test batch 120 with loss 2.35097336769104.\n",
      "Test batch 121 with loss 2.4343161582946777.\n",
      "Test batch 122 with loss 3.3092682361602783.\n",
      "Test batch 123 with loss 2.726900577545166.\n",
      "Test batch 124 with loss 2.0728259086608887.\n",
      "Test batch 125 with loss 3.0246658325195312.\n",
      "Test batch 126 with loss 2.4856154918670654.\n",
      "Test batch 127 with loss 2.610107660293579.\n",
      "Test batch 128 with loss 2.3440966606140137.\n",
      "Test batch 129 with loss 2.6954023838043213.\n",
      "Test batch 130 with loss 2.688387870788574.\n",
      "Test batch 131 with loss 2.0942811965942383.\n",
      "Test batch 132 with loss 2.3470683097839355.\n",
      "Test batch 133 with loss 2.5751702785491943.\n",
      "Test batch 134 with loss 2.460075616836548.\n",
      "Test batch 135 with loss 2.226254463195801.\n",
      "Test batch 136 with loss 2.929532051086426.\n",
      "Test batch 137 with loss 2.417980432510376.\n",
      "Test batch 138 with loss 2.3986308574676514.\n",
      "Test batch 139 with loss 2.346144676208496.\n",
      "Test batch 140 with loss 2.588013172149658.\n",
      "Test batch 141 with loss 2.233208179473877.\n",
      "Test batch 142 with loss 2.9251437187194824.\n",
      "Test batch 143 with loss 2.468250036239624.\n",
      "Test batch 144 with loss 2.271639347076416.\n",
      "Test batch 145 with loss 2.090026378631592.\n",
      "Test batch 146 with loss 2.2684340476989746.\n",
      "Test batch 147 with loss 2.3341968059539795.\n",
      "Test batch 148 with loss 2.240852117538452.\n",
      "Test batch 149 with loss 2.225207805633545.\n",
      "Test batch 150 with loss 2.558356523513794.\n",
      "Test batch 151 with loss 2.349203586578369.\n",
      "Test batch 152 with loss 2.426297903060913.\n",
      "Test batch 153 with loss 3.0282962322235107.\n",
      "Test batch 154 with loss 2.4818077087402344.\n",
      "Test batch 155 with loss 2.6017913818359375.\n",
      "Test batch 156 with loss 2.236510753631592.\n",
      "Test batch 157 with loss 3.12661075592041.\n",
      "Test batch 158 with loss 2.526055335998535.\n",
      "Test batch 159 with loss 2.544053554534912.\n",
      "Test batch 160 with loss 2.5081639289855957.\n",
      "Test batch 161 with loss 2.5051660537719727.\n",
      "Test batch 162 with loss 2.407500982284546.\n",
      "Test batch 163 with loss 2.7494239807128906.\n",
      "Test batch 164 with loss 2.4604408740997314.\n",
      "Test batch 165 with loss 2.5948386192321777.\n",
      "Test batch 166 with loss 2.2542994022369385.\n",
      "Test batch 167 with loss 1.9412531852722168.\n",
      "Test batch 168 with loss 2.481818437576294.\n",
      "Test batch 169 with loss 2.259430170059204.\n",
      "Test batch 170 with loss 2.312134027481079.\n",
      "Test batch 171 with loss 2.631293773651123.\n",
      "Test batch 172 with loss 2.504241943359375.\n",
      "Test batch 173 with loss 2.7102601528167725.\n",
      "Test batch 174 with loss 2.565202236175537.\n",
      "Test batch 175 with loss 2.310601234436035.\n",
      "Test batch 176 with loss 3.099440574645996.\n",
      "Test batch 177 with loss 3.0476372241973877.\n",
      "Test batch 178 with loss 2.643308162689209.\n",
      "Test batch 179 with loss 2.4049108028411865.\n",
      "Test batch 180 with loss 2.016723394393921.\n",
      "Test batch 181 with loss 2.622230052947998.\n",
      "Test batch 182 with loss 2.303908109664917.\n",
      "Test batch 183 with loss 2.081669807434082.\n",
      "Test batch 184 with loss 2.9358439445495605.\n",
      "Test batch 185 with loss 2.2660224437713623.\n",
      "Test batch 186 with loss 2.346341371536255.\n",
      "Test batch 187 with loss 2.51151180267334.\n",
      "Test batch 188 with loss 2.116057872772217.\n",
      "Test batch 189 with loss 2.89350962638855.\n",
      "Test batch 190 with loss 2.6498875617980957.\n",
      "Test batch 191 with loss 2.1467602252960205.\n",
      "Test batch 192 with loss 2.2824008464813232.\n",
      "Test batch 193 with loss 2.9760830402374268.\n",
      "Test batch 194 with loss 2.6608927249908447.\n",
      "Test batch 195 with loss 2.973506212234497.\n",
      "Test batch 196 with loss 2.832627773284912.\n",
      "Test batch 197 with loss 3.1622188091278076.\n",
      "Test batch 198 with loss 2.8692259788513184.\n",
      "Test batch 199 with loss 2.360537052154541.\n",
      "Test batch 200 with loss 3.127528667449951.\n",
      "Test batch 201 with loss 2.594850778579712.\n",
      "Test batch 202 with loss 2.544837474822998.\n",
      "Test batch 203 with loss 2.368535280227661.\n",
      "Test batch 204 with loss 2.516655206680298.\n",
      "Test batch 205 with loss 2.582817554473877.\n",
      "Test batch 206 with loss 2.436816930770874.\n",
      "Test batch 207 with loss 2.7077085971832275.\n",
      "Test batch 208 with loss 2.9166080951690674.\n",
      "Test batch 209 with loss 2.660595655441284.\n",
      "Test batch 210 with loss 2.563708782196045.\n",
      "Test batch 211 with loss 2.494361400604248.\n",
      "Test batch 212 with loss 2.5632574558258057.\n",
      "Test batch 213 with loss 2.4527838230133057.\n",
      "Test batch 214 with loss 2.6627719402313232.\n",
      "Test batch 215 with loss 1.8677747249603271.\n",
      "Test batch 216 with loss 2.445906639099121.\n",
      "Test batch 217 with loss 2.2432920932769775.\n",
      "Test batch 218 with loss 3.0319557189941406.\n",
      "Test batch 219 with loss 2.4000730514526367.\n",
      "Test batch 220 with loss 1.8130344152450562.\n",
      "Test batch 221 with loss 2.587977886199951.\n",
      "Test batch 222 with loss 2.7760138511657715.\n",
      "Test batch 223 with loss 2.2026565074920654.\n",
      "Test batch 224 with loss 2.4258224964141846.\n",
      "Test batch 225 with loss 2.6700820922851562.\n",
      "Test batch 226 with loss 2.7541615962982178.\n",
      "Test batch 227 with loss 2.988790273666382.\n",
      "Test batch 228 with loss 3.014868974685669.\n",
      "Test batch 229 with loss 2.759984016418457.\n",
      "Test batch 230 with loss 2.57452392578125.\n",
      "Test batch 231 with loss 2.045095205307007.\n",
      "Test batch 232 with loss 2.5810370445251465.\n",
      "Test batch 233 with loss 2.139078378677368.\n",
      "Test batch 234 with loss 2.703317403793335.\n",
      "Test batch 235 with loss 2.2551026344299316.\n",
      "Test batch 236 with loss 2.8513290882110596.\n",
      "Test batch 237 with loss 2.574645757675171.\n",
      "Test batch 238 with loss 3.0668375492095947.\n",
      "Test batch 239 with loss 2.5995004177093506.\n",
      "Test batch 240 with loss 2.577404737472534.\n",
      "Test batch 241 with loss 2.6359658241271973.\n",
      "Test batch 242 with loss 2.0370473861694336.\n",
      "Test batch 243 with loss 2.2472283840179443.\n",
      "Test batch 244 with loss 2.9264440536499023.\n",
      "Test batch 245 with loss 2.617767095565796.\n",
      "Test batch 246 with loss 2.227781057357788.\n",
      "Test batch 247 with loss 2.489640712738037.\n",
      "Test batch 248 with loss 2.4674270153045654.\n",
      "Test batch 249 with loss 2.400190830230713.\n",
      "Test batch 250 with loss 2.728879690170288.\n",
      "Test batch 251 with loss 2.5496835708618164.\n",
      "Test batch 252 with loss 2.4641432762145996.\n",
      "Test batch 253 with loss 2.549412250518799.\n",
      "Test batch 254 with loss 2.403259515762329.\n",
      "Test batch 255 with loss 2.0773141384124756.\n",
      "Test batch 256 with loss 2.350879192352295.\n",
      "Test batch 257 with loss 2.618516683578491.\n",
      "Test batch 258 with loss 2.9531283378601074.\n",
      "Test batch 259 with loss 2.4733734130859375.\n",
      "Test batch 260 with loss 2.985692262649536.\n",
      "Test batch 261 with loss 2.636671304702759.\n",
      "Test batch 262 with loss 2.50825572013855.\n",
      "Test batch 263 with loss 2.6471338272094727.\n",
      "Test batch 264 with loss 2.477069616317749.\n",
      "Test batch 265 with loss 2.6360130310058594.\n",
      "Test batch 266 with loss 2.4336724281311035.\n",
      "Test batch 267 with loss 2.1134629249572754.\n",
      "Test batch 268 with loss 2.3015730381011963.\n",
      "Test batch 269 with loss 2.236246347427368.\n",
      "Test batch 270 with loss 2.64939022064209.\n",
      "Test batch 271 with loss 2.4748573303222656.\n",
      "Test batch 272 with loss 2.3760476112365723.\n",
      "Test batch 273 with loss 2.8865292072296143.\n",
      "Test batch 274 with loss 2.7690818309783936.\n",
      "Test batch 275 with loss 2.2190937995910645.\n",
      "Test batch 276 with loss 2.6465775966644287.\n",
      "Test batch 277 with loss 2.6925885677337646.\n",
      "Test batch 278 with loss 2.6223556995391846.\n",
      "Test batch 279 with loss 2.7837893962860107.\n",
      "Test batch 280 with loss 2.3992226123809814.\n",
      "Test batch 281 with loss 2.137794017791748.\n",
      "Test batch 282 with loss 2.389298677444458.\n",
      "Test batch 283 with loss 2.672743082046509.\n",
      "Test batch 284 with loss 2.195618152618408.\n",
      "Test batch 285 with loss 2.3128890991210938.\n",
      "Test batch 286 with loss 2.1211206912994385.\n",
      "Test batch 287 with loss 2.180424690246582.\n",
      "Test batch 288 with loss 2.034355878829956.\n",
      "Test batch 289 with loss 2.8347525596618652.\n",
      "Test batch 290 with loss 2.2640931606292725.\n",
      "Test batch 291 with loss 2.406810760498047.\n",
      "Test batch 292 with loss 2.550640344619751.\n",
      "Test batch 293 with loss 2.2883870601654053.\n",
      "Test batch 294 with loss 2.5062944889068604.\n",
      "Test batch 295 with loss 2.4474666118621826.\n",
      "Test batch 296 with loss 3.0758512020111084.\n",
      "Test batch 297 with loss 2.72731614112854.\n",
      "Test batch 298 with loss 3.0730504989624023.\n",
      "Test batch 299 with loss 2.3027472496032715.\n",
      "Test batch 300 with loss 2.2911605834960938.\n",
      "Test batch 301 with loss 2.177088737487793.\n",
      "Test batch 302 with loss 2.59883975982666.\n",
      "Test batch 303 with loss 2.6655919551849365.\n",
      "Test batch 304 with loss 2.357206106185913.\n",
      "Test batch 305 with loss 2.865061044692993.\n",
      "Test batch 306 with loss 2.404989004135132.\n",
      "Test batch 307 with loss 2.3453660011291504.\n",
      "Test batch 308 with loss 2.3084073066711426.\n",
      "Test batch 309 with loss 2.246469020843506.\n",
      "Test batch 310 with loss 2.2670676708221436.\n",
      "Test batch 311 with loss 2.2193796634674072.\n",
      "Test batch 312 with loss 2.2843739986419678.\n",
      "Test batch 313 with loss 2.3303349018096924.\n",
      "Test batch 314 with loss 3.1733057498931885.\n",
      "Test batch 315 with loss 2.3854658603668213.\n",
      "Test batch 316 with loss 3.3634767532348633.\n",
      "Test batch 317 with loss 2.0671141147613525.\n",
      "Test batch 318 with loss 2.2624006271362305.\n",
      "Test batch 319 with loss 2.8738608360290527.\n",
      "Test batch 320 with loss 2.3875319957733154.\n",
      "Test batch 321 with loss 2.433203935623169.\n",
      "Test batch 322 with loss 2.204209804534912.\n",
      "Test batch 323 with loss 2.4330098628997803.\n",
      "Test batch 324 with loss 2.38022780418396.\n",
      "Test batch 325 with loss 2.6436798572540283.\n",
      "Test batch 326 with loss 2.6236038208007812.\n",
      "Test batch 327 with loss 2.7508020401000977.\n",
      "Test batch 328 with loss 2.485511064529419.\n",
      "Test batch 329 with loss 2.8046133518218994.\n",
      "Test batch 330 with loss 2.679219961166382.\n",
      "Test batch 331 with loss 2.3989017009735107.\n",
      "Test batch 332 with loss 2.3486082553863525.\n",
      "Test batch 333 with loss 2.312908887863159.\n",
      "Training epoch 4 batch 0 with loss 1.9323960542678833.\n",
      "Training epoch 4 batch 1 with loss 1.691672444343567.\n",
      "Training epoch 4 batch 2 with loss 1.8600410223007202.\n",
      "Training epoch 4 batch 3 with loss 1.5324280261993408.\n",
      "Training epoch 4 batch 4 with loss 1.3939911127090454.\n",
      "Training epoch 4 batch 5 with loss 1.7783336639404297.\n",
      "Training epoch 4 batch 6 with loss 1.4153249263763428.\n",
      "Training epoch 4 batch 7 with loss 1.7271692752838135.\n",
      "Training epoch 4 batch 8 with loss 1.6947952508926392.\n",
      "Training epoch 4 batch 9 with loss 1.83883535861969.\n",
      "Training epoch 4 batch 10 with loss 1.330135703086853.\n",
      "Training epoch 4 batch 11 with loss 1.9528815746307373.\n",
      "Training epoch 4 batch 12 with loss 1.6808229684829712.\n",
      "Training epoch 4 batch 13 with loss 1.2925407886505127.\n",
      "Training epoch 4 batch 14 with loss 1.890705943107605.\n",
      "Training epoch 4 batch 15 with loss 1.1952002048492432.\n",
      "Training epoch 4 batch 16 with loss 1.7033476829528809.\n",
      "Training epoch 4 batch 17 with loss 1.6074315309524536.\n",
      "Training epoch 4 batch 18 with loss 1.4470478296279907.\n",
      "Training epoch 4 batch 19 with loss 1.665255069732666.\n",
      "Training epoch 4 batch 20 with loss 1.814689040184021.\n",
      "Training epoch 4 batch 21 with loss 1.372636079788208.\n",
      "Training epoch 4 batch 22 with loss 1.3787167072296143.\n",
      "Training epoch 4 batch 23 with loss 1.9759694337844849.\n",
      "Training epoch 4 batch 24 with loss 1.9704997539520264.\n",
      "Training epoch 4 batch 25 with loss 1.4897156953811646.\n",
      "Training epoch 4 batch 26 with loss 1.6724520921707153.\n",
      "Training epoch 4 batch 27 with loss 1.6316982507705688.\n",
      "Training epoch 4 batch 28 with loss 1.2707889080047607.\n",
      "Training epoch 4 batch 29 with loss 1.542975902557373.\n",
      "Training epoch 4 batch 30 with loss 1.6061500310897827.\n",
      "Training epoch 4 batch 31 with loss 1.5954116582870483.\n",
      "Training epoch 4 batch 32 with loss 1.643200159072876.\n",
      "Training epoch 4 batch 33 with loss 1.6821845769882202.\n",
      "Training epoch 4 batch 34 with loss 1.5334292650222778.\n",
      "Training epoch 4 batch 35 with loss 1.538537621498108.\n",
      "Training epoch 4 batch 36 with loss 1.72823166847229.\n",
      "Training epoch 4 batch 37 with loss 1.7309668064117432.\n",
      "Training epoch 4 batch 38 with loss 1.7027314901351929.\n",
      "Training epoch 4 batch 39 with loss 1.857105016708374.\n",
      "Training epoch 4 batch 40 with loss 1.5135502815246582.\n",
      "Training epoch 4 batch 41 with loss 1.64353609085083.\n",
      "Training epoch 4 batch 42 with loss 1.6733559370040894.\n",
      "Training epoch 4 batch 43 with loss 1.6468884944915771.\n",
      "Training epoch 4 batch 44 with loss 2.168064832687378.\n",
      "Training epoch 4 batch 45 with loss 1.564404845237732.\n",
      "Training epoch 4 batch 46 with loss 1.7710191011428833.\n",
      "Training epoch 4 batch 47 with loss 1.451185941696167.\n",
      "Training epoch 4 batch 48 with loss 2.0979106426239014.\n",
      "Training epoch 4 batch 49 with loss 1.6848037242889404.\n",
      "Training epoch 4 batch 50 with loss 1.3413728475570679.\n",
      "Training epoch 4 batch 51 with loss 1.6788026094436646.\n",
      "Training epoch 4 batch 52 with loss 1.5411392450332642.\n",
      "Training epoch 4 batch 53 with loss 1.8334364891052246.\n",
      "Training epoch 4 batch 54 with loss 1.209761381149292.\n",
      "Training epoch 4 batch 55 with loss 1.8376424312591553.\n",
      "Training epoch 4 batch 56 with loss 1.6817890405654907.\n",
      "Training epoch 4 batch 57 with loss 1.2125486135482788.\n",
      "Training epoch 4 batch 58 with loss 1.6416618824005127.\n",
      "Training epoch 4 batch 59 with loss 2.2137246131896973.\n",
      "Training epoch 4 batch 60 with loss 1.9997822046279907.\n",
      "Training epoch 4 batch 61 with loss 1.3845237493515015.\n",
      "Training epoch 4 batch 62 with loss 1.9203122854232788.\n",
      "Training epoch 4 batch 63 with loss 1.805497646331787.\n",
      "Training epoch 4 batch 64 with loss 1.6410073041915894.\n",
      "Training epoch 4 batch 65 with loss 1.5497725009918213.\n",
      "Training epoch 4 batch 66 with loss 1.6834862232208252.\n",
      "Training epoch 4 batch 67 with loss 1.3533872365951538.\n",
      "Training epoch 4 batch 68 with loss 1.731102705001831.\n",
      "Training epoch 4 batch 69 with loss 1.415608525276184.\n",
      "Training epoch 4 batch 70 with loss 1.9013067483901978.\n",
      "Training epoch 4 batch 71 with loss 1.7912920713424683.\n",
      "Training epoch 4 batch 72 with loss 1.3399250507354736.\n",
      "Training epoch 4 batch 73 with loss 1.5115264654159546.\n",
      "Training epoch 4 batch 74 with loss 1.6453077793121338.\n",
      "Training epoch 4 batch 75 with loss 1.9339478015899658.\n",
      "Training epoch 4 batch 76 with loss 1.8607429265975952.\n",
      "Training epoch 4 batch 77 with loss 2.2001707553863525.\n",
      "Training epoch 4 batch 78 with loss 1.5795822143554688.\n",
      "Training epoch 4 batch 79 with loss 1.8134597539901733.\n",
      "Training epoch 4 batch 80 with loss 1.630781888961792.\n",
      "Training epoch 4 batch 81 with loss 1.7783524990081787.\n",
      "Training epoch 4 batch 82 with loss 1.8798434734344482.\n",
      "Training epoch 4 batch 83 with loss 1.6852840185165405.\n",
      "Training epoch 4 batch 84 with loss 1.3288079500198364.\n",
      "Training epoch 4 batch 85 with loss 1.5796371698379517.\n",
      "Training epoch 4 batch 86 with loss 1.687645435333252.\n",
      "Training epoch 4 batch 87 with loss 1.1850759983062744.\n",
      "Training epoch 4 batch 88 with loss 1.529434323310852.\n",
      "Training epoch 4 batch 89 with loss 2.1371850967407227.\n",
      "Training epoch 4 batch 90 with loss 1.6349965333938599.\n",
      "Training epoch 4 batch 91 with loss 2.1071667671203613.\n",
      "Training epoch 4 batch 92 with loss 1.5966604948043823.\n",
      "Training epoch 4 batch 93 with loss 1.6586474180221558.\n",
      "Training epoch 4 batch 94 with loss 1.459126353263855.\n",
      "Training epoch 4 batch 95 with loss 1.5090223550796509.\n",
      "Training epoch 4 batch 96 with loss 1.5735745429992676.\n",
      "Training epoch 4 batch 97 with loss 1.656220555305481.\n",
      "Training epoch 4 batch 98 with loss 1.7285763025283813.\n",
      "Training epoch 4 batch 99 with loss 1.4409687519073486.\n",
      "Training epoch 4 batch 100 with loss 1.8015514612197876.\n",
      "Training epoch 4 batch 101 with loss 1.335170865058899.\n",
      "Training epoch 4 batch 102 with loss 1.6881146430969238.\n",
      "Training epoch 4 batch 103 with loss 1.5545903444290161.\n",
      "Training epoch 4 batch 104 with loss 1.3943054676055908.\n",
      "Training epoch 4 batch 105 with loss 1.7715436220169067.\n",
      "Training epoch 4 batch 106 with loss 1.6378544569015503.\n",
      "Training epoch 4 batch 107 with loss 1.7858308553695679.\n",
      "Training epoch 4 batch 108 with loss 1.5143893957138062.\n",
      "Training epoch 4 batch 109 with loss 2.0147833824157715.\n",
      "Training epoch 4 batch 110 with loss 1.732171893119812.\n",
      "Training epoch 4 batch 111 with loss 1.3683007955551147.\n",
      "Training epoch 4 batch 112 with loss 1.4937556982040405.\n",
      "Training epoch 4 batch 113 with loss 1.775455117225647.\n",
      "Training epoch 4 batch 114 with loss 2.2950098514556885.\n",
      "Training epoch 4 batch 115 with loss 1.9770532846450806.\n",
      "Training epoch 4 batch 116 with loss 1.799422025680542.\n",
      "Training epoch 4 batch 117 with loss 1.4936175346374512.\n",
      "Training epoch 4 batch 118 with loss 1.3660297393798828.\n",
      "Training epoch 4 batch 119 with loss 1.71861732006073.\n",
      "Training epoch 4 batch 120 with loss 1.4936763048171997.\n",
      "Training epoch 4 batch 121 with loss 1.4985631704330444.\n",
      "Training epoch 4 batch 122 with loss 1.5578690767288208.\n",
      "Training epoch 4 batch 123 with loss 1.7169685363769531.\n",
      "Training epoch 4 batch 124 with loss 1.7471749782562256.\n",
      "Training epoch 4 batch 125 with loss 1.7496424913406372.\n",
      "Training epoch 4 batch 126 with loss 1.8518574237823486.\n",
      "Training epoch 4 batch 127 with loss 1.5697416067123413.\n",
      "Training epoch 4 batch 128 with loss 1.3249013423919678.\n",
      "Training epoch 4 batch 129 with loss 1.4302126169204712.\n",
      "Training epoch 4 batch 130 with loss 1.925557255744934.\n",
      "Training epoch 4 batch 131 with loss 1.8962143659591675.\n",
      "Training epoch 4 batch 132 with loss 1.594849705696106.\n",
      "Training epoch 4 batch 133 with loss 1.695086121559143.\n",
      "Training epoch 4 batch 134 with loss 1.5863381624221802.\n",
      "Training epoch 4 batch 135 with loss 2.0246946811676025.\n",
      "Training epoch 4 batch 136 with loss 1.7017461061477661.\n",
      "Training epoch 4 batch 137 with loss 1.944429636001587.\n",
      "Training epoch 4 batch 138 with loss 1.9549120664596558.\n",
      "Training epoch 4 batch 139 with loss 1.651932954788208.\n",
      "Training epoch 4 batch 140 with loss 1.725057601928711.\n",
      "Training epoch 4 batch 141 with loss 1.6201081275939941.\n",
      "Training epoch 4 batch 142 with loss 1.3821511268615723.\n",
      "Training epoch 4 batch 143 with loss 1.5393847227096558.\n",
      "Training epoch 4 batch 144 with loss 1.8093414306640625.\n",
      "Training epoch 4 batch 145 with loss 1.6645604372024536.\n",
      "Training epoch 4 batch 146 with loss 1.8851689100265503.\n",
      "Training epoch 4 batch 147 with loss 1.7695262432098389.\n",
      "Training epoch 4 batch 148 with loss 1.734635591506958.\n",
      "Training epoch 4 batch 149 with loss 2.6102936267852783.\n",
      "Training epoch 4 batch 150 with loss 1.8389917612075806.\n",
      "Training epoch 4 batch 151 with loss 1.9987510442733765.\n",
      "Training epoch 4 batch 152 with loss 1.9223922491073608.\n",
      "Training epoch 4 batch 153 with loss 1.615256428718567.\n",
      "Training epoch 4 batch 154 with loss 1.6850978136062622.\n",
      "Training epoch 4 batch 155 with loss 2.0212607383728027.\n",
      "Training epoch 4 batch 156 with loss 1.8431304693222046.\n",
      "Training epoch 4 batch 157 with loss 1.492709994316101.\n",
      "Training epoch 4 batch 158 with loss 1.9368970394134521.\n",
      "Training epoch 4 batch 159 with loss 1.523385763168335.\n",
      "Training epoch 4 batch 160 with loss 1.6648273468017578.\n",
      "Training epoch 4 batch 161 with loss 1.655017614364624.\n",
      "Training epoch 4 batch 162 with loss 1.825019121170044.\n",
      "Training epoch 4 batch 163 with loss 1.33191978931427.\n",
      "Training epoch 4 batch 164 with loss 1.9943455457687378.\n",
      "Training epoch 4 batch 165 with loss 2.011655569076538.\n",
      "Training epoch 4 batch 166 with loss 1.6404849290847778.\n",
      "Training epoch 4 batch 167 with loss 1.8857595920562744.\n",
      "Training epoch 4 batch 168 with loss 1.8825081586837769.\n",
      "Training epoch 4 batch 169 with loss 2.2090821266174316.\n",
      "Training epoch 4 batch 170 with loss 1.755428433418274.\n",
      "Training epoch 4 batch 171 with loss 1.8238810300827026.\n",
      "Training epoch 4 batch 172 with loss 1.7800744771957397.\n",
      "Training epoch 4 batch 173 with loss 1.7201976776123047.\n",
      "Training epoch 4 batch 174 with loss 1.8592815399169922.\n",
      "Training epoch 4 batch 175 with loss 2.0384867191314697.\n",
      "Training epoch 4 batch 176 with loss 1.9295824766159058.\n",
      "Training epoch 4 batch 177 with loss 1.5655243396759033.\n",
      "Training epoch 4 batch 178 with loss 1.7766352891921997.\n",
      "Training epoch 4 batch 179 with loss 1.665567398071289.\n",
      "Training epoch 4 batch 180 with loss 1.7743842601776123.\n",
      "Training epoch 4 batch 181 with loss 1.93143892288208.\n",
      "Training epoch 4 batch 182 with loss 1.6975573301315308.\n",
      "Training epoch 4 batch 183 with loss 1.4933584928512573.\n",
      "Training epoch 4 batch 184 with loss 1.7132649421691895.\n",
      "Training epoch 4 batch 185 with loss 1.7201248407363892.\n",
      "Training epoch 4 batch 186 with loss 1.7552762031555176.\n",
      "Training epoch 4 batch 187 with loss 1.3084079027175903.\n",
      "Training epoch 4 batch 188 with loss 1.538483738899231.\n",
      "Training epoch 4 batch 189 with loss 1.965520977973938.\n",
      "Training epoch 4 batch 190 with loss 1.7946587800979614.\n",
      "Training epoch 4 batch 191 with loss 2.123788356781006.\n",
      "Training epoch 4 batch 192 with loss 1.8221898078918457.\n",
      "Training epoch 4 batch 193 with loss 1.6086480617523193.\n",
      "Training epoch 4 batch 194 with loss 1.8551520109176636.\n",
      "Training epoch 4 batch 195 with loss 1.7167775630950928.\n",
      "Training epoch 4 batch 196 with loss 1.6385952234268188.\n",
      "Training epoch 4 batch 197 with loss 1.560058355331421.\n",
      "Training epoch 4 batch 198 with loss 1.3296970129013062.\n",
      "Training epoch 4 batch 199 with loss 1.735690951347351.\n",
      "Training epoch 4 batch 200 with loss 1.616782546043396.\n",
      "Training epoch 4 batch 201 with loss 1.6901301145553589.\n",
      "Training epoch 4 batch 202 with loss 2.035365581512451.\n",
      "Training epoch 4 batch 203 with loss 1.4586589336395264.\n",
      "Training epoch 4 batch 204 with loss 1.7335340976715088.\n",
      "Training epoch 4 batch 205 with loss 1.8004951477050781.\n",
      "Training epoch 4 batch 206 with loss 1.999497890472412.\n",
      "Training epoch 4 batch 207 with loss 2.0907580852508545.\n",
      "Training epoch 4 batch 208 with loss 2.180818796157837.\n",
      "Training epoch 4 batch 209 with loss 1.7974814176559448.\n",
      "Training epoch 4 batch 210 with loss 1.6399766206741333.\n",
      "Training epoch 4 batch 211 with loss 1.396329402923584.\n",
      "Training epoch 4 batch 212 with loss 1.7094573974609375.\n",
      "Training epoch 4 batch 213 with loss 1.8988239765167236.\n",
      "Training epoch 4 batch 214 with loss 1.363698959350586.\n",
      "Training epoch 4 batch 215 with loss 1.882195234298706.\n",
      "Training epoch 4 batch 216 with loss 1.2346512079238892.\n",
      "Training epoch 4 batch 217 with loss 1.8462823629379272.\n",
      "Training epoch 4 batch 218 with loss 1.7496501207351685.\n",
      "Training epoch 4 batch 219 with loss 1.650773525238037.\n",
      "Training epoch 4 batch 220 with loss 1.6919732093811035.\n",
      "Training epoch 4 batch 221 with loss 1.9022148847579956.\n",
      "Training epoch 4 batch 222 with loss 2.044935703277588.\n",
      "Training epoch 4 batch 223 with loss 2.067885160446167.\n",
      "Training epoch 4 batch 224 with loss 1.9357210397720337.\n",
      "Training epoch 4 batch 225 with loss 2.0162079334259033.\n",
      "Training epoch 4 batch 226 with loss 1.8776785135269165.\n",
      "Training epoch 4 batch 227 with loss 1.6998978853225708.\n",
      "Training epoch 4 batch 228 with loss 1.5528292655944824.\n",
      "Training epoch 4 batch 229 with loss 2.0150861740112305.\n",
      "Training epoch 4 batch 230 with loss 2.009117364883423.\n",
      "Training epoch 4 batch 231 with loss 1.435912013053894.\n",
      "Training epoch 4 batch 232 with loss 1.4697372913360596.\n",
      "Training epoch 4 batch 233 with loss 1.6859130859375.\n",
      "Training epoch 4 batch 234 with loss 1.9982515573501587.\n",
      "Training epoch 4 batch 235 with loss 1.713948130607605.\n",
      "Training epoch 4 batch 236 with loss 1.7715175151824951.\n",
      "Training epoch 4 batch 237 with loss 2.3731529712677.\n",
      "Training epoch 4 batch 238 with loss 2.1106748580932617.\n",
      "Training epoch 4 batch 239 with loss 2.14831280708313.\n",
      "Training epoch 4 batch 240 with loss 1.8983508348464966.\n",
      "Training epoch 4 batch 241 with loss 1.574190616607666.\n",
      "Training epoch 4 batch 242 with loss 1.6015467643737793.\n",
      "Training epoch 4 batch 243 with loss 1.4743188619613647.\n",
      "Training epoch 4 batch 244 with loss 1.8289337158203125.\n",
      "Training epoch 4 batch 245 with loss 1.7695232629776.\n",
      "Training epoch 4 batch 246 with loss 2.0096001625061035.\n",
      "Training epoch 4 batch 247 with loss 1.8884413242340088.\n",
      "Training epoch 4 batch 248 with loss 1.686518907546997.\n",
      "Training epoch 4 batch 249 with loss 1.861403226852417.\n",
      "Training epoch 4 batch 250 with loss 1.8628379106521606.\n",
      "Training epoch 4 batch 251 with loss 1.5574746131896973.\n",
      "Training epoch 4 batch 252 with loss 1.6213772296905518.\n",
      "Training epoch 4 batch 253 with loss 1.8840610980987549.\n",
      "Training epoch 4 batch 254 with loss 2.0141677856445312.\n",
      "Training epoch 4 batch 255 with loss 2.072685480117798.\n",
      "Training epoch 4 batch 256 with loss 1.6100257635116577.\n",
      "Training epoch 4 batch 257 with loss 2.0511322021484375.\n",
      "Training epoch 4 batch 258 with loss 2.158093214035034.\n",
      "Training epoch 4 batch 259 with loss 1.794325590133667.\n",
      "Training epoch 4 batch 260 with loss 1.611326813697815.\n",
      "Training epoch 4 batch 261 with loss 2.172747850418091.\n",
      "Training epoch 4 batch 262 with loss 1.8950591087341309.\n",
      "Training epoch 4 batch 263 with loss 1.896817922592163.\n",
      "Training epoch 4 batch 264 with loss 1.7916258573532104.\n",
      "Training epoch 4 batch 265 with loss 1.84392511844635.\n",
      "Training epoch 4 batch 266 with loss 1.6670290231704712.\n",
      "Training epoch 4 batch 267 with loss 1.7344365119934082.\n",
      "Training epoch 4 batch 268 with loss 2.129922389984131.\n",
      "Training epoch 4 batch 269 with loss 2.0079867839813232.\n",
      "Training epoch 4 batch 270 with loss 1.7189140319824219.\n",
      "Training epoch 4 batch 271 with loss 1.8800469636917114.\n",
      "Training epoch 4 batch 272 with loss 1.4547820091247559.\n",
      "Training epoch 4 batch 273 with loss 1.749240517616272.\n",
      "Training epoch 4 batch 274 with loss 1.886141300201416.\n",
      "Training epoch 4 batch 275 with loss 1.375421166419983.\n",
      "Training epoch 4 batch 276 with loss 1.6834590435028076.\n",
      "Training epoch 4 batch 277 with loss 2.0333547592163086.\n",
      "Training epoch 4 batch 278 with loss 2.1603200435638428.\n",
      "Training epoch 4 batch 279 with loss 1.9366511106491089.\n",
      "Training epoch 4 batch 280 with loss 2.0025289058685303.\n",
      "Training epoch 4 batch 281 with loss 1.6636559963226318.\n",
      "Training epoch 4 batch 282 with loss 2.095729351043701.\n",
      "Training epoch 4 batch 283 with loss 1.9589506387710571.\n",
      "Training epoch 4 batch 284 with loss 2.0947954654693604.\n",
      "Training epoch 4 batch 285 with loss 1.7689892053604126.\n",
      "Training epoch 4 batch 286 with loss 1.85807466506958.\n",
      "Training epoch 4 batch 287 with loss 2.1979477405548096.\n",
      "Training epoch 4 batch 288 with loss 2.1529626846313477.\n",
      "Training epoch 4 batch 289 with loss 1.878679633140564.\n",
      "Training epoch 4 batch 290 with loss 1.810600996017456.\n",
      "Training epoch 4 batch 291 with loss 1.9883182048797607.\n",
      "Training epoch 4 batch 292 with loss 1.7236405611038208.\n",
      "Training epoch 4 batch 293 with loss 2.355717897415161.\n",
      "Training epoch 4 batch 294 with loss 1.6201759576797485.\n",
      "Training epoch 4 batch 295 with loss 1.8495053052902222.\n",
      "Training epoch 4 batch 296 with loss 1.6591880321502686.\n",
      "Training epoch 4 batch 297 with loss 1.5782803297042847.\n",
      "Training epoch 4 batch 298 with loss 1.722765326499939.\n",
      "Training epoch 4 batch 299 with loss 1.9837309122085571.\n",
      "Training epoch 4 batch 300 with loss 1.6282826662063599.\n",
      "Training epoch 4 batch 301 with loss 1.6395443677902222.\n",
      "Training epoch 4 batch 302 with loss 1.7229443788528442.\n",
      "Training epoch 4 batch 303 with loss 1.6808873414993286.\n",
      "Training epoch 4 batch 304 with loss 1.746131181716919.\n",
      "Training epoch 4 batch 305 with loss 1.8804799318313599.\n",
      "Training epoch 4 batch 306 with loss 1.6454524993896484.\n",
      "Training epoch 4 batch 307 with loss 1.6661503314971924.\n",
      "Training epoch 4 batch 308 with loss 2.1863510608673096.\n",
      "Training epoch 4 batch 309 with loss 2.351020336151123.\n",
      "Training epoch 4 batch 310 with loss 2.2573366165161133.\n",
      "Training epoch 4 batch 311 with loss 1.6283050775527954.\n",
      "Training epoch 4 batch 312 with loss 1.8500339984893799.\n",
      "Training epoch 4 batch 313 with loss 1.9978994131088257.\n",
      "Training epoch 4 batch 314 with loss 1.5739158391952515.\n",
      "Training epoch 4 batch 315 with loss 1.589223027229309.\n",
      "Training epoch 4 batch 316 with loss 1.9268726110458374.\n",
      "Training epoch 4 batch 317 with loss 1.5586541891098022.\n",
      "Training epoch 4 batch 318 with loss 1.9047645330429077.\n",
      "Training epoch 4 batch 319 with loss 2.1840522289276123.\n",
      "Training epoch 4 batch 320 with loss 1.7044281959533691.\n",
      "Training epoch 4 batch 321 with loss 2.0267624855041504.\n",
      "Training epoch 4 batch 322 with loss 1.6071891784667969.\n",
      "Training epoch 4 batch 323 with loss 1.6541281938552856.\n",
      "Training epoch 4 batch 324 with loss 1.4383509159088135.\n",
      "Training epoch 4 batch 325 with loss 1.6847469806671143.\n",
      "Training epoch 4 batch 326 with loss 1.8102004528045654.\n",
      "Training epoch 4 batch 327 with loss 2.070502758026123.\n",
      "Training epoch 4 batch 328 with loss 2.1916661262512207.\n",
      "Training epoch 4 batch 329 with loss 2.0935680866241455.\n",
      "Training epoch 4 batch 330 with loss 2.0705275535583496.\n",
      "Training epoch 4 batch 331 with loss 2.164921998977661.\n",
      "Training epoch 4 batch 332 with loss 2.1150426864624023.\n",
      "Training epoch 4 batch 333 with loss 1.5948326587677002.\n",
      "Training epoch 4 batch 334 with loss 1.9275307655334473.\n",
      "Training epoch 4 batch 335 with loss 1.6996105909347534.\n",
      "Training epoch 4 batch 336 with loss 2.1687421798706055.\n",
      "Training epoch 4 batch 337 with loss 2.0838756561279297.\n",
      "Training epoch 4 batch 338 with loss 1.5750715732574463.\n",
      "Training epoch 4 batch 339 with loss 1.650484323501587.\n",
      "Training epoch 4 batch 340 with loss 1.6981520652770996.\n",
      "Training epoch 4 batch 341 with loss 1.701715350151062.\n",
      "Training epoch 4 batch 342 with loss 1.7412747144699097.\n",
      "Training epoch 4 batch 343 with loss 1.8360445499420166.\n",
      "Training epoch 4 batch 344 with loss 2.0331506729125977.\n",
      "Training epoch 4 batch 345 with loss 1.6153453588485718.\n",
      "Training epoch 4 batch 346 with loss 1.5810621976852417.\n",
      "Training epoch 4 batch 347 with loss 1.6317179203033447.\n",
      "Training epoch 4 batch 348 with loss 2.0496020317077637.\n",
      "Training epoch 4 batch 349 with loss 2.344250202178955.\n",
      "Training epoch 4 batch 350 with loss 1.805862545967102.\n",
      "Training epoch 4 batch 351 with loss 2.1549301147460938.\n",
      "Training epoch 4 batch 352 with loss 2.010057210922241.\n",
      "Training epoch 4 batch 353 with loss 1.6371619701385498.\n",
      "Training epoch 4 batch 354 with loss 1.7838194370269775.\n",
      "Training epoch 4 batch 355 with loss 1.9449882507324219.\n",
      "Training epoch 4 batch 356 with loss 1.6666195392608643.\n",
      "Training epoch 4 batch 357 with loss 1.8059931993484497.\n",
      "Training epoch 4 batch 358 with loss 1.991410493850708.\n",
      "Training epoch 4 batch 359 with loss 2.2098705768585205.\n",
      "Training epoch 4 batch 360 with loss 1.9065954685211182.\n",
      "Training epoch 4 batch 361 with loss 1.7702972888946533.\n",
      "Training epoch 4 batch 362 with loss 1.6907691955566406.\n",
      "Training epoch 4 batch 363 with loss 1.890507459640503.\n",
      "Training epoch 4 batch 364 with loss 2.3111813068389893.\n",
      "Training epoch 4 batch 365 with loss 1.7901496887207031.\n",
      "Training epoch 4 batch 366 with loss 1.7598620653152466.\n",
      "Training epoch 4 batch 367 with loss 2.0978217124938965.\n",
      "Training epoch 4 batch 368 with loss 1.9714815616607666.\n",
      "Training epoch 4 batch 369 with loss 1.8960509300231934.\n",
      "Training epoch 4 batch 370 with loss 1.8504396677017212.\n",
      "Training epoch 4 batch 371 with loss 1.9134960174560547.\n",
      "Training epoch 4 batch 372 with loss 1.678829312324524.\n",
      "Training epoch 4 batch 373 with loss 2.1457366943359375.\n",
      "Training epoch 4 batch 374 with loss 2.2983481884002686.\n",
      "Training epoch 4 batch 375 with loss 1.6700152158737183.\n",
      "Training epoch 4 batch 376 with loss 1.8525607585906982.\n",
      "Training epoch 4 batch 377 with loss 2.2278449535369873.\n",
      "Training epoch 4 batch 378 with loss 1.8737598657608032.\n",
      "Training epoch 4 batch 379 with loss 1.8475592136383057.\n",
      "Training epoch 4 batch 380 with loss 1.928602933883667.\n",
      "Training epoch 4 batch 381 with loss 1.7624355554580688.\n",
      "Training epoch 4 batch 382 with loss 1.7990435361862183.\n",
      "Training epoch 4 batch 383 with loss 1.8227039575576782.\n",
      "Training epoch 4 batch 384 with loss 2.2732324600219727.\n",
      "Training epoch 4 batch 385 with loss 2.070733070373535.\n",
      "Training epoch 4 batch 386 with loss 1.4871288537979126.\n",
      "Training epoch 4 batch 387 with loss 1.850606083869934.\n",
      "Training epoch 4 batch 388 with loss 1.9937248229980469.\n",
      "Training epoch 4 batch 389 with loss 1.8667634725570679.\n",
      "Training epoch 4 batch 390 with loss 1.9020631313323975.\n",
      "Training epoch 4 batch 391 with loss 2.2066829204559326.\n",
      "Training epoch 4 batch 392 with loss 1.6677581071853638.\n",
      "Training epoch 4 batch 393 with loss 1.7291910648345947.\n",
      "Training epoch 4 batch 394 with loss 2.2116739749908447.\n",
      "Training epoch 4 batch 395 with loss 1.837684154510498.\n",
      "Training epoch 4 batch 396 with loss 2.1435773372650146.\n",
      "Training epoch 4 batch 397 with loss 1.6967415809631348.\n",
      "Training epoch 4 batch 398 with loss 1.8800474405288696.\n",
      "Training epoch 4 batch 399 with loss 2.208001136779785.\n",
      "Training epoch 4 batch 400 with loss 2.3935060501098633.\n",
      "Training epoch 4 batch 401 with loss 1.8628647327423096.\n",
      "Training epoch 4 batch 402 with loss 2.1125926971435547.\n",
      "Training epoch 4 batch 403 with loss 1.752864956855774.\n",
      "Training epoch 4 batch 404 with loss 2.0504231452941895.\n",
      "Training epoch 4 batch 405 with loss 2.0595309734344482.\n",
      "Training epoch 4 batch 406 with loss 2.175083875656128.\n",
      "Training epoch 4 batch 407 with loss 2.1980361938476562.\n",
      "Training epoch 4 batch 408 with loss 2.686535358428955.\n",
      "Training epoch 4 batch 409 with loss 2.0135092735290527.\n",
      "Training epoch 4 batch 410 with loss 1.8768919706344604.\n",
      "Training epoch 4 batch 411 with loss 1.8964202404022217.\n",
      "Training epoch 4 batch 412 with loss 1.6653298139572144.\n",
      "Training epoch 4 batch 413 with loss 1.808300495147705.\n",
      "Training epoch 4 batch 414 with loss 2.3775594234466553.\n",
      "Training epoch 4 batch 415 with loss 2.149472951889038.\n",
      "Training epoch 4 batch 416 with loss 2.1040830612182617.\n",
      "Training epoch 4 batch 417 with loss 1.4629369974136353.\n",
      "Training epoch 4 batch 418 with loss 2.3534927368164062.\n",
      "Training epoch 4 batch 419 with loss 1.766593098640442.\n",
      "Training epoch 4 batch 420 with loss 1.904202938079834.\n",
      "Training epoch 4 batch 421 with loss 2.5564053058624268.\n",
      "Training epoch 4 batch 422 with loss 1.9713616371154785.\n",
      "Training epoch 4 batch 423 with loss 1.459191083908081.\n",
      "Training epoch 4 batch 424 with loss 1.6896986961364746.\n",
      "Training epoch 4 batch 425 with loss 1.898071050643921.\n",
      "Training epoch 4 batch 426 with loss 1.9083077907562256.\n",
      "Training epoch 4 batch 427 with loss 2.341886043548584.\n",
      "Training epoch 4 batch 428 with loss 2.1963462829589844.\n",
      "Training epoch 4 batch 429 with loss 2.1299281120300293.\n",
      "Training epoch 4 batch 430 with loss 1.827621340751648.\n",
      "Training epoch 4 batch 431 with loss 1.8533341884613037.\n",
      "Training epoch 4 batch 432 with loss 2.261543035507202.\n",
      "Training epoch 4 batch 433 with loss 1.5566965341567993.\n",
      "Training epoch 4 batch 434 with loss 1.9293100833892822.\n",
      "Training epoch 4 batch 435 with loss 2.1122262477874756.\n",
      "Training epoch 4 batch 436 with loss 1.8732876777648926.\n",
      "Training epoch 4 batch 437 with loss 1.3936363458633423.\n",
      "Training epoch 4 batch 438 with loss 1.9305697679519653.\n",
      "Training epoch 4 batch 439 with loss 1.8728997707366943.\n",
      "Training epoch 4 batch 440 with loss 2.0179851055145264.\n",
      "Training epoch 4 batch 441 with loss 1.830029010772705.\n",
      "Training epoch 4 batch 442 with loss 1.7045408487319946.\n",
      "Training epoch 4 batch 443 with loss 2.0220255851745605.\n",
      "Training epoch 4 batch 444 with loss 1.6379002332687378.\n",
      "Training epoch 4 batch 445 with loss 2.0658087730407715.\n",
      "Training epoch 4 batch 446 with loss 1.720054268836975.\n",
      "Training epoch 4 batch 447 with loss 2.514133930206299.\n",
      "Training epoch 4 batch 448 with loss 1.6504355669021606.\n",
      "Training epoch 4 batch 449 with loss 1.7110918760299683.\n",
      "Training epoch 4 batch 450 with loss 1.5834863185882568.\n",
      "Training epoch 4 batch 451 with loss 2.0851240158081055.\n",
      "Training epoch 4 batch 452 with loss 2.2342567443847656.\n",
      "Training epoch 4 batch 453 with loss 2.076354742050171.\n",
      "Training epoch 4 batch 454 with loss 2.14663028717041.\n",
      "Training epoch 4 batch 455 with loss 2.166822671890259.\n",
      "Training epoch 4 batch 456 with loss 1.3719855546951294.\n",
      "Training epoch 4 batch 457 with loss 1.7473180294036865.\n",
      "Training epoch 4 batch 458 with loss 1.7110836505889893.\n",
      "Training epoch 4 batch 459 with loss 1.6088058948516846.\n",
      "Training epoch 4 batch 460 with loss 2.3297133445739746.\n",
      "Training epoch 4 batch 461 with loss 2.0495269298553467.\n",
      "Training epoch 4 batch 462 with loss 1.9279075860977173.\n",
      "Training epoch 4 batch 463 with loss 1.8948395252227783.\n",
      "Training epoch 4 batch 464 with loss 2.103419780731201.\n",
      "Training epoch 4 batch 465 with loss 1.9810529947280884.\n",
      "Training epoch 4 batch 466 with loss 2.147538423538208.\n",
      "Training epoch 4 batch 467 with loss 1.6614421606063843.\n",
      "Training epoch 4 batch 468 with loss 2.2832908630371094.\n",
      "Training epoch 4 batch 469 with loss 1.748348593711853.\n",
      "Training epoch 4 batch 470 with loss 2.0336220264434814.\n",
      "Training epoch 4 batch 471 with loss 2.091447114944458.\n",
      "Training epoch 4 batch 472 with loss 1.7102653980255127.\n",
      "Training epoch 4 batch 473 with loss 2.1017942428588867.\n",
      "Training epoch 4 batch 474 with loss 1.8524302244186401.\n",
      "Training epoch 4 batch 475 with loss 1.9575788974761963.\n",
      "Training epoch 4 batch 476 with loss 2.1009104251861572.\n",
      "Training epoch 4 batch 477 with loss 1.7485109567642212.\n",
      "Training epoch 4 batch 478 with loss 2.0071897506713867.\n",
      "Training epoch 4 batch 479 with loss 1.8489267826080322.\n",
      "Training epoch 4 batch 480 with loss 1.785422921180725.\n",
      "Training epoch 4 batch 481 with loss 2.1306281089782715.\n",
      "Training epoch 4 batch 482 with loss 1.664815068244934.\n",
      "Training epoch 4 batch 483 with loss 2.0057737827301025.\n",
      "Training epoch 4 batch 484 with loss 2.117486000061035.\n",
      "Training epoch 4 batch 485 with loss 2.1108756065368652.\n",
      "Training epoch 4 batch 486 with loss 1.8738409280776978.\n",
      "Training epoch 4 batch 487 with loss 2.10207462310791.\n",
      "Training epoch 4 batch 488 with loss 1.857875108718872.\n",
      "Training epoch 4 batch 489 with loss 2.143702268600464.\n",
      "Training epoch 4 batch 490 with loss 1.4669129848480225.\n",
      "Training epoch 4 batch 491 with loss 1.6650564670562744.\n",
      "Training epoch 4 batch 492 with loss 1.92636239528656.\n",
      "Training epoch 4 batch 493 with loss 2.2949399948120117.\n",
      "Training epoch 4 batch 494 with loss 2.128922939300537.\n",
      "Training epoch 4 batch 495 with loss 2.2018513679504395.\n",
      "Training epoch 4 batch 496 with loss 1.9888553619384766.\n",
      "Training epoch 4 batch 497 with loss 1.9539331197738647.\n",
      "Training epoch 4 batch 498 with loss 1.8793396949768066.\n",
      "Training epoch 4 batch 499 with loss 1.8235106468200684.\n",
      "Training epoch 4 batch 500 with loss 1.789039134979248.\n",
      "Training epoch 4 batch 501 with loss 2.3354475498199463.\n",
      "Training epoch 4 batch 502 with loss 1.6864430904388428.\n",
      "Training epoch 4 batch 503 with loss 2.105520009994507.\n",
      "Training epoch 4 batch 504 with loss 1.9389073848724365.\n",
      "Training epoch 4 batch 505 with loss 1.7416409254074097.\n",
      "Training epoch 4 batch 506 with loss 2.060790538787842.\n",
      "Training epoch 4 batch 507 with loss 1.862922191619873.\n",
      "Training epoch 4 batch 508 with loss 1.7923157215118408.\n",
      "Training epoch 4 batch 509 with loss 2.264695405960083.\n",
      "Training epoch 4 batch 510 with loss 1.875510573387146.\n",
      "Training epoch 4 batch 511 with loss 2.045316219329834.\n",
      "Training epoch 4 batch 512 with loss 1.8360600471496582.\n",
      "Training epoch 4 batch 513 with loss 2.229365587234497.\n",
      "Training epoch 4 batch 514 with loss 1.6503838300704956.\n",
      "Training epoch 4 batch 515 with loss 2.527451515197754.\n",
      "Training epoch 4 batch 516 with loss 1.8398640155792236.\n",
      "Training epoch 4 batch 517 with loss 2.088104486465454.\n",
      "Training epoch 4 batch 518 with loss 1.9421526193618774.\n",
      "Training epoch 4 batch 519 with loss 1.9799450635910034.\n",
      "Training epoch 4 batch 520 with loss 2.041236639022827.\n",
      "Training epoch 4 batch 521 with loss 1.8033559322357178.\n",
      "Training epoch 4 batch 522 with loss 1.6270556449890137.\n",
      "Training epoch 4 batch 523 with loss 1.8364911079406738.\n",
      "Training epoch 4 batch 524 with loss 2.1940829753875732.\n",
      "Training epoch 4 batch 525 with loss 1.9983769655227661.\n",
      "Training epoch 4 batch 526 with loss 2.151978015899658.\n",
      "Training epoch 4 batch 527 with loss 1.5917047262191772.\n",
      "Training epoch 4 batch 528 with loss 1.5744963884353638.\n",
      "Training epoch 4 batch 529 with loss 2.0544447898864746.\n",
      "Training epoch 4 batch 530 with loss 1.893259048461914.\n",
      "Training epoch 4 batch 531 with loss 1.9271740913391113.\n",
      "Training epoch 4 batch 532 with loss 1.9056899547576904.\n",
      "Training epoch 4 batch 533 with loss 2.1254420280456543.\n",
      "Training epoch 4 batch 534 with loss 1.9746230840682983.\n",
      "Training epoch 4 batch 535 with loss 1.7399746179580688.\n",
      "Training epoch 4 batch 536 with loss 1.912262201309204.\n",
      "Training epoch 4 batch 537 with loss 1.6318529844284058.\n",
      "Training epoch 4 batch 538 with loss 2.070406436920166.\n",
      "Training epoch 4 batch 539 with loss 1.9486644268035889.\n",
      "Training epoch 4 batch 540 with loss 1.6563031673431396.\n",
      "Training epoch 4 batch 541 with loss 1.8036357164382935.\n",
      "Training epoch 4 batch 542 with loss 1.4436678886413574.\n",
      "Training epoch 4 batch 543 with loss 2.5964913368225098.\n",
      "Training epoch 4 batch 544 with loss 1.96251380443573.\n",
      "Training epoch 4 batch 545 with loss 1.8859142065048218.\n",
      "Training epoch 4 batch 546 with loss 1.8594715595245361.\n",
      "Training epoch 4 batch 547 with loss 2.069455623626709.\n",
      "Training epoch 4 batch 548 with loss 1.9226195812225342.\n",
      "Training epoch 4 batch 549 with loss 2.0581583976745605.\n",
      "Training epoch 4 batch 550 with loss 2.118894338607788.\n",
      "Training epoch 4 batch 551 with loss 1.9605516195297241.\n",
      "Training epoch 4 batch 552 with loss 1.8643615245819092.\n",
      "Training epoch 4 batch 553 with loss 2.1065728664398193.\n",
      "Training epoch 4 batch 554 with loss 2.022228717803955.\n",
      "Training epoch 4 batch 555 with loss 2.197150945663452.\n",
      "Training epoch 4 batch 556 with loss 1.7320020198822021.\n",
      "Training epoch 4 batch 557 with loss 2.0686495304107666.\n",
      "Training epoch 4 batch 558 with loss 2.002678632736206.\n",
      "Training epoch 4 batch 559 with loss 1.9823462963104248.\n",
      "Training epoch 4 batch 560 with loss 1.8187216520309448.\n",
      "Training epoch 4 batch 561 with loss 1.936174988746643.\n",
      "Training epoch 4 batch 562 with loss 1.873878002166748.\n",
      "Training epoch 4 batch 563 with loss 1.6515265703201294.\n",
      "Training epoch 4 batch 564 with loss 1.7059000730514526.\n",
      "Training epoch 4 batch 565 with loss 2.243422746658325.\n",
      "Training epoch 4 batch 566 with loss 1.9302345514297485.\n",
      "Training epoch 4 batch 567 with loss 2.1901071071624756.\n",
      "Training epoch 4 batch 568 with loss 1.7725574970245361.\n",
      "Training epoch 4 batch 569 with loss 1.7322214841842651.\n",
      "Training epoch 4 batch 570 with loss 1.8988655805587769.\n",
      "Training epoch 4 batch 571 with loss 2.35958194732666.\n",
      "Training epoch 4 batch 572 with loss 2.145148277282715.\n",
      "Training epoch 4 batch 573 with loss 2.056759834289551.\n",
      "Training epoch 4 batch 574 with loss 1.5040968656539917.\n",
      "Training epoch 4 batch 575 with loss 2.179035186767578.\n",
      "Training epoch 4 batch 576 with loss 1.5383574962615967.\n",
      "Training epoch 4 batch 577 with loss 1.9140795469284058.\n",
      "Training epoch 4 batch 578 with loss 2.1035103797912598.\n",
      "Training epoch 4 batch 579 with loss 2.1373724937438965.\n",
      "Training epoch 4 batch 580 with loss 2.1224260330200195.\n",
      "Training epoch 4 batch 581 with loss 1.8099092245101929.\n",
      "Training epoch 4 batch 582 with loss 2.1101205348968506.\n",
      "Training epoch 4 batch 583 with loss 2.1214189529418945.\n",
      "Training epoch 4 batch 584 with loss 2.034010171890259.\n",
      "Training epoch 4 batch 585 with loss 2.028661012649536.\n",
      "Training epoch 4 batch 586 with loss 2.1320507526397705.\n",
      "Training epoch 4 batch 587 with loss 2.4289815425872803.\n",
      "Training epoch 4 batch 588 with loss 2.350499153137207.\n",
      "Training epoch 4 batch 589 with loss 2.4746692180633545.\n",
      "Training epoch 4 batch 590 with loss 1.8576414585113525.\n",
      "Training epoch 4 batch 591 with loss 1.9332160949707031.\n",
      "Training epoch 4 batch 592 with loss 1.9795182943344116.\n",
      "Training epoch 4 batch 593 with loss 2.435702323913574.\n",
      "Training epoch 4 batch 594 with loss 1.8649746179580688.\n",
      "Training epoch 4 batch 595 with loss 2.108154058456421.\n",
      "Training epoch 4 batch 596 with loss 2.2866079807281494.\n",
      "Training epoch 4 batch 597 with loss 2.019968271255493.\n",
      "Training epoch 4 batch 598 with loss 2.1112799644470215.\n",
      "Training epoch 4 batch 599 with loss 1.9181424379348755.\n",
      "Training epoch 4 batch 600 with loss 2.0236687660217285.\n",
      "Training epoch 4 batch 601 with loss 1.910740613937378.\n",
      "Training epoch 4 batch 602 with loss 1.9104962348937988.\n",
      "Training epoch 4 batch 603 with loss 1.9867488145828247.\n",
      "Training epoch 4 batch 604 with loss 2.0113418102264404.\n",
      "Training epoch 4 batch 605 with loss 2.090515375137329.\n",
      "Training epoch 4 batch 606 with loss 2.3512606620788574.\n",
      "Training epoch 4 batch 607 with loss 1.9743517637252808.\n",
      "Training epoch 4 batch 608 with loss 1.974711537361145.\n",
      "Training epoch 4 batch 609 with loss 2.1337103843688965.\n",
      "Training epoch 4 batch 610 with loss 1.845408320426941.\n",
      "Training epoch 4 batch 611 with loss 1.9156157970428467.\n",
      "Training epoch 4 batch 612 with loss 1.7527801990509033.\n",
      "Training epoch 4 batch 613 with loss 1.6114917993545532.\n",
      "Training epoch 4 batch 614 with loss 2.18992280960083.\n",
      "Training epoch 4 batch 615 with loss 2.1398634910583496.\n",
      "Training epoch 4 batch 616 with loss 2.060476541519165.\n",
      "Training epoch 4 batch 617 with loss 1.8648178577423096.\n",
      "Training epoch 4 batch 618 with loss 2.093257427215576.\n",
      "Training epoch 4 batch 619 with loss 2.426192283630371.\n",
      "Training epoch 4 batch 620 with loss 2.346303701400757.\n",
      "Training epoch 4 batch 621 with loss 1.6932556629180908.\n",
      "Training epoch 4 batch 622 with loss 2.184678077697754.\n",
      "Training epoch 4 batch 623 with loss 2.1718525886535645.\n",
      "Training epoch 4 batch 624 with loss 1.9145814180374146.\n",
      "Training epoch 4 batch 625 with loss 1.975988745689392.\n",
      "Training epoch 4 batch 626 with loss 2.085238218307495.\n",
      "Training epoch 4 batch 627 with loss 2.2606136798858643.\n",
      "Training epoch 4 batch 628 with loss 1.9742333889007568.\n",
      "Training epoch 4 batch 629 with loss 1.9110506772994995.\n",
      "Training epoch 4 batch 630 with loss 2.2793662548065186.\n",
      "Training epoch 4 batch 631 with loss 1.980647325515747.\n",
      "Training epoch 4 batch 632 with loss 2.287076473236084.\n",
      "Training epoch 4 batch 633 with loss 1.9668388366699219.\n",
      "Training epoch 4 batch 634 with loss 1.8896397352218628.\n",
      "Training epoch 4 batch 635 with loss 1.9347248077392578.\n",
      "Training epoch 4 batch 636 with loss 1.650612711906433.\n",
      "Training epoch 4 batch 637 with loss 1.990662693977356.\n",
      "Training epoch 4 batch 638 with loss 2.193305492401123.\n",
      "Training epoch 4 batch 639 with loss 2.051353693008423.\n",
      "Training epoch 4 batch 640 with loss 2.177624225616455.\n",
      "Training epoch 4 batch 641 with loss 2.0523388385772705.\n",
      "Training epoch 4 batch 642 with loss 1.6920745372772217.\n",
      "Training epoch 4 batch 643 with loss 2.09446120262146.\n",
      "Training epoch 4 batch 644 with loss 1.8233656883239746.\n",
      "Training epoch 4 batch 645 with loss 2.146920680999756.\n",
      "Training epoch 4 batch 646 with loss 2.1490375995635986.\n",
      "Training epoch 4 batch 647 with loss 2.0755505561828613.\n",
      "Training epoch 4 batch 648 with loss 2.1787500381469727.\n",
      "Training epoch 4 batch 649 with loss 2.122985363006592.\n",
      "Training epoch 4 batch 650 with loss 2.2263689041137695.\n",
      "Training epoch 4 batch 651 with loss 2.12900972366333.\n",
      "Training epoch 4 batch 652 with loss 2.17932391166687.\n",
      "Training epoch 4 batch 653 with loss 2.0440750122070312.\n",
      "Training epoch 4 batch 654 with loss 2.1886703968048096.\n",
      "Training epoch 4 batch 655 with loss 1.869507908821106.\n",
      "Training epoch 4 batch 656 with loss 2.152289390563965.\n",
      "Training epoch 4 batch 657 with loss 2.2821359634399414.\n",
      "Training epoch 4 batch 658 with loss 2.3859851360321045.\n",
      "Training epoch 4 batch 659 with loss 2.1328606605529785.\n",
      "Training epoch 4 batch 660 with loss 2.051683187484741.\n",
      "Training epoch 4 batch 661 with loss 1.973298192024231.\n",
      "Training epoch 4 batch 662 with loss 2.111133337020874.\n",
      "Training epoch 4 batch 663 with loss 2.097478151321411.\n",
      "Training epoch 4 batch 664 with loss 1.8677639961242676.\n",
      "Training epoch 4 batch 665 with loss 2.3582751750946045.\n",
      "Training epoch 4 batch 666 with loss 2.4292759895324707.\n",
      "Training epoch 4 batch 667 with loss 1.9271883964538574.\n",
      "Training epoch 4 batch 668 with loss 1.9394973516464233.\n",
      "Training epoch 4 batch 669 with loss 2.279010772705078.\n",
      "Training epoch 4 batch 670 with loss 2.3820149898529053.\n",
      "Training epoch 4 batch 671 with loss 1.9312433004379272.\n",
      "Training epoch 4 batch 672 with loss 1.9472023248672485.\n",
      "Training epoch 4 batch 673 with loss 1.9722025394439697.\n",
      "Training epoch 4 batch 674 with loss 2.2971460819244385.\n",
      "Training epoch 4 batch 675 with loss 1.7846125364303589.\n",
      "Training epoch 4 batch 676 with loss 1.8448617458343506.\n",
      "Training epoch 4 batch 677 with loss 1.7289472818374634.\n",
      "Training epoch 4 batch 678 with loss 2.3268895149230957.\n",
      "Training epoch 4 batch 679 with loss 1.7409523725509644.\n",
      "Training epoch 4 batch 680 with loss 2.4867777824401855.\n",
      "Training epoch 4 batch 681 with loss 2.367722511291504.\n",
      "Training epoch 4 batch 682 with loss 1.9750404357910156.\n",
      "Training epoch 4 batch 683 with loss 1.8944727182388306.\n",
      "Training epoch 4 batch 684 with loss 2.1452174186706543.\n",
      "Training epoch 4 batch 685 with loss 1.7932946681976318.\n",
      "Training epoch 4 batch 686 with loss 1.6649863719940186.\n",
      "Training epoch 4 batch 687 with loss 1.6517524719238281.\n",
      "Training epoch 4 batch 688 with loss 1.8940062522888184.\n",
      "Training epoch 4 batch 689 with loss 2.496901750564575.\n",
      "Training epoch 4 batch 690 with loss 1.8874757289886475.\n",
      "Training epoch 4 batch 691 with loss 2.075892686843872.\n",
      "Training epoch 4 batch 692 with loss 1.87940514087677.\n",
      "Training epoch 4 batch 693 with loss 1.8972868919372559.\n",
      "Training epoch 4 batch 694 with loss 1.987285852432251.\n",
      "Training epoch 4 batch 695 with loss 1.8548678159713745.\n",
      "Training epoch 4 batch 696 with loss 2.057929039001465.\n",
      "Training epoch 4 batch 697 with loss 2.418442726135254.\n",
      "Training epoch 4 batch 698 with loss 1.8627409934997559.\n",
      "Training epoch 4 batch 699 with loss 2.0010440349578857.\n",
      "Training epoch 4 batch 700 with loss 1.791076421737671.\n",
      "Training epoch 4 batch 701 with loss 2.1391892433166504.\n",
      "Training epoch 4 batch 702 with loss 2.2142553329467773.\n",
      "Training epoch 4 batch 703 with loss 1.8981989622116089.\n",
      "Training epoch 4 batch 704 with loss 2.1912033557891846.\n",
      "Training epoch 4 batch 705 with loss 2.1256964206695557.\n",
      "Training epoch 4 batch 706 with loss 2.3896644115448.\n",
      "Training epoch 4 batch 707 with loss 2.0237371921539307.\n",
      "Training epoch 4 batch 708 with loss 2.086095094680786.\n",
      "Training epoch 4 batch 709 with loss 1.7374508380889893.\n",
      "Training epoch 4 batch 710 with loss 1.9671072959899902.\n",
      "Training epoch 4 batch 711 with loss 2.4825997352600098.\n",
      "Training epoch 4 batch 712 with loss 1.8695504665374756.\n",
      "Training epoch 4 batch 713 with loss 2.392465114593506.\n",
      "Training epoch 4 batch 714 with loss 2.1662518978118896.\n",
      "Training epoch 4 batch 715 with loss 1.6513903141021729.\n",
      "Training epoch 4 batch 716 with loss 2.0396227836608887.\n",
      "Training epoch 4 batch 717 with loss 2.233219861984253.\n",
      "Training epoch 4 batch 718 with loss 1.8914231061935425.\n",
      "Training epoch 4 batch 719 with loss 1.9149625301361084.\n",
      "Training epoch 4 batch 720 with loss 2.069859266281128.\n",
      "Training epoch 4 batch 721 with loss 1.9849058389663696.\n",
      "Training epoch 4 batch 722 with loss 2.1326138973236084.\n",
      "Training epoch 4 batch 723 with loss 1.9671143293380737.\n",
      "Training epoch 4 batch 724 with loss 2.3283376693725586.\n",
      "Training epoch 4 batch 725 with loss 2.238821506500244.\n",
      "Training epoch 4 batch 726 with loss 2.1112210750579834.\n",
      "Training epoch 4 batch 727 with loss 2.1172919273376465.\n",
      "Training epoch 4 batch 728 with loss 1.8570448160171509.\n",
      "Training epoch 4 batch 729 with loss 2.0129506587982178.\n",
      "Training epoch 4 batch 730 with loss 2.1334214210510254.\n",
      "Training epoch 4 batch 731 with loss 2.1718013286590576.\n",
      "Training epoch 4 batch 732 with loss 2.1233115196228027.\n",
      "Training epoch 4 batch 733 with loss 1.8905333280563354.\n",
      "Training epoch 4 batch 734 with loss 2.192180633544922.\n",
      "Training epoch 4 batch 735 with loss 2.3167965412139893.\n",
      "Training epoch 4 batch 736 with loss 2.250844717025757.\n",
      "Training epoch 4 batch 737 with loss 2.0770533084869385.\n",
      "Training epoch 4 batch 738 with loss 2.0535895824432373.\n",
      "Training epoch 4 batch 739 with loss 2.3107099533081055.\n",
      "Training epoch 4 batch 740 with loss 2.018517255783081.\n",
      "Training epoch 4 batch 741 with loss 1.5923746824264526.\n",
      "Training epoch 4 batch 742 with loss 1.9168950319290161.\n",
      "Training epoch 4 batch 743 with loss 2.3837649822235107.\n",
      "Training epoch 4 batch 744 with loss 2.201233386993408.\n",
      "Training epoch 4 batch 745 with loss 2.350186586380005.\n",
      "Training epoch 4 batch 746 with loss 1.912909746170044.\n",
      "Training epoch 4 batch 747 with loss 1.8220903873443604.\n",
      "Training epoch 4 batch 748 with loss 2.006659507751465.\n",
      "Training epoch 4 batch 749 with loss 1.6915401220321655.\n",
      "Training epoch 4 batch 750 with loss 1.6495888233184814.\n",
      "Training epoch 4 batch 751 with loss 2.376004219055176.\n",
      "Training epoch 4 batch 752 with loss 2.1116228103637695.\n",
      "Training epoch 4 batch 753 with loss 1.8954169750213623.\n",
      "Training epoch 4 batch 754 with loss 2.4354169368743896.\n",
      "Training epoch 4 batch 755 with loss 2.0104174613952637.\n",
      "Training epoch 4 batch 756 with loss 1.7596007585525513.\n",
      "Training epoch 4 batch 757 with loss 2.060007095336914.\n",
      "Training epoch 4 batch 758 with loss 2.064070224761963.\n",
      "Training epoch 4 batch 759 with loss 2.102414131164551.\n",
      "Training epoch 4 batch 760 with loss 2.3169195652008057.\n",
      "Training epoch 4 batch 761 with loss 1.9404927492141724.\n",
      "Training epoch 4 batch 762 with loss 2.0579254627227783.\n",
      "Training epoch 4 batch 763 with loss 1.9536919593811035.\n",
      "Training epoch 4 batch 764 with loss 1.7514801025390625.\n",
      "Training epoch 4 batch 765 with loss 2.073479413986206.\n",
      "Training epoch 4 batch 766 with loss 2.270744800567627.\n",
      "Training epoch 4 batch 767 with loss 1.8446576595306396.\n",
      "Training epoch 4 batch 768 with loss 2.3329672813415527.\n",
      "Training epoch 4 batch 769 with loss 1.7299941778182983.\n",
      "Training epoch 4 batch 770 with loss 2.398632287979126.\n",
      "Training epoch 4 batch 771 with loss 1.8849979639053345.\n",
      "Training epoch 4 batch 772 with loss 2.2829132080078125.\n",
      "Training epoch 4 batch 773 with loss 1.9169548749923706.\n",
      "Training epoch 4 batch 774 with loss 2.0298073291778564.\n",
      "Training epoch 4 batch 775 with loss 1.893879771232605.\n",
      "Training epoch 4 batch 776 with loss 1.8474012613296509.\n",
      "Training epoch 4 batch 777 with loss 1.7679622173309326.\n",
      "Training epoch 4 batch 778 with loss 2.2468693256378174.\n",
      "Training epoch 4 batch 779 with loss 1.6660996675491333.\n",
      "Training epoch 4 batch 780 with loss 2.339386224746704.\n",
      "Training epoch 4 batch 781 with loss 2.218524694442749.\n",
      "Training epoch 4 batch 782 with loss 1.9287662506103516.\n",
      "Training epoch 4 batch 783 with loss 1.850974678993225.\n",
      "Training epoch 4 batch 784 with loss 2.2358505725860596.\n",
      "Training epoch 4 batch 785 with loss 1.749218225479126.\n",
      "Training epoch 4 batch 786 with loss 2.0111570358276367.\n",
      "Training epoch 4 batch 787 with loss 1.9115909337997437.\n",
      "Training epoch 4 batch 788 with loss 2.5145604610443115.\n",
      "Training epoch 4 batch 789 with loss 2.098252058029175.\n",
      "Training epoch 4 batch 790 with loss 1.9558171033859253.\n",
      "Training epoch 4 batch 791 with loss 2.072349786758423.\n",
      "Training epoch 4 batch 792 with loss 2.1501352787017822.\n",
      "Training epoch 4 batch 793 with loss 1.5353913307189941.\n",
      "Training epoch 4 batch 794 with loss 1.9608104228973389.\n",
      "Training epoch 4 batch 795 with loss 2.1035687923431396.\n",
      "Training epoch 4 batch 796 with loss 1.573344111442566.\n",
      "Training epoch 4 batch 797 with loss 1.715260624885559.\n",
      "Training epoch 4 batch 798 with loss 1.8530738353729248.\n",
      "Training epoch 4 batch 799 with loss 1.7891731262207031.\n",
      "Training epoch 4 batch 800 with loss 2.151076078414917.\n",
      "Training epoch 4 batch 801 with loss 2.0771608352661133.\n",
      "Training epoch 4 batch 802 with loss 2.169649600982666.\n",
      "Training epoch 4 batch 803 with loss 2.68466854095459.\n",
      "Training epoch 4 batch 804 with loss 2.337261438369751.\n",
      "Training epoch 4 batch 805 with loss 2.184603214263916.\n",
      "Training epoch 4 batch 806 with loss 1.9839345216751099.\n",
      "Training epoch 4 batch 807 with loss 2.738612174987793.\n",
      "Training epoch 4 batch 808 with loss 1.807558536529541.\n",
      "Training epoch 4 batch 809 with loss 2.0940051078796387.\n",
      "Training epoch 4 batch 810 with loss 1.7761515378952026.\n",
      "Training epoch 4 batch 811 with loss 2.335216522216797.\n",
      "Training epoch 4 batch 812 with loss 2.631856679916382.\n",
      "Training epoch 4 batch 813 with loss 2.3101274967193604.\n",
      "Training epoch 4 batch 814 with loss 2.386363983154297.\n",
      "Training epoch 4 batch 815 with loss 1.861045002937317.\n",
      "Training epoch 4 batch 816 with loss 2.2886452674865723.\n",
      "Training epoch 4 batch 817 with loss 1.9052785634994507.\n",
      "Training epoch 4 batch 818 with loss 2.09250545501709.\n",
      "Training epoch 4 batch 819 with loss 1.478434443473816.\n",
      "Training epoch 4 batch 820 with loss 1.9432995319366455.\n",
      "Training epoch 4 batch 821 with loss 1.9196780920028687.\n",
      "Training epoch 4 batch 822 with loss 1.9741524457931519.\n",
      "Training epoch 4 batch 823 with loss 2.204420804977417.\n",
      "Training epoch 4 batch 824 with loss 1.822629690170288.\n",
      "Training epoch 4 batch 825 with loss 1.8091771602630615.\n",
      "Training epoch 4 batch 826 with loss 2.014711380004883.\n",
      "Training epoch 4 batch 827 with loss 1.871716022491455.\n",
      "Training epoch 4 batch 828 with loss 2.1234006881713867.\n",
      "Training epoch 4 batch 829 with loss 2.3420569896698.\n",
      "Training epoch 4 batch 830 with loss 2.3575286865234375.\n",
      "Training epoch 4 batch 831 with loss 2.099722146987915.\n",
      "Training epoch 4 batch 832 with loss 2.0030903816223145.\n",
      "Training epoch 4 batch 833 with loss 1.7730295658111572.\n",
      "Training epoch 4 batch 834 with loss 2.1845786571502686.\n",
      "Training epoch 4 batch 835 with loss 1.908826231956482.\n",
      "Training epoch 4 batch 836 with loss 1.9976708889007568.\n",
      "Training epoch 4 batch 837 with loss 2.003066301345825.\n",
      "Training epoch 4 batch 838 with loss 2.0719261169433594.\n",
      "Training epoch 4 batch 839 with loss 2.198256492614746.\n",
      "Training epoch 4 batch 840 with loss 1.787266492843628.\n",
      "Training epoch 4 batch 841 with loss 2.0807502269744873.\n",
      "Training epoch 4 batch 842 with loss 2.3261032104492188.\n",
      "Training epoch 4 batch 843 with loss 2.296509265899658.\n",
      "Training epoch 4 batch 844 with loss 2.1194989681243896.\n",
      "Training epoch 4 batch 845 with loss 1.636271357536316.\n",
      "Training epoch 4 batch 846 with loss 2.0719473361968994.\n",
      "Training epoch 4 batch 847 with loss 2.27253794670105.\n",
      "Training epoch 4 batch 848 with loss 1.9667404890060425.\n",
      "Training epoch 4 batch 849 with loss 2.376124858856201.\n",
      "Training epoch 4 batch 850 with loss 2.1904046535491943.\n",
      "Training epoch 4 batch 851 with loss 1.6762566566467285.\n",
      "Training epoch 4 batch 852 with loss 1.7450659275054932.\n",
      "Training epoch 4 batch 853 with loss 1.9551446437835693.\n",
      "Training epoch 4 batch 854 with loss 1.8623117208480835.\n",
      "Training epoch 4 batch 855 with loss 2.3550403118133545.\n",
      "Training epoch 4 batch 856 with loss 2.1850569248199463.\n",
      "Training epoch 4 batch 857 with loss 2.0576679706573486.\n",
      "Training epoch 4 batch 858 with loss 2.496856927871704.\n",
      "Training epoch 4 batch 859 with loss 1.8944249153137207.\n",
      "Training epoch 4 batch 860 with loss 1.841121792793274.\n",
      "Training epoch 4 batch 861 with loss 1.7785276174545288.\n",
      "Training epoch 4 batch 862 with loss 2.0313005447387695.\n",
      "Training epoch 4 batch 863 with loss 2.1142189502716064.\n",
      "Training epoch 4 batch 864 with loss 1.8275772333145142.\n",
      "Training epoch 4 batch 865 with loss 2.5314550399780273.\n",
      "Training epoch 4 batch 866 with loss 1.8653918504714966.\n",
      "Training epoch 4 batch 867 with loss 2.0604336261749268.\n",
      "Training epoch 4 batch 868 with loss 2.164231300354004.\n",
      "Training epoch 4 batch 869 with loss 1.8759855031967163.\n",
      "Training epoch 4 batch 870 with loss 1.5841789245605469.\n",
      "Training epoch 4 batch 871 with loss 1.9872535467147827.\n",
      "Training epoch 4 batch 872 with loss 1.9640544652938843.\n",
      "Training epoch 4 batch 873 with loss 2.245849370956421.\n",
      "Training epoch 4 batch 874 with loss 2.1181235313415527.\n",
      "Training epoch 4 batch 875 with loss 2.2245776653289795.\n",
      "Training epoch 4 batch 876 with loss 1.990160346031189.\n",
      "Training epoch 4 batch 877 with loss 1.6919411420822144.\n",
      "Training epoch 4 batch 878 with loss 1.8154844045639038.\n",
      "Training epoch 4 batch 879 with loss 2.1522397994995117.\n",
      "Training epoch 4 batch 880 with loss 1.9958112239837646.\n",
      "Training epoch 4 batch 881 with loss 1.9662091732025146.\n",
      "Training epoch 4 batch 882 with loss 2.2359366416931152.\n",
      "Training epoch 4 batch 883 with loss 2.2009685039520264.\n",
      "Training epoch 4 batch 884 with loss 2.0875935554504395.\n",
      "Training epoch 4 batch 885 with loss 1.8372950553894043.\n",
      "Training epoch 4 batch 886 with loss 2.0011818408966064.\n",
      "Training epoch 4 batch 887 with loss 2.4436230659484863.\n",
      "Training epoch 4 batch 888 with loss 1.8949713706970215.\n",
      "Training epoch 4 batch 889 with loss 2.1275579929351807.\n",
      "Training epoch 4 batch 890 with loss 2.4193763732910156.\n",
      "Training epoch 4 batch 891 with loss 2.1331326961517334.\n",
      "Training epoch 4 batch 892 with loss 2.145070791244507.\n",
      "Training epoch 4 batch 893 with loss 2.115000009536743.\n",
      "Training epoch 4 batch 894 with loss 2.654538631439209.\n",
      "Training epoch 4 batch 895 with loss 2.215536594390869.\n",
      "Training epoch 4 batch 896 with loss 2.07946515083313.\n",
      "Training epoch 4 batch 897 with loss 2.189526319503784.\n",
      "Training epoch 4 batch 898 with loss 2.019994020462036.\n",
      "Training epoch 4 batch 899 with loss 2.3890748023986816.\n",
      "Training epoch 4 batch 900 with loss 2.1193888187408447.\n",
      "Training epoch 4 batch 901 with loss 2.008643865585327.\n",
      "Training epoch 4 batch 902 with loss 2.255269765853882.\n",
      "Training epoch 4 batch 903 with loss 2.15289044380188.\n",
      "Training epoch 4 batch 904 with loss 2.240424156188965.\n",
      "Training epoch 4 batch 905 with loss 2.134512424468994.\n",
      "Training epoch 4 batch 906 with loss 2.4492287635803223.\n",
      "Training epoch 4 batch 907 with loss 2.134803533554077.\n",
      "Training epoch 4 batch 908 with loss 2.008911609649658.\n",
      "Training epoch 4 batch 909 with loss 2.28891921043396.\n",
      "Training epoch 4 batch 910 with loss 2.0456786155700684.\n",
      "Training epoch 4 batch 911 with loss 1.9442564249038696.\n",
      "Training epoch 4 batch 912 with loss 2.455176591873169.\n",
      "Training epoch 4 batch 913 with loss 2.198803663253784.\n",
      "Training epoch 4 batch 914 with loss 1.8722509145736694.\n",
      "Training epoch 4 batch 915 with loss 2.013357400894165.\n",
      "Training epoch 4 batch 916 with loss 2.131558895111084.\n",
      "Training epoch 4 batch 917 with loss 1.8405121564865112.\n",
      "Training epoch 4 batch 918 with loss 1.9919276237487793.\n",
      "Training epoch 4 batch 919 with loss 1.7271648645401.\n",
      "Training epoch 4 batch 920 with loss 2.1467649936676025.\n",
      "Training epoch 4 batch 921 with loss 2.2477614879608154.\n",
      "Training epoch 4 batch 922 with loss 1.9635274410247803.\n",
      "Training epoch 4 batch 923 with loss 2.010859489440918.\n",
      "Training epoch 4 batch 924 with loss 2.090385675430298.\n",
      "Training epoch 4 batch 925 with loss 1.718055248260498.\n",
      "Training epoch 4 batch 926 with loss 1.9233028888702393.\n",
      "Training epoch 4 batch 927 with loss 2.104163885116577.\n",
      "Training epoch 4 batch 928 with loss 2.1974503993988037.\n",
      "Training epoch 4 batch 929 with loss 2.266684055328369.\n",
      "Training epoch 4 batch 930 with loss 2.1399619579315186.\n",
      "Training epoch 4 batch 931 with loss 1.8385130167007446.\n",
      "Training epoch 4 batch 932 with loss 2.310218334197998.\n",
      "Training epoch 4 batch 933 with loss 2.155822515487671.\n",
      "Training epoch 4 batch 934 with loss 2.269214153289795.\n",
      "Training epoch 4 batch 935 with loss 2.3618874549865723.\n",
      "Training epoch 4 batch 936 with loss 2.074178695678711.\n",
      "Training epoch 4 batch 937 with loss 2.4164724349975586.\n",
      "Training epoch 4 batch 938 with loss 2.170055866241455.\n",
      "Training epoch 4 batch 939 with loss 2.8298113346099854.\n",
      "Training epoch 4 batch 940 with loss 2.2887957096099854.\n",
      "Training epoch 4 batch 941 with loss 1.9952492713928223.\n",
      "Training epoch 4 batch 942 with loss 2.3107571601867676.\n",
      "Training epoch 4 batch 943 with loss 1.9117032289505005.\n",
      "Training epoch 4 batch 944 with loss 2.2489030361175537.\n",
      "Training epoch 4 batch 945 with loss 1.9053294658660889.\n",
      "Training epoch 4 batch 946 with loss 1.9335854053497314.\n",
      "Training epoch 4 batch 947 with loss 2.1664652824401855.\n",
      "Training epoch 4 batch 948 with loss 2.155195713043213.\n",
      "Training epoch 4 batch 949 with loss 2.3595693111419678.\n",
      "Training epoch 4 batch 950 with loss 1.9121102094650269.\n",
      "Training epoch 4 batch 951 with loss 1.7783938646316528.\n",
      "Training epoch 4 batch 952 with loss 2.4828784465789795.\n",
      "Training epoch 4 batch 953 with loss 2.305783987045288.\n",
      "Training epoch 4 batch 954 with loss 2.1282737255096436.\n",
      "Training epoch 4 batch 955 with loss 2.080798625946045.\n",
      "Training epoch 4 batch 956 with loss 2.036465883255005.\n",
      "Training epoch 4 batch 957 with loss 2.051957368850708.\n",
      "Training epoch 4 batch 958 with loss 1.8581938743591309.\n",
      "Training epoch 4 batch 959 with loss 2.433298349380493.\n",
      "Training epoch 4 batch 960 with loss 2.128664970397949.\n",
      "Training epoch 4 batch 961 with loss 2.463472843170166.\n",
      "Training epoch 4 batch 962 with loss 2.0267724990844727.\n",
      "Training epoch 4 batch 963 with loss 2.306922197341919.\n",
      "Training epoch 4 batch 964 with loss 2.516713857650757.\n",
      "Training epoch 4 batch 965 with loss 1.40206778049469.\n",
      "Training epoch 4 batch 966 with loss 2.196373462677002.\n",
      "Training epoch 4 batch 967 with loss 2.00437593460083.\n",
      "Training epoch 4 batch 968 with loss 2.1961631774902344.\n",
      "Training epoch 4 batch 969 with loss 2.288137912750244.\n",
      "Training epoch 4 batch 970 with loss 2.4460370540618896.\n",
      "Training epoch 4 batch 971 with loss 2.521505832672119.\n",
      "Training epoch 4 batch 972 with loss 1.995232343673706.\n",
      "Training epoch 4 batch 973 with loss 2.018153667449951.\n",
      "Training epoch 4 batch 974 with loss 1.8274133205413818.\n",
      "Training epoch 4 batch 975 with loss 2.0362110137939453.\n",
      "Training epoch 4 batch 976 with loss 2.6825368404388428.\n",
      "Training epoch 4 batch 977 with loss 2.275606632232666.\n",
      "Training epoch 4 batch 978 with loss 2.1855719089508057.\n",
      "Training epoch 4 batch 979 with loss 2.049095392227173.\n",
      "Training epoch 4 batch 980 with loss 1.8710507154464722.\n",
      "Training epoch 4 batch 981 with loss 2.053652286529541.\n",
      "Training epoch 4 batch 982 with loss 1.9992491006851196.\n",
      "Training epoch 4 batch 983 with loss 1.9264286756515503.\n",
      "Training epoch 4 batch 984 with loss 1.9368996620178223.\n",
      "Training epoch 4 batch 985 with loss 2.2707650661468506.\n",
      "Training epoch 4 batch 986 with loss 1.7655936479568481.\n",
      "Training epoch 4 batch 987 with loss 1.873779296875.\n",
      "Training epoch 4 batch 988 with loss 2.117457866668701.\n",
      "Training epoch 4 batch 989 with loss 1.8692065477371216.\n",
      "Training epoch 4 batch 990 with loss 1.9392263889312744.\n",
      "Training epoch 4 batch 991 with loss 2.26982045173645.\n",
      "Training epoch 4 batch 992 with loss 2.3238754272460938.\n",
      "Training epoch 4 batch 993 with loss 2.0601441860198975.\n",
      "Training epoch 4 batch 994 with loss 2.2871644496917725.\n",
      "Training epoch 4 batch 995 with loss 2.3005781173706055.\n",
      "Training epoch 4 batch 996 with loss 2.2264490127563477.\n",
      "Training epoch 4 batch 997 with loss 2.570017099380493.\n",
      "Training epoch 4 batch 998 with loss 2.0502753257751465.\n",
      "Training epoch 4 batch 999 with loss 2.010823965072632.\n",
      "Test batch 0 with loss 2.744716167449951.\n",
      "Test batch 1 with loss 2.4220573902130127.\n",
      "Test batch 2 with loss 2.890465021133423.\n",
      "Test batch 3 with loss 3.003695249557495.\n",
      "Test batch 4 with loss 2.5679330825805664.\n",
      "Test batch 5 with loss 2.6326253414154053.\n",
      "Test batch 6 with loss 3.220167875289917.\n",
      "Test batch 7 with loss 2.5859885215759277.\n",
      "Test batch 8 with loss 2.6987650394439697.\n",
      "Test batch 9 with loss 2.731694221496582.\n",
      "Test batch 10 with loss 2.539902448654175.\n",
      "Test batch 11 with loss 2.7546238899230957.\n",
      "Test batch 12 with loss 2.8490402698516846.\n",
      "Test batch 13 with loss 2.363891363143921.\n",
      "Test batch 14 with loss 2.3896443843841553.\n",
      "Test batch 15 with loss 2.920975685119629.\n",
      "Test batch 16 with loss 2.5965828895568848.\n",
      "Test batch 17 with loss 2.7976748943328857.\n",
      "Test batch 18 with loss 2.233630418777466.\n",
      "Test batch 19 with loss 2.3801345825195312.\n",
      "Test batch 20 with loss 2.712738037109375.\n",
      "Test batch 21 with loss 2.641139507293701.\n",
      "Test batch 22 with loss 3.359477996826172.\n",
      "Test batch 23 with loss 2.2500388622283936.\n",
      "Test batch 24 with loss 2.9813907146453857.\n",
      "Test batch 25 with loss 2.239546060562134.\n",
      "Test batch 26 with loss 2.377427101135254.\n",
      "Test batch 27 with loss 2.469646692276001.\n",
      "Test batch 28 with loss 3.111853837966919.\n",
      "Test batch 29 with loss 2.4855353832244873.\n",
      "Test batch 30 with loss 2.928039789199829.\n",
      "Test batch 31 with loss 2.77370548248291.\n",
      "Test batch 32 with loss 2.676358222961426.\n",
      "Test batch 33 with loss 3.0130455493927.\n",
      "Test batch 34 with loss 2.9518322944641113.\n",
      "Test batch 35 with loss 2.714478015899658.\n",
      "Test batch 36 with loss 2.7253334522247314.\n",
      "Test batch 37 with loss 2.5741827487945557.\n",
      "Test batch 38 with loss 2.865009069442749.\n",
      "Test batch 39 with loss 2.7343647480010986.\n",
      "Test batch 40 with loss 2.3896749019622803.\n",
      "Test batch 41 with loss 2.3650381565093994.\n",
      "Test batch 42 with loss 2.604069471359253.\n",
      "Test batch 43 with loss 2.703496217727661.\n",
      "Test batch 44 with loss 2.359891414642334.\n",
      "Test batch 45 with loss 2.348170757293701.\n",
      "Test batch 46 with loss 2.122859239578247.\n",
      "Test batch 47 with loss 2.565459728240967.\n",
      "Test batch 48 with loss 2.4539055824279785.\n",
      "Test batch 49 with loss 2.873736619949341.\n",
      "Test batch 50 with loss 2.7779619693756104.\n",
      "Test batch 51 with loss 2.868051767349243.\n",
      "Test batch 52 with loss 2.060126304626465.\n",
      "Test batch 53 with loss 2.562004566192627.\n",
      "Test batch 54 with loss 2.351853370666504.\n",
      "Test batch 55 with loss 2.8719515800476074.\n",
      "Test batch 56 with loss 3.399235963821411.\n",
      "Test batch 57 with loss 3.177142381668091.\n",
      "Test batch 58 with loss 2.6397223472595215.\n",
      "Test batch 59 with loss 2.3593027591705322.\n",
      "Test batch 60 with loss 2.6702444553375244.\n",
      "Test batch 61 with loss 2.5824456214904785.\n",
      "Test batch 62 with loss 2.4352104663848877.\n",
      "Test batch 63 with loss 2.605085611343384.\n",
      "Test batch 64 with loss 2.8456599712371826.\n",
      "Test batch 65 with loss 2.343140125274658.\n",
      "Test batch 66 with loss 2.7007696628570557.\n",
      "Test batch 67 with loss 2.631199836730957.\n",
      "Test batch 68 with loss 2.709080219268799.\n",
      "Test batch 69 with loss 2.4090142250061035.\n",
      "Test batch 70 with loss 2.699770450592041.\n",
      "Test batch 71 with loss 2.1814799308776855.\n",
      "Test batch 72 with loss 2.6610493659973145.\n",
      "Test batch 73 with loss 2.3895061016082764.\n",
      "Test batch 74 with loss 2.918985605239868.\n",
      "Test batch 75 with loss 2.6397643089294434.\n",
      "Test batch 76 with loss 2.3180253505706787.\n",
      "Test batch 77 with loss 2.0915074348449707.\n",
      "Test batch 78 with loss 2.6678216457366943.\n",
      "Test batch 79 with loss 2.1786282062530518.\n",
      "Test batch 80 with loss 2.5173676013946533.\n",
      "Test batch 81 with loss 2.1604044437408447.\n",
      "Test batch 82 with loss 2.769117832183838.\n",
      "Test batch 83 with loss 2.43489670753479.\n",
      "Test batch 84 with loss 2.592639207839966.\n",
      "Test batch 85 with loss 2.82871150970459.\n",
      "Test batch 86 with loss 2.4935595989227295.\n",
      "Test batch 87 with loss 2.398242712020874.\n",
      "Test batch 88 with loss 2.5793042182922363.\n",
      "Test batch 89 with loss 2.31257700920105.\n",
      "Test batch 90 with loss 2.392961025238037.\n",
      "Test batch 91 with loss 2.8415913581848145.\n",
      "Test batch 92 with loss 2.4112613201141357.\n",
      "Test batch 93 with loss 2.655902147293091.\n",
      "Test batch 94 with loss 2.214057207107544.\n",
      "Test batch 95 with loss 2.9301040172576904.\n",
      "Test batch 96 with loss 2.5962538719177246.\n",
      "Test batch 97 with loss 2.5467183589935303.\n",
      "Test batch 98 with loss 2.3525004386901855.\n",
      "Test batch 99 with loss 2.762648582458496.\n",
      "Test batch 100 with loss 2.2849247455596924.\n",
      "Test batch 101 with loss 2.6515839099884033.\n",
      "Test batch 102 with loss 2.4676971435546875.\n",
      "Test batch 103 with loss 1.7861195802688599.\n",
      "Test batch 104 with loss 2.285104274749756.\n",
      "Test batch 105 with loss 2.310351848602295.\n",
      "Test batch 106 with loss 1.9817723035812378.\n",
      "Test batch 107 with loss 2.745281934738159.\n",
      "Test batch 108 with loss 2.7569727897644043.\n",
      "Test batch 109 with loss 2.3893957138061523.\n",
      "Test batch 110 with loss 2.510986328125.\n",
      "Test batch 111 with loss 2.5257833003997803.\n",
      "Test batch 112 with loss 2.7522008419036865.\n",
      "Test batch 113 with loss 2.568080186843872.\n",
      "Test batch 114 with loss 2.399714708328247.\n",
      "Test batch 115 with loss 2.411203622817993.\n",
      "Test batch 116 with loss 2.745248317718506.\n",
      "Test batch 117 with loss 2.6363093852996826.\n",
      "Test batch 118 with loss 2.6462275981903076.\n",
      "Test batch 119 with loss 2.3911545276641846.\n",
      "Test batch 120 with loss 2.4694275856018066.\n",
      "Test batch 121 with loss 2.490206241607666.\n",
      "Test batch 122 with loss 2.7051899433135986.\n",
      "Test batch 123 with loss 2.6248326301574707.\n",
      "Test batch 124 with loss 2.644859552383423.\n",
      "Test batch 125 with loss 2.799668788909912.\n",
      "Test batch 126 with loss 2.176636219024658.\n",
      "Test batch 127 with loss 2.7224814891815186.\n",
      "Test batch 128 with loss 2.4210803508758545.\n",
      "Test batch 129 with loss 2.2893993854522705.\n",
      "Test batch 130 with loss 2.0930335521698.\n",
      "Test batch 131 with loss 2.563896656036377.\n",
      "Test batch 132 with loss 2.9647560119628906.\n",
      "Test batch 133 with loss 2.9845874309539795.\n",
      "Test batch 134 with loss 2.575402021408081.\n",
      "Test batch 135 with loss 2.734539270401001.\n",
      "Test batch 136 with loss 2.595524311065674.\n",
      "Test batch 137 with loss 2.8444199562072754.\n",
      "Test batch 138 with loss 3.160412311553955.\n",
      "Test batch 139 with loss 2.286263942718506.\n",
      "Test batch 140 with loss 2.279682159423828.\n",
      "Test batch 141 with loss 2.3943307399749756.\n",
      "Test batch 142 with loss 2.8137528896331787.\n",
      "Test batch 143 with loss 2.807159900665283.\n",
      "Test batch 144 with loss 2.7367260456085205.\n",
      "Test batch 145 with loss 2.9060685634613037.\n",
      "Test batch 146 with loss 2.686460018157959.\n",
      "Test batch 147 with loss 3.2214512825012207.\n",
      "Test batch 148 with loss 2.300274610519409.\n",
      "Test batch 149 with loss 2.6123316287994385.\n",
      "Test batch 150 with loss 1.7179949283599854.\n",
      "Test batch 151 with loss 2.078002452850342.\n",
      "Test batch 152 with loss 2.7720015048980713.\n",
      "Test batch 153 with loss 2.50821590423584.\n",
      "Test batch 154 with loss 2.4533979892730713.\n",
      "Test batch 155 with loss 2.530202627182007.\n",
      "Test batch 156 with loss 2.2189154624938965.\n",
      "Test batch 157 with loss 3.4029593467712402.\n",
      "Test batch 158 with loss 3.280071496963501.\n",
      "Test batch 159 with loss 2.7127463817596436.\n",
      "Test batch 160 with loss 2.5429131984710693.\n",
      "Test batch 161 with loss 2.5920374393463135.\n",
      "Test batch 162 with loss 2.4552793502807617.\n",
      "Test batch 163 with loss 2.6314537525177.\n",
      "Test batch 164 with loss 2.204718589782715.\n",
      "Test batch 165 with loss 2.7074344158172607.\n",
      "Test batch 166 with loss 2.464627981185913.\n",
      "Test batch 167 with loss 2.692816972732544.\n",
      "Test batch 168 with loss 3.4973626136779785.\n",
      "Test batch 169 with loss 2.311899423599243.\n",
      "Test batch 170 with loss 2.8951425552368164.\n",
      "Test batch 171 with loss 2.4656498432159424.\n",
      "Test batch 172 with loss 2.940762758255005.\n",
      "Test batch 173 with loss 2.859300136566162.\n",
      "Test batch 174 with loss 2.880977153778076.\n",
      "Test batch 175 with loss 2.6829941272735596.\n",
      "Test batch 176 with loss 2.4994957447052.\n",
      "Test batch 177 with loss 2.3900556564331055.\n",
      "Test batch 178 with loss 2.596658945083618.\n",
      "Test batch 179 with loss 2.4601335525512695.\n",
      "Test batch 180 with loss 2.087923765182495.\n",
      "Test batch 181 with loss 2.537261486053467.\n",
      "Test batch 182 with loss 2.297337770462036.\n",
      "Test batch 183 with loss 3.1201088428497314.\n",
      "Test batch 184 with loss 2.3181793689727783.\n",
      "Test batch 185 with loss 2.344571590423584.\n",
      "Test batch 186 with loss 2.4809226989746094.\n",
      "Test batch 187 with loss 2.3385205268859863.\n",
      "Test batch 188 with loss 1.7520332336425781.\n",
      "Test batch 189 with loss 3.3162009716033936.\n",
      "Test batch 190 with loss 2.4355523586273193.\n",
      "Test batch 191 with loss 2.4250152111053467.\n",
      "Test batch 192 with loss 2.150852680206299.\n",
      "Test batch 193 with loss 2.9687256813049316.\n",
      "Test batch 194 with loss 2.49560809135437.\n",
      "Test batch 195 with loss 2.251488447189331.\n",
      "Test batch 196 with loss 2.4814536571502686.\n",
      "Test batch 197 with loss 2.096562147140503.\n",
      "Test batch 198 with loss 3.1329591274261475.\n",
      "Test batch 199 with loss 2.6171376705169678.\n",
      "Test batch 200 with loss 2.7288880348205566.\n",
      "Test batch 201 with loss 2.449174642562866.\n",
      "Test batch 202 with loss 2.420414924621582.\n",
      "Test batch 203 with loss 2.7072935104370117.\n",
      "Test batch 204 with loss 2.6809134483337402.\n",
      "Test batch 205 with loss 2.8655083179473877.\n",
      "Test batch 206 with loss 2.5340893268585205.\n",
      "Test batch 207 with loss 3.487257719039917.\n",
      "Test batch 208 with loss 2.660527229309082.\n",
      "Test batch 209 with loss 2.3609824180603027.\n",
      "Test batch 210 with loss 2.8717758655548096.\n",
      "Test batch 211 with loss 2.5193307399749756.\n",
      "Test batch 212 with loss 3.1530847549438477.\n",
      "Test batch 213 with loss 2.4559524059295654.\n",
      "Test batch 214 with loss 2.357541561126709.\n",
      "Test batch 215 with loss 2.032135486602783.\n",
      "Test batch 216 with loss 2.4875528812408447.\n",
      "Test batch 217 with loss 2.949206590652466.\n",
      "Test batch 218 with loss 2.9874062538146973.\n",
      "Test batch 219 with loss 2.364668369293213.\n",
      "Test batch 220 with loss 3.3059914112091064.\n",
      "Test batch 221 with loss 2.5493507385253906.\n",
      "Test batch 222 with loss 3.3997321128845215.\n",
      "Test batch 223 with loss 1.9815784692764282.\n",
      "Test batch 224 with loss 2.8608925342559814.\n",
      "Test batch 225 with loss 3.0079903602600098.\n",
      "Test batch 226 with loss 2.7439486980438232.\n",
      "Test batch 227 with loss 2.9729363918304443.\n",
      "Test batch 228 with loss 1.9459247589111328.\n",
      "Test batch 229 with loss 2.5254106521606445.\n",
      "Test batch 230 with loss 2.3901944160461426.\n",
      "Test batch 231 with loss 2.5295608043670654.\n",
      "Test batch 232 with loss 2.1374337673187256.\n",
      "Test batch 233 with loss 2.77546763420105.\n",
      "Test batch 234 with loss 2.2558250427246094.\n",
      "Test batch 235 with loss 2.2903432846069336.\n",
      "Test batch 236 with loss 2.624817371368408.\n",
      "Test batch 237 with loss 2.3728187084198.\n",
      "Test batch 238 with loss 2.5399343967437744.\n",
      "Test batch 239 with loss 2.4612889289855957.\n",
      "Test batch 240 with loss 2.0626890659332275.\n",
      "Test batch 241 with loss 2.7085719108581543.\n",
      "Test batch 242 with loss 2.1797194480895996.\n",
      "Test batch 243 with loss 3.019386053085327.\n",
      "Test batch 244 with loss 2.5641965866088867.\n",
      "Test batch 245 with loss 2.216013193130493.\n",
      "Test batch 246 with loss 2.6823527812957764.\n",
      "Test batch 247 with loss 2.8425180912017822.\n",
      "Test batch 248 with loss 2.5913102626800537.\n",
      "Test batch 249 with loss 2.8670244216918945.\n",
      "Test batch 250 with loss 2.3720438480377197.\n",
      "Test batch 251 with loss 2.8966798782348633.\n",
      "Test batch 252 with loss 2.60953950881958.\n",
      "Test batch 253 with loss 2.3413031101226807.\n",
      "Test batch 254 with loss 2.484001636505127.\n",
      "Test batch 255 with loss 2.376370906829834.\n",
      "Test batch 256 with loss 2.2234175205230713.\n",
      "Test batch 257 with loss 2.977517604827881.\n",
      "Test batch 258 with loss 2.0052390098571777.\n",
      "Test batch 259 with loss 1.9847522974014282.\n",
      "Test batch 260 with loss 2.935622453689575.\n",
      "Test batch 261 with loss 3.036330461502075.\n",
      "Test batch 262 with loss 2.6825058460235596.\n",
      "Test batch 263 with loss 2.6971592903137207.\n",
      "Test batch 264 with loss 2.549471139907837.\n",
      "Test batch 265 with loss 2.0399255752563477.\n",
      "Test batch 266 with loss 2.7393407821655273.\n",
      "Test batch 267 with loss 2.5765225887298584.\n",
      "Test batch 268 with loss 2.649671792984009.\n",
      "Test batch 269 with loss 2.502285957336426.\n",
      "Test batch 270 with loss 2.819140672683716.\n",
      "Test batch 271 with loss 2.5995664596557617.\n",
      "Test batch 272 with loss 2.4266672134399414.\n",
      "Test batch 273 with loss 2.5917856693267822.\n",
      "Test batch 274 with loss 2.347252368927002.\n",
      "Test batch 275 with loss 2.3240530490875244.\n",
      "Test batch 276 with loss 2.2988386154174805.\n",
      "Test batch 277 with loss 2.5117671489715576.\n",
      "Test batch 278 with loss 2.020987033843994.\n",
      "Test batch 279 with loss 2.4325754642486572.\n",
      "Test batch 280 with loss 2.6805808544158936.\n",
      "Test batch 281 with loss 2.5822360515594482.\n",
      "Test batch 282 with loss 2.345601797103882.\n",
      "Test batch 283 with loss 2.5691192150115967.\n",
      "Test batch 284 with loss 2.2478291988372803.\n",
      "Test batch 285 with loss 2.4746828079223633.\n",
      "Test batch 286 with loss 2.9948456287384033.\n",
      "Test batch 287 with loss 2.1038806438446045.\n",
      "Test batch 288 with loss 2.265900135040283.\n",
      "Test batch 289 with loss 2.62874698638916.\n",
      "Test batch 290 with loss 2.458507776260376.\n",
      "Test batch 291 with loss 2.938403367996216.\n",
      "Test batch 292 with loss 2.716191291809082.\n",
      "Test batch 293 with loss 3.5019097328186035.\n",
      "Test batch 294 with loss 3.0921645164489746.\n",
      "Test batch 295 with loss 2.555161714553833.\n",
      "Test batch 296 with loss 3.0346693992614746.\n",
      "Test batch 297 with loss 2.4704582691192627.\n",
      "Test batch 298 with loss 2.60762882232666.\n",
      "Test batch 299 with loss 2.7203595638275146.\n",
      "Test batch 300 with loss 2.2316431999206543.\n",
      "Test batch 301 with loss 2.547701835632324.\n",
      "Test batch 302 with loss 2.035388946533203.\n",
      "Test batch 303 with loss 2.3309454917907715.\n",
      "Test batch 304 with loss 2.6492228507995605.\n",
      "Test batch 305 with loss 2.3384203910827637.\n",
      "Test batch 306 with loss 2.0428662300109863.\n",
      "Test batch 307 with loss 3.0772931575775146.\n",
      "Test batch 308 with loss 2.493354558944702.\n",
      "Test batch 309 with loss 2.548396110534668.\n",
      "Test batch 310 with loss 2.5052201747894287.\n",
      "Test batch 311 with loss 2.194565773010254.\n",
      "Test batch 312 with loss 2.245814561843872.\n",
      "Test batch 313 with loss 2.5188944339752197.\n",
      "Test batch 314 with loss 2.7781195640563965.\n",
      "Test batch 315 with loss 2.7742385864257812.\n",
      "Test batch 316 with loss 2.793025493621826.\n",
      "Test batch 317 with loss 2.2743337154388428.\n",
      "Test batch 318 with loss 2.3899500370025635.\n",
      "Test batch 319 with loss 2.481954336166382.\n",
      "Test batch 320 with loss 2.5137240886688232.\n",
      "Test batch 321 with loss 3.0541272163391113.\n",
      "Test batch 322 with loss 2.2693307399749756.\n",
      "Test batch 323 with loss 3.0077590942382812.\n",
      "Test batch 324 with loss 2.299687623977661.\n",
      "Test batch 325 with loss 2.95407772064209.\n",
      "Test batch 326 with loss 3.0008387565612793.\n",
      "Test batch 327 with loss 2.601440668106079.\n",
      "Test batch 328 with loss 2.8507041931152344.\n",
      "Test batch 329 with loss 2.630950927734375.\n",
      "Test batch 330 with loss 2.0911810398101807.\n",
      "Test batch 331 with loss 2.2037763595581055.\n",
      "Test batch 332 with loss 2.6586203575134277.\n",
      "Test batch 333 with loss 2.172720193862915.\n",
      "Training epoch 5 batch 0 with loss 1.7123469114303589.\n",
      "Training epoch 5 batch 1 with loss 1.487222671508789.\n",
      "Training epoch 5 batch 2 with loss 1.5053420066833496.\n",
      "Training epoch 5 batch 3 with loss 1.3106085062026978.\n",
      "Training epoch 5 batch 4 with loss 1.546388030052185.\n",
      "Training epoch 5 batch 5 with loss 1.3282546997070312.\n",
      "Training epoch 5 batch 6 with loss 1.6846978664398193.\n",
      "Training epoch 5 batch 7 with loss 1.8665523529052734.\n",
      "Training epoch 5 batch 8 with loss 1.233349323272705.\n",
      "Training epoch 5 batch 9 with loss 1.5844446420669556.\n",
      "Training epoch 5 batch 10 with loss 1.373022198677063.\n",
      "Training epoch 5 batch 11 with loss 1.788828730583191.\n",
      "Training epoch 5 batch 12 with loss 1.6324098110198975.\n",
      "Training epoch 5 batch 13 with loss 1.8287285566329956.\n",
      "Training epoch 5 batch 14 with loss 1.598596215248108.\n",
      "Training epoch 5 batch 15 with loss 1.4431554079055786.\n",
      "Training epoch 5 batch 16 with loss 1.197494387626648.\n",
      "Training epoch 5 batch 17 with loss 1.9658976793289185.\n",
      "Training epoch 5 batch 18 with loss 1.479447841644287.\n",
      "Training epoch 5 batch 19 with loss 1.7548713684082031.\n",
      "Training epoch 5 batch 20 with loss 1.7832942008972168.\n",
      "Training epoch 5 batch 21 with loss 1.4321367740631104.\n",
      "Training epoch 5 batch 22 with loss 1.7671889066696167.\n",
      "Training epoch 5 batch 23 with loss 1.5362681150436401.\n",
      "Training epoch 5 batch 24 with loss 1.5065703392028809.\n",
      "Training epoch 5 batch 25 with loss 1.5523276329040527.\n",
      "Training epoch 5 batch 26 with loss 1.80196213722229.\n",
      "Training epoch 5 batch 27 with loss 1.6472530364990234.\n",
      "Training epoch 5 batch 28 with loss 1.7012542486190796.\n",
      "Training epoch 5 batch 29 with loss 1.3992199897766113.\n",
      "Training epoch 5 batch 30 with loss 1.3171640634536743.\n",
      "Training epoch 5 batch 31 with loss 1.6458724737167358.\n",
      "Training epoch 5 batch 32 with loss 1.7212461233139038.\n",
      "Training epoch 5 batch 33 with loss 1.7904739379882812.\n",
      "Training epoch 5 batch 34 with loss 1.549903392791748.\n",
      "Training epoch 5 batch 35 with loss 1.6652870178222656.\n",
      "Training epoch 5 batch 36 with loss 1.8812261819839478.\n",
      "Training epoch 5 batch 37 with loss 1.8688377141952515.\n",
      "Training epoch 5 batch 38 with loss 1.6642168760299683.\n",
      "Training epoch 5 batch 39 with loss 1.5401135683059692.\n",
      "Training epoch 5 batch 40 with loss 1.463998556137085.\n",
      "Training epoch 5 batch 41 with loss 1.7818738222122192.\n",
      "Training epoch 5 batch 42 with loss 1.7815386056900024.\n",
      "Training epoch 5 batch 43 with loss 1.4151062965393066.\n",
      "Training epoch 5 batch 44 with loss 1.4290703535079956.\n",
      "Training epoch 5 batch 45 with loss 1.5321753025054932.\n",
      "Training epoch 5 batch 46 with loss 1.810718059539795.\n",
      "Training epoch 5 batch 47 with loss 1.4493072032928467.\n",
      "Training epoch 5 batch 48 with loss 1.4463403224945068.\n",
      "Training epoch 5 batch 49 with loss 1.9497027397155762.\n",
      "Training epoch 5 batch 50 with loss 1.2678322792053223.\n",
      "Training epoch 5 batch 51 with loss 1.6987816095352173.\n",
      "Training epoch 5 batch 52 with loss 1.6706652641296387.\n",
      "Training epoch 5 batch 53 with loss 1.8264033794403076.\n",
      "Training epoch 5 batch 54 with loss 1.4628552198410034.\n",
      "Training epoch 5 batch 55 with loss 1.3726937770843506.\n",
      "Training epoch 5 batch 56 with loss 1.8273961544036865.\n",
      "Training epoch 5 batch 57 with loss 1.8213545083999634.\n",
      "Training epoch 5 batch 58 with loss 1.7206155061721802.\n",
      "Training epoch 5 batch 59 with loss 1.7636990547180176.\n",
      "Training epoch 5 batch 60 with loss 1.950555443763733.\n",
      "Training epoch 5 batch 61 with loss 1.8005809783935547.\n",
      "Training epoch 5 batch 62 with loss 1.4833693504333496.\n",
      "Training epoch 5 batch 63 with loss 1.0070289373397827.\n",
      "Training epoch 5 batch 64 with loss 1.6571441888809204.\n",
      "Training epoch 5 batch 65 with loss 1.760224461555481.\n",
      "Training epoch 5 batch 66 with loss 1.953332781791687.\n",
      "Training epoch 5 batch 67 with loss 1.407909870147705.\n",
      "Training epoch 5 batch 68 with loss 1.7219723463058472.\n",
      "Training epoch 5 batch 69 with loss 1.7656697034835815.\n",
      "Training epoch 5 batch 70 with loss 1.38004469871521.\n",
      "Training epoch 5 batch 71 with loss 1.7646450996398926.\n",
      "Training epoch 5 batch 72 with loss 1.7744663953781128.\n",
      "Training epoch 5 batch 73 with loss 1.4998949766159058.\n",
      "Training epoch 5 batch 74 with loss 1.5149140357971191.\n",
      "Training epoch 5 batch 75 with loss 1.5269845724105835.\n",
      "Training epoch 5 batch 76 with loss 1.449364185333252.\n",
      "Training epoch 5 batch 77 with loss 1.5560791492462158.\n",
      "Training epoch 5 batch 78 with loss 1.7070547342300415.\n",
      "Training epoch 5 batch 79 with loss 1.4778187274932861.\n",
      "Training epoch 5 batch 80 with loss 1.7268376350402832.\n",
      "Training epoch 5 batch 81 with loss 1.5915007591247559.\n",
      "Training epoch 5 batch 82 with loss 1.7064990997314453.\n",
      "Training epoch 5 batch 83 with loss 1.9105125665664673.\n",
      "Training epoch 5 batch 84 with loss 1.59260892868042.\n",
      "Training epoch 5 batch 85 with loss 1.6311256885528564.\n",
      "Training epoch 5 batch 86 with loss 1.8356101512908936.\n",
      "Training epoch 5 batch 87 with loss 1.467976450920105.\n",
      "Training epoch 5 batch 88 with loss 1.7260949611663818.\n",
      "Training epoch 5 batch 89 with loss 1.423707365989685.\n",
      "Training epoch 5 batch 90 with loss 1.5351685285568237.\n",
      "Training epoch 5 batch 91 with loss 1.6650009155273438.\n",
      "Training epoch 5 batch 92 with loss 1.453866720199585.\n",
      "Training epoch 5 batch 93 with loss 1.6257798671722412.\n",
      "Training epoch 5 batch 94 with loss 1.5732477903366089.\n",
      "Training epoch 5 batch 95 with loss 1.914243221282959.\n",
      "Training epoch 5 batch 96 with loss 1.9945266246795654.\n",
      "Training epoch 5 batch 97 with loss 1.3315932750701904.\n",
      "Training epoch 5 batch 98 with loss 1.437657117843628.\n",
      "Training epoch 5 batch 99 with loss 1.783940076828003.\n",
      "Training epoch 5 batch 100 with loss 1.4161241054534912.\n",
      "Training epoch 5 batch 101 with loss 1.79360830783844.\n",
      "Training epoch 5 batch 102 with loss 1.852233648300171.\n",
      "Training epoch 5 batch 103 with loss 1.5820896625518799.\n",
      "Training epoch 5 batch 104 with loss 1.7291300296783447.\n",
      "Training epoch 5 batch 105 with loss 1.5905333757400513.\n",
      "Training epoch 5 batch 106 with loss 1.5912315845489502.\n",
      "Training epoch 5 batch 107 with loss 1.6762138605117798.\n",
      "Training epoch 5 batch 108 with loss 1.9542902708053589.\n",
      "Training epoch 5 batch 109 with loss 1.785516381263733.\n",
      "Training epoch 5 batch 110 with loss 1.5838698148727417.\n",
      "Training epoch 5 batch 111 with loss 1.652740716934204.\n",
      "Training epoch 5 batch 112 with loss 1.6765843629837036.\n",
      "Training epoch 5 batch 113 with loss 1.5808162689208984.\n",
      "Training epoch 5 batch 114 with loss 1.3066030740737915.\n",
      "Training epoch 5 batch 115 with loss 2.2304461002349854.\n",
      "Training epoch 5 batch 116 with loss 1.830625295639038.\n",
      "Training epoch 5 batch 117 with loss 1.7860527038574219.\n",
      "Training epoch 5 batch 118 with loss 2.075908660888672.\n",
      "Training epoch 5 batch 119 with loss 1.6996172666549683.\n",
      "Training epoch 5 batch 120 with loss 1.8103197813034058.\n",
      "Training epoch 5 batch 121 with loss 1.8976857662200928.\n",
      "Training epoch 5 batch 122 with loss 1.7153748273849487.\n",
      "Training epoch 5 batch 123 with loss 1.757705807685852.\n",
      "Training epoch 5 batch 124 with loss 1.5760926008224487.\n",
      "Training epoch 5 batch 125 with loss 1.5566589832305908.\n",
      "Training epoch 5 batch 126 with loss 1.7483086585998535.\n",
      "Training epoch 5 batch 127 with loss 1.848792552947998.\n",
      "Training epoch 5 batch 128 with loss 1.4070625305175781.\n",
      "Training epoch 5 batch 129 with loss 1.6157689094543457.\n",
      "Training epoch 5 batch 130 with loss 2.033569812774658.\n",
      "Training epoch 5 batch 131 with loss 1.9674274921417236.\n",
      "Training epoch 5 batch 132 with loss 1.888029932975769.\n",
      "Training epoch 5 batch 133 with loss 2.011369228363037.\n",
      "Training epoch 5 batch 134 with loss 2.1096889972686768.\n",
      "Training epoch 5 batch 135 with loss 1.7146364450454712.\n",
      "Training epoch 5 batch 136 with loss 1.4569318294525146.\n",
      "Training epoch 5 batch 137 with loss 1.4778825044631958.\n",
      "Training epoch 5 batch 138 with loss 1.7657573223114014.\n",
      "Training epoch 5 batch 139 with loss 1.7388877868652344.\n",
      "Training epoch 5 batch 140 with loss 1.9282432794570923.\n",
      "Training epoch 5 batch 141 with loss 1.6171663999557495.\n",
      "Training epoch 5 batch 142 with loss 1.6789904832839966.\n",
      "Training epoch 5 batch 143 with loss 1.9108415842056274.\n",
      "Training epoch 5 batch 144 with loss 1.8625229597091675.\n",
      "Training epoch 5 batch 145 with loss 1.9106920957565308.\n",
      "Training epoch 5 batch 146 with loss 1.6536781787872314.\n",
      "Training epoch 5 batch 147 with loss 1.7256907224655151.\n",
      "Training epoch 5 batch 148 with loss 1.7872587442398071.\n",
      "Training epoch 5 batch 149 with loss 1.9383450746536255.\n",
      "Training epoch 5 batch 150 with loss 1.9303611516952515.\n",
      "Training epoch 5 batch 151 with loss 1.8022273778915405.\n",
      "Training epoch 5 batch 152 with loss 1.8310692310333252.\n",
      "Training epoch 5 batch 153 with loss 1.4851093292236328.\n",
      "Training epoch 5 batch 154 with loss 2.052518844604492.\n",
      "Training epoch 5 batch 155 with loss 1.6653350591659546.\n",
      "Training epoch 5 batch 156 with loss 1.5487213134765625.\n",
      "Training epoch 5 batch 157 with loss 1.9827734231948853.\n",
      "Training epoch 5 batch 158 with loss 1.7694588899612427.\n",
      "Training epoch 5 batch 159 with loss 2.2070443630218506.\n",
      "Training epoch 5 batch 160 with loss 1.9094222784042358.\n",
      "Training epoch 5 batch 161 with loss 1.5121421813964844.\n",
      "Training epoch 5 batch 162 with loss 2.0723516941070557.\n",
      "Training epoch 5 batch 163 with loss 1.8722033500671387.\n",
      "Training epoch 5 batch 164 with loss 2.157456636428833.\n",
      "Training epoch 5 batch 165 with loss 1.7500052452087402.\n",
      "Training epoch 5 batch 166 with loss 1.6783279180526733.\n",
      "Training epoch 5 batch 167 with loss 1.7583081722259521.\n",
      "Training epoch 5 batch 168 with loss 1.6713558435440063.\n",
      "Training epoch 5 batch 169 with loss 1.4804797172546387.\n",
      "Training epoch 5 batch 170 with loss 1.879409670829773.\n",
      "Training epoch 5 batch 171 with loss 1.3742201328277588.\n",
      "Training epoch 5 batch 172 with loss 1.7776597738265991.\n",
      "Training epoch 5 batch 173 with loss 1.9629274606704712.\n",
      "Training epoch 5 batch 174 with loss 1.5437705516815186.\n",
      "Training epoch 5 batch 175 with loss 1.7688921689987183.\n",
      "Training epoch 5 batch 176 with loss 1.5332940816879272.\n",
      "Training epoch 5 batch 177 with loss 1.5752692222595215.\n",
      "Training epoch 5 batch 178 with loss 1.7718372344970703.\n",
      "Training epoch 5 batch 179 with loss 2.1088180541992188.\n",
      "Training epoch 5 batch 180 with loss 1.6801162958145142.\n",
      "Training epoch 5 batch 181 with loss 1.7531317472457886.\n",
      "Training epoch 5 batch 182 with loss 1.972489833831787.\n",
      "Training epoch 5 batch 183 with loss 1.9308923482894897.\n",
      "Training epoch 5 batch 184 with loss 1.9148972034454346.\n",
      "Training epoch 5 batch 185 with loss 1.845706820487976.\n",
      "Training epoch 5 batch 186 with loss 2.1483235359191895.\n",
      "Training epoch 5 batch 187 with loss 1.5939505100250244.\n",
      "Training epoch 5 batch 188 with loss 1.7655216455459595.\n",
      "Training epoch 5 batch 189 with loss 2.3147828578948975.\n",
      "Training epoch 5 batch 190 with loss 2.002061367034912.\n",
      "Training epoch 5 batch 191 with loss 1.6745699644088745.\n",
      "Training epoch 5 batch 192 with loss 1.6094279289245605.\n",
      "Training epoch 5 batch 193 with loss 2.0165815353393555.\n",
      "Training epoch 5 batch 194 with loss 1.4761688709259033.\n",
      "Training epoch 5 batch 195 with loss 1.8186123371124268.\n",
      "Training epoch 5 batch 196 with loss 1.961368203163147.\n",
      "Training epoch 5 batch 197 with loss 2.5997300148010254.\n",
      "Training epoch 5 batch 198 with loss 1.7439484596252441.\n",
      "Training epoch 5 batch 199 with loss 1.8207024335861206.\n",
      "Training epoch 5 batch 200 with loss 2.025677442550659.\n",
      "Training epoch 5 batch 201 with loss 1.7165201902389526.\n",
      "Training epoch 5 batch 202 with loss 1.673509955406189.\n",
      "Training epoch 5 batch 203 with loss 2.0412018299102783.\n",
      "Training epoch 5 batch 204 with loss 1.7796021699905396.\n",
      "Training epoch 5 batch 205 with loss 1.8618736267089844.\n",
      "Training epoch 5 batch 206 with loss 1.6041439771652222.\n",
      "Training epoch 5 batch 207 with loss 1.6141079664230347.\n",
      "Training epoch 5 batch 208 with loss 1.9901020526885986.\n",
      "Training epoch 5 batch 209 with loss 1.9417634010314941.\n",
      "Training epoch 5 batch 210 with loss 1.5082794427871704.\n",
      "Training epoch 5 batch 211 with loss 1.6229816675186157.\n",
      "Training epoch 5 batch 212 with loss 1.9869178533554077.\n",
      "Training epoch 5 batch 213 with loss 1.596986174583435.\n",
      "Training epoch 5 batch 214 with loss 1.523747205734253.\n",
      "Training epoch 5 batch 215 with loss 1.8235594034194946.\n",
      "Training epoch 5 batch 216 with loss 1.642923355102539.\n",
      "Training epoch 5 batch 217 with loss 1.4039446115493774.\n",
      "Training epoch 5 batch 218 with loss 1.465283989906311.\n",
      "Training epoch 5 batch 219 with loss 1.8333264589309692.\n",
      "Training epoch 5 batch 220 with loss 1.8777705430984497.\n",
      "Training epoch 5 batch 221 with loss 1.785415768623352.\n",
      "Training epoch 5 batch 222 with loss 1.6648567914962769.\n",
      "Training epoch 5 batch 223 with loss 1.5890426635742188.\n",
      "Training epoch 5 batch 224 with loss 1.9095056056976318.\n",
      "Training epoch 5 batch 225 with loss 1.6922662258148193.\n",
      "Training epoch 5 batch 226 with loss 1.6027830839157104.\n",
      "Training epoch 5 batch 227 with loss 1.9128483533859253.\n",
      "Training epoch 5 batch 228 with loss 1.683807134628296.\n",
      "Training epoch 5 batch 229 with loss 1.5077641010284424.\n",
      "Training epoch 5 batch 230 with loss 1.8062937259674072.\n",
      "Training epoch 5 batch 231 with loss 1.6066433191299438.\n",
      "Training epoch 5 batch 232 with loss 2.374819755554199.\n",
      "Training epoch 5 batch 233 with loss 2.0145795345306396.\n",
      "Training epoch 5 batch 234 with loss 2.2856669425964355.\n",
      "Training epoch 5 batch 235 with loss 1.9561331272125244.\n",
      "Training epoch 5 batch 236 with loss 1.8886282444000244.\n",
      "Training epoch 5 batch 237 with loss 1.768998622894287.\n",
      "Training epoch 5 batch 238 with loss 2.220586061477661.\n",
      "Training epoch 5 batch 239 with loss 1.6366046667099.\n",
      "Training epoch 5 batch 240 with loss 2.0678553581237793.\n",
      "Training epoch 5 batch 241 with loss 1.6244614124298096.\n",
      "Training epoch 5 batch 242 with loss 1.4912511110305786.\n",
      "Training epoch 5 batch 243 with loss 1.8733792304992676.\n",
      "Training epoch 5 batch 244 with loss 1.5797371864318848.\n",
      "Training epoch 5 batch 245 with loss 1.4121911525726318.\n",
      "Training epoch 5 batch 246 with loss 1.6500226259231567.\n",
      "Training epoch 5 batch 247 with loss 1.8730851411819458.\n",
      "Training epoch 5 batch 248 with loss 1.79800546169281.\n",
      "Training epoch 5 batch 249 with loss 2.1390867233276367.\n",
      "Training epoch 5 batch 250 with loss 1.9827206134796143.\n",
      "Training epoch 5 batch 251 with loss 1.8616350889205933.\n",
      "Training epoch 5 batch 252 with loss 1.5806758403778076.\n",
      "Training epoch 5 batch 253 with loss 1.746049165725708.\n",
      "Training epoch 5 batch 254 with loss 1.8194013833999634.\n",
      "Training epoch 5 batch 255 with loss 1.8819140195846558.\n",
      "Training epoch 5 batch 256 with loss 1.5161560773849487.\n",
      "Training epoch 5 batch 257 with loss 1.7033193111419678.\n",
      "Training epoch 5 batch 258 with loss 1.9356039762496948.\n",
      "Training epoch 5 batch 259 with loss 1.7232248783111572.\n",
      "Training epoch 5 batch 260 with loss 1.476876139640808.\n",
      "Training epoch 5 batch 261 with loss 1.9250408411026.\n",
      "Training epoch 5 batch 262 with loss 2.212239980697632.\n",
      "Training epoch 5 batch 263 with loss 1.8981382846832275.\n",
      "Training epoch 5 batch 264 with loss 1.9683985710144043.\n",
      "Training epoch 5 batch 265 with loss 2.025143623352051.\n",
      "Training epoch 5 batch 266 with loss 1.7164742946624756.\n",
      "Training epoch 5 batch 267 with loss 2.1919310092926025.\n",
      "Training epoch 5 batch 268 with loss 1.8572478294372559.\n",
      "Training epoch 5 batch 269 with loss 1.9134713411331177.\n",
      "Training epoch 5 batch 270 with loss 1.7530876398086548.\n",
      "Training epoch 5 batch 271 with loss 1.9918957948684692.\n",
      "Training epoch 5 batch 272 with loss 2.158608913421631.\n",
      "Training epoch 5 batch 273 with loss 1.5355241298675537.\n",
      "Training epoch 5 batch 274 with loss 1.944859266281128.\n",
      "Training epoch 5 batch 275 with loss 1.79198157787323.\n",
      "Training epoch 5 batch 276 with loss 1.9544010162353516.\n",
      "Training epoch 5 batch 277 with loss 1.4559333324432373.\n",
      "Training epoch 5 batch 278 with loss 1.9946963787078857.\n",
      "Training epoch 5 batch 279 with loss 1.6051262617111206.\n",
      "Training epoch 5 batch 280 with loss 1.7842411994934082.\n",
      "Training epoch 5 batch 281 with loss 1.767118215560913.\n",
      "Training epoch 5 batch 282 with loss 1.7364140748977661.\n",
      "Training epoch 5 batch 283 with loss 1.5909007787704468.\n",
      "Training epoch 5 batch 284 with loss 1.9194027185440063.\n",
      "Training epoch 5 batch 285 with loss 1.124969482421875.\n",
      "Training epoch 5 batch 286 with loss 1.993409276008606.\n",
      "Training epoch 5 batch 287 with loss 1.5800721645355225.\n",
      "Training epoch 5 batch 288 with loss 1.4585219621658325.\n",
      "Training epoch 5 batch 289 with loss 1.8729958534240723.\n",
      "Training epoch 5 batch 290 with loss 1.8801275491714478.\n",
      "Training epoch 5 batch 291 with loss 2.0447957515716553.\n",
      "Training epoch 5 batch 292 with loss 1.6540857553482056.\n",
      "Training epoch 5 batch 293 with loss 2.0692174434661865.\n",
      "Training epoch 5 batch 294 with loss 1.721730351448059.\n",
      "Training epoch 5 batch 295 with loss 1.5271573066711426.\n",
      "Training epoch 5 batch 296 with loss 1.5819801092147827.\n",
      "Training epoch 5 batch 297 with loss 1.994744896888733.\n",
      "Training epoch 5 batch 298 with loss 1.7094446420669556.\n",
      "Training epoch 5 batch 299 with loss 2.0446934700012207.\n",
      "Training epoch 5 batch 300 with loss 1.7063636779785156.\n",
      "Training epoch 5 batch 301 with loss 1.4595592021942139.\n",
      "Training epoch 5 batch 302 with loss 1.733322024345398.\n",
      "Training epoch 5 batch 303 with loss 1.73736572265625.\n",
      "Training epoch 5 batch 304 with loss 1.5769718885421753.\n",
      "Training epoch 5 batch 305 with loss 1.9423645734786987.\n",
      "Training epoch 5 batch 306 with loss 1.9625587463378906.\n",
      "Training epoch 5 batch 307 with loss 2.0365920066833496.\n",
      "Training epoch 5 batch 308 with loss 1.6798006296157837.\n",
      "Training epoch 5 batch 309 with loss 2.2527918815612793.\n",
      "Training epoch 5 batch 310 with loss 1.8059449195861816.\n",
      "Training epoch 5 batch 311 with loss 1.8732647895812988.\n",
      "Training epoch 5 batch 312 with loss 1.9261337518692017.\n",
      "Training epoch 5 batch 313 with loss 2.092459201812744.\n",
      "Training epoch 5 batch 314 with loss 2.0582263469696045.\n",
      "Training epoch 5 batch 315 with loss 1.8215337991714478.\n",
      "Training epoch 5 batch 316 with loss 2.2602245807647705.\n",
      "Training epoch 5 batch 317 with loss 2.1082372665405273.\n",
      "Training epoch 5 batch 318 with loss 1.6992937326431274.\n",
      "Training epoch 5 batch 319 with loss 1.7322540283203125.\n",
      "Training epoch 5 batch 320 with loss 1.9616578817367554.\n",
      "Training epoch 5 batch 321 with loss 2.0173285007476807.\n",
      "Training epoch 5 batch 322 with loss 1.5520460605621338.\n",
      "Training epoch 5 batch 323 with loss 2.020078182220459.\n",
      "Training epoch 5 batch 324 with loss 2.0955824851989746.\n",
      "Training epoch 5 batch 325 with loss 1.8914674520492554.\n",
      "Training epoch 5 batch 326 with loss 1.685806155204773.\n",
      "Training epoch 5 batch 327 with loss 1.9078490734100342.\n",
      "Training epoch 5 batch 328 with loss 1.7881437540054321.\n",
      "Training epoch 5 batch 329 with loss 1.913962721824646.\n",
      "Training epoch 5 batch 330 with loss 1.7019731998443604.\n",
      "Training epoch 5 batch 331 with loss 1.981899380683899.\n",
      "Training epoch 5 batch 332 with loss 1.6558804512023926.\n",
      "Training epoch 5 batch 333 with loss 1.586003065109253.\n",
      "Training epoch 5 batch 334 with loss 2.1213090419769287.\n",
      "Training epoch 5 batch 335 with loss 1.6941522359848022.\n",
      "Training epoch 5 batch 336 with loss 2.1644155979156494.\n",
      "Training epoch 5 batch 337 with loss 1.8167424201965332.\n",
      "Training epoch 5 batch 338 with loss 1.7958838939666748.\n",
      "Training epoch 5 batch 339 with loss 1.8445862531661987.\n",
      "Training epoch 5 batch 340 with loss 1.607933759689331.\n",
      "Training epoch 5 batch 341 with loss 2.078477621078491.\n",
      "Training epoch 5 batch 342 with loss 1.8305530548095703.\n",
      "Training epoch 5 batch 343 with loss 1.8965930938720703.\n",
      "Training epoch 5 batch 344 with loss 2.002305746078491.\n",
      "Training epoch 5 batch 345 with loss 1.9026196002960205.\n",
      "Training epoch 5 batch 346 with loss 1.817016363143921.\n",
      "Training epoch 5 batch 347 with loss 1.953155279159546.\n",
      "Training epoch 5 batch 348 with loss 2.169581174850464.\n",
      "Training epoch 5 batch 349 with loss 1.9704833030700684.\n",
      "Training epoch 5 batch 350 with loss 1.9479764699935913.\n",
      "Training epoch 5 batch 351 with loss 1.905663013458252.\n",
      "Training epoch 5 batch 352 with loss 1.820647954940796.\n",
      "Training epoch 5 batch 353 with loss 1.9645823240280151.\n",
      "Training epoch 5 batch 354 with loss 1.5590893030166626.\n",
      "Training epoch 5 batch 355 with loss 1.7372256517410278.\n",
      "Training epoch 5 batch 356 with loss 1.8130759000778198.\n",
      "Training epoch 5 batch 357 with loss 1.8327317237854004.\n",
      "Training epoch 5 batch 358 with loss 1.9514029026031494.\n",
      "Training epoch 5 batch 359 with loss 1.7997233867645264.\n",
      "Training epoch 5 batch 360 with loss 1.829108476638794.\n",
      "Training epoch 5 batch 361 with loss 1.583443284034729.\n",
      "Training epoch 5 batch 362 with loss 1.8093063831329346.\n",
      "Training epoch 5 batch 363 with loss 1.671963095664978.\n",
      "Training epoch 5 batch 364 with loss 1.6958776712417603.\n",
      "Training epoch 5 batch 365 with loss 1.2753891944885254.\n",
      "Training epoch 5 batch 366 with loss 1.689841628074646.\n",
      "Training epoch 5 batch 367 with loss 1.6553409099578857.\n",
      "Training epoch 5 batch 368 with loss 1.7153286933898926.\n",
      "Training epoch 5 batch 369 with loss 1.5855952501296997.\n",
      "Training epoch 5 batch 370 with loss 1.8885835409164429.\n",
      "Training epoch 5 batch 371 with loss 2.1923558712005615.\n",
      "Training epoch 5 batch 372 with loss 1.6531070470809937.\n",
      "Training epoch 5 batch 373 with loss 1.4753590822219849.\n",
      "Training epoch 5 batch 374 with loss 1.7750232219696045.\n",
      "Training epoch 5 batch 375 with loss 1.8040122985839844.\n",
      "Training epoch 5 batch 376 with loss 2.1321518421173096.\n",
      "Training epoch 5 batch 377 with loss 1.8117204904556274.\n",
      "Training epoch 5 batch 378 with loss 1.9415614604949951.\n",
      "Training epoch 5 batch 379 with loss 2.0310111045837402.\n",
      "Training epoch 5 batch 380 with loss 1.8614442348480225.\n",
      "Training epoch 5 batch 381 with loss 1.6736055612564087.\n",
      "Training epoch 5 batch 382 with loss 1.9529247283935547.\n",
      "Training epoch 5 batch 383 with loss 2.0694425106048584.\n",
      "Training epoch 5 batch 384 with loss 1.6310851573944092.\n",
      "Training epoch 5 batch 385 with loss 1.772005558013916.\n",
      "Training epoch 5 batch 386 with loss 2.347689390182495.\n",
      "Training epoch 5 batch 387 with loss 1.638738751411438.\n",
      "Training epoch 5 batch 388 with loss 1.776938796043396.\n",
      "Training epoch 5 batch 389 with loss 2.2241978645324707.\n",
      "Training epoch 5 batch 390 with loss 2.500124454498291.\n",
      "Training epoch 5 batch 391 with loss 1.532397985458374.\n",
      "Training epoch 5 batch 392 with loss 2.0280799865722656.\n",
      "Training epoch 5 batch 393 with loss 2.1392440795898438.\n",
      "Training epoch 5 batch 394 with loss 1.8826369047164917.\n",
      "Training epoch 5 batch 395 with loss 1.979487657546997.\n",
      "Training epoch 5 batch 396 with loss 1.7716782093048096.\n",
      "Training epoch 5 batch 397 with loss 2.041017532348633.\n",
      "Training epoch 5 batch 398 with loss 2.071620225906372.\n",
      "Training epoch 5 batch 399 with loss 2.4395837783813477.\n",
      "Training epoch 5 batch 400 with loss 1.7846378087997437.\n",
      "Training epoch 5 batch 401 with loss 1.7289053201675415.\n",
      "Training epoch 5 batch 402 with loss 2.2667224407196045.\n",
      "Training epoch 5 batch 403 with loss 2.28985595703125.\n",
      "Training epoch 5 batch 404 with loss 1.7238490581512451.\n",
      "Training epoch 5 batch 405 with loss 1.9699456691741943.\n",
      "Training epoch 5 batch 406 with loss 1.8276515007019043.\n",
      "Training epoch 5 batch 407 with loss 1.5975042581558228.\n",
      "Training epoch 5 batch 408 with loss 2.038341522216797.\n",
      "Training epoch 5 batch 409 with loss 1.9552096128463745.\n",
      "Training epoch 5 batch 410 with loss 1.6020556688308716.\n",
      "Training epoch 5 batch 411 with loss 1.718397855758667.\n",
      "Training epoch 5 batch 412 with loss 1.4405255317687988.\n",
      "Training epoch 5 batch 413 with loss 1.4271912574768066.\n",
      "Training epoch 5 batch 414 with loss 2.025069236755371.\n",
      "Training epoch 5 batch 415 with loss 2.053623914718628.\n",
      "Training epoch 5 batch 416 with loss 1.816072702407837.\n",
      "Training epoch 5 batch 417 with loss 1.572542428970337.\n",
      "Training epoch 5 batch 418 with loss 1.7705528736114502.\n",
      "Training epoch 5 batch 419 with loss 1.8498719930648804.\n",
      "Training epoch 5 batch 420 with loss 2.0963265895843506.\n",
      "Training epoch 5 batch 421 with loss 1.7822006940841675.\n",
      "Training epoch 5 batch 422 with loss 1.9559088945388794.\n",
      "Training epoch 5 batch 423 with loss 1.966109037399292.\n",
      "Training epoch 5 batch 424 with loss 2.2815449237823486.\n",
      "Training epoch 5 batch 425 with loss 1.8802053928375244.\n",
      "Training epoch 5 batch 426 with loss 1.9807003736495972.\n",
      "Training epoch 5 batch 427 with loss 1.7715506553649902.\n",
      "Training epoch 5 batch 428 with loss 1.928056001663208.\n",
      "Training epoch 5 batch 429 with loss 1.8794941902160645.\n",
      "Training epoch 5 batch 430 with loss 1.907868504524231.\n",
      "Training epoch 5 batch 431 with loss 1.5057014226913452.\n",
      "Training epoch 5 batch 432 with loss 1.9754544496536255.\n",
      "Training epoch 5 batch 433 with loss 1.8814269304275513.\n",
      "Training epoch 5 batch 434 with loss 2.146134614944458.\n",
      "Training epoch 5 batch 435 with loss 1.8698683977127075.\n",
      "Training epoch 5 batch 436 with loss 2.0023159980773926.\n",
      "Training epoch 5 batch 437 with loss 2.1915581226348877.\n",
      "Training epoch 5 batch 438 with loss 1.6471083164215088.\n",
      "Training epoch 5 batch 439 with loss 1.8652777671813965.\n",
      "Training epoch 5 batch 440 with loss 1.632601261138916.\n",
      "Training epoch 5 batch 441 with loss 1.888325572013855.\n",
      "Training epoch 5 batch 442 with loss 1.6639115810394287.\n",
      "Training epoch 5 batch 443 with loss 2.0077459812164307.\n",
      "Training epoch 5 batch 444 with loss 2.425137996673584.\n",
      "Training epoch 5 batch 445 with loss 2.140324831008911.\n",
      "Training epoch 5 batch 446 with loss 1.8132362365722656.\n",
      "Training epoch 5 batch 447 with loss 2.131438732147217.\n",
      "Training epoch 5 batch 448 with loss 1.889168620109558.\n",
      "Training epoch 5 batch 449 with loss 1.973881483078003.\n",
      "Training epoch 5 batch 450 with loss 2.2023584842681885.\n",
      "Training epoch 5 batch 451 with loss 1.8854501247406006.\n",
      "Training epoch 5 batch 452 with loss 1.7744828462600708.\n",
      "Training epoch 5 batch 453 with loss 2.218761920928955.\n",
      "Training epoch 5 batch 454 with loss 1.8408198356628418.\n",
      "Training epoch 5 batch 455 with loss 1.7551292181015015.\n",
      "Training epoch 5 batch 456 with loss 2.0720722675323486.\n",
      "Training epoch 5 batch 457 with loss 1.8610635995864868.\n",
      "Training epoch 5 batch 458 with loss 1.6493438482284546.\n",
      "Training epoch 5 batch 459 with loss 1.59574556350708.\n",
      "Training epoch 5 batch 460 with loss 1.8999255895614624.\n",
      "Training epoch 5 batch 461 with loss 2.4025917053222656.\n",
      "Training epoch 5 batch 462 with loss 1.8379110097885132.\n",
      "Training epoch 5 batch 463 with loss 2.2488479614257812.\n",
      "Training epoch 5 batch 464 with loss 2.2105612754821777.\n",
      "Training epoch 5 batch 465 with loss 1.952638030052185.\n",
      "Training epoch 5 batch 466 with loss 2.0176117420196533.\n",
      "Training epoch 5 batch 467 with loss 1.8007551431655884.\n",
      "Training epoch 5 batch 468 with loss 1.902718186378479.\n",
      "Training epoch 5 batch 469 with loss 1.966407299041748.\n",
      "Training epoch 5 batch 470 with loss 2.0544512271881104.\n",
      "Training epoch 5 batch 471 with loss 1.593192219734192.\n",
      "Training epoch 5 batch 472 with loss 2.019132614135742.\n",
      "Training epoch 5 batch 473 with loss 1.5985945463180542.\n",
      "Training epoch 5 batch 474 with loss 1.8036935329437256.\n",
      "Training epoch 5 batch 475 with loss 2.2084295749664307.\n",
      "Training epoch 5 batch 476 with loss 2.053642511367798.\n",
      "Training epoch 5 batch 477 with loss 1.7905153036117554.\n",
      "Training epoch 5 batch 478 with loss 1.8425488471984863.\n",
      "Training epoch 5 batch 479 with loss 2.256640911102295.\n",
      "Training epoch 5 batch 480 with loss 1.5320019721984863.\n",
      "Training epoch 5 batch 481 with loss 2.110461473464966.\n",
      "Training epoch 5 batch 482 with loss 2.288067579269409.\n",
      "Training epoch 5 batch 483 with loss 1.7597286701202393.\n",
      "Training epoch 5 batch 484 with loss 1.838005781173706.\n",
      "Training epoch 5 batch 485 with loss 1.8927786350250244.\n",
      "Training epoch 5 batch 486 with loss 1.8814635276794434.\n",
      "Training epoch 5 batch 487 with loss 2.039811372756958.\n",
      "Training epoch 5 batch 488 with loss 2.156904935836792.\n",
      "Training epoch 5 batch 489 with loss 2.328465700149536.\n",
      "Training epoch 5 batch 490 with loss 1.9767595529556274.\n",
      "Training epoch 5 batch 491 with loss 1.898314356803894.\n",
      "Training epoch 5 batch 492 with loss 1.7939929962158203.\n",
      "Training epoch 5 batch 493 with loss 1.9119051694869995.\n",
      "Training epoch 5 batch 494 with loss 1.965277910232544.\n",
      "Training epoch 5 batch 495 with loss 2.0147299766540527.\n",
      "Training epoch 5 batch 496 with loss 1.7122939825057983.\n",
      "Training epoch 5 batch 497 with loss 1.9090723991394043.\n",
      "Training epoch 5 batch 498 with loss 2.3476545810699463.\n",
      "Training epoch 5 batch 499 with loss 2.0943551063537598.\n",
      "Training epoch 5 batch 500 with loss 2.1706581115722656.\n",
      "Training epoch 5 batch 501 with loss 1.9268642663955688.\n",
      "Training epoch 5 batch 502 with loss 1.6426249742507935.\n",
      "Training epoch 5 batch 503 with loss 1.9307429790496826.\n",
      "Training epoch 5 batch 504 with loss 2.064455270767212.\n",
      "Training epoch 5 batch 505 with loss 1.677368402481079.\n",
      "Training epoch 5 batch 506 with loss 1.8708300590515137.\n",
      "Training epoch 5 batch 507 with loss 1.9345651865005493.\n",
      "Training epoch 5 batch 508 with loss 2.379438877105713.\n",
      "Training epoch 5 batch 509 with loss 2.2844560146331787.\n",
      "Training epoch 5 batch 510 with loss 1.9291746616363525.\n",
      "Training epoch 5 batch 511 with loss 1.8502261638641357.\n",
      "Training epoch 5 batch 512 with loss 1.5768753290176392.\n",
      "Training epoch 5 batch 513 with loss 2.3324716091156006.\n",
      "Training epoch 5 batch 514 with loss 1.8997658491134644.\n",
      "Training epoch 5 batch 515 with loss 1.7317770719528198.\n",
      "Training epoch 5 batch 516 with loss 1.857804536819458.\n",
      "Training epoch 5 batch 517 with loss 1.719551682472229.\n",
      "Training epoch 5 batch 518 with loss 2.0314409732818604.\n",
      "Training epoch 5 batch 519 with loss 2.012535572052002.\n",
      "Training epoch 5 batch 520 with loss 1.8645657300949097.\n",
      "Training epoch 5 batch 521 with loss 1.8200894594192505.\n",
      "Training epoch 5 batch 522 with loss 2.0056116580963135.\n",
      "Training epoch 5 batch 523 with loss 1.6857458353042603.\n",
      "Training epoch 5 batch 524 with loss 2.38706636428833.\n",
      "Training epoch 5 batch 525 with loss 2.1412289142608643.\n",
      "Training epoch 5 batch 526 with loss 2.160970449447632.\n",
      "Training epoch 5 batch 527 with loss 1.7327383756637573.\n",
      "Training epoch 5 batch 528 with loss 1.7741307020187378.\n",
      "Training epoch 5 batch 529 with loss 2.0707812309265137.\n",
      "Training epoch 5 batch 530 with loss 2.069770097732544.\n",
      "Training epoch 5 batch 531 with loss 2.0093109607696533.\n",
      "Training epoch 5 batch 532 with loss 1.8504502773284912.\n",
      "Training epoch 5 batch 533 with loss 2.169617176055908.\n",
      "Training epoch 5 batch 534 with loss 1.6992121934890747.\n",
      "Training epoch 5 batch 535 with loss 1.6802639961242676.\n",
      "Training epoch 5 batch 536 with loss 1.8520734310150146.\n",
      "Training epoch 5 batch 537 with loss 1.7926597595214844.\n",
      "Training epoch 5 batch 538 with loss 1.9799253940582275.\n",
      "Training epoch 5 batch 539 with loss 1.5637133121490479.\n",
      "Training epoch 5 batch 540 with loss 1.8795896768569946.\n",
      "Training epoch 5 batch 541 with loss 1.975602388381958.\n",
      "Training epoch 5 batch 542 with loss 2.5727181434631348.\n",
      "Training epoch 5 batch 543 with loss 1.946940302848816.\n",
      "Training epoch 5 batch 544 with loss 2.2439234256744385.\n",
      "Training epoch 5 batch 545 with loss 1.897877812385559.\n",
      "Training epoch 5 batch 546 with loss 2.173741340637207.\n",
      "Training epoch 5 batch 547 with loss 1.910698413848877.\n",
      "Training epoch 5 batch 548 with loss 1.8798854351043701.\n",
      "Training epoch 5 batch 549 with loss 2.1159160137176514.\n",
      "Training epoch 5 batch 550 with loss 1.9039973020553589.\n",
      "Training epoch 5 batch 551 with loss 2.2361721992492676.\n",
      "Training epoch 5 batch 552 with loss 1.9153516292572021.\n",
      "Training epoch 5 batch 553 with loss 1.6552790403366089.\n",
      "Training epoch 5 batch 554 with loss 1.4859699010849.\n",
      "Training epoch 5 batch 555 with loss 2.2766377925872803.\n",
      "Training epoch 5 batch 556 with loss 1.7951911687850952.\n",
      "Training epoch 5 batch 557 with loss 2.104924440383911.\n",
      "Training epoch 5 batch 558 with loss 2.1620612144470215.\n",
      "Training epoch 5 batch 559 with loss 2.086616039276123.\n",
      "Training epoch 5 batch 560 with loss 1.7946122884750366.\n",
      "Training epoch 5 batch 561 with loss 2.1473934650421143.\n",
      "Training epoch 5 batch 562 with loss 2.0034568309783936.\n",
      "Training epoch 5 batch 563 with loss 1.8346161842346191.\n",
      "Training epoch 5 batch 564 with loss 2.100625514984131.\n",
      "Training epoch 5 batch 565 with loss 2.2085916996002197.\n",
      "Training epoch 5 batch 566 with loss 2.073565721511841.\n",
      "Training epoch 5 batch 567 with loss 1.634878158569336.\n",
      "Training epoch 5 batch 568 with loss 1.8998394012451172.\n",
      "Training epoch 5 batch 569 with loss 1.9722771644592285.\n",
      "Training epoch 5 batch 570 with loss 1.7520209550857544.\n",
      "Training epoch 5 batch 571 with loss 1.8290259838104248.\n",
      "Training epoch 5 batch 572 with loss 1.9579554796218872.\n",
      "Training epoch 5 batch 573 with loss 2.0806758403778076.\n",
      "Training epoch 5 batch 574 with loss 2.216412305831909.\n",
      "Training epoch 5 batch 575 with loss 2.3114612102508545.\n",
      "Training epoch 5 batch 576 with loss 1.9700164794921875.\n",
      "Training epoch 5 batch 577 with loss 2.2320563793182373.\n",
      "Training epoch 5 batch 578 with loss 2.460561990737915.\n",
      "Training epoch 5 batch 579 with loss 2.039285182952881.\n",
      "Training epoch 5 batch 580 with loss 2.1854350566864014.\n",
      "Training epoch 5 batch 581 with loss 2.024198532104492.\n",
      "Training epoch 5 batch 582 with loss 1.8470709323883057.\n",
      "Training epoch 5 batch 583 with loss 1.998246669769287.\n",
      "Training epoch 5 batch 584 with loss 2.1361820697784424.\n",
      "Training epoch 5 batch 585 with loss 2.039172887802124.\n",
      "Training epoch 5 batch 586 with loss 2.1303887367248535.\n",
      "Training epoch 5 batch 587 with loss 2.5401458740234375.\n",
      "Training epoch 5 batch 588 with loss 1.8133260011672974.\n",
      "Training epoch 5 batch 589 with loss 2.0662596225738525.\n",
      "Training epoch 5 batch 590 with loss 2.156672239303589.\n",
      "Training epoch 5 batch 591 with loss 1.9690449237823486.\n",
      "Training epoch 5 batch 592 with loss 1.6480234861373901.\n",
      "Training epoch 5 batch 593 with loss 2.040893316268921.\n",
      "Training epoch 5 batch 594 with loss 1.6668362617492676.\n",
      "Training epoch 5 batch 595 with loss 1.960801601409912.\n",
      "Training epoch 5 batch 596 with loss 2.0169084072113037.\n",
      "Training epoch 5 batch 597 with loss 1.9508625268936157.\n",
      "Training epoch 5 batch 598 with loss 2.5700149536132812.\n",
      "Training epoch 5 batch 599 with loss 2.056361198425293.\n",
      "Training epoch 5 batch 600 with loss 2.3924551010131836.\n",
      "Training epoch 5 batch 601 with loss 2.0634114742279053.\n",
      "Training epoch 5 batch 602 with loss 1.9951624870300293.\n",
      "Training epoch 5 batch 603 with loss 2.5426642894744873.\n",
      "Training epoch 5 batch 604 with loss 1.610797643661499.\n",
      "Training epoch 5 batch 605 with loss 1.721072793006897.\n",
      "Training epoch 5 batch 606 with loss 2.1800296306610107.\n",
      "Training epoch 5 batch 607 with loss 2.758591413497925.\n",
      "Training epoch 5 batch 608 with loss 2.27710223197937.\n",
      "Training epoch 5 batch 609 with loss 2.145987033843994.\n",
      "Training epoch 5 batch 610 with loss 1.717374563217163.\n",
      "Training epoch 5 batch 611 with loss 1.9714573621749878.\n",
      "Training epoch 5 batch 612 with loss 1.639357089996338.\n",
      "Training epoch 5 batch 613 with loss 2.164724826812744.\n",
      "Training epoch 5 batch 614 with loss 1.9531376361846924.\n",
      "Training epoch 5 batch 615 with loss 2.2799949645996094.\n",
      "Training epoch 5 batch 616 with loss 2.596362352371216.\n",
      "Training epoch 5 batch 617 with loss 1.9807288646697998.\n",
      "Training epoch 5 batch 618 with loss 1.6386864185333252.\n",
      "Training epoch 5 batch 619 with loss 2.152257204055786.\n",
      "Training epoch 5 batch 620 with loss 2.0484323501586914.\n",
      "Training epoch 5 batch 621 with loss 2.599238872528076.\n",
      "Training epoch 5 batch 622 with loss 2.069718599319458.\n",
      "Training epoch 5 batch 623 with loss 2.1940393447875977.\n",
      "Training epoch 5 batch 624 with loss 2.1148791313171387.\n",
      "Training epoch 5 batch 625 with loss 1.935921549797058.\n",
      "Training epoch 5 batch 626 with loss 2.041229248046875.\n",
      "Training epoch 5 batch 627 with loss 1.915517807006836.\n",
      "Training epoch 5 batch 628 with loss 2.2530462741851807.\n",
      "Training epoch 5 batch 629 with loss 1.6629821062088013.\n",
      "Training epoch 5 batch 630 with loss 2.536858320236206.\n",
      "Training epoch 5 batch 631 with loss 1.8260893821716309.\n",
      "Training epoch 5 batch 632 with loss 2.1286556720733643.\n",
      "Training epoch 5 batch 633 with loss 1.926364541053772.\n",
      "Training epoch 5 batch 634 with loss 1.8806037902832031.\n",
      "Training epoch 5 batch 635 with loss 2.122380495071411.\n",
      "Training epoch 5 batch 636 with loss 2.0123465061187744.\n",
      "Training epoch 5 batch 637 with loss 1.7844501733779907.\n",
      "Training epoch 5 batch 638 with loss 2.3082330226898193.\n",
      "Training epoch 5 batch 639 with loss 2.136155605316162.\n",
      "Training epoch 5 batch 640 with loss 1.6842602491378784.\n",
      "Training epoch 5 batch 641 with loss 1.745734453201294.\n",
      "Training epoch 5 batch 642 with loss 2.003863573074341.\n",
      "Training epoch 5 batch 643 with loss 1.9393422603607178.\n",
      "Training epoch 5 batch 644 with loss 1.8350026607513428.\n",
      "Training epoch 5 batch 645 with loss 2.0540802478790283.\n",
      "Training epoch 5 batch 646 with loss 2.003014087677002.\n",
      "Training epoch 5 batch 647 with loss 2.0270113945007324.\n",
      "Training epoch 5 batch 648 with loss 1.648102045059204.\n",
      "Training epoch 5 batch 649 with loss 1.8832752704620361.\n",
      "Training epoch 5 batch 650 with loss 2.424440860748291.\n",
      "Training epoch 5 batch 651 with loss 1.7658154964447021.\n",
      "Training epoch 5 batch 652 with loss 2.2641427516937256.\n",
      "Training epoch 5 batch 653 with loss 1.7424168586730957.\n",
      "Training epoch 5 batch 654 with loss 1.8610451221466064.\n",
      "Training epoch 5 batch 655 with loss 2.245717763900757.\n",
      "Training epoch 5 batch 656 with loss 1.9895046949386597.\n",
      "Training epoch 5 batch 657 with loss 1.861619234085083.\n",
      "Training epoch 5 batch 658 with loss 1.9298996925354004.\n",
      "Training epoch 5 batch 659 with loss 1.7679370641708374.\n",
      "Training epoch 5 batch 660 with loss 1.5515172481536865.\n",
      "Training epoch 5 batch 661 with loss 2.0100197792053223.\n",
      "Training epoch 5 batch 662 with loss 2.426225423812866.\n",
      "Training epoch 5 batch 663 with loss 1.7980653047561646.\n",
      "Training epoch 5 batch 664 with loss 2.125164270401001.\n",
      "Training epoch 5 batch 665 with loss 2.036987066268921.\n",
      "Training epoch 5 batch 666 with loss 1.7662429809570312.\n",
      "Training epoch 5 batch 667 with loss 1.9326642751693726.\n",
      "Training epoch 5 batch 668 with loss 2.084681272506714.\n",
      "Training epoch 5 batch 669 with loss 1.9203447103500366.\n",
      "Training epoch 5 batch 670 with loss 1.9927011728286743.\n",
      "Training epoch 5 batch 671 with loss 2.4180262088775635.\n",
      "Training epoch 5 batch 672 with loss 2.3280954360961914.\n",
      "Training epoch 5 batch 673 with loss 1.7162145376205444.\n",
      "Training epoch 5 batch 674 with loss 2.195216178894043.\n",
      "Training epoch 5 batch 675 with loss 1.734948754310608.\n",
      "Training epoch 5 batch 676 with loss 2.230109691619873.\n",
      "Training epoch 5 batch 677 with loss 2.2779018878936768.\n",
      "Training epoch 5 batch 678 with loss 2.3541324138641357.\n",
      "Training epoch 5 batch 679 with loss 1.9071568250656128.\n",
      "Training epoch 5 batch 680 with loss 2.1580796241760254.\n",
      "Training epoch 5 batch 681 with loss 2.1906180381774902.\n",
      "Training epoch 5 batch 682 with loss 2.2105653285980225.\n",
      "Training epoch 5 batch 683 with loss 1.9771956205368042.\n",
      "Training epoch 5 batch 684 with loss 2.2304506301879883.\n",
      "Training epoch 5 batch 685 with loss 2.0101118087768555.\n",
      "Training epoch 5 batch 686 with loss 1.754809021949768.\n",
      "Training epoch 5 batch 687 with loss 2.085407257080078.\n",
      "Training epoch 5 batch 688 with loss 1.9158068895339966.\n",
      "Training epoch 5 batch 689 with loss 2.2158241271972656.\n",
      "Training epoch 5 batch 690 with loss 1.9077038764953613.\n",
      "Training epoch 5 batch 691 with loss 2.1730356216430664.\n",
      "Training epoch 5 batch 692 with loss 2.186321258544922.\n",
      "Training epoch 5 batch 693 with loss 2.044670581817627.\n",
      "Training epoch 5 batch 694 with loss 1.8936086893081665.\n",
      "Training epoch 5 batch 695 with loss 2.2884299755096436.\n",
      "Training epoch 5 batch 696 with loss 1.7809609174728394.\n",
      "Training epoch 5 batch 697 with loss 1.695844054222107.\n",
      "Training epoch 5 batch 698 with loss 1.9012941122055054.\n",
      "Training epoch 5 batch 699 with loss 1.905924677848816.\n",
      "Training epoch 5 batch 700 with loss 2.36483097076416.\n",
      "Training epoch 5 batch 701 with loss 1.7329611778259277.\n",
      "Training epoch 5 batch 702 with loss 2.140669822692871.\n",
      "Training epoch 5 batch 703 with loss 1.9255870580673218.\n",
      "Training epoch 5 batch 704 with loss 1.7541152238845825.\n",
      "Training epoch 5 batch 705 with loss 2.2964043617248535.\n",
      "Training epoch 5 batch 706 with loss 1.9564906358718872.\n",
      "Training epoch 5 batch 707 with loss 1.7476147413253784.\n",
      "Training epoch 5 batch 708 with loss 1.9272030591964722.\n",
      "Training epoch 5 batch 709 with loss 2.0556983947753906.\n",
      "Training epoch 5 batch 710 with loss 1.8917701244354248.\n",
      "Training epoch 5 batch 711 with loss 1.891455888748169.\n",
      "Training epoch 5 batch 712 with loss 1.9123517274856567.\n",
      "Training epoch 5 batch 713 with loss 2.4704203605651855.\n",
      "Training epoch 5 batch 714 with loss 2.0168864727020264.\n",
      "Training epoch 5 batch 715 with loss 2.4941465854644775.\n",
      "Training epoch 5 batch 716 with loss 2.193255662918091.\n",
      "Training epoch 5 batch 717 with loss 2.1764042377471924.\n",
      "Training epoch 5 batch 718 with loss 2.130882740020752.\n",
      "Training epoch 5 batch 719 with loss 2.3910369873046875.\n",
      "Training epoch 5 batch 720 with loss 2.191883087158203.\n",
      "Training epoch 5 batch 721 with loss 2.0376760959625244.\n",
      "Training epoch 5 batch 722 with loss 2.5152926445007324.\n",
      "Training epoch 5 batch 723 with loss 1.8152897357940674.\n",
      "Training epoch 5 batch 724 with loss 2.0508389472961426.\n",
      "Training epoch 5 batch 725 with loss 1.8890455961227417.\n",
      "Training epoch 5 batch 726 with loss 1.9186030626296997.\n",
      "Training epoch 5 batch 727 with loss 1.777878999710083.\n",
      "Training epoch 5 batch 728 with loss 2.1033542156219482.\n",
      "Training epoch 5 batch 729 with loss 2.0531320571899414.\n",
      "Training epoch 5 batch 730 with loss 2.230898857116699.\n",
      "Training epoch 5 batch 731 with loss 1.9439276456832886.\n",
      "Training epoch 5 batch 732 with loss 2.3004064559936523.\n",
      "Training epoch 5 batch 733 with loss 1.8775980472564697.\n",
      "Training epoch 5 batch 734 with loss 2.5828053951263428.\n",
      "Training epoch 5 batch 735 with loss 2.079530954360962.\n",
      "Training epoch 5 batch 736 with loss 1.815688967704773.\n",
      "Training epoch 5 batch 737 with loss 1.7863142490386963.\n",
      "Training epoch 5 batch 738 with loss 2.307119846343994.\n",
      "Training epoch 5 batch 739 with loss 2.335897922515869.\n",
      "Training epoch 5 batch 740 with loss 1.9076836109161377.\n",
      "Training epoch 5 batch 741 with loss 1.9834754467010498.\n",
      "Training epoch 5 batch 742 with loss 1.9717917442321777.\n",
      "Training epoch 5 batch 743 with loss 2.0233089923858643.\n",
      "Training epoch 5 batch 744 with loss 2.4269888401031494.\n",
      "Training epoch 5 batch 745 with loss 1.9409005641937256.\n",
      "Training epoch 5 batch 746 with loss 1.8735510110855103.\n",
      "Training epoch 5 batch 747 with loss 2.345494031906128.\n",
      "Training epoch 5 batch 748 with loss 2.092992067337036.\n",
      "Training epoch 5 batch 749 with loss 2.312941074371338.\n",
      "Training epoch 5 batch 750 with loss 2.005082607269287.\n",
      "Training epoch 5 batch 751 with loss 2.2030274868011475.\n",
      "Training epoch 5 batch 752 with loss 2.489286184310913.\n",
      "Training epoch 5 batch 753 with loss 2.3623363971710205.\n",
      "Training epoch 5 batch 754 with loss 1.9908777475357056.\n",
      "Training epoch 5 batch 755 with loss 2.5297510623931885.\n",
      "Training epoch 5 batch 756 with loss 2.415349006652832.\n",
      "Training epoch 5 batch 757 with loss 1.9453601837158203.\n",
      "Training epoch 5 batch 758 with loss 2.1784539222717285.\n",
      "Training epoch 5 batch 759 with loss 2.330066680908203.\n",
      "Training epoch 5 batch 760 with loss 2.2567555904388428.\n",
      "Training epoch 5 batch 761 with loss 2.188999652862549.\n",
      "Training epoch 5 batch 762 with loss 2.4215028285980225.\n",
      "Training epoch 5 batch 763 with loss 2.122596263885498.\n",
      "Training epoch 5 batch 764 with loss 1.9730452299118042.\n",
      "Training epoch 5 batch 765 with loss 2.252676486968994.\n",
      "Training epoch 5 batch 766 with loss 1.5777068138122559.\n",
      "Training epoch 5 batch 767 with loss 2.3793084621429443.\n",
      "Training epoch 5 batch 768 with loss 2.4654958248138428.\n",
      "Training epoch 5 batch 769 with loss 1.59823477268219.\n",
      "Training epoch 5 batch 770 with loss 1.8746740818023682.\n",
      "Training epoch 5 batch 771 with loss 2.1625025272369385.\n",
      "Training epoch 5 batch 772 with loss 2.1635963916778564.\n",
      "Training epoch 5 batch 773 with loss 2.075528621673584.\n",
      "Training epoch 5 batch 774 with loss 2.064985990524292.\n",
      "Training epoch 5 batch 775 with loss 2.1098132133483887.\n",
      "Training epoch 5 batch 776 with loss 2.1895782947540283.\n",
      "Training epoch 5 batch 777 with loss 1.9242571592330933.\n",
      "Training epoch 5 batch 778 with loss 1.827169418334961.\n",
      "Training epoch 5 batch 779 with loss 1.9187504053115845.\n",
      "Training epoch 5 batch 780 with loss 2.28788161277771.\n",
      "Training epoch 5 batch 781 with loss 2.223128080368042.\n",
      "Training epoch 5 batch 782 with loss 2.2128775119781494.\n",
      "Training epoch 5 batch 783 with loss 2.179363965988159.\n",
      "Training epoch 5 batch 784 with loss 1.784040093421936.\n",
      "Training epoch 5 batch 785 with loss 2.310305118560791.\n",
      "Training epoch 5 batch 786 with loss 1.9464895725250244.\n",
      "Training epoch 5 batch 787 with loss 2.1506998538970947.\n",
      "Training epoch 5 batch 788 with loss 2.112565755844116.\n",
      "Training epoch 5 batch 789 with loss 2.086975336074829.\n",
      "Training epoch 5 batch 790 with loss 2.475435733795166.\n",
      "Training epoch 5 batch 791 with loss 2.4115970134735107.\n",
      "Training epoch 5 batch 792 with loss 2.3008010387420654.\n",
      "Training epoch 5 batch 793 with loss 2.1623709201812744.\n",
      "Training epoch 5 batch 794 with loss 2.56038761138916.\n",
      "Training epoch 5 batch 795 with loss 2.1107099056243896.\n",
      "Training epoch 5 batch 796 with loss 1.7309130430221558.\n",
      "Training epoch 5 batch 797 with loss 2.014338254928589.\n",
      "Training epoch 5 batch 798 with loss 2.037177801132202.\n",
      "Training epoch 5 batch 799 with loss 2.10184383392334.\n",
      "Training epoch 5 batch 800 with loss 2.0931317806243896.\n",
      "Training epoch 5 batch 801 with loss 1.6921916007995605.\n",
      "Training epoch 5 batch 802 with loss 1.9860458374023438.\n",
      "Training epoch 5 batch 803 with loss 2.3872125148773193.\n",
      "Training epoch 5 batch 804 with loss 2.33388352394104.\n",
      "Training epoch 5 batch 805 with loss 2.510291814804077.\n",
      "Training epoch 5 batch 806 with loss 1.9574625492095947.\n",
      "Training epoch 5 batch 807 with loss 2.364650011062622.\n",
      "Training epoch 5 batch 808 with loss 1.9206339120864868.\n",
      "Training epoch 5 batch 809 with loss 2.1288602352142334.\n",
      "Training epoch 5 batch 810 with loss 2.5221238136291504.\n",
      "Training epoch 5 batch 811 with loss 2.24661922454834.\n",
      "Training epoch 5 batch 812 with loss 1.8275108337402344.\n",
      "Training epoch 5 batch 813 with loss 1.9750477075576782.\n",
      "Training epoch 5 batch 814 with loss 2.3579161167144775.\n",
      "Training epoch 5 batch 815 with loss 1.615576148033142.\n",
      "Training epoch 5 batch 816 with loss 1.789354681968689.\n",
      "Training epoch 5 batch 817 with loss 2.2187869548797607.\n",
      "Training epoch 5 batch 818 with loss 2.7019577026367188.\n",
      "Training epoch 5 batch 819 with loss 2.2232604026794434.\n",
      "Training epoch 5 batch 820 with loss 1.9712352752685547.\n",
      "Training epoch 5 batch 821 with loss 2.123958110809326.\n",
      "Training epoch 5 batch 822 with loss 1.9144128561019897.\n",
      "Training epoch 5 batch 823 with loss 2.2114899158477783.\n",
      "Training epoch 5 batch 824 with loss 2.336087465286255.\n",
      "Training epoch 5 batch 825 with loss 2.0436830520629883.\n",
      "Training epoch 5 batch 826 with loss 2.4520986080169678.\n",
      "Training epoch 5 batch 827 with loss 2.164027214050293.\n",
      "Training epoch 5 batch 828 with loss 2.1510419845581055.\n",
      "Training epoch 5 batch 829 with loss 1.8660125732421875.\n",
      "Training epoch 5 batch 830 with loss 2.353182077407837.\n",
      "Training epoch 5 batch 831 with loss 2.3052165508270264.\n",
      "Training epoch 5 batch 832 with loss 1.7768563032150269.\n",
      "Training epoch 5 batch 833 with loss 2.2393860816955566.\n",
      "Training epoch 5 batch 834 with loss 1.9253345727920532.\n",
      "Training epoch 5 batch 835 with loss 2.0755186080932617.\n",
      "Training epoch 5 batch 836 with loss 1.4772632122039795.\n",
      "Training epoch 5 batch 837 with loss 1.9941898584365845.\n",
      "Training epoch 5 batch 838 with loss 2.371155023574829.\n",
      "Training epoch 5 batch 839 with loss 1.8431836366653442.\n",
      "Training epoch 5 batch 840 with loss 1.8717906475067139.\n",
      "Training epoch 5 batch 841 with loss 1.99879789352417.\n",
      "Training epoch 5 batch 842 with loss 2.1747829914093018.\n",
      "Training epoch 5 batch 843 with loss 2.1722922325134277.\n",
      "Training epoch 5 batch 844 with loss 1.7090117931365967.\n",
      "Training epoch 5 batch 845 with loss 1.9974710941314697.\n",
      "Training epoch 5 batch 846 with loss 2.0268096923828125.\n",
      "Training epoch 5 batch 847 with loss 2.323127031326294.\n",
      "Training epoch 5 batch 848 with loss 1.957809567451477.\n",
      "Training epoch 5 batch 849 with loss 2.1982853412628174.\n",
      "Training epoch 5 batch 850 with loss 1.7908626794815063.\n",
      "Training epoch 5 batch 851 with loss 1.8696520328521729.\n",
      "Training epoch 5 batch 852 with loss 2.404313325881958.\n",
      "Training epoch 5 batch 853 with loss 2.061685085296631.\n",
      "Training epoch 5 batch 854 with loss 2.0738861560821533.\n",
      "Training epoch 5 batch 855 with loss 2.3630757331848145.\n",
      "Training epoch 5 batch 856 with loss 2.2213926315307617.\n",
      "Training epoch 5 batch 857 with loss 1.980438232421875.\n",
      "Training epoch 5 batch 858 with loss 2.1769440174102783.\n",
      "Training epoch 5 batch 859 with loss 2.2675137519836426.\n",
      "Training epoch 5 batch 860 with loss 2.079078197479248.\n",
      "Training epoch 5 batch 861 with loss 2.039259910583496.\n",
      "Training epoch 5 batch 862 with loss 2.0853793621063232.\n",
      "Training epoch 5 batch 863 with loss 2.4030940532684326.\n",
      "Training epoch 5 batch 864 with loss 2.0250227451324463.\n",
      "Training epoch 5 batch 865 with loss 1.9838992357254028.\n",
      "Training epoch 5 batch 866 with loss 2.019918203353882.\n",
      "Training epoch 5 batch 867 with loss 2.023529529571533.\n",
      "Training epoch 5 batch 868 with loss 2.2131800651550293.\n",
      "Training epoch 5 batch 869 with loss 1.9675911664962769.\n",
      "Training epoch 5 batch 870 with loss 2.173358201980591.\n",
      "Training epoch 5 batch 871 with loss 2.109257459640503.\n",
      "Training epoch 5 batch 872 with loss 2.035447120666504.\n",
      "Training epoch 5 batch 873 with loss 2.345992088317871.\n",
      "Training epoch 5 batch 874 with loss 2.1124086380004883.\n",
      "Training epoch 5 batch 875 with loss 2.0890347957611084.\n",
      "Training epoch 5 batch 876 with loss 2.482647657394409.\n",
      "Training epoch 5 batch 877 with loss 2.108430862426758.\n",
      "Training epoch 5 batch 878 with loss 1.906553030014038.\n",
      "Training epoch 5 batch 879 with loss 1.6534250974655151.\n",
      "Training epoch 5 batch 880 with loss 2.3255884647369385.\n",
      "Training epoch 5 batch 881 with loss 2.581467866897583.\n",
      "Training epoch 5 batch 882 with loss 1.965010404586792.\n",
      "Training epoch 5 batch 883 with loss 2.2496871948242188.\n",
      "Training epoch 5 batch 884 with loss 2.0726428031921387.\n",
      "Training epoch 5 batch 885 with loss 2.144348382949829.\n",
      "Training epoch 5 batch 886 with loss 1.9674041271209717.\n",
      "Training epoch 5 batch 887 with loss 1.8564568758010864.\n",
      "Training epoch 5 batch 888 with loss 2.2965457439422607.\n",
      "Training epoch 5 batch 889 with loss 2.1318342685699463.\n",
      "Training epoch 5 batch 890 with loss 1.9734344482421875.\n",
      "Training epoch 5 batch 891 with loss 2.044680118560791.\n",
      "Training epoch 5 batch 892 with loss 1.9774543046951294.\n",
      "Training epoch 5 batch 893 with loss 1.6372352838516235.\n",
      "Training epoch 5 batch 894 with loss 1.9468696117401123.\n",
      "Training epoch 5 batch 895 with loss 1.8955563306808472.\n",
      "Training epoch 5 batch 896 with loss 2.0765013694763184.\n",
      "Training epoch 5 batch 897 with loss 2.435628652572632.\n",
      "Training epoch 5 batch 898 with loss 2.1875228881835938.\n",
      "Training epoch 5 batch 899 with loss 1.6977826356887817.\n",
      "Training epoch 5 batch 900 with loss 1.9544689655303955.\n",
      "Training epoch 5 batch 901 with loss 2.0998570919036865.\n",
      "Training epoch 5 batch 902 with loss 1.7331674098968506.\n",
      "Training epoch 5 batch 903 with loss 2.138838291168213.\n",
      "Training epoch 5 batch 904 with loss 2.2951138019561768.\n",
      "Training epoch 5 batch 905 with loss 2.5732619762420654.\n",
      "Training epoch 5 batch 906 with loss 1.7069168090820312.\n",
      "Training epoch 5 batch 907 with loss 1.790014386177063.\n",
      "Training epoch 5 batch 908 with loss 1.7618846893310547.\n",
      "Training epoch 5 batch 909 with loss 2.514594793319702.\n",
      "Training epoch 5 batch 910 with loss 2.504319190979004.\n",
      "Training epoch 5 batch 911 with loss 2.118116617202759.\n",
      "Training epoch 5 batch 912 with loss 2.0038368701934814.\n",
      "Training epoch 5 batch 913 with loss 2.191692590713501.\n",
      "Training epoch 5 batch 914 with loss 2.3673319816589355.\n",
      "Training epoch 5 batch 915 with loss 2.474065065383911.\n",
      "Training epoch 5 batch 916 with loss 1.8751083612442017.\n",
      "Training epoch 5 batch 917 with loss 2.1370577812194824.\n",
      "Training epoch 5 batch 918 with loss 2.296231508255005.\n",
      "Training epoch 5 batch 919 with loss 2.1848950386047363.\n",
      "Training epoch 5 batch 920 with loss 1.731185793876648.\n",
      "Training epoch 5 batch 921 with loss 2.0190420150756836.\n",
      "Training epoch 5 batch 922 with loss 2.295781135559082.\n",
      "Training epoch 5 batch 923 with loss 2.039729595184326.\n",
      "Training epoch 5 batch 924 with loss 1.7581062316894531.\n",
      "Training epoch 5 batch 925 with loss 1.9482206106185913.\n",
      "Training epoch 5 batch 926 with loss 1.7956079244613647.\n",
      "Training epoch 5 batch 927 with loss 2.0216991901397705.\n",
      "Training epoch 5 batch 928 with loss 1.9730098247528076.\n",
      "Training epoch 5 batch 929 with loss 2.093723773956299.\n",
      "Training epoch 5 batch 930 with loss 2.3991520404815674.\n",
      "Training epoch 5 batch 931 with loss 2.249112367630005.\n",
      "Training epoch 5 batch 932 with loss 1.7668757438659668.\n",
      "Training epoch 5 batch 933 with loss 2.1796951293945312.\n",
      "Training epoch 5 batch 934 with loss 2.2751879692077637.\n",
      "Training epoch 5 batch 935 with loss 1.9536184072494507.\n",
      "Training epoch 5 batch 936 with loss 1.893524169921875.\n",
      "Training epoch 5 batch 937 with loss 2.042454957962036.\n",
      "Training epoch 5 batch 938 with loss 1.7834566831588745.\n",
      "Training epoch 5 batch 939 with loss 2.05536150932312.\n",
      "Training epoch 5 batch 940 with loss 1.838997483253479.\n",
      "Training epoch 5 batch 941 with loss 2.0987634658813477.\n",
      "Training epoch 5 batch 942 with loss 2.1086108684539795.\n",
      "Training epoch 5 batch 943 with loss 2.1241559982299805.\n",
      "Training epoch 5 batch 944 with loss 2.183799982070923.\n",
      "Training epoch 5 batch 945 with loss 2.0756328105926514.\n",
      "Training epoch 5 batch 946 with loss 1.9479917287826538.\n",
      "Training epoch 5 batch 947 with loss 1.5289666652679443.\n",
      "Training epoch 5 batch 948 with loss 2.118609666824341.\n",
      "Training epoch 5 batch 949 with loss 2.217965602874756.\n",
      "Training epoch 5 batch 950 with loss 2.153912305831909.\n",
      "Training epoch 5 batch 951 with loss 1.6988232135772705.\n",
      "Training epoch 5 batch 952 with loss 2.263017177581787.\n",
      "Training epoch 5 batch 953 with loss 1.7171951532363892.\n",
      "Training epoch 5 batch 954 with loss 2.285315752029419.\n",
      "Training epoch 5 batch 955 with loss 1.9028397798538208.\n",
      "Training epoch 5 batch 956 with loss 2.257213830947876.\n",
      "Training epoch 5 batch 957 with loss 2.488391160964966.\n",
      "Training epoch 5 batch 958 with loss 1.7513102293014526.\n",
      "Training epoch 5 batch 959 with loss 2.101318836212158.\n",
      "Training epoch 5 batch 960 with loss 1.5441797971725464.\n",
      "Training epoch 5 batch 961 with loss 2.2860183715820312.\n",
      "Training epoch 5 batch 962 with loss 2.1144015789031982.\n",
      "Training epoch 5 batch 963 with loss 2.11916446685791.\n",
      "Training epoch 5 batch 964 with loss 2.2310988903045654.\n",
      "Training epoch 5 batch 965 with loss 2.0534589290618896.\n",
      "Training epoch 5 batch 966 with loss 2.094212532043457.\n",
      "Training epoch 5 batch 967 with loss 2.036997079849243.\n",
      "Training epoch 5 batch 968 with loss 1.7509015798568726.\n",
      "Training epoch 5 batch 969 with loss 2.043523073196411.\n",
      "Training epoch 5 batch 970 with loss 1.980236291885376.\n",
      "Training epoch 5 batch 971 with loss 1.6526867151260376.\n",
      "Training epoch 5 batch 972 with loss 2.0408568382263184.\n",
      "Training epoch 5 batch 973 with loss 1.8835463523864746.\n",
      "Training epoch 5 batch 974 with loss 1.9409042596817017.\n",
      "Training epoch 5 batch 975 with loss 2.2913551330566406.\n",
      "Training epoch 5 batch 976 with loss 2.3153605461120605.\n",
      "Training epoch 5 batch 977 with loss 1.9401094913482666.\n",
      "Training epoch 5 batch 978 with loss 2.1698455810546875.\n",
      "Training epoch 5 batch 979 with loss 1.636451244354248.\n",
      "Training epoch 5 batch 980 with loss 2.684629201889038.\n",
      "Training epoch 5 batch 981 with loss 2.533997058868408.\n",
      "Training epoch 5 batch 982 with loss 1.860695719718933.\n",
      "Training epoch 5 batch 983 with loss 2.5822842121124268.\n",
      "Training epoch 5 batch 984 with loss 2.045975685119629.\n",
      "Training epoch 5 batch 985 with loss 2.201416254043579.\n",
      "Training epoch 5 batch 986 with loss 2.301588296890259.\n",
      "Training epoch 5 batch 987 with loss 1.9421050548553467.\n",
      "Training epoch 5 batch 988 with loss 2.060331106185913.\n",
      "Training epoch 5 batch 989 with loss 2.461966037750244.\n",
      "Training epoch 5 batch 990 with loss 2.235924482345581.\n",
      "Training epoch 5 batch 991 with loss 2.0026116371154785.\n",
      "Training epoch 5 batch 992 with loss 1.9035742282867432.\n",
      "Training epoch 5 batch 993 with loss 2.402859687805176.\n",
      "Training epoch 5 batch 994 with loss 1.885966420173645.\n",
      "Training epoch 5 batch 995 with loss 2.3336308002471924.\n",
      "Training epoch 5 batch 996 with loss 2.208854913711548.\n",
      "Training epoch 5 batch 997 with loss 2.1823346614837646.\n",
      "Training epoch 5 batch 998 with loss 1.7515531778335571.\n",
      "Training epoch 5 batch 999 with loss 1.7844054698944092.\n",
      "Test batch 0 with loss 2.4532389640808105.\n",
      "Test batch 1 with loss 2.8627231121063232.\n",
      "Test batch 2 with loss 2.836094856262207.\n",
      "Test batch 3 with loss 2.6110517978668213.\n",
      "Test batch 4 with loss 2.6472208499908447.\n",
      "Test batch 5 with loss 2.794480323791504.\n",
      "Test batch 6 with loss 2.6096878051757812.\n",
      "Test batch 7 with loss 2.645052194595337.\n",
      "Test batch 8 with loss 2.9611265659332275.\n",
      "Test batch 9 with loss 2.8447983264923096.\n",
      "Test batch 10 with loss 2.5822269916534424.\n",
      "Test batch 11 with loss 2.3859987258911133.\n",
      "Test batch 12 with loss 2.8030636310577393.\n",
      "Test batch 13 with loss 2.612504720687866.\n",
      "Test batch 14 with loss 2.544593095779419.\n",
      "Test batch 15 with loss 2.996527910232544.\n",
      "Test batch 16 with loss 2.602015972137451.\n",
      "Test batch 17 with loss 2.4064595699310303.\n",
      "Test batch 18 with loss 2.719121217727661.\n",
      "Test batch 19 with loss 2.421257257461548.\n",
      "Test batch 20 with loss 2.48698353767395.\n",
      "Test batch 21 with loss 2.4274213314056396.\n",
      "Test batch 22 with loss 2.7901272773742676.\n",
      "Test batch 23 with loss 3.2285146713256836.\n",
      "Test batch 24 with loss 2.2402658462524414.\n",
      "Test batch 25 with loss 2.9862759113311768.\n",
      "Test batch 26 with loss 2.3701558113098145.\n",
      "Test batch 27 with loss 2.8924648761749268.\n",
      "Test batch 28 with loss 2.5759191513061523.\n",
      "Test batch 29 with loss 2.9743270874023438.\n",
      "Test batch 30 with loss 2.2085320949554443.\n",
      "Test batch 31 with loss 2.3382158279418945.\n",
      "Test batch 32 with loss 2.7008166313171387.\n",
      "Test batch 33 with loss 3.1510367393493652.\n",
      "Test batch 34 with loss 2.387760639190674.\n",
      "Test batch 35 with loss 2.5766563415527344.\n",
      "Test batch 36 with loss 2.53006649017334.\n",
      "Test batch 37 with loss 2.7638370990753174.\n",
      "Test batch 38 with loss 2.4037859439849854.\n",
      "Test batch 39 with loss 2.4654955863952637.\n",
      "Test batch 40 with loss 2.3366119861602783.\n",
      "Test batch 41 with loss 2.6549558639526367.\n",
      "Test batch 42 with loss 2.682340383529663.\n",
      "Test batch 43 with loss 2.870206117630005.\n",
      "Test batch 44 with loss 2.800297260284424.\n",
      "Test batch 45 with loss 2.9418985843658447.\n",
      "Test batch 46 with loss 2.7909162044525146.\n",
      "Test batch 47 with loss 2.5757386684417725.\n",
      "Test batch 48 with loss 3.357750654220581.\n",
      "Test batch 49 with loss 2.925997018814087.\n",
      "Test batch 50 with loss 2.3000566959381104.\n",
      "Test batch 51 with loss 2.550631284713745.\n",
      "Test batch 52 with loss 2.305542230606079.\n",
      "Test batch 53 with loss 2.5454344749450684.\n",
      "Test batch 54 with loss 2.1173300743103027.\n",
      "Test batch 55 with loss 2.868748664855957.\n",
      "Test batch 56 with loss 2.7497260570526123.\n",
      "Test batch 57 with loss 2.2491061687469482.\n",
      "Test batch 58 with loss 1.991084337234497.\n",
      "Test batch 59 with loss 2.8519692420959473.\n",
      "Test batch 60 with loss 2.9247922897338867.\n",
      "Test batch 61 with loss 3.016934394836426.\n",
      "Test batch 62 with loss 2.3745696544647217.\n",
      "Test batch 63 with loss 2.3679873943328857.\n",
      "Test batch 64 with loss 2.556756019592285.\n",
      "Test batch 65 with loss 2.657456874847412.\n",
      "Test batch 66 with loss 2.402970790863037.\n",
      "Test batch 67 with loss 2.203697919845581.\n",
      "Test batch 68 with loss 2.830488681793213.\n",
      "Test batch 69 with loss 2.5042152404785156.\n",
      "Test batch 70 with loss 2.8319175243377686.\n",
      "Test batch 71 with loss 2.4647669792175293.\n",
      "Test batch 72 with loss 2.627934694290161.\n",
      "Test batch 73 with loss 2.5677881240844727.\n",
      "Test batch 74 with loss 2.9735701084136963.\n",
      "Test batch 75 with loss 2.8599495887756348.\n",
      "Test batch 76 with loss 2.398503541946411.\n",
      "Test batch 77 with loss 3.218871831893921.\n",
      "Test batch 78 with loss 2.1037721633911133.\n",
      "Test batch 79 with loss 3.4154560565948486.\n",
      "Test batch 80 with loss 2.5442068576812744.\n",
      "Test batch 81 with loss 2.5054216384887695.\n",
      "Test batch 82 with loss 2.69994854927063.\n",
      "Test batch 83 with loss 2.776397943496704.\n",
      "Test batch 84 with loss 2.840984582901001.\n",
      "Test batch 85 with loss 2.7890210151672363.\n",
      "Test batch 86 with loss 3.0040059089660645.\n",
      "Test batch 87 with loss 2.4264976978302.\n",
      "Test batch 88 with loss 2.5225672721862793.\n",
      "Test batch 89 with loss 2.642353057861328.\n",
      "Test batch 90 with loss 2.7058565616607666.\n",
      "Test batch 91 with loss 2.47434139251709.\n",
      "Test batch 92 with loss 2.3583972454071045.\n",
      "Test batch 93 with loss 2.91152286529541.\n",
      "Test batch 94 with loss 2.469942331314087.\n",
      "Test batch 95 with loss 2.2671687602996826.\n",
      "Test batch 96 with loss 2.690023899078369.\n",
      "Test batch 97 with loss 2.1472465991973877.\n",
      "Test batch 98 with loss 2.825568914413452.\n",
      "Test batch 99 with loss 2.5088860988616943.\n",
      "Test batch 100 with loss 3.432307720184326.\n",
      "Test batch 101 with loss 2.518444538116455.\n",
      "Test batch 102 with loss 3.183384895324707.\n",
      "Test batch 103 with loss 3.040534019470215.\n",
      "Test batch 104 with loss 3.226646900177002.\n",
      "Test batch 105 with loss 2.5914862155914307.\n",
      "Test batch 106 with loss 3.0462563037872314.\n",
      "Test batch 107 with loss 3.1099419593811035.\n",
      "Test batch 108 with loss 2.5725622177124023.\n",
      "Test batch 109 with loss 2.9344797134399414.\n",
      "Test batch 110 with loss 3.156405448913574.\n",
      "Test batch 111 with loss 2.55975079536438.\n",
      "Test batch 112 with loss 3.5681612491607666.\n",
      "Test batch 113 with loss 2.728358507156372.\n",
      "Test batch 114 with loss 2.705437183380127.\n",
      "Test batch 115 with loss 2.6126997470855713.\n",
      "Test batch 116 with loss 2.607616424560547.\n",
      "Test batch 117 with loss 2.4304487705230713.\n",
      "Test batch 118 with loss 2.501697063446045.\n",
      "Test batch 119 with loss 2.3920106887817383.\n",
      "Test batch 120 with loss 2.281363010406494.\n",
      "Test batch 121 with loss 2.892639636993408.\n",
      "Test batch 122 with loss 2.56535267829895.\n",
      "Test batch 123 with loss 3.20074462890625.\n",
      "Test batch 124 with loss 2.4367682933807373.\n",
      "Test batch 125 with loss 2.8925089836120605.\n",
      "Test batch 126 with loss 2.5489540100097656.\n",
      "Test batch 127 with loss 2.830202341079712.\n",
      "Test batch 128 with loss 2.4499475955963135.\n",
      "Test batch 129 with loss 2.8813421726226807.\n",
      "Test batch 130 with loss 3.055917263031006.\n",
      "Test batch 131 with loss 2.7330615520477295.\n",
      "Test batch 132 with loss 2.1205532550811768.\n",
      "Test batch 133 with loss 3.2195892333984375.\n",
      "Test batch 134 with loss 2.7955892086029053.\n",
      "Test batch 135 with loss 2.515970468521118.\n",
      "Test batch 136 with loss 2.800638437271118.\n",
      "Test batch 137 with loss 2.2371938228607178.\n",
      "Test batch 138 with loss 3.2530646324157715.\n",
      "Test batch 139 with loss 2.70154070854187.\n",
      "Test batch 140 with loss 2.47821044921875.\n",
      "Test batch 141 with loss 2.127361297607422.\n",
      "Test batch 142 with loss 2.672053337097168.\n",
      "Test batch 143 with loss 2.768502712249756.\n",
      "Test batch 144 with loss 2.9436089992523193.\n",
      "Test batch 145 with loss 2.492861747741699.\n",
      "Test batch 146 with loss 2.6612894535064697.\n",
      "Test batch 147 with loss 2.4931271076202393.\n",
      "Test batch 148 with loss 2.4066989421844482.\n",
      "Test batch 149 with loss 2.75162935256958.\n",
      "Test batch 150 with loss 2.4829232692718506.\n",
      "Test batch 151 with loss 2.337287187576294.\n",
      "Test batch 152 with loss 2.7654221057891846.\n",
      "Test batch 153 with loss 2.4391191005706787.\n",
      "Test batch 154 with loss 2.8514676094055176.\n",
      "Test batch 155 with loss 2.514347553253174.\n",
      "Test batch 156 with loss 2.8727002143859863.\n",
      "Test batch 157 with loss 2.8326616287231445.\n",
      "Test batch 158 with loss 2.4502131938934326.\n",
      "Test batch 159 with loss 2.7386081218719482.\n",
      "Test batch 160 with loss 2.1647534370422363.\n",
      "Test batch 161 with loss 2.276743173599243.\n",
      "Test batch 162 with loss 2.6598665714263916.\n",
      "Test batch 163 with loss 2.836601972579956.\n",
      "Test batch 164 with loss 2.0413100719451904.\n",
      "Test batch 165 with loss 2.86666202545166.\n",
      "Test batch 166 with loss 2.586919069290161.\n",
      "Test batch 167 with loss 2.2498257160186768.\n",
      "Test batch 168 with loss 3.266233205795288.\n",
      "Test batch 169 with loss 2.8114404678344727.\n",
      "Test batch 170 with loss 3.07458758354187.\n",
      "Test batch 171 with loss 2.2414114475250244.\n",
      "Test batch 172 with loss 3.2337825298309326.\n",
      "Test batch 173 with loss 3.0123491287231445.\n",
      "Test batch 174 with loss 2.673753499984741.\n",
      "Test batch 175 with loss 2.6313793659210205.\n",
      "Test batch 176 with loss 2.4124770164489746.\n",
      "Test batch 177 with loss 2.5679383277893066.\n",
      "Test batch 178 with loss 2.703570604324341.\n",
      "Test batch 179 with loss 2.1023242473602295.\n",
      "Test batch 180 with loss 2.7713258266448975.\n",
      "Test batch 181 with loss 2.8502421379089355.\n",
      "Test batch 182 with loss 2.4890716075897217.\n",
      "Test batch 183 with loss 2.720968008041382.\n",
      "Test batch 184 with loss 2.3306491374969482.\n",
      "Test batch 185 with loss 2.9851441383361816.\n",
      "Test batch 186 with loss 2.731252908706665.\n",
      "Test batch 187 with loss 2.232287883758545.\n",
      "Test batch 188 with loss 2.952082395553589.\n",
      "Test batch 189 with loss 3.024658441543579.\n",
      "Test batch 190 with loss 2.342334508895874.\n",
      "Test batch 191 with loss 2.7925174236297607.\n",
      "Test batch 192 with loss 2.912703514099121.\n",
      "Test batch 193 with loss 2.5800726413726807.\n",
      "Test batch 194 with loss 2.472837209701538.\n",
      "Test batch 195 with loss 2.0822768211364746.\n",
      "Test batch 196 with loss 2.826601505279541.\n",
      "Test batch 197 with loss 2.4472098350524902.\n",
      "Test batch 198 with loss 2.405181646347046.\n",
      "Test batch 199 with loss 3.0733730792999268.\n",
      "Test batch 200 with loss 2.7593908309936523.\n",
      "Test batch 201 with loss 3.1222453117370605.\n",
      "Test batch 202 with loss 2.746271848678589.\n",
      "Test batch 203 with loss 2.590087652206421.\n",
      "Test batch 204 with loss 2.549389362335205.\n",
      "Test batch 205 with loss 2.1930458545684814.\n",
      "Test batch 206 with loss 3.076569080352783.\n",
      "Test batch 207 with loss 2.7409794330596924.\n",
      "Test batch 208 with loss 2.2311487197875977.\n",
      "Test batch 209 with loss 2.743905782699585.\n",
      "Test batch 210 with loss 2.5519680976867676.\n",
      "Test batch 211 with loss 2.8101325035095215.\n",
      "Test batch 212 with loss 2.385874032974243.\n",
      "Test batch 213 with loss 2.73486590385437.\n",
      "Test batch 214 with loss 2.114574670791626.\n",
      "Test batch 215 with loss 2.8934366703033447.\n",
      "Test batch 216 with loss 2.364663600921631.\n",
      "Test batch 217 with loss 2.373509645462036.\n",
      "Test batch 218 with loss 2.500354766845703.\n",
      "Test batch 219 with loss 3.101494312286377.\n",
      "Test batch 220 with loss 3.066837787628174.\n",
      "Test batch 221 with loss 2.4408762454986572.\n",
      "Test batch 222 with loss 3.1457715034484863.\n",
      "Test batch 223 with loss 2.571039915084839.\n",
      "Test batch 224 with loss 2.509572744369507.\n",
      "Test batch 225 with loss 2.516446352005005.\n",
      "Test batch 226 with loss 2.6019091606140137.\n",
      "Test batch 227 with loss 3.3075215816497803.\n",
      "Test batch 228 with loss 2.675891637802124.\n",
      "Test batch 229 with loss 2.560386896133423.\n",
      "Test batch 230 with loss 2.566128969192505.\n",
      "Test batch 231 with loss 2.419926404953003.\n",
      "Test batch 232 with loss 2.6029984951019287.\n",
      "Test batch 233 with loss 2.7799506187438965.\n",
      "Test batch 234 with loss 2.3564095497131348.\n",
      "Test batch 235 with loss 2.716597318649292.\n",
      "Test batch 236 with loss 2.2526044845581055.\n",
      "Test batch 237 with loss 1.9256114959716797.\n",
      "Test batch 238 with loss 2.3452789783477783.\n",
      "Test batch 239 with loss 2.6437811851501465.\n",
      "Test batch 240 with loss 2.296375274658203.\n",
      "Test batch 241 with loss 2.8999242782592773.\n",
      "Test batch 242 with loss 2.620347023010254.\n",
      "Test batch 243 with loss 2.202578067779541.\n",
      "Test batch 244 with loss 2.706263780593872.\n",
      "Test batch 245 with loss 2.4397430419921875.\n",
      "Test batch 246 with loss 2.731992483139038.\n",
      "Test batch 247 with loss 3.156198263168335.\n",
      "Test batch 248 with loss 3.0498900413513184.\n",
      "Test batch 249 with loss 2.5586678981781006.\n",
      "Test batch 250 with loss 2.8842873573303223.\n",
      "Test batch 251 with loss 2.151726484298706.\n",
      "Test batch 252 with loss 2.5123867988586426.\n",
      "Test batch 253 with loss 2.061457872390747.\n",
      "Test batch 254 with loss 2.8858797550201416.\n",
      "Test batch 255 with loss 2.273106336593628.\n",
      "Test batch 256 with loss 2.3523833751678467.\n",
      "Test batch 257 with loss 3.2721495628356934.\n",
      "Test batch 258 with loss 2.3433072566986084.\n",
      "Test batch 259 with loss 2.294678211212158.\n",
      "Test batch 260 with loss 2.0963809490203857.\n",
      "Test batch 261 with loss 2.3509130477905273.\n",
      "Test batch 262 with loss 2.0534117221832275.\n",
      "Test batch 263 with loss 2.39764404296875.\n",
      "Test batch 264 with loss 2.733058452606201.\n",
      "Test batch 265 with loss 3.078228235244751.\n",
      "Test batch 266 with loss 2.585597515106201.\n",
      "Test batch 267 with loss 2.4089033603668213.\n",
      "Test batch 268 with loss 2.916766405105591.\n",
      "Test batch 269 with loss 2.6457738876342773.\n",
      "Test batch 270 with loss 2.28155779838562.\n",
      "Test batch 271 with loss 2.2195088863372803.\n",
      "Test batch 272 with loss 2.116631269454956.\n",
      "Test batch 273 with loss 2.2418909072875977.\n",
      "Test batch 274 with loss 2.4575998783111572.\n",
      "Test batch 275 with loss 2.5942530632019043.\n",
      "Test batch 276 with loss 2.6697323322296143.\n",
      "Test batch 277 with loss 2.533639669418335.\n",
      "Test batch 278 with loss 2.4195733070373535.\n",
      "Test batch 279 with loss 2.4449174404144287.\n",
      "Test batch 280 with loss 2.3913254737854004.\n",
      "Test batch 281 with loss 2.843261241912842.\n",
      "Test batch 282 with loss 2.1987195014953613.\n",
      "Test batch 283 with loss 2.3655521869659424.\n",
      "Test batch 284 with loss 2.9614617824554443.\n",
      "Test batch 285 with loss 2.096456289291382.\n",
      "Test batch 286 with loss 2.746222496032715.\n",
      "Test batch 287 with loss 2.1651551723480225.\n",
      "Test batch 288 with loss 3.0660860538482666.\n",
      "Test batch 289 with loss 3.3406896591186523.\n",
      "Test batch 290 with loss 2.040837287902832.\n",
      "Test batch 291 with loss 2.7618813514709473.\n",
      "Test batch 292 with loss 3.0710599422454834.\n",
      "Test batch 293 with loss 1.9854241609573364.\n",
      "Test batch 294 with loss 2.6751043796539307.\n",
      "Test batch 295 with loss 2.407529354095459.\n",
      "Test batch 296 with loss 2.799107551574707.\n",
      "Test batch 297 with loss 2.95293927192688.\n",
      "Test batch 298 with loss 3.0343406200408936.\n",
      "Test batch 299 with loss 2.8204164505004883.\n",
      "Test batch 300 with loss 2.471531629562378.\n",
      "Test batch 301 with loss 2.423037052154541.\n",
      "Test batch 302 with loss 3.175645351409912.\n",
      "Test batch 303 with loss 1.9618700742721558.\n",
      "Test batch 304 with loss 2.492579221725464.\n",
      "Test batch 305 with loss 2.361807107925415.\n",
      "Test batch 306 with loss 2.445543050765991.\n",
      "Test batch 307 with loss 2.6828625202178955.\n",
      "Test batch 308 with loss 2.36814546585083.\n",
      "Test batch 309 with loss 2.2083964347839355.\n",
      "Test batch 310 with loss 2.179499626159668.\n",
      "Test batch 311 with loss 2.9492735862731934.\n",
      "Test batch 312 with loss 2.5570714473724365.\n",
      "Test batch 313 with loss 2.726745128631592.\n",
      "Test batch 314 with loss 2.498828887939453.\n",
      "Test batch 315 with loss 2.7609105110168457.\n",
      "Test batch 316 with loss 2.3645265102386475.\n",
      "Test batch 317 with loss 2.4662280082702637.\n",
      "Test batch 318 with loss 2.5769801139831543.\n",
      "Test batch 319 with loss 2.6466705799102783.\n",
      "Test batch 320 with loss 2.399852991104126.\n",
      "Test batch 321 with loss 2.9467947483062744.\n",
      "Test batch 322 with loss 2.462320566177368.\n",
      "Test batch 323 with loss 2.070452928543091.\n",
      "Test batch 324 with loss 2.941145896911621.\n",
      "Test batch 325 with loss 2.9042937755584717.\n",
      "Test batch 326 with loss 2.0898983478546143.\n",
      "Test batch 327 with loss 3.153385639190674.\n",
      "Test batch 328 with loss 2.5252060890197754.\n",
      "Test batch 329 with loss 2.547008752822876.\n",
      "Test batch 330 with loss 2.584500312805176.\n",
      "Test batch 331 with loss 2.639019727706909.\n",
      "Test batch 332 with loss 2.661073923110962.\n",
      "Test batch 333 with loss 2.487173080444336.\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu' # set so 'cuda:xx' if you have a GPU, xx is GPU index. L'entraînement des réseaux de neurones est grandement accéléré par l'utilisation d'un GPU \n",
    "x0 = next(iter(dataloader_train))[0]\n",
    "model = DumbModel(x0.size(1), x0.size(2), 6)  # vous instanciez ici votre modèle\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy # définissez la fonction de perte selon laquelle le modèle sera optimisé\n",
    "\n",
    "# en pratique on utilise pas une simple descente de gradient mais une procédure d'optimisation plus sophistiquée qui est implémentée sous la forme d'un objet Optimizer.\n",
    "# Il en existe beaucoup d'optimizers différents, vous pouvez en tester différents, \n",
    "# je vous propose d'utiliser en premier lieu l'algorithme Adam\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "n_epochs = 6 # le nombre d'itérations dans l'entrainement \n",
    "\n",
    "chemin_vers_sauvegarde_model = Path(\"saved_models\")/\"dumb_model\" # chemin vers un fichier où vous sauvegarderez votre modèle après optimisation pour le réutiliser plus tard. \n",
    "\n",
    "model.to(device) # on place le modèle dans le GPU si nécessaire\n",
    "training_loss = []\n",
    "test_loss = []\n",
    "for epoch in range(n_epochs):\n",
    "    training_loss.append([])\n",
    "    i = 0\n",
    "    for batch_x,batch_y in dataloader_train:\n",
    "        batch_x.to(device)\n",
    "        batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_y_predicted = model(batch_x)\n",
    "        \n",
    "        l = loss(batch_y_predicted, batch_y)\n",
    "        # loggez la loss sur le batch d'entraînement\n",
    "        print(f\"Training epoch {epoch} batch {i} with loss {l}.\")\n",
    "        training_loss[epoch].append(float(l))\n",
    "        i += 1\n",
    "        \n",
    "        l.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    i = 0\n",
    "    test_loss.append([])\n",
    "    for batch_x,batch_y in dataloader_valid:\n",
    "        \n",
    "        batch_x.to(device)\n",
    "        batch_y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_y_predicted = model(batch_x)  \n",
    "            \n",
    "        # loggez la loss et les métriques sur le batch de validation\n",
    "        l = loss(batch_y_predicted, batch_y)\n",
    "        print(f\"Test batch {i} with loss {l}.\")\n",
    "        test_loss[epoch].append(float(l))\n",
    "\n",
    "        i += 1\n",
    "torch.save(model, chemin_vers_sauvegarde_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1a455335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHFCAYAAAAaD0bAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACRLklEQVR4nO3dd1wT5x8H8M+FETYIyhJEnIgTJ+DCuq2rrdUOV5e1dbW2tWJt1S5rl6Nau6zUXyvaFldbtwLWihvcWxREECdLdp7fH0ggZF3mXZLv+/XKS3N57u7Jcbn73jM5xhgDIYQQQogNkQidAUIIIYQQc6MAiBBCCCE2hwIgQgghhNgcCoAIIYQQYnMoACKEEEKIzaEAiBBCCCE2hwIgQgghhNgcCoAIIYQQYnMoACKEEEKIzaEAiBDCC8dxvF5JSUkG7Wf+/PngOE6vdZOSkoySB0vbNyFEd/ZCZ4AQYhlSUlIU3n/00UdITEzE3r17FZaHh4cbtJ+XX34ZgwYN0mvdjh07IiUlxeA8EEKsHwVAhBBeIiMjFd43aNAAEolEaXldDx8+hIuLC+/9BAUFISgoSK88enh4aM0PIYQAVAVGCDGimJgYtGnTBvv27UN0dDRcXFzw4osvAgDWr1+PAQMGICAgAM7OzmjVqhVmz56NoqIihW2oqgJr3Lgxhg4diu3bt6Njx45wdnZGWFgYfv75Z4V0qqqhJk6cCDc3N1y+fBlDhgyBm5sbgoOD8dZbb6G0tFRh/Rs3bmDUqFFwd3eHl5cXnn/+eRw5cgQcxyEuLk6vY7JlyxZERUXBxcUF7u7u6N+/v1Jp2u3btzFp0iQEBwdDKpWiQYMG6N69O3bv3i1Pk5qaiqFDh8LX1xdSqRSBgYF4/PHHcePGDb3yRYitoxIgQohRZWdnY+zYsZg1axY+/fRTSCRVz1mXLl3CkCFD8MYbb8DV1RXnz5/HokWLcPjwYaVqNFVOnDiBt956C7Nnz4afnx9++uknvPTSS2jWrBl69eqlcd3y8nIMHz4cL730Et566y3s27cPH330ETw9PfHBBx8AAIqKitCnTx/cu3cPixYtQrNmzbB9+3aMGTNG72Oxdu1aPP/88xgwYADi4+NRWlqKzz//HDExMdizZw969OgBABg3bhyOHz+OTz75BC1atMCDBw9w/Phx3L17V563/v37IzQ0FCtWrICfnx9ycnKQmJiIgoICvfNHiE1jhBCihwkTJjBXV1eFZb1792YA2J49ezSuK5PJWHl5OUtOTmYA2IkTJ+SfzZs3j9W9NIWEhDAnJyd2/fp1+bLi4mLm7e3NXn31VfmyxMREBoAlJiYq5BMA+/333xW2OWTIENayZUv5+xUrVjAAbNu2bQrpXn31VQaArV69WuN3qrvvyspKFhgYyNq2bcsqKyvl6QoKCpivry+Ljo6WL3Nzc2NvvPGG2m0fPXqUAWCbNm3SmAdCCH9UBUYIMap69erhscceU1p+9epVPPfcc/D394ednR0cHBzQu3dvAMC5c+e0brdDhw5o1KiR/L2TkxNatGiB69eva12X4zgMGzZMYVm7du0U1k1OToa7u7tSA+xnn31W6/ZVuXDhAm7evIlx48bJS8EAwM3NDU899RQOHjyIhw8fAgC6du2KuLg4fPzxxzh48CDKy8sVttWsWTPUq1cP7777Lr777jucPXtWrzwRQmpQAEQIMaqAgAClZYWFhejZsycOHTqEjz/+GElJSThy5Ag2bNgAACguLta6XR8fH6VlUqmU17ouLi5wcnJSWrekpET+/u7du/Dz81NaV9UyPqqrr1Qdj8DAQMhkMty/fx9AVfuoCRMm4KeffkJUVBS8vb0xfvx45OTkAAA8PT2RnJyMDh06YM6cOWjdujUCAwMxb948pWCJEMIPtQEihBiVqjF89u7di5s3byIpKUle6gMADx48MGPONPPx8cHhw4eVllcHIfpsD6hqE1XXzZs3IZFIUK9ePQBA/fr1sWTJEixZsgQZGRnYsmULZs+ejdzcXGzfvh0A0LZtW6xbtw6MMZw8eRJxcXH48MMP4ezsjNmzZ+uVR0JsGZUAEUJMrjookkqlCsu///57IbKjUu/evVFQUIBt27YpLF+3bp1e22vZsiUaNmyItWvXgjEmX15UVISEhAR5z7C6GjVqhKlTp6J///44fvy40uccx6F9+/ZYvHgxvLy8VKYhhGhHJUCEEJOLjo5GvXr1MHnyZMybNw8ODg747bffcOLECaGzJjdhwgQsXrwYY8eOxccff4xmzZph27Zt2LFjBwAotOPhQyKR4PPPP8fzzz+PoUOH4tVXX0VpaSm++OILPHjwAJ999hkAIC8vD3369MFzzz2HsLAwuLu748iRI9i+fTuefPJJAMDff/+Nb7/9FiNHjkSTJk3AGMOGDRvw4MED9O/f37gHghAbQQEQIcTkfHx88M8//+Ctt97C2LFj4erqihEjRmD9+vXo2LGj0NkDALi6umLv3r144403MGvWLHAchwEDBuDbb7/FkCFD4OXlpfM2n3vuObi6umLhwoUYM2YM7OzsEBkZicTERERHRwOoaszdrVs3/O9//8O1a9dQXl6ORo0a4d1338WsWbMAAM2bN4eXlxc+//xz3Lx5E46OjmjZsiXi4uIwYcIEYx4GQmwGx2qXzRJCCFHw6aefYu7cucjIyNB7hGpCiPhQCRAhhDyyfPlyAEBYWBjKy8uxd+9eLFu2DGPHjqXghxArQwEQIYQ84uLigsWLF+PatWsoLS2VV0XNnTtX6KwRQoyMqsAIIYQQYnOoGzwhhBBCbA4FQIQQQgixORQAEUIIIcTmUCNoFWQyGW7evAl3d3eVw/oTQgghRHwYYygoKEBgYKDWwUspAFLh5s2bCA4OFjobhBBCCNFDZmam1qErKABSwd3dHUDVAfTw8BA4N4QQQgjhIz8/H8HBwfL7uCYUAKlQXe3l4eFBARAhhBBiYfg0X6FG0IQQQgixORQAEUIIIcTmUABECCGEEJtDbYAIIYQHmUyGsrIyobNBiM1zdHTU2sWdDwqACCFEi7KyMqSnp0MmkwmdFUJsnkQiQWhoKBwdHQ3aDgVAhBCiAWMM2dnZsLOzQ3BwsFGePAkh+qkeqDg7OxuNGjUyaLBiCoAIIUSDiooKPHz4EIGBgXBxcRE6O4TYvAYNGuDmzZuoqKiAg4OD3tuhRxlCCNGgsrISAAwubieEGEf1b7H6t6kvCoAIIYQHmheQEHEw1m+RAiBCCCGE2BwKgAghhBArxXEcNm3aJHQ2NIqJicEbb7xh9v1SAEQIIeYgqwTS/wVO/Vn1r8yw9gvaTJw4ESNHjjTpPojl4TgO165dEzobokC9wAghNqGkvBKOdhJIJAK05Tm7Bdj+LpB/s2aZRyAwaBEQPtz8+SFWp7y83KAeUbaISoAIIVYvr7gcrT7Yjqe+O2D+nZ/dAvw+XjH4AYD87KrlZ7eYP08AkpOT0bVrV0ilUgQEBGD27NmoqKiQf/7nn3+ibdu2cHZ2ho+PD/r164eioiIAQFJSErp27QpXV1d4eXmhe/fuuH79usr9XLt2DRzH4ffff0fPnj3h7OyMLl264OLFizhy5Ag6d+4MNzc3DBo0CLdv35avJ5PJ8OGHHyIoKAhSqRQdOnTA9u3b5Z8/9thjmDp1qsK+7t69C6lUir179wKoGsBy1qxZaNiwIVxdXdGtWzckJSXJ08fFxcHLyws7duxAq1at5PnIzs6Wp6kuSfvyyy8REBAAHx8fTJkyBeXl5fI02vajyvz589GoUSNIpVIEBgZi+vTp8s9UVVt5eXkhLi5O6ZjGxMTAyckJv/76q8b9qXP27FkMGTIEbm5u8PPzw7hx43Dnzh355zExMZg6dSqmTp0KLy8v+Pj4YO7cuWCMydPcv38f48ePR7169eDi4oLBgwfj0qVLCvv577//0Lt3b7i4uKBevXoYOHAg7t+/L/9cJpNh1qxZ8Pb2hr+/P+bPn6/X99EFBUCEEKu37+JtMAakZjwwfGOMAWVF/F4l+cC2WQCYqg1V/bP93ap0fLbHVG1Hd1lZWRgyZAi6dOmCEydOYOXKlVi1ahU+/vhjAEB2djaeffZZvPjiizh37hySkpLw5JNPgjGGiooKjBw5Er1798bJkyeRkpKCSZMmae2ZM2/ePMydOxfHjx+Hvb09nn32WcyaNQtLly7Fv//+iytXruCDDz6Qp1+6dCm++uorfPnllzh58iQGDhyI4cOHy2+sL7/8MtauXYvS0lL5Or/99hsCAwPRp08fAMALL7yA//77D+vWrcPJkyfx9NNPY9CgQQo354cPH+LLL7/E//73P+zbtw8ZGRl4++23FfKemJiIK1euIDExEb/88gvi4uLkwQjf/dT2559/YvHixfj+++9x6dIlbNq0CW3btuXxl1P07rvvYvr06Th37hwGDhyo8/rZ2dno3bs3OnTogKNHj2L79u24desWRo8erZDul19+gb29PQ4dOoRly5Zh8eLF+Omnn+SfT5w4EUePHsWWLVuQkpICxhiGDBkiDxLT0tLQt29ftG7dGikpKdi/fz+GDRum0I39l19+gaurKw4dOoTPP/8cH374IXbt2qXzd9IJI0ry8vIYAJaXlyd0VgghRrAlLYuFvPs3C3n3b53XLS4uZmfPnmXFxcVVC0oLGZvnIcyrtJB3vidMmMBGjBih8rM5c+awli1bMplMJl+2YsUK5ubmxiorK9mxY8cYAHbt2jWlde/evcsAsKSkJF75SE9PZwDYTz/9JF8WHx/PALA9e/bIly1cuJC1bNlS/j4wMJB98sknCtvq0qULe/311xljjJWUlDBvb2+2fv16+ecdOnRg8+fPZ4wxdvnyZcZxHMvKylLYRt++fVlsbCxjjLHVq1czAOzy5csKx8HPz0/+fsKECSwkJIRVVFTIlz399NNszJgxvPdT11dffcVatGjBysrKVH4OgG3cuFFhmaenJ1u9ejVjrOaYLlmyROX62rZV7f3332cDBgxQWJaZmckAsAsXLjDGGOvduzdr1aqVwrny7rvvslatWjHGGLt48SIDwP777z/553fu3GHOzs7s999/Z4wx9uyzz7Lu3burzWPv3r1Zjx49FJZ16dKFvfvuuyrTK/0ma9Hl/k0lQIQQYmPOnTuHqKgohVKb7t27o7CwEDdu3ED79u3Rt29ftG3bFk8//TR+/PFHeXWFt7c3Jk6ciIEDB2LYsGFYunSpQpWROu3atZP/38/PDwAUSj38/PyQm5sLAMjPz8fNmzfRvXt3hW10794d586dAwBIpVKMHTsWP//8M4CqUoYTJ05g4sSJAIDjx4+DMYYWLVrAzc1N/kpOTsaVK1fk23RxcUHTpk3l7wMCAuT5qNa6dWvY2dmpTMN3P7U9/fTTKC4uRpMmTfDKK69g48aNCtWPfHXu3FnndWo7duwYEhMTFfIdFhYGAAp5j4yMVDhXoqKicOnSJVRWVuLcuXOwt7dHt27d5J/7+PigZcuW8r9VdQmQJrXPD0D138HYqBE0IcTqGafi6BEHF2DOTe3pAOD6AeC3UdrTPf8nEBLNb99GwBhTqrJij6rXOI6DnZ0ddu3ahQMHDmDnzp345ptv8N577+HQoUMIDQ3F6tWrMX36dGzfvh3r16/H3LlzsWvXLkRGRqrPeq0GutX7rrus7mSzqvJYe9nLL7+MDh064MaNG/j555/Rt29fhISEAKhqU2JnZ4djx44pBC8A4ObmpjJf1ftkdaoaVaWpzivf/dQWHByMCxcuYNeuXdi9ezdef/11fPHFF0hOToaDg4PKPNRuc1TN1dVV5fb5kslkGDZsGBYtWqT0WUBAAK9t1M1n7eXVfytnZ2et29F0jE1F0BKglStXol27dvDw8ICHhweioqKwbds2jeskJyejU6dOcHJyQpMmTfDdd98ppUlISEB4eDikUinCw8OxceNGU30FQoit4TjA0ZXfq+ljVb29oK59DAd4NKxKx2d7RhoBNzw8HAcOHFC4eR04cADu7u5o2LDho6/JoXv37liwYAFSU1Ph6OiocC2NiIhAbGwsDhw4gDZt2mDt2rVGyRsAeHh4IDAwEPv371dYfuDAAbRq1Ur+vm3btujcuTN+/PFHrF27Fi+++KJC/iorK5Gbm4tmzZopvPz9/Y2WV3334+zsjOHDh2PZsmVISkpCSkoKTp06BaBqrqvapWqXLl3Cw4cPjZbnah07dsSZM2fQuHFjpbzXDq4OHjyosN7BgwfRvHlz2NnZITw8HBUVFTh06JD887t37+LixYvyv1W7du2wZ88eo+ffUIIGQEFBQfjss89w9OhRHD16FI899hhGjBiBM2fOqEyfnp6OIUOGoGfPnkhNTcWcOXMwffp0JCQkyNOkpKRgzJgxGDduHE6cOIFx48Zh9OjRCn8cQggxC4ldVVd3AMpB0KP3gz6rSmcCeXl5SEtLU3hlZGTg9ddfR2ZmJqZNm4bz589j8+bNmDdvHmbOnAmJRIJDhw7h008/xdGjR5GRkYENGzbg9u3baNWqFdLT0xEbG4uUlBRcv34dO3fuVLjZGcs777yDRYsWYf369bhw4QJmz56NtLQ0zJgxQyHdyy+/jM8++wyVlZV44okn5MtbtGiB559/HuPHj8eGDRuQnp6OI0eOYNGiRdi6davR8qnPfuLi4rBq1SqcPn0aV69exf/+9z84OzvLS68ee+wxLF++HMePH8fRo0cxefJkk3RxnzJlCu7du4dnn30Whw8fxtWrV7Fz5068+OKLCg2UMzMzMXPmTFy4cAHx8fH45ptv5H+H5s2bY8SIEXjllVewf/9+nDhxAmPHjkXDhg0xYsQIAEBsbCyOHDmC119/HSdPnsT58+excuVKhd5mgtDaSsjM6tWrp9BYrrZZs2axsLAwhWWvvvoqi4yMlL8fPXo0GzRokEKagQMHsmeeeYZ3HqgRNCHWZbMxG0Hr48xmxr4KU2zQ/FWrquUmMmHCBIaq2j+F14QJExhjjCUlJbEuXbowR0dH5u/vz959911WXl7OGGPs7NmzbODAgaxBgwZMKpWyFi1asG+++YYxxlhOTg4bOXIkCwgIYI6OjiwkJIR98MEHrLKyUmU+qhvspqamypclJiYyAOz+/fvyZatXr2aenp7y95WVlWzBggWsYcOGzMHBgbVv355t27ZNafsFBQXMxcVF3ji6trKyMvbBBx+wxo0bMwcHB+bv78+eeOIJdvLkSZX7ZIyxjRs3stq3RlWNyWfMmMF69+7Nez91bdy4kXXr1o15eHgwV1dXFhkZyXbv3i3/PCsriw0YMIC5urqy5s2bs61bt6psBF37mKoDDY2gGatqxPzEE08wLy8v5uzszMLCwtgbb7whb/Tcu3dv9vrrr7PJkyczDw8PVq9ePTZ79myFRtH37t1j48aNY56enszZ2ZkNHDiQXbx4UWE/SUlJLDo6mkmlUubl5cUGDhwo//v37t2bzZgxQyH9iBEj5OdqXcZqBM0xZqR+lQaqrKzEH3/8gQkTJiA1NRXh4eFKaXr16oWIiAgsXbpUvmzjxo0YPXo0Hj58CAcHBzRq1Ahvvvkm3nzzTXmaxYsXY8mSJWrHqSgtLVXoSpmfn4/g4GDk5eXBw8PDiN+SECKEzWlZmLEuDQBw7bPHdVq3pKQE6enpCA0NhZOTk/6ZkFVWtQkqvAW4+VW1+TFRyY8tyczMROPGjXHkyBF07NhR6OxYnZiYGHTo0AFLliwROitymn6T+fn58PT05HX/FrwR9KlTpxAVFYWSkhK4ublh48aNKoMfAMjJyZH3Hqjm5+eHiooK3LlzBwEBAWrT5OTkqM3DwoULsWDBAsO/DCGEqCOxA0J7Cp0Lq1FeXo7s7GzMnj0bkZGRFPwQnQneDb5ly5ZIS0vDwYMH8dprr2HChAk4e/as2vSaei5oSqNpkK7Y2Fjk5eXJX5mZmfp8FUIIIWby33//ISQkBMeOHVPZGYYQbQQvAXJ0dESzZs0AVI1pcOTIESxduhTff/+9Ulp/f3+lkpzc3FzY29vDx8dHY5q6pUK1SaVSSKVSQ78KIUSktI1STCxPTEyM2i7YxHi0TelhyQQvAaqLMabQHqe2qKgopaGxd+7cic6dO8tbyKtLEx3NY4wNQohVohslIaQuQUuA5syZg8GDByM4OBgFBQVYt24dkpKS5BPexcbGIisrC2vWrAEATJ48GcuXL8fMmTPxyiuvICUlBatWrUJ8fLx8mzNmzECvXr2waNEijBgxAps3b8bu3buVxpMghBBdUBBFiDgY67coaAB069YtjBs3DtnZ2fD09ES7du2wfft29O/fH0DVRG0ZGRny9KGhodi6dSvefPNNrFixAoGBgVi2bBmeeuopeZro6GisW7cOc+fOxfvvv4+mTZti/fr1CsN0E0IIX9Wj+5aVlfEa0ZYQYlplZWUAoDTytq5E0w1eTHTpRkcIET9DusEzxpCRkYHy8nIEBgZCIhFdywFCbIZMJsPNmzflw97Ubd9nUd3gCSFEzDiOQ0BAANLT09WOJUYIMR+JRKIy+NEVBUCEEKKFo6MjmjdvLi96J4QIx9HR0SglsRQAEUIIDxKJxLCRoAkhokKV2YQQQgixORQAEUIIIcTmUABECCGEEJtDARAhhBBCbA4FQIQQQgixORQAEUKsHg33SgipiwIgQgghhNgcCoAIIYQQYnMoACKEEEKIzaEAiBBi9RioERAhRBEFQIQQQgixORQAEUIEtff8LTzx7X+4crtQ6KwQQmwIBUCEEEG9GHcUqRkPMGNdqsn2wYEz2bYJIZaJAiBCiCg8eFhusm1TGyBCSF0UABFCRKWkvBIHLt9BeaVM6KwQQqwYBUCEEFGZsS4Vz/10CAu3nhc6K4QQK0YBECFEVHacuQUAiDuQLnBOCCHWjAIgQojVo7nAiLF9vv08vthBpZSWzF7oDBBCCCGW5F5RGb5NugIAmNSrKTydHQTOEdEHlQARQkShbikNFdoQsaqo1UBfJqMz1VJRAEQIIYQQm0MBECGEEKInKv+xXBQAEUJEiRouE9GigcWtAgVAhBBR4OimQiwFBedWgQIgQgghRE8Ut1suCoAIIaJkzBIhqk4jpkKnluWiAIgQIgpK3eDpzkLEiop9rIKgAdDChQvRpUsXuLu7w9fXFyNHjsSFCxc0rjNx4kRwHKf0at26tTxNXFycyjQlJSWm/kqEEEIIsQCCBkDJycmYMmUKDh48iF27dqGiogIDBgxAUVGR2nWWLl2K7Oxs+SszMxPe3t54+umnFdJ5eHgopMvOzoaTk5OpvxIhhBBiUcprDexoSwSdCmP79u0K71evXg1fX18cO3YMvXr1UrmOp6cnPD095e83bdqE+/fv44UXXlBIx3Ec/P39jZ9pQojFodo0QlR7c30atp3Oxr+zHkMDd6nQ2TErUbUBysvLAwB4e3vzXmfVqlXo168fQkJCFJYXFhYiJCQEQUFBGDp0KFJTU9Vuo7S0FPn5+QovQojwMu4+FDoLhFi1jalZKCmX4fejmUJnxexEEwAxxjBz5kz06NEDbdq04bVOdnY2tm3bhpdffllheVhYGOLi4rBlyxbEx8fDyckJ3bt3x6VLl1RuZ+HChfKSJU9PTwQHBxv8fQghhuv1RSJ+O3Rd6GzYlOKySuw9fwsl5ZVCZ4UQkxJNADR16lScPHkS8fHxvNeJi4uDl5cXRo4cqbA8MjISY8eORfv27dGzZ0/8/vvvaNGiBb755huV24mNjUVeXp78lZlpe5EwIWK1bI/qBxddUKcd/t5NOIkX445idsJJobMiiG+TLmPuplNgPLsh8k1HxEcUAdC0adOwZcsWJCYmIigoiNc6jDH8/PPPGDduHBwdHTWmlUgk6NKli9oSIKlUCg8PD4UXIcS8TDkSNN2i+Nty4iYAYFPaTYFzIozPt1/ArwczcOam+qYQHIXUVkHQAIgxhqlTp2LDhg3Yu3cvQkNDea+bnJyMy5cv46WXXuK1n7S0NAQEBBiSXUKICdGDNBETqgK0foIGQFOmTMGvv/6KtWvXwt3dHTk5OcjJyUFxcbE8TWxsLMaPH6+07qpVq9CtWzeV7YUWLFiAHTt24OrVq0hLS8NLL72EtLQ0TJ482aTfhxBifMYOjKpLOAghtk3QAGjlypXIy8tDTEwMAgIC5K/169fL02RnZyMjI0Nhvby8PCQkJKgt/Xnw4AEmTZqEVq1aYcCAAcjKysK+ffvQtWtXk34fS3evqEzoLBBictPj1fcIJYTYDkHHAeLTeCwuLk5pmaenJx4+VN89dvHixVi8eLEhWbM5X++6iGV7LmHhk23xbNdGZtlnWYUMiRdy0S3UG14umttxEWIIaqhKCKlLFI2gifCqe9q8v+m02fa5dM9FvPq/Y3jmh4Nm2yexPKZsHE0IMcx3yVfw6dZzQmdDLxQAEcFsftTL5HxOgcA5IWJmrYU3cf+lY/6WM1Q6ZeFs/a/32bbz+GHfVVy5XSh0VnQmaBUYIYTYqvl/nQUADGsfiE4h9QTOjfgUllagpLwS9d3ENz0DlUoqKy6zvF5zVAJECLFKMhnDrfwSobOh1cOyCqGzIEpt5+9A5493474ZO2fkFZfzSle70E7GGH769yrSMh+YJlNmYq6SyB/3XcWI5fuRX8LvWJsSBUBEAT3ZELFRdVmeu+kUFu+6qHG9afGp6PbpHuw5d8s0GSMmVX0/PpttvrkZV+1P13mdzak38fE/5zByxX8myJH1+WTrOZy4kYef/tX9WBsbBUA24HRWHmauT0PWg2Ltic3gTmEpZDLbrDlfkXgZi7afFzobFu1ybgF+PZiBpVqmyPjnVDaAqkaaYkZNgMSjtIJfNU7tB8ULt6gNoz5KRTDQJLUBsgFDv9kPALh6pwibpnQXNC//XrqNcasOY0hbf0HzIYTyShm+2HEBADA2MgQNvZwFzpFlqFsoWVIuEyQfYnUrvwT/nMzGqM5B8HByEDo7hPAihrifAiAbciVX+Fb63ydfBQBsPZWDoHq2FQDIaj3ql1XQTdxcGBPHxdZURn+fgut3HyI18wG+eTZC6OwYnTlLyPSZ44taDehHDL0fqQrMhgh/uhGiO2s/bw39ftfvVg0Km3Q+1/DMELXOZeejx6K92JSapbCc2k1aLgqAiFnkFZfjxbgj2H/5jtBZEQUxPP0QYgmECjDq7veNdWm4cb8Yb6xPU0xHZUB6EcMlkAIgYhZLd1/CXht/QqULpTBEcJ0lBhDqRll3v+oaSDM6w/QihqNGARDRy/2iMsQfzlAay+HszXx8tfMCikoVxza5/5AmWiUCEsPVlohe7VIfOmVMSwwlQNQImijgW0rxypqjOHr9Pvaez8WP4zvLlw9Z9i8AoKCkAvOHt0ZuQQk+335B5SBhVHdOiPGI4H5iVb5Pvooujb3l77laFyxrvHSZOyARQ8kZBUBEL0ev3wcA7DqrepC5s9n52H32Fl5ec1TtNlT94ErKK7Hr7C30at4Ani7Updfa3C0sBcdx8HZ1NMr2KmUMfx7LRJfG3mjSwE19Qmu8Y9kIoR6UdtcZQJPa7Smz9GNCVWBEq8LSCr0mutMU/Kjz0d9nMS0+Fe0/3CmagRuJcZRWVKLTx7vR8aNdqKg0zjAA645k4N2EU3jsq2S1aY49CtbFytJvIqYmxsNTO0vUts9yUQBkQ/S90MZ8kYi+XyXjhBnmuqndxXTM9ykm359Qpq5NxZt1epNYu7uFNe3ASnQYB0nTacs3uLlTWMp7f5ZK6NvwvaIyzNl4CidvPBA4J/rTdAw5NUVRVJWvHzEEthQAEa3uPLpx1S0SrutCTs2Q8HyuCdouHDfuW28J0NnsfGxMzUJhqe1MhCnk9Y4GnjS9uZtOYe2hDAxfbv1zYtW+dInhRk70QwEQMdjXOy+AMYaBS/bpvQ2ZjEEmY2qfsqyBFX81nelyKIxx3GyhmqJA4GD60i1hRpq/cf8hdp+9ZdFViZvTstD7i0SczzHfxK91fbXrIvKKdZuh3ZBDLoa/FwVAhDd1t5Bley8j5epdnbdX+/x//Jv9GLBkn8J0EcS6GOuCp89mKPi0Xj0WJeLlNUfVdsgwNWOcWzPWpeH63Yd4Y12a4RszwLdJl822LzFc6SkAIkZRu32HPs5l5+NybiEelgk/Q7DYFZVW4Md9V5F576HQWdGbLjcNY8RNFP9Yv8Pp9wzeBt/z0lQl1UJX1T4sVX/9vVdUBplMDGGL8VAARIxCn5+FrT2VG6tw6+N/zuGTrecwyIAqRyEY6/vrc96I+VyzrluK9VJs91PzVxPzuWUsp7Py0PGjXZiw+rDWtHnF5Ri0ZB+W7r6kMd320znGyp7eaBwgwtudIhrNWQxSrlTNp1ZkwaVlhrTJOaFHLyNrbltGqhgjkDS0rVhFpQz2duItV7h2pwj/Xr6DMZ2D4WivnE91P5NfD14HAPx7SXEuR1XH/JcD13A+pwDncwpQKZPhiY5BCK3vqpQut0D4npni/UsRo+N1gdDw+197KAOpGaq7Hf9+JFOvPNk6fdrFWOvNnO+4T+9tPK1yedKFXOTklRgzS4TopMXcbSpHvefNxD/tmC+T8P6m0/jx36sAgO2nsxU+N0YpbXmtMb6W7b2MIUv/NXyjJkIBENHJT/vTVVa91J3l3Urv0WZVUl6pcvya2oc25Yrujc+t1cTVRxC5cI/Kz0R9PlIdmMVR9xAiY8AHm1UH6GJy9FpVe6nJvx7XextXbhci4dgN+Xt1wVNxuXhLqikAIjr552Q2ztca74eYTswXSej88W6FkpHdZ2/h6p0i+ftnfzwoRNaIwEpEfFMxtnf+PMGrpNQojeU1joRo+PbFTt33V7W871fJmJVw0rQZMjEKgKxcdp71DiaoK8YYHohsVnpN1Vk5+VXVOfsv3QZQNVaSPtOLiJG5S2SsaRyg1Iz7CHt/Oz7866zQWTGL7LwSnM0WbnwcVRQDMus5twxR/Zu2pJFMKACycjtE0NJeLN5NOIkOH+7Cv48CCktRfUGxlOsKYwzbTmUrddPX/8Jo+DcXdRWYjr7YcQEA8PN/6bzXyc4rxuVcYQYqNAahu4drYonnlinaEVpS4FONAiAbYoknqDH9frSqvnrZHs3dM02FqbmRF5ZWoNKA8TVOZD4QTZVIdl4xQmO34rXfjqPn54lG3/7WU9naE6lggfcoo4pauBf9vk626jnR1P2+zM0SrrOMMRy5pjxu0tZThj8wW1JASAGQldP1t1hWIUNJeSXGfJ8iWKBgyRhjOj2t3i4oRZt5O/DEt/rPnzRixX946Zcjeq9vTO/8ob5NgDFuUK//pn+jTUtSKWPYc+6WXgHLoat3MXjpvzh2XfkGd61W+zFLxxhDRaVxS4ZU3btPZ+XhzfVpyDLD3ITmjB2+S7qitEz9+VaTs81pWUY/7kKhcYCswNmb+aiUMbQN8lT6rLBE9fxAV28XYsa6NDzeLgAH6vQkWrzrIg6l38Oh9HuY3re5XnmypjYXuhi76hCOpN/Hkff6wcPZHvklFfB0dlCbfueZqieukzfytG5bU0PQ/y6LozfYTZ5d2XVj3eeSqsAw/nAG5m6q6k20e2ZvNPN1q0mvJY4c80NVw/hR36UgfeHjxsuogFR95RfjjuB4xgP5e1Ndc4Z+s1+n9JZQAmJIFdiMdWm4rWEMH0soAasmaAnQwoUL0aVLF7i7u8PX1xcjR47EhQsXNK6TlJQEjuOUXufPn1dIl5CQgPDwcEilUoSHh2Pjxo2m/CqCKauQYciyfzFs+X4UqZgM8atdF1Wu9+bvJ3AqKw+fbTuPfRcV28Tkl+g2IR6p8d/luyirlGHP+VtY8NdZtF+wE3vOqZ+jyIKuFQYzRRugc9mqeyTurzNg297zufruXBC157Xq93WyXr9JVcfblDdnc9/4Ey/cVpi8U6gqsLWHMhTeW1IAwFelTLHEJ/miZbWjVEfQACg5ORlTpkzBwYMHsWvXLlRUVGDAgAEoKtJeTHvhwgVkZ2fLX82b15RUpKSkYMyYMRg3bhxOnDiBcePGYfTo0Th06JApv44gSipq2n7oMpNvgca0FvAIYwHiDlwDAHy+XXNQz5elD4BoivuCuifRsasUf+uHjDBPlJBy82sGeBTraWDKG/97G0/jzfVpptsBqsa1WbaX32Sg1hDj8P0OeQ/L5e0n5etawwGAwFVg27dvV3i/evVq+Pr64tixY+jVq5fGdX19feHl5aXysyVLlqB///6IjY0FAMTGxiI5ORlLlixBfHy8UfJuzcR6gbUkxrxAWNK1Rte8nrmpWPX3fbJyuwRdFVvYFCHmvZlY5o/7XHY+zmXn452BLRHo5WySfcQmnDLJdsWIA//f6paTN5WWaSpts6T7h6gaQeflVV0Mvb29taaNiIhAQEAA+vbti8RExd4mKSkpGDBggMKygQMH4sCBAyq3VVpaivz8fIWXNVI4aUVwklZWav8JXrkt/q67q/an438p1xSWpVw1bpucb5MuYyKPiQgtzePLFNtXLNx2XimNrgFCqw+2a09kwYwVMJXzaMg64efDOHDljtZ05iLT8OX5Hhd1PS5rl6Zbkl8PXjd7lZSmY63qM7HOIi+aAIgxhpkzZ6JHjx5o06aN2nQBAQH44YcfkJCQgA0bNqBly5bo27cv9u2rmZ4hJycHfn5+Cuv5+fkhJ0d1F7+FCxfC09NT/goODjbOl7JhfAKAmzzmbXr6uxRjZMdk7haW4qO/z+L9zWcUuqL/eeyGUlpVFwa+F+3Pt19QmohQjMwfV4vzwqoLfk/Mxj2yf524iebvbcOm1CyN6ZIv3sZzP1pP04H3Np5Cp4938e5dVyryoOjUjTzM3XQaE3423cPROSMMQinWX6loeoFNnToVJ0+exP79mlvct2zZEi1btpS/j4qKQmZmJr788kuFarO67SUYY2rbUMTGxmLmzJny9/n5+aINgk5n5eHMzTyM7hxs8W1C+Lgn8hnoa89zU6HHU07douS7haWo5+IIicQy/7aajoA+E78CllWkrg8h2lNMi08FALyxPg0jIxqaPwMCKCmvxG+PGiz/evA63ujXQus6h83cdkzXa7o5Rvqv28gb0P2cZYwh814xVv93zTiZMhJRBEDTpk3Dli1bsG/fPgQFBem8fmRkJH799Vf5e39/f6XSntzcXKVSoWpSqRRSqVTn/Qqhukuml4sjBrb2Fzg3pDZ9bvC1Vzl09S7G/HAQA8L98P7QcCPmzLLdKRR3EGxumu6RX+xQrkLku661W/DXGfn/l+y+hND6rhjRwTaCP2M7dv2+2s/qtuurNmH1YVy9La5xqAStAmOMYerUqdiwYQP27t2L0NBQvbaTmpqKgIAA+fuoqCjs2rVLIc3OnTsRHR1tUH7F5IIJJyS14WukVmdv5uP9TaflRejGLIX78d+qqQ12nr2lMIqytfS4MORrPCxTPZ6VLdJ0PqxIVGxEzhgzafVIbWIIrmQyhsTzuSqruOIPZyq8n7EuTe/9iOU3qW82DP1blWloP5Z4Qbk9EgNEF/wAApcATZkyBWvXrsXmzZvh7u4uL7Xx9PSEs3NVS//Y2FhkZWVhzZo1AKp6eDVu3BitW7dGWVkZfv31VyQkJCAhIUG+3RkzZqBXr15YtGgRRowYgc2bN2P37t1aq9eIbTBkwLQhy/4FUDXg38In26K41o1ZJNdEq1RYUgEXR1EUWBsdn5upvjes8zkFCg1kRRCjmNT6o5mI3XAK3q6OOP5+f53WFUNQY66/D2P6V0nrQ1WbSDEQ9IqycuVKAEBMTIzC8tWrV2PixIkAgOzsbGRk1NRBlpWV4e2330ZWVhacnZ3RunVr/PPPPxgyZIg8TXR0NNatW4e5c+fi/fffR9OmTbF+/Xp069bN5N/JUmj6oYnhSU6Mfnk0rg9Q1Ti066d7hMuMhdh+Ogf1XBzQrYmPSW4wVD2mmSFzzOlKDAFE9QCSYm87qI45D6E5O2atO6zcjkgMBA2A+ESgcXFxCu9nzZqFWbNmaV1v1KhRGDVqlL5Zs0hZD4o1jpFR+3BrOvLWPo2FviPGzttS04ZAnwbPhSpG6jamnLwS+Hs6mXQfupr86zEAwLXPFKdkOHb9Pjo3ridElogVM1ephljGmjLk61rLaM6GEE03eGK4D/86K3QWLF7d8SpKyiuReD7XKLOtf6NlctndGqbM4GPkCv0nVDW35386hBnxabzTi6BwgVgAY58n6h4Ge32RqHK54fszjyITtKlLzVTfMFqsKACyIrqMWWHdZTz6yckrQcePd+GTf6oCScYY3v7jBF6IO4LYDYaPEququobPEyvfEqucfO3jKunjcm4hUq4Yf7LV7WdUj8tF+FdD61riYc1DZzDGDCoRUXVohJpfzBJ9sPmM+g9Fet5RAGSlDCmxEOm5ajTqnupWJl3Gg4fl+PHfdFy9XYiOH+3C3yezAQAbtQwYpy8+F+x0gXtP9Ps6Gc/+eBCXcw3teaj/zcTKT0kldc+L6u//2bbzRh9lnFiH9Dvi62UlJ4YGYipQAGSFUjPuI+z97QrjXtRlzU+Cuki8kIs31qUqTST70d9ncf+hbjNwX7plmqEJftqfbpLt6urSLfFPS2KJdLk1fKfjXGnqAilTENslhW/p2HsbT2HM9ykqG4xbUnvI4cupl7OurLNfqY37YkfV7ON1R92s/fM2ZxdIMXth9REAgKezg8Hbul2ge88TW/or0ClnfObs5SV2DIoNe/ecy4W/pxPaNPTEf5fVTyPzm4qRjoViSBBZUFLVric3vwSO9hJ4uTjqtZ30O0U4l52PAeF+sLez7jIS6/52NsrQG82alOvGyYhIqarXz84rMTgY0Seo/HrXRQP3aj58vp2pAmtbu83zvRFWjwxvyHaPXLuHsgrtE6NqI7YA9+U1R+XH5/mfLGM+M92PoeIKBSXl6PrpHnT4cJea9Nr1+TIJr/92HKuMWfIstuLBRygAslFUBabIGNfuU1mqh4AnVQw5xowBDx5a5tguqhy5pnmOKb43wvNaRoT/etcFrdt4+rsUtJi7DX+fvMlvp0QjIRtOX7/7kFc6PufXwm3nrb6EkarArIjYnsAsmT6HUtvM2rbQo8RUgfXLa47gdJbhs1KLxdPfpcj/z6fUTN/jqmpaAnWmrk1FZBMfvfZDjMNcz6V896NqShFrQiVAVuTSo+7Kmm60KxIv4/Fl/+Jyru02aLWkho2WxlRVYMYOfs7etJ5gypjuW+gIypoYck7qE5CI5fryy4FrSuOa6UrXQ9dj0V6d0huaP0NRAGRlnv3xoMbPv9hxAWfo4m8S1l++YxgxlVAu2S2etleqSndMVRJgypuzWGvVDbnH6nPOmrOkV1P+5m05g7/UVGvy/V66fpcb94tVLj+R+UDl8r9PZeu0fWOjAMjC5eow+J0xGjraCn2u5eouKvkl5bj5oFhUAYCtO3lDPO21xNQjUyw5+eek7jdGdYdRTMdXG2MHqNraiGlj6kN3TeCxi6gNkIU6cu0etp3Kxmu/HVf6zJgnbQbPRnWWrmp25Frv9dmGmrWy80oQ/ZluRcNiZOh5JaY2UKYaNdtYTHXj0VZK87qK64kQpqw1Xj40zdt3PEPz9A1iLdWqVjd/6vIrkzGM+SFFa7q6xPOLNQ0KgCzUv5fu4N9L6se2MBZTzXljjcRS96+Ph2UVcHGky4FQGGM4n1OAFn7usJOoqBIzUz6ssW2gpkk/n/z2gMZ1yyvFXWpeN1BWFTiXlFfimR8OIk1NNZTm7Vt3CERVYFboULrmLrbENMT+tKjO59vPI/yDHUi6kKsxnaHfz8qvpXpjAL7ZexmDl/6L2QknAVjmuaTp77v3/C3M3XRKp/kKdd6/mvKKwhL9J/6c+GigVHNhYHgp7gjGrTpklOCDA7Bqf7pewQ9g/b9ZCoAsQGFpBbacuInCUuPP4EtqGFpFY6kXi2+TqqZX+OjvsxrTWer3swTVjbL/OHZD4JyYxotxR/HrwQysOWD+QVYt6bQtKq3EnvO5+PfSHWTn6V5NWzdwZgBuFyh3Zef7W7b2bvBU5m0B3lyfhl1nb6F/uJ/QWbFihl8mxdTGRQjanlgpgFKvbjMVaz1W+tzUeW/7geptW1I1jq55rZva2F/1CS1VhJaOSoBE6MzNPDz2VRK2n67qCbHr7C2Ff4lpGNqG51a+ZT8tGXLtlMkYRqz4D9dspNG8rgpKytVW/zwsM1/Jrimr1vhsu/oh4XD6PTzzQ4pRx2Pac151Fa7lhD80Qr+5UQAkQq//dhxXbxdh8q/i6JFhGzibL8ExRPrdIlF1LReT/JJytJ2/E9ELVfcEXLRNeboKvr17LNXo71Nw8Oo9DFn2L1YkXjbpviypBKg2Prmu2yxC6bwBsO8S/9HAzU3oPw0FQCJUVGq6hoJEHcu8SBLxqx4E7u6jUZbXH1GcfdycXfKPXtPc7dvUVN3wvthxAel3itDnyyT8fiTT6Pu05umsFmw5o/Hz4vJKXL2tPNaOtQXU+qIASOSGfvOv0FmwSUI/mVgaPseLStiAA1fu4N2EU4Ltf96WMzgswl6i7208hfQ7RZj1qBecNmL8DkIoKtP8sFxSLu6HaaEDMQqARM6aJoA0pwodx++guaGq5BaU4I+jmSovnIYEMNY4vgxftduWpQs88i0AHLx61yTb5RMEq7vhlfIcpX7bqRwAVVVoRLsCNUMAiOUBb9vpHEH3TwEQsTqJ53PRYu42nYrTb+aVoKJSJFcFoTDgiRUH8M6fJ/HFDuV2KbVtSs1CqpZRdGu7W1gqmouuNowx/O/gdRwyUqBQ+6av7zEw5iCbQv4dDN33J1vP6dxo3JZre/7WY0oRczqXLeyDJwVAxOq8suYoZAy8i9OrldcKgIQumhVCYWkFsh5UTWa455z6HofHrt/HG+vT6nSRtZDohoeUK3fx/qbTGPOD5omF+dL1VMp7WG6U/VoSXRoql5aLe3Tm8grzzj4v5v2IHY0DJErWczOxJMZ4UrdkuSoGTFNFn2ocSzqc1+8pd+XPe1gOqYMETg52Jt9/+w93mnwfpmCsm6q26mixn0uVOl48btyvOd8Ur0Gm+6bW3DBcF1QCRGyPmgv16Szqxl1N9+uj9T5S5j0sR/sPd6LLJ7uVPjt7Mx8Lt55DXrGGUhvrPTRGU3v8myHLbKvjR/zhDO2JeOJbVUqNyKtQCZDI/LDvCu4UlgmdDYum9UlUzd39fE6B0fNiO7SHTIlqBqoTu7QbDwCoblBafbO+W1SGL59ur3L92jclMTx4PygW3/XFlKUd5j7mhkygamhbL+ppqRsKgETm063nhc6CxbPF6itzMOS4MgZ8teui8TJjQvp8T03VNmuN8IRvzDYbRUaeUzB2wymomMDeZI5fF3YsI22u6zgaurqgh65jpkcBECFEibHvZ8Ys5rc0f524KXQWTOZuYan8b+vnIdWa3hglPS+vOapT+n9FPBJyXYaW4Bizt6AtoDZAxOpoe1o+fO0eLlB1l0aqLsOajqu2+9oWKwsCsh4U47kfjdNLjI/M+8abY82YJQuVtVrTirVh7dZTwo41o4vaAcylXN2vUZ9sPWvM7Fg9CoBERCbWK4gVGrhkn8XOESQUOlw1YjecwoErNeMEmbpb8dZTxhvPxVSXGTo/DFf7PDqkR0Pl/y6bZpBLayVoALRw4UJ06dIF7u7u8PX1xciRI3HhguYB2DZs2ID+/fujQYMG8PDwQFRUFHbs2KGQJi4uDhzHKb1KSsw3544+JsYdEToLVoFvMfCec5bZKFeMrHlcEVWB8p06QwZUJ8nOKzZJYL059abR2u4kHL9hlO0Q41D726GA0uQEDYCSk5MxZcoUHDx4ELt27UJFRQUGDBiAoiL144zs27cP/fv3x9atW3Hs2DH06dMHw4YNQ2pqqkI6Dw8PZGdnK7ycnJxM/ZUMsu+i5dRVixnfenRNbQno2qMbevoH/jiaiaiFezFno/q5vu4X6dcDq6C0ArEbhJtDjA9DgmA6fbRjjKGgxPYGyTQlQRtBb9++XeH96tWr4evri2PHjqFXr14q11myZInC+08//RSbN2/GX3/9hYiICPlyjuPg7+9v9DwTQogqX+6sKr2OP5yJhU+2U5nmawN6wom9HRW/CXFJXZya/9c9VnM2nkL84UysnxSJbk18zJAz6yeqNkB5eVUD0Xl7e/NeRyaToaCgQGmdwsJChISEICgoCEOHDlUqIRKb8zk0GaexUE8I4/nv8h35/6/eVl0yWyljCu1hLF3dEkSOR9GGNVcBamSr39vMikorEH+4am7DpXsuCZwb6yGaAIgxhpkzZ6JHjx5o06YN7/W++uorFBUVYfTo0fJlYWFhiIuLw5YtWxAfHw8nJyd0794dly6pPnFKS0uRn5+v8DK3QUtsa/RTsbP163r103ztyQoX71ZdevFd8hXM23LGHNkyqnt6VkfZIrXtmmot5hMEUlWpZuoOz9pDNcNI0DE0HtEEQFOnTsXJkycRHx/Pe534+HjMnz8f69evh6+vr3x5ZGQkxo4di/bt26Nnz574/fff0aJFC3zzzTcqt7Nw4UJ4enrKX8HBwQZ/H2LZkqk9Fm9/HM0UOgt6yS3g1ylCn0bNV28X6ryOpaMbs+HUxZClFZXy/9Noz8YjigBo2rRp2LJlCxITExEUFMRrnfXr1+Oll17C77//jn79+mlMK5FI0KVLF7UlQLGxscjLy5O/MjNNe0G/cf8hfvr3KgqNPCKrLdtw/AbWH1E92N69ojJM/t8xM+dIPGQyhuMZ9/GwjP/5pu5pXtX4SZZ6ObZT8yWNcSMvrRD3jOW6UntMdCwqtdmqwkdKyiuRWXey3VoHpXZ1Kw3TYXqCNoJmjGHatGnYuHEjkpKSEBoaymu9+Ph4vPjii4iPj8fjjz/Oaz9paWlo27atys+lUimkUu2jmBrL8OX/4V5RGS7eKsDHI9vixKO5hoh+SisqMfP3EwCA/uHKDd8XbTuP7WcsZzA0Y/vtcAbe33QaHRt5YcPr3XmtU33trdv+ZUXiZfRq0cDYWRSEnRnmb1C62VkoY92Kbf2ePmDxPmTce4i/pvZA2yBPjWlt/ViZg6AB0JQpU7B27Vps3rwZ7u7uyMmpukl5enrC2dkZQFXpTFZWFtasWQOgKvgZP348li5disjISPk6zs7O8PSsOqEWLFiAyMhING/eHPn5+Vi2bBnS0tKwYsUKAb6lsuq2B/9dvos5G0/hz2M0Lochao9G+7CsQumplG9Vh7WqLhk7nvFA53XrPoVuOXFTKQCy1Id6dQGQMUopqg9bbp3xgohty3gUEG89na01ALpV69xRLBkyTd5skaBVYCtXrkReXh5iYmIQEBAgf61fv16eJjs7GxkZNVUb33//PSoqKjBlyhSFdWbMmCFP8+DBA0yaNAmtWrXCgAEDkJWVhX379qFr165m/X58UPBjOG0XBD69eGxFypW7uHTLsGlADtTqGWbJJHRe8FY7EP775E2c1LPU2hbar0QZ0EW99hn514mbuHanqudl7eNv/UfQfASvAtMmLi5O4X1SUpLWdRYvXozFixfrmStibeg2V+PZR/NXXftMc9Vx9ZOqquBxQ2qW8TMmAImaEqC6lyXDAmjrul2dyHyAqWurhhTRdA7N33IG84aF8z521lSqwbdqtaS8Ev9euoPiWm3z6h6GmC+TsPblbkbMHamNZoMnFk/bNZZKgEzLUu9dfBuZ6tMY1dpKOqq/zZVavdvS7xTBVWqnMn3cgWt4LMzXatqLGRtjwPubTuOPOjUAqs61DalZaOzjYq6s2RRR9AIjxBDaq8DMkw9iWayp1EEIfb5MwumsPLWf5xXTtA2a1A1+APUPawrL6bw1GgqABEQ3ZtOrqJTBDJ19RM3UI2NTIKH8W7a2Y1LTK1Bx+a6zt9Suo+r6xpjqASjTMh8YkDtxMfZ1ve4UKNWli5qOPeGHAiAB3bhfLHQWrN66I5k239jV1NUxGRba1duYQcrprHyF3ojWht85pJxGJmOoqLSuMZGMQZffZFmFTGXV2CsaJnMm/FAbIGJV6hYhZ9x7SCVtJnQr33KHGDB2YHinULFk43ZBKaw4JnpE3VSeVYav2I+cPMWhAGy9RJaIBwVAxOoozKjMGE2OagBtR+6zbefNkg9TMGU11f7Ld/DZtvPw8zDfAKumJK8Cq3NGKD5cKB/Q01mK8yr+dijD6puw8Dmv1F2T1P3e6o4DZC0DbAqNAiAiShLI0FVyHr54AKR7ACHRgER1jxNtqATIdDZacJd4dfcpY9ygv0u+AgC4lW89AyHeyi/BG+vTeKdXd5OvPbEnUcTn3Dt6/T56fp5o8rzYAgqAiOgMlBzGPIc1COTuVS34ZTngEQgMWgSED1dKX7trbt26csaoGzxRjeZa0s2S3arnUqyhqXSI1Kau+lXdOUnnqmlQI2giKgMlh7HSYQn8cU/xg/xs4PfxwNktSuss23NZ7fb+vWQdoxYbgqoADVO3bQ8f1na/YgyolCk3ZqYzy7jUPawV0MTZJkEBkBndLyrDtlPZQmdDtCSQYZ5D1Zxvyg0lH91Rts8GZJWqP4PyBeTCrQLcLbSeaghjoZ45/Ku63v7jhEnzYcmolEeZKY5JfjEFQKZAAZAZpd8twmu/HRc6G6LVVXIegdw9Db1EGJCfBVw/oLhUy53swJW7RsmfpVJV3N52/k6UVdh2EKT2vLG24hsjUFdlQ6WLeqJTTBQoACKi4YsH/BLm1x0YTBE9lWpXXF6JNSnXtKaz7mNpuruQNbbZUPWV/nfweu0UZsuLtVLfC8ys2bAZFAAR0ciFF7+E294Btr4DZBwCVLRLIPzw6UprhfdxObXfzQh3m/wS66qy0Oc8sOZzx1TokJkX9QIzIwriNTssC8NN5g1/aKoG44CSPODwD1Uvz2A8xaKQw3XAWRaCykq6hNSlrpriYVklZNY/Up9aar853blV0jUufH/zadNkxBroeCzp3mEaFAAR0ZBBggXl47HSYQlkrG5D6EdvRq0GpG7AqT+B838DeZl4HJl4XPo7LssCceLX/gioaId0BAjxFSzKH8duaJ3GwpqL3inO4U+fQ6Vqzi/yCJ17okBVYERUdsi64rXyN5ADb8UPPAKB0WuANk8AzfsDT34PvHMZePoXHHHpiVLmgGaSmxh2/xckSt/CX45z8Ird3wiAbTeA1uZQ+j21n7326zEs+OusGXNjXr8duq49EQHAt02TFUfLRpZ4IVfoLBBQCZBZ0YB8/OyQdcWu0s7ykaCXvTJI9UjQDs5A65H45lAgjt+7jgGSoxhul4IeklNoK7mGtpJreM9hLQ7LWmJLZTS2VnbDPXgI86Us0LbTOUJnwaTWpFzHhyPaaEyzIlH9GFO2hkrMjOfirULtiWqhW4dpUABEREkGCQ7KwgEAy0J7akzLGEMhXLBB1gsbZL3gjXwMtjuM4XYH0E1yHl0lF9BVcgHz7X/Bf7I2+EsWhR2VXVAAF3N8FbMrKa/Ey78cRUzLBni5ZxOhs2Nxat/nv9hxQbB8iAm/2IciJFOh4QZMgwIgM6JT2DzuwQO/VfbDb5X94I+7GGp3EMPtDqCdJB297U6it91JfGL/MxJlHbClMgp7ZREogXVMWllQUo6EYzew//Id7L98hwIgYjQU3pieunuEunGYiGEoACIWT1PRfA588FPl4/ip8nE05rIxTJKC4XYpaC7JwiC7IxhkdwSFzAm7ZJ2wpTIa+2VtUW7BP4vMe8V4WF53pGyiC00PKuWVMpTb4CjajGmvAqMqMsPRITQvvRpB//LLL/jnn3/k72fNmgUvLy9ER0fj+nVqWEjMi+/T0TUWgG8qn0T/ss8xqPQzfFsxHJmyBnDjSvCE3X9Y7fgFDktfx6f2PyJKcgYSWOaNjorLTafX54kI/2AHHpZZ1zg/2jDGqBRCQBJqBGQSegVAn376KZydnQEAKSkpWL58OT7//HPUr18fb775plEzaE3oHBYLDudZI3xe8Qx6li3Bk6XzsbpiIG4zT9TjCvGcfSLiHT9BinQq3rf/H9pzl2FJz2Z0nhlG0186O68EAHDtrvZBJK3JXyduak9ETIZ+0qahV1l/ZmYmmjVrBgDYtGkTRo0ahUmTJqF79+6IiYkxZv4I0cqwoncOx1kLHK9ogY8qxiFSchbDJQcw2O4w/LgHeMl+G16y34brMl/8JYvClspoXGTBxsq60XGc4sVy19lbguWFWI/d53JR38062slZol9SqGbFFPQqAXJzc8Pdu1Xjq+zcuRP9+vUDADg5OaG4uNh4uSOEB2O1PZBBggOyNphdMQldSlfipbK3sLkyGg+ZFCGSXEy134yd0nex3fFdTLHbhGBOnMFF7eLyV9YcxamsPAFzY3n2XbwjdBZEp9KGRwwn1kuvEqD+/fvj5ZdfRkREBC5evIjHH38cAHDmzBk0btzYmPmzKtQ2wzRM0TahDA7YI+uEPbJOcEYJ+kmOY5hdCnpLTiBMkokwSSbewe9IkzXFlspo/F0ZiVzUM3o+9EFVYIbZfU6cga2Q+ARAFCJVkdrbaU+kBv10zUuvAGjFihWYO3cuMjMzkZCQAB8fHwDAsWPH8Oyzzxo1g4RoY+qH02I44S9ZNP6SRcMDhRhodxTDJQcQLTmDDpIr6CC5grn2v+KQrBW2yKKwrbIrHsDdtJkixIwqZdobQfMbLdr6NfN1EzoLhCe9AiAvLy8sX75cafmCBQsMzhAhOjPjdTcfbvijMgZ/VMagAR5gsN0hDLdLQWfJRUTZnUWU3Vl8aB+HfbJ22FIZhd2yTiiCs/kyCBpxnBhfJQU3ZiGj42xWerUB2r59O/bv3y9/v2LFCnTo0AHPPfcc7t+/b7TMWRu6Lxnf59vP4/A19fNZmdJteGFN5UCMKpuPHqVL8Vn5MzgjC4EDV4m+dqlY6vgtjkpfw3KHpRgoOQIpTD85ZN1G0IQYA7UB4s+QKnlb610oNL0CoHfeeQf5+fkAgFOnTuGtt97CkCFDcPXqVcycOdOoGSREndNZefg26YrQ2QAA3GAN8F3lcDxethB9S7/A0ooncVXmD2euDEPtDuF7x8U4In0NXzp8h16SE7CDaQYr/HTrea0zvBOiKwZqw0isj15VYOnp6QgPr5qnKSEhAUOHDsWnn36K48ePY8iQIUbNICHq5JeUC50Fla6whlhcMQqL8RTacOkYZpeCYXYpCOTuYZTdPoyy24c7zANbK7vhr8ooHGUtwPR7FlGy7+Jt7Lt42yjbIkSOx0CI9x+K8/dIiDp6XXUdHR3x8GHVU+bu3bsxYMAAAIC3t7e8ZIiPhQsXokuXLnB3d4evry9GjhyJCxe0Tz6YnJyMTp06wcnJCU2aNMF3332nlCYhIQHh4eGQSqUIDw/Hxo0beeeLEOPgcJo1wcKK59G9dBmeLv0A/6voh7vMHfW5fIy334U/pB9iv3QGYu1/Q2suHdSXhoiRjIFOTb7oOFkMvQKgHj16YObMmfjoo49w+PBheTf4ixcvIigoiPd2kpOTMWXKFBw8eBC7du1CRUUFBgwYgKKiIrXrpKenY8iQIejZsydSU1MxZ84cTJ8+HQkJCfI0KSkpGDNmDMaNG4cTJ05g3LhxGD16NA4dOqTP1yXEYAwSHGFheL/iRXQrXYHxZe/iz8peyGfOaMjdxav2/+Af6XvY4/g23rT/E025LKGzTIgcTYNBrJFeVWDLly/H66+/jj///BMrV65Ew4YNAQDbtm3DoEGDeG9n+/btCu9Xr14NX19fHDt2DL169VK5znfffYdGjRphyZIlAIBWrVrh6NGj+PLLL/HUU08BAJYsWYL+/fsjNjYWABAbG4vk5GQsWbIE8fHxun5do6FG0PphjIHjOJRXyvDR32eRk1eC57o1gqO9caqNzK0C9tgna499svaQ4kXESNIwzC4F/STH0VSSjRmSDZhhvwFnZCHyMYay0EDt9iSQoavkPHzxALnwwmFZGGRGqlIjBKCJTnVBh8py6BUANWrUCH///bfS8sWLFxuUmby8qhFrvb291aZJSUmRV7lVGzhwIFatWoXy8nI4ODggJSVFaU6ygQMHyoMmYpnWHc7AmkdDwu88ewtrX+kmcI4MVwpH7JB1xQ5ZV7iiGP0lxzDc7gB6Sk6hteQ6WkuuI9YhHkdlLbClMgpbKyNxB57y9QdKDmOewxoEcjU94W4ybywoH48dsq5CfCWLtOH4DVTIGEZ3Fu80J4QQ49IrAAKAyspKbNq0CefOnQPHcWjVqhVGjBgBOzv9RsFkjGHmzJno0aMH2rRpozZdTk4O/Pz8FJb5+fmhoqICd+7cQUBAgNo0OTk5KrdZWlqK0tJS+Xtd2jER88nJLxE6CyZVBGdskvXAJlkPeKEAg+0OY7gkBd0k59BZchGdJRcxz34N/pO1wV+yKFQwO3zlsFJpO/64h5UOS/Ba+RsUBPFQXFaJmb+fAAAMDPcXODfiRCVA/KkaEJIxhm2nVd9/iHD0CoAuX76MIUOGICsrCy1btgRjDBcvXkRwcDD++ecfNG3aVOdtTp06FSdPnlQYX0idugO9VZ9wtZerSqNugLiFCxeaZRBH6kaqH8Zsr/rwAdwRX9kX8ZV94Yd7GGp3EMPsUtBBcgW97E6hl90p+U2p7rGRcFWNVuc5/A+7SjtTdZgWpRU1QxIUl5tmeAJLR22gDZN08TZe/+240Nkgdeh1ZZw+fTqaNm2KzMxMHD9+HKmpqcjIyEBoaCimT5+u8/amTZuGLVu2IDExUWsjan9/f6WSnNzcXNjb28un5FCXpm6pULXY2Fjk5eXJX5mZmTp/B0JM5Ra8sapyCEaWfYRepYvxRfloZMjqVw16qCYwlHBAIHcXc+x/w3DJf+gjSUVn7jxachloiNvwQBEkkJn3i4hQxt2HCqUbthZo80XTXBgmLeOB0FkgKuhVApScnIyDBw8qtNXx8fHBZ599hu7du/PeDmMM06ZNw8aNG5GUlITQ0FCt60RFReGvv/5SWLZz50507twZDg4O8jS7du1SaAe0c+dOREdHq9ymVCqFVCrlnW9ifuey87EiUXHQQ1u8JmcwP6yoHIlM5otljsrT0dT1sv02jZ8XMGcUwBmFzBkFcEEBc0EBnOv864IC5oxCVKXJV1jmgnL9a9LNrm6D8Tfi3fHjxJq2ZBT/qJZ+pwj3ikw/krk1UHVdssFLlUXQ68ollUpRUFCgtLywsBCOjo68tzNlyhSsXbsWmzdvhru7u7zUxtPTE87OVfMnxcbGIisrC2vWrAEATJ48GcuXL8fMmTPxyiuvICUlBatWrVLo3TVjxgz06tULixYtwogRI7B582bs3r2bV/WaKdHTpX4YgMFL/1Va/tqvx8yfGZHIhRevdIcrW6AC9nDnHsIdxXDjiuGOh5ByFQAAd64Y7ig26M5fwhxqBUZ1g6nqQEpxWeGjwKs6mCqBI0wdfqhsMH7bG+8tHA+gqq1UfkmFSfNgqUorZMgtKNWekBALolcANHToUEyaNAmrVq1C165VF45Dhw5h8uTJGD58OO/trFxZ1YAzJiZGYfnq1asxceJEAEB2djYyMjLkn4WGhmLr1q148803sWLFCgQGBmLZsmXyLvAAEB0djXXr1mHu3Ll4//330bRpU6xfvx7dull+ryFSw5ZvVodlYbjJvOGPe5CoiBtkDMiBD54p/0BlGyApyh4FRFWBUVWA9BAe3EO4oVhhmbs8TbH8vRuK4cpV3RCduHI4IQ8NuDy9v085s6sqYaoOlBRKo5SDp/xapVPVAVchnNSOqD1QchgrHZYoLffHPXxrvwSvsaoG48k0ijYx0E/709HM1w3PdG1Us9AWi6stgF4B0LJlyzBhwgRERUXJq53Ky8sxYsQInbqa86lXjouLU1rWu3dvHD+uuUHZqFGjMGrUKN55sXWnFwxEm3k7hM4G4UkGCRaUj8dKhyWQMSgEQdXzVi4oH6e2AXQpHFEKR9xhj7rU63F9tkNlVbDEPYQHHsr/767070N58OQmD6KK5etIOAYHrhL1UIh6XKHuGXlExjgUwkkhKKqu4usrSQUH9Q3GP3b4GdfL/ODx0AMN8ADFcEQxpKiEfr1arQGNL6W/2RtOKQRAFP6Ik14BkJeXFzZv3ozLly/j3LlzYIwhPDwczZo1M3b+rIqYq8DcpOZpx9G2oSdOZelWUkANMFXbIeuK18rfqKrWQU21Tg58sKB8nMm7wFfCDnlwQx5zq1mo45+KgwwuKFUIkmqXNFW/d0MxPKrfqyi5cuQqIeEYPFAMDx2r9CQc0AD52C6NBQ4ATzvVfFbG7FACaVVAxKr+LYEUxawqQFL5/lG64kfLS2r9vxjSqve10pTCwWhzwRkLjS+lu7oBI2SDcDq7EMv2XIK9nYgv/gKofayQ7gGERAMS8z9s8L7raZvlPSkpSf7/r7/+Wu8MEUL42yHril2lnS32SZ1BgiI4owjOyGHyhTpvRYpypWq76qApkjuLp+y1t//LZ85wtgfsKkog4aoy4chVwhFVJVymbKJUFRw9CpBqBVPFTKo1gFJ6r2b9ctiBz5fQVF1I40uppipgfLDwe3xT9Dx20rFSoHSsflkOeAQCgxYB4fyb0BgD7wAoNTWVVzp1Y+0QGgcIEHcpmKWSQYKDsnChsyEgTmOVXqbEF09BewA0qfwtDBn0ND7YfBpSlMMJZXBGKZy5qn+dUAZnrhTOj5Y7PVru/Gi5PH3d91wZnFEGp1rbckYZpFzN7OnVaYBCkwVaFUxSK2BSV4LlgEGSo2qrCxkDPnP4Ea7lxaiAAyogQSXsav1rh0pIUMEe/Qs7+ecVsIMMkqq0rFZaKKcVW4mYJuoCRo+y2xQw1qHuWCE/G/h9PDB6jVmDIN4BUGJioinzYRNoQkH90FEjhuDbYPywLAxV0zrXBFR5cFM8AY14MkogUwiSnFATXCm8fxR01Q2gnDg16eXvy+CCEtg9Ks2y52RVpWQG9PrjOKAeivC14/fGOxAqyBinHFQ9+rdSHkCp/rwCdnU+V/y3svo9UxO8KQRxqrZjBxk4VLCqf+c7rNHYvuwTh59xr8wdFbCHDBxkkDx6cZCBQyUkYI/+lS9nEpVpq/9fnbZmPQ6sKhcm/bsYQgIZ5jlU9eZW/h0yABywfTYQ9rjZqsMsZwAPK3DjXrHQWRAc359naH1XpN8pMmleiG3QpcF4auYDs+brIZzwEE7KgZXRAi0GB1TyLsHqxF3AE/YHtG71nCwY95gH7DgZ7FAJe9T9txJ2kMGee/Tvo9BDKR2nejBOCcfgiEoAakbmFu99XoGEA+ojH39IPzLL/ioZVytgkigEWLJHwSOrFXjJIAFjikFUTSCmHHgpbIOpCd6U1qt67418hSpCZQzIzwKuHwBCe5rleFEAZEYyaszLm52qR3VC9MS3wfiG41lCZdFEOJTDHuWwRz5ctZZmXZY0xBPQHgAtqJhgpGpXBgmY+gAJlbDjZDUBlYp0Ek5DgMVjfeUgrnba6iBO9bb9uPtoKbmh9VvmyjxQDKeqyj2uquzGTh4isEeVgwyc/P+yWv9n8jZp2thxDHbqgkZ1xHapLbxltl1RAERE48qnQ9B0zlYA4vtNEstn6Q3GzUGX6kLjqCopKIMEgIPqJNru/QI+V0ZKzmKd48da002vmG5gwMgetYySPQqcGK8gqjrYql5X3TZqynyqgq3aaWv+z+TrqQ7SZEr7qZ3HxlwOxtnv0f5V3VRPWWUKFAAR0VBX6kMFZ8RYqMG4ZoaOL2VrzBcw1lQp6TT8qz7XThNdbyWQoa9dqtpjBXBVvcFCVE9ZZZo8EWJOHIfnujVS+3G/Vr4AgJd7ap8XjhBifNXVhTnwVlieAx/q0VRHdcAI1ASI8s8oYFSg6VjJy/wHfWbW8YCoBIiY3YvdG2PtoQyVn30/rjNuPiiGp4sD3k04ZeacEUIAqi7UhdADkloSdceqahygz8Q7DhCxXuOjQsy6P01VWnYSDsHeLsgvqRkjhYYPIMT8qLqQPwoY+at7rJa9MkiwkaDpr2NGYr2Nuzvxi4NHdw5S+1nP5vV574/PcaBG0IQQS1IdMG6RReOgLJyCHw1qHyuE9hQk+AEoACI6+HxUe4O3wYEaNRNCCFE1IKKZ9y/s7m2LPn/rhl7ORs+HPv6e1sNo29K1SqvX5zQKOSGEWJunOwULun8KgESuSQNXobMAAAjwdNKeiAeOA4LqufBIVxMu3sovNcq+CSGEiIdE4AiEAiCRmzVQ8/gRy5+LMEs+jFVr9XSnYLhJldscffJEGyPtgRBCCNGOAiAz0ieI0NZAeWi7QP0yo8HbA1rgwxGtdVonPNBD4+ejOwdhw+vReLar6iLP57uZtycaIYQQYQndHpQCIKIktL4bxkc1Vlim7UR9o28LTO/bHH9P64E1L1aNezG9b3P55y6O9ujYqJ5C1ZYm1AuMEEKIKVEARJTwaaT8xah2Cu+dHe0ws38LtGnoiV4tGuDch4Mws3+Lmm0KHepbqa+ebo8nIhoKnQ1CCLE4FACJHM8CE16a1HfFry91U/v5ExENEeLjgn6tDJ+MztlRmHEdxKC5r5tZ90fBJSGE6I5GghY5iREjoLZBnqjnqjzjsr9nVVf7xWM6gDGmsprK0NGY+VZ91aQ3aHeCMnc4QuEPIcQSCf3sRiVAZtS2oafO6wTVc8ZjYb5o7KO967gqYf7u8v8/3y0EXK3WNd+P64QXujfGM11qGibrGqgQZeYskaE/FyGE6IcCIDPydZdq/HxSryZKyziOw88Tu2DpM8rd3b8erX1kZkd7Ca58OgSH5/RF11DF2Z0fC/PFvGGt4WCn+jT4bmynmjdUzMCbPofqnYEt9dpXVFMfvdYjhOiuXZDuD7FEvCgAEhF7DeOCy+qUKrwe0xRPdlQ/N1c1DlUTjPp6KA9kqK3woFudgMlUts3oqbSMs7F+YFP6NNMpvZvUHkfe64cAT2fYCT2evMjVc1Gu9iVEH0JX2VgboSe6pgDIjAypXqp9msS/EqnQw8qSje4chFYBmscQIsqcHCRo8KhEUVPgTICBrf2FzgIhRIQoADIjbbcpTfFR7SePqKY+sFdTbVXX5N5Nee9DE1Vx+ovdQwEAT/EoibIpZn6osRN6PHmRs7an9ue6NRI6CzaL2txZF+oFZkaG/Hjquznqtd7gtgF671NbfucMCcPQ9gF6Ne6W70NNWGjJFxpz3G9r39Sl9hQAaRPm747zOQVCZ8Mo9O0QQQhRRFdOgbX0c9eeCECIjysWj2mPXx6NsqwvvUuAVNzV7e0k6NiontpG1LbK3OPyDO9g/OlQrI0jBYnECCz4uUyUhC6dpauCGalqA+ThzL8Q7omIIPRu0cCYWdKodjUbNbTlz9tVv9I6XdQ+lTo2qmfy/VkyoRtaEitiyUXTIvTHsRuC7p8CIIF9Mao92gV5YuXzHQ3eVo9m9RHVxAevxzTVnhjaG2W7Se0xuXdTvNIzVN7glmi3eEwHtZ+1aUgNvgkh1kvbBN5iImgAtG/fPgwbNgyBgYHgOA6bNm3SmH7ixIngOE7p1bp1zczlcXFxKtOUlJSY+Nvop3F9V2yZ2gOD2wYY3PW7f7gf4idFwt9Tucu7vmYPDsN7j4cbbXu2IMTHVe1nf09T7vJf1+dPtaMSNyNylVrOBZkPoasNbJmt/ypVDVliyQQNgIqKitC+fXssX76cV/qlS5ciOztb/srMzIS3tzeefvpphXQeHh4K6bKzs+HkZLygwBCdQkxfXaHpAmlr4+tYIj9PJ/Rr5St0NqzG9MeaC50FYqUSXosSOgtmkxL7GK8hSyzpDiPoo9HgwYMxePBg3uk9PT3h6VnT42jTpk24f/8+XnjhBYV0HMfB31+cY39oerA3tHq5en1zNMKlKTNMhwM95Rtq+XMRaObrhpZ+7nSuEqOpeyo5O1hX6aImAY/mjNTGkn5vFt0GaNWqVejXrx9CQkIUlhcWFiIkJARBQUEYOnQoUlNTBcqhMnsNY7YY2puq+rTTdO809Nx0cqjKo6lHibaU35C6+u5BNPieoBztJAjz97CoizGxbMPbU29MS2Ox4Wt2dja2bduGtWvXKiwPCwtDXFwc2rZti/z8fCxduhTdu3fHiRMn0Ly56qLw0tJSlJaWyt/n5+ebLN/2duovyBO7N8bWU9no28oXJ2/koVdz/Xp8mbL04Ojc/sgvLkegF7+nAW0s/f70/tBwzPrzpNDZIISYQd3LVe3rV5MG6tv+EdWE7lxjsQFQXFwcvLy8MHLkSIXlkZGRiIyMlL/v3r07OnbsiG+++QbLli1Tua2FCxdiwYIFpsyunKOGUh4PJwdsf6OXwftoXJ/fQGn6xB5uUnu4WVmjUkPUPYbqBqxMejsG9VxM3z2eWD+qHRVO3RJFS3+AMwW+x6RpA1eDx7UzlEVWgTHG8PPPP2PcuHFwdNR8U5FIJOjSpQsuXbqkNk1sbCzy8vLkr8zMTGNnWc7J0c5k264+8/q09MUHQ8OxflKkchLT7d2oLLWx9sbXuwNQHnvG39MJnjQpp9nYatVXUyqFMKvape2Wes3i44mIhrzT8j0Kr/ZuiqB6wo5qbpEBUHJyMi5fvoyXXnpJa1rGGNLS0hAQoH5KCKlUCg8PD4WXqTiYsHtzk/pVFz+O4/Bij1B0a+KjMT09SdbYPKW7wdt4vlsjBHtX/aANqYa00Xu3UdnqIdzzVozGz1sH0jhUhtB0Xlnz7zbMn9+MBZZG0ACosLAQaWlpSEtLAwCkp6cjLS0NGRkZAKpKZsaPH6+03qpVq9CtWze0adNG6bMFCxZgx44duHr1KtLS0vDSSy8hLS0NkydPNul34Wt6X+N3yd3wejQ+e7ItujerrzWtNf9IDdE+2Euv9WqXNFTKaqIeCi6FFeAljmEvxKR9sBf+ntZD6GwIJtjb+O0WbeV6OiG6sUHrO2ho+yokQQOgo0ePIiIiAhEREQCAmTNnIiIiAh988AGAqobO1cFQtby8PCQkJKgt/Xnw4AEmTZqEVq1aYcCAAcjKysK+ffvQtauwdY3VmjRwk4+l0MtI01p0bFQPz3TlN0O0pl5oYiKWC0vP5pqDytpDDtQOgAwRYMSBLG1V60D9J+i1Vhxst2oQAH5/1Thj9qya0Fnlcms+srr0UFZ1jj3TRcX9SQRPiYK2Zo2JidE4Zk1cXJzSMk9PTzx8+FDtOosXL8bixYuNkT2T+d9LXfHXiZs61asaS4iPC57uFAQPZwcabVgLH1dHRDX1wb+X7igsf3tAC3y586JSek0BEJ/7ztpXuiE3vxTNfK2zuJkYh77VqzYc+wAApPbGaX/Zs3kDhPi4wNeGpgfS5dSxpNOMuvMIoL6bFC90DxVk3xzH4Yun25t9v+pGwBbTRdnXXYrcgprhEAaoGcvn1d5N5QFQ7aeditpVYBpuUu8OCsOi7eeVlkc31V6FSQjRjzEuNRxXVRqy960YSDjgwq0Chc+sSfsgT5y4kafzes6OdkCRCTJkApZRH0IsVvI7MVj+XASGtlPfCF0VIa4lUx9rptd6zz6qfny9T+1JaNVHQBGNvBTex7RUrgql0jlibHRGGa66p5edhLP66sQOtdpF6vJV6w6Tour6JhYUABGTCvFxxdB2gRZxsZCoyCOf6oaFT7bF+Y8GIcy/poeNpgk4uzb2xmNhvpgY3Ri/vtQN3z7fUSnNnCGtEOjphPeGtOKXeSu3emIXtAuidj2GsITfoCmZ4usrdIO3suPr7FhzDeNb7aqqofnswWHGypLRURUYITriVPzfyUGxfcGcIa2QfPE2HjwsV1pfIuHw88QuGvcR7O2C/2Y/Bo7j8MnWcwbm2PJENPJCasYD+fs+Yb7oE+aLxrP/ES5TFk7X2/PA1n7YceaWSfJiqYZ3UD/dhZXFP2hYzxlvD2gBN6k9JDqUSNcNBMP8PZTGRRMLKgEioiTE05Q+u1T3s/bzcDJ4bKHqY/B426rqQ69aAynGtLTu2eK/GGX+dmrWTtP5XfezbTN64vtxqns7WaLxUSFGGajwOQ29ba1xIMSpjzXHRA3tVZ/vxq/3sVhRAEQEJp6LhlhnYP9qdHvEvdAFH42oGfdq/vDWAubIuDo28sKTHRV7RHq7OmKKQpsqYih1N+g3+jVXmqaleqgOa6GpSloXmkpCrK0EiM8Fke95ItbgkAIgQkTOycEOMS190dinZpoDa5qPLbppfXw9uoPS8hZ+NByAKnpXJ6i5BwnVI9XsTHAPVhgU0fibF726Z6K6mEmsVWDWcxUlFqW5rxsu5RZipJo6dSEuJvr8RM2Zz7ZBnvh4ZBsE1TPOiLZiN6xdIB48LFfqNUeMy+pKLgRCx1E3YgiKqASICOKvaT2Q/E6M1vnKiKKxkSFmb/8TK1AvDomEw4ToxmgX5CXI/q2NpvuzpgFprYWjltGMPxqpPLWSLsRazaMvXmeEivPGko4CBUBEEE4OdgjxEdnM1Uo/Zuu/KViDMZ2D0dDLNkrFdLWgVlsxdSUUlnTDMoTUXvPt7ulOQTpvU7EbvM6rW6Xmfm5CZ4E3CoAI0ZGtDVIoRBioyxFeNKod3XzUUGyjUvXmYwNLOizR052CIJFwGNkhEJFNvPFs10ZGmxzVWulTKMgYMH+Y5XTQoACIiJIgNzSlnXIqqwb4dtH3cKrptm5nwXdoU9eOqDo0xt6lJQxS56ilhKKatr+Hn0fNHFXdQmuqmKsPwdjIEOx6s5fiNvll0SKdmj8ATRpUlUoseSYC6yZFYeGTbbHvnT7yNIueamvwfizhHKur7sTLP4zrpNP6qs6beq6O6CPi0Z9rowCI2JwwfzW9i4x8p6/n6ogfxnVC3AtdYK/DbMqWrL6b7hNEqh6B27h/i0VPtYWPqyM+GiHep9P97/ZRuTw8wAN9w/i3+/pnek8sfLIt/pneAy1rnetqq8As8MatC/daDyK11f7ens6OKtPowhKP4qg61X6P6XCeAeovmUPbVXVu0VTK1lwEvTxt46pMiIlou3cMaO1v9YMW1jaojZ9O6R3tJXhRRTdsY5dIhPl74OjcfhgX1djIWzYeX3cnlcv/md4DP03gPyhhfTcpnu3aCK0Dq6YOmfZYM3AcEDu4ZloVay7xMaYlYzrwDgosMY4cGVEz/tbJ+QMUHtQMeQh5smND/P5qFP6e1lPl5yuf74iOjVRPkG1OFAARUTLlU2ntbXds5IX4VyIB0E1BHVN2V018OwaeLspP6DIT1LtZakkHx1VNvDkuMgQN3KU6j7771oCWuPjxYLRpqHouNTEdFV3+RG0aGnewRlX7HtDaD11DvY26H7F4q38Lhap5bb3kVFEXJHEch66h3vB0Vl36NritbpNjmwqNA0Rs2obXNU9XYQO9g7Uy5TFQdwGl467so5FtsGB4a53mZarmUOfmJqagpzYJx6GS5x+/hZ87TmflmzQ/HDiN56IlD4TIYPhDn7TOHIiWhkqACCGiY0gApLaNlxWoG/zMHxZulO2KJeA0VgfL6tKh9kGqS72MRSzHTV8ujjUBTN22eHy+2hMRDdGlsfBVWfqiEiBic9SVOoRb2fxHlkBdtZQh1W5xL3TVe11LU99d90bndYmpZrCqqz6/v72mgQd/ntAF645k4pmuwQblR5fz0NKqWDlUTdo8e3AYXBztePdCrM3JwQ5/TI5G49n/GD+DZkAlQIQ80rmxt87dQIkiY42GKzPgydrfU3VjYlJDeQ4ncRRlGCuG8PVwwvS+zdU2LCc158Dk3k0x3sSdA0RyeimhAIiQWga09tcpvYU99OnFlDdHdYdPLDdkfT0Z0VB7IgD13RS7X2sbrdgcNk/R3C7OlFQNiWAO6hrr6hLQ28K1oLZ3BrYUOgsGE/7XRoiZ2dvxv1JZ9m3Y/FRVGfh78HsKrz0Xk4XHP/h6TAde6Zr7KrZX+ndWH7QyQ1Usp/B/TqH6pn2wl8n3r44ubYCMEXB8/lQ7vNIzFJFNrKun1+kFA7Wm0fYb0z7gJv/SNbEGhxQAEZtQe4A+Sx6VWQgBnrpNGeBbp13KwTl9ebUvGNWxZlA2Sw+A9OXr4YSxkfy7uetb5SjWKrCopvXNur/RXYLx3uPh4Dj1vb1Gdw6Ci6MdRnYI1LgtMV1VHO0k6NjIy6Bt6HNGWFqvTgqAiE2o/WRpa3N5Gcrblf8ouRw4/D2th/IHPC6AtYMkbzfdRuZ98lHwZOpeP8amKhYf09mwhrvGyINQvhjVjndac2Xbx02Kk/MGYMkzEZoTiulA8qBvR4N2j35jljLdhSbUC4zYhNoD61EAZFq+Hk54smNDbDiepdN6dhIO/87qg/JKGdykul2apj3WDBHBXuhkwV1yq5lj2pS6vwAhHtB/HN8Zr6w5Kn//x+Qo1NMh2AaAb56NwLT4VGNnTQmfv4mYrip1Y7Fjc/uhtEKG6M/2Grztja93R2lFJVwc+f9GxRobUgkQsQkrx9b07tIlABJr0a1ZmfDiVffCGOztIp+4UhcOdhL0CfNVmIDWFrQOtNyhG+r+DLs01r0dzrD2mqulSBUfNykCvXSrylZXnWUn4XQKfqq2pVNys6EAiFgdVTeFLo29seet3tj/bh+jddUGjNft29o9p+P0DbaigZ7j+ByY/Rj+ntYDjeu7GjlHlkOspQrmoO933zZD9dxcxiLSOEctCoCIaHUO0a86o0ez+oh7oYvS8qYN3BBUz8XQbBEN5BfmOlfCOUNaIe6FLuggYA8jsag9h9Xcx8PRN8wXP43nP9kpAAR6Oaud24sPsdyoVE2Ea6nMdUwlHLB9Ri+t6VTlp3YPQ22lMm0NOL8sBbUBIqKl9wWFA2Ja+mJUpyD8eeyGMbNE9ORoL0FMS1/0at4AtwpKELWwqi2CJZegtW3oiVNZeTqv1y7ISz6HVQN3KVZNVA7WzUmokhSOA5wc9H8GF9u5c+DyHZPvo0ez+ljyTAfkF5ebbB973+qNq7eL0K2Jj8n2IRZUAkSsjxEfxUw5E7pYnZw/QOG9LrcZZy2TI0okHNxrtdOR6HgF0me4flNp5K25NDHx7RjzZEQPSn9TgU5za/p1FZZWmHwfHUPqob6b1CjTbqg79k0auKFfuJ/B21fnj8lRJtu2rsRzNSGkDlONTaItqJn+WDPUd5Niet9mJtm/GNk/apH60cg2KhsSp8Q+xm87jwaZ1HSE3aT2mBAVgme7NtJ5qoL1kyLl3XCFpu08Cq3vikAV03KIoUGoCLIgOlJ7CZo00L9NldjmAgvzN38DeT7ntj6N3U2FqsCIaBlSBWaImQNa4s3+LUR3QTOlZ7oGI3ZwK7iq6X4e4OmMn8Z3RsLxG/DzcELcgWsq0/Gtllgwoo32RCpENKqHLVN7oN/XybicW6jXNszJEgINIauSxPQL4zgOu97sjYx7D9HnyyRe69S+4Zvju3B1/tUkdkgY3J3sMVwEPeXGRzXGb4cy0K+Vr9BZUSBoCdC+ffswbNgwBAYGguM4bNq0SWP6pKQkcByn9Dp//rxCuoSEBISHh0MqlSI8PBwbN2404bcgpiLkkzLf4MeaYqTawc+WqcrzQfUL98PKsZ0we3AYugr8FCeGkYv1DRyaiLDnlhBH09DAyxS/PTsJJy8N1XUf5rwW1N1XQxVd3D2cHDBnSCuDGsvrSt0xaOnvjpPzB+BHHRv7m5qgAVBRURHat2+P5cuX67TehQsXkJ2dLX81b95c/llKSgrGjBmDcePG4cSJExg3bhxGjx6NQ4cOGTv7xMT07SKszdB2VU9EIT7ae4SJ4D4riHZBXvL/1w0GnRzs8PvkKIUbhS3i0z5MVXXihOjGeC2mKdZPijRKPrxcDBv7yFKDeLHlW2TZ0c4EFzdNm/RwchBdqbqgVWCDBw/G4MGDdV7P19cXXl5eKj9bsmQJ+vfvj9jYWABAbGwskpOTsWTJEsTHxxuSXWJmPjqOCsvXc10bobGPq0108zQlTZfP57o1wsbULHQLFU99vxBUXe8d7SV4d1CY0fZhyXPbif35QluMYO5DX72/uqVndfNhqw9uurLIRtAREREICAhA3759kZiYqPBZSkoKBgxQ7MUycOBAHDhwQO32SktLkZ+fr/Ai1ksi4dCjeX14GvjkbE2MdcGsvhB3aeyNQ3P64reXuxlnw3XY69p9zMQsOAbRyJK+VzPfqhHEn+1q/YNuavq7DGrtL6rekmJmUUcpICAAP/zwAxISErBhwwa0bNkSffv2xb59++RpcnJy4Oen2IXPz88POTk5are7cOFCeHp6yl/BweadjJCoRk8xlqN6Bvj+tbrP+nk4mWxeq6/HtIefhxSf6zB5pim9PaCl1jSfPdlWZdsqQxn6M7GgGEejF7uHIuntGHwyUr8G9qqILQBU126qdj6/G9dJZZq66PJqYb3AWrZsiZYtay40UVFRyMzMxJdffolevWpGxqxbz8gY01j3GBsbi5kzZ8rf5+fnUxBEbIqmCz2fe0DSOzG4lV+KUDM18G0d6ImDsX0FbVNQO0Dn0w7nGR1KJoLqOePG/WJ9sqUXdY3KOZjwRmnwn065GkjIqUHMcS6KLSCry9LGTbOoEiBVIiMjcenSJfl7f39/pdKe3NxcpVKh2qRSKTw8PBRehAD0lMSXi6O92YKfamJrUKmKvqWYcS90Rb9Wfvhrag+taQ09CpqOoxDHeOkzHeDp7KC1jZ4Y2r2Yuxu8fF8G7Ky6qvDxdgFGyo3lsvgAKDU1FQEBNX/IqKgo7Nq1SyHNzp07ER0dbe6sEQPp+zRhynFNerdoYLJtC4mqG3Vnytigma8bfprQGW15DPpojD+ds5rZvYUIMUd0aIi0D/oL1oC+dttAOwvp6ajLNW/r9J44NKevSQZKtLSOJYJWgRUWFuLy5cvy9+np6UhLS4O3tzcaNWqE2NhYZGVlYc2aNQCqeng1btwYrVu3RllZGX799VckJCQgISFBvo0ZM2agV69eWLRoEUaMGIHNmzdj9+7d2L9/v9m/HzGMGG7KEbUm70x4LQqtAy3rB25KYhiLx1y+GNUOszecQqWs5jvz+foWUEgFAFg1oTPeWJ+G2MHG652mioujHR6WVWpNx6fkyVSH1sPJAesmRcLBjoODDm3YDP1bezjZI7+E33QatY/P+0PD8YuagUlVcbSXwM9DtxHYtdn1Zi+sP5KJ12KaGnW7piZoCdDRo0cRERGBiIgIAMDMmTMRERGBDz74AACQnZ2NjIwMefqysjK8/fbbaNeuHXr27In9+/fjn3/+wZNPPilPEx0djXXr1mH16tVo164d4uLisH79enTrZpreKMR0xHB7jWnZAN+N7Yi9b/VGpxBvOGmZ68oaWcpN3JR8PZz06m5uCTEiB6B9sBcS347BgNb+Jt3XwifbKuxXE4ma0hf3RwN29jXhqMKRTXzQKcR8JVAujnb4bqz2xsuqjsiIDoGC/0ab+7lj7tBw+LiZZuw2UxG0BCgmJkbjU2RcXJzC+1mzZmHWrFlatztq1CiMGjXK0OwRAo7jMKiN9deVD9MwXL6qUWZtTZi/u2gbePZsXh+b027Ke+JZi0m9muCHfVeVlu+b1QdXbheiU0g9heVi/fvwwbexeXX7nbrxDj2j6MeieoER22IJT8/WYGBrP0Q28VFavuH1aNwtLEOTBm4C5EpcVFUZiOX8/GhkG7QJ9MQQPRu1auwBaOQ7a+0RxrWp7ybFukmReOaHgwrL67k6orOrOAbYNNbxYVB9PvVp2QCJF24DAKb0aYpBbfyNul9bZ/GNoIn1suQnOjGrfoqsNrJDQ5XpOjaqpzCuj7Vpz3NW+b+nVfXE0tTQVMgJRT2cHPBKryYmKakz9vfStaegqsBcvHQ7VjP7t1B4L1MRAc0Z0kr+/5d7NLGIno+WhAIgYrWcHEx/elvDBamnnj3bLDU8fb5b1Xg8b/EYvBCAyskk+Y60O6lXEwCw6kBSaM0fBfR9w8x/jI02gjr0/z1xsI7rkBCoCoxYrRl9W+DotfsY04UGtdTE1i6dH49sg1kDwwyaCuXo3H6Y9cdJreme6hSEjiH1EFxPvO2oNN48TXhycByHF7uHYv2RTAzX0AZNm20zeqKorBKezsJObaNrDFI3uaoSIPXr2tqv1jQoACLiZeDTVQN3Kba/0Ut7QhvSwF2q1PHA1h4eOY7jHfx8/lTNVBufj2qHN9an4Z2BLVXO8q6OuQeI5MPFkV9vRmOeGklvxygta+AuxdH3+qnt8cWHvZ0Ens7CV2YYfKwMuN7xGYmcKKMAiIiWpVaxiJkxb2hiaQRsKkH1nDG6VunhyIiG6NvKF+46BD9iFVTPBW/1bwEPM5aaqJumwpDgx5qoavNY++FE3YMKx3H4enQHvLk+DVP6NDNR7qwTBUBEtFoFuGNjqtC5sC6qLqJUnM5f7eDH0hvpT+vbXGsaV6k9SivKzJAby2doSaq2BwqFaTfq7Cu0vis2TTH+RLvWTvhyQ0LUmBgdqtd6tlalowtjBjt0nGtY67Ho2by+ybZtpYeMN8XSHU5lAOSiZooSYhwUABHR4tvTRkiWdhHnOMDf0zjD4Ft7FRgBJNYa2RmJIYdnSFvFcZtqt0vbPbM39r7VW+1UHPRXMQ4KLwmxIRyAz0e1R/fP9gqdFatS38KmAOCLbrSatfBzR/sgTzRwl+pUuvr+0HClAUY7h9TD6zFN0aSBm3ysrtyCEq3bor+R/sT/iE2IjpzszTdfl5vUsp4hOI5DQy9n7Hunj9BZsSr9TDgvlZCqRx4WkphLgu0kHDZN6Y6fJnTRaT1vV+XG5xzHYdagMIzqFKTTtqggVn/iPbMI0dHIDoHoEOyFF3s0Nvm+5g0Lx9OdgtBbz0EEhUI1GqbBcRyeezTAojURwwCO+2f1gaMOs7KbW/U4Srr8tgyuPqbfsVFY1uMrISqM7BCI1/s0Qws/d7Pt84Xu+jXQJpZjYnRjjZ/bQhsoU44wzHfTvh5O6NWiAXafu2WyvBiDIYeKz6r08GJ84g2rCeHJw9nBrMGPJaseAM/Su3Cbw4s6Brl0f9KNLgNESkVcDaaP6uC5vpsjAKBbE+2Tu6oLuOsObEr4oxIgQmzA8uci8PXOi1j2bITQWbEYNECf7rxcHPDgYbnGNHvf6o37D8sRVM+F93bnPN4K57LzMbF7YwNzaDr6DDGx8fXu2HA8C+OjQnTaZu3lFP7ojwIgYjGa+7ohwMsZ+y7eRgs/N1y8VQiAnrwB4M/JUfg26Qp6t2iAeVvOKH0+tF0ghrarmW+JBj8kQqnb+4mPhl7O2KtiKg1LVR20BHu7YEY/7QNSEtOgAIhYDAZg6ZgO+P1oJp6IaIiun+4ROkui0bmxN36e6I2kC7m80lMVmGpD2wVAxhjGdlP9RK6JqlnjbQ2NG2Rc9Ds1LQqAiMVgjKGeqyNe7d1U6KyIlj6NVumeVaOBuxTzhrXWa93RnYNRXilDl8ba23MQYojav1lqAqQ/CoCIxYhpaZ1jrRiTORtE+rpLkVtQCncLGwvJWOoeaTsJh/FRjYXIimjYdCxtxi9v08fZiKyraT2xWtFNffDOwJZCZ4PU8tvL3TC4jT/+eC1K6KwYDbWNMgyVJpoOHVvjowCIWIRnujaCk4P5Rni2VKYct6Wu5n7uWDm2E8L8Pcy2TyI+7YO95P/vFFJPuIwITNMvr3r4iWr6lNSq7QZP7YT0RgEQIVaE74XVy8VR/n876u6tF2p7UWX9pEj5/98ZGIYPhoYLmBvhNNEwrlGnkHqYN0z346K2GzwVBxmFbVbeE6tCFwPdeTo7YN2kSDjYcWpnnLZFkTwGpCOKOA74eGQb5BWXo5lv1USeH/59VuhsmZ1USwn1C91DseCvquNCsbM4UABEiBXRJRiMbOJjwpxYlq+ebo96rg7oo0NDe4q7a4yN1H3YAGIkFE3pjR79CCE264dxnfDOwJZ4smNDPBbmp1MASVVg6olhElVze64r/8lwfVwdtSeC+vY9FHsbB5UAEWJFOjbyEjoLFmVAa38M0G/YH/KIqnYqS8Z0wL+X7mB54iWczsoXIFfmV49HULP0mQ44nZWHx8KMN6QHxeH6oxIgQqyIu5MDzn04iOb8IoJyldpjUBt/OFPPTQUjOjTEe4+HGzxgKVW/GgcFQIRYGWdHOwxvH4jGPvwnmyT6oGdvwo8xOmpQlavxUQBEiJWi6yUxBkd7uk0YSt8R2vkMzEmBkf7ozCYWgUp8CRGIlhssVceYX+2xu2gcL/0JGgDt27cPw4YNQ2BgIDiOw6ZNmzSm37BhA/r3748GDRrAw8MDUVFR2LFjh0KauLg4cByn9CopKTHhNyGE2B668RB+jD1WmbuTA17sHorxUSFo4C416rZtiaABUFFREdq3b4/ly5fzSr9v3z70798fW7duxbFjx9CnTx8MGzYMqampCuk8PDyQnZ2t8HJycjLFVyBmQk+ZRHxspO6Bfnt6md63ucHb0DTNxQfDwvHhiDYG78OWCdoNfvDgwRg8eDDv9EuWLFF4/+mnn2Lz5s3466+/EBFR0+uF4zj4+/sbK5tEBCQUARFCLMjM/i2wbM8lobNBNLDoNkAymQwFBQXw9lYcvr6wsBAhISEICgrC0KFDlUqI6iotLUV+fr7Ci4gLhT+6o8aRxBzot0kslUUHQF999RWKioowevRo+bKwsDDExcVhy5YtiI+Ph5OTE7p3745Ll9RH4gsXLoSnp6f8FRwcbI7sE0IsmM0EmLbyPUWITy8woj+LDYDi4+Mxf/58rF+/Hr6+NaNqRkZGYuzYsWjfvj169uyJ33//HS1atMA333yjdluxsbHIy8uTvzIzM83xFYgONNWAUe2Yah2CvYTOAiGEiJZFToWxfv16vPTSS/jjjz/Qr18/jWklEgm6dOmisQRIKpVCKqWW9OKmPsqxmSdxHX00og2CvZ3xRERDobNCbFTXUG8cuXZf6GwQopLFBUDx8fF48cUXER8fj8cff1xresYY0tLS0LZtWzPkjpgKlfLoztPFAe8MDBM6G8TSafntaeriPe2x5qjvJkWflsab+0rMAjydkJ1HQ65YCkEDoMLCQly+fFn+Pj09HWlpafD29kajRo0QGxuLrKwsrFmzBkBV8DN+/HgsXboUkZGRyMnJAQA4OzvD09MTALBgwQJERkaiefPmyM/Px7Jly5CWloYVK1aY/wsSo6H4hxCBGFDC6uRghxe6hxovLyI3MqIhViZdETobhCdB2wAdPXoUERER8i7sM2fOREREBD744AMAQHZ2NjIyMuTpv//+e1RUVGDKlCkICAiQv2bMmCFP8+DBA0yaNAmtWrXCgAEDkJWVhX379qFr167m/XLEqDQ9ZVLpECFEzPS9RGkaB4gYTtASoJiYGI1zpMTFxSm8T0pK0rrNxYsXY/HixQbmjIgNxThEbOjWROpSd52ic0WcLLYXGLEtVMpDiPD+m/0Ynu0ajN9e7iZ0VkTJ2Ncp6gZvWhQAEYtAARAhwqj922vo5YyFT7aDvydNLaRKI28XobNAdGBxvcCIbaInIUKE8cuLXfHar8do3ikeRnUKRua9YkQ19VFYTlcvcaIAiFiExvVdhc4CITYpsokPjr/fX6Ejgqsj3TpUsZNweHtgS6GzQXiis5iI2j/Te+BOYRlCKQAiRDB1e2H6ezphzpAwuDjaw05C5RvEMlEAREStdaCn1jRUPUaEoKkHqy2Y1Kup0FkgxCDUCJoQQgghNocCIGLxmjSg6jFCCCG6oQCIWKyE16Iws38LPNMlWOisEBvUIbie0FkghBiA2gARi9UpxBudQryFzgaxUZNjmsBVaofeLRoInRVCiB4oACKEED1I7e3wcs8mQmeDEKInqgIjhBBCTIhGshcnCoAIIYQQE7LxERNEiwIgQgghRIScHe3k/5c60O3a2KgNECGEEGJC+laBuUnt8e3zHQEALjT9iNHRESWEEEJEakjbAKGzYLWoTI0QQggxIWoDJE4UABFCCCHE5lAARAghhJgQdYMXJwqACCGEEGJzKAAihBBCiM2hAIgQQgghNocCIEIIIYTYHAqACCGEEGJzKAAihBBCiM2hAIgQQgghNocCIEIIIYTYHAqACCGEEBOicRDFiQIgQgghxIRoKjBxogCIEEIIITZH0ABo3759GDZsGAIDA8FxHDZt2qR1neTkZHTq1AlOTk5o0qQJvvvuO6U0CQkJCA8Ph1QqRXh4ODZu3GiC3BNCCCHaURWYOAkaABUVFaF9+/ZYvnw5r/Tp6ekYMmQIevbsidTUVMyZMwfTp09HQkKCPE1KSgrGjBmDcePG4cSJExg3bhxGjx6NQ4cOmeprEEIIIcTCcIwxUVRPchyHjRs3YuTIkWrTvPvuu9iyZQvOnTsnXzZ58mScOHECKSkpAIAxY8YgPz8f27Ztk6cZNGgQ6tWrh/j4eF55yc/Ph6enJ/Ly8uDh4aHfFyKEEGLTGs/+BwAwINwPP4zvLHBubIMu92+LagOUkpKCAQMGKCwbOHAgjh49ivLyco1pDhw4oHa7paWlyM/PV3gRQgghhvh4ZBs0qe+K94eGC50VooJFBUA5OTnw8/NTWObn54eKigrcuXNHY5qcnBy12124cCE8PT3lr+DgYONnnhBCiE0ZGxmCvW/HINjbReisEBUsKgACqqrKaquuwau9XFWaustqi42NRV5envyVmZlpxBwTQgghRGzshc6ALvz9/ZVKcnJzc2Fvbw8fHx+NaeqWCtUmlUohlUqNn2FCCCGEiJJFlQBFRUVh165dCst27tyJzp07w8HBQWOa6Ohos+WTEEIIIeImaAlQYWEhLl++LH+fnp6OtLQ0eHt7o1GjRoiNjUVWVhbWrFkDoKrH1/LlyzFz5ky88sorSElJwapVqxR6d82YMQO9evXCokWLMGLECGzevBm7d+/G/v37zf79CCGEECJOgpYAHT16FBEREYiIiAAAzJw5ExEREfjggw8AANnZ2cjIyJCnDw0NxdatW5GUlIQOHTrgo48+wrJly/DUU0/J00RHR2PdunVYvXo12rVrh7i4OKxfvx7dunUz75cjhBBCiGiJZhwgMaFxgAghhBDLY7XjABFCCCGEGAMFQIQQQgixORQAEUIIIcTmUABECCGEEJtDARAhhBBCbA4FQIQQQgixORQAEUIIIcTmUABECCGEEJtjUZOhmkv12JD5+fkC54QQQgghfFXft/mM8UwBkAoFBQUAgODgYIFzQgghhBBdFRQUwNPTU2MamgpDBZlMhps3b8Ld3R0cxxl12/n5+QgODkZmZiZNs6EFHSv+6FjxR8eKPzpWuqHjxZ+pjhVjDAUFBQgMDIREormVD5UAqSCRSBAUFGTSfXh4eNAPhCc6VvzRseKPjhV/dKx0Q8eLP1McK20lP9WoETQhhBBCbA4FQIQQQgixORQAmZlUKsW8efMglUqFzoro0bHij44Vf3Ss+KNjpRs6XvyJ4VhRI2hCCCGE2BwqASKEEEKIzaEAiBBCCCE2hwIgQgghhNgcCoAIIYQQYnMoADKjb7/9FqGhoXByckKnTp3w77//Cp0lk9u3bx+GDRuGwMBAcByHTZs2KXzOGMP8+fMRGBgIZ2dnxMTE4MyZMwppSktLMW3aNNSvXx+urq4YPnw4bty4oZDm/v37GDduHDw9PeHp6Ylx48bhwYMHJv52xrNw4UJ06dIF7u7u8PX1xciRI3HhwgWFNHSsaqxcuRLt2rWTD6IWFRWFbdu2yT+nY6XawoULwXEc3njjDfkyOlY15s+fD47jFF7+/v7yz+lYKcrKysLYsWPh4+MDFxcXdOjQAceOHZN/LvrjxYhZrFu3jjk4OLAff/yRnT17ls2YMYO5urqy69evC501k9q6dSt77733WEJCAgPANm7cqPD5Z599xtzd3VlCQgI7deoUGzNmDAsICGD5+fnyNJMnT2YNGzZku3btYsePH2d9+vRh7du3ZxUVFfI0gwYNYm3atGEHDhxgBw4cYG3atGFDhw4119c02MCBA9nq1avZ6dOnWVpaGnv88cdZo0aNWGFhoTwNHasaW7ZsYf/88w+7cOECu3DhApszZw5zcHBgp0+fZozRsVLl8OHDrHHjxqxdu3ZsxowZ8uV0rGrMmzePtW7dmmVnZ8tfubm58s/pWNW4d+8eCwkJYRMnTmSHDh1i6enpbPfu3ezy5cvyNGI/XhQAmUnXrl3Z5MmTFZaFhYWx2bNnC5Qj86sbAMlkMubv788+++wz+bKSkhLm6enJvvvuO8YYYw8ePGAODg5s3bp18jRZWVlMIpGw7du3M8YYO3v2LAPADh48KE+TkpLCALDz58+b+FuZRm5uLgPAkpOTGWN0rPioV68e++mnn+hYqVBQUMCaN2/Odu3axXr37i0PgOhYKZo3bx5r3769ys/oWCl69913WY8ePdR+bgnHi6rAzKCsrAzHjh3DgAEDFJYPGDAABw4cEChXwktPT0dOTo7CcZFKpejdu7f8uBw7dgzl5eUKaQIDA9GmTRt5mpSUFHh6eqJbt27yNJGRkfD09LTY45uXlwcA8Pb2BkDHSpPKykqsW7cORUVFiIqKomOlwpQpU/D444+jX79+CsvpWCm7dOkSAgMDERoaimeeeQZXr14FQMeqri1btqBz5854+umn4evri4iICPz444/yzy3heFEAZAZ37txBZWUl/Pz8FJb7+fkhJydHoFwJr/q7azouOTk5cHR0RL169TSm8fX1Vdq+r6+vRR5fxhhmzpyJHj16oE2bNgDoWKly6tQpuLm5QSqVYvLkydi4cSPCw8PpWNWxbt06HD9+HAsXLlT6jI6Vom7dumHNmjXYsWMHfvzxR+Tk5CA6Ohp3796lY1XH1atXsXLlSjRv3hw7duzA5MmTMX36dKxZswaAZZxbNBu8GXEcp/CeMaa0zBbpc1zqplGV3lKP79SpU3Hy5Ens379f6TM6VjVatmyJtLQ0PHjwAAkJCZgwYQKSk5Pln9OxAjIzMzFjxgzs3LkTTk5OatPRsaoyePBg+f/btm2LqKgoNG3aFL/88gsiIyMB0LGqJpPJ0LlzZ3z66acAgIiICJw5cwYrV67E+PHj5enEfLyoBMgM6tevDzs7O6VoNTc3Vyk6tiXVvSs0HRd/f3+UlZXh/v37GtPcunVLafu3b9+2uOM7bdo0bNmyBYmJiQgKCpIvp2OlzNHREc2aNUPnzp2xcOFCtG/fHkuXLqVjVcuxY8eQm5uLTp06wd7eHvb29khOTsayZctgb28v/x50rFRzdXVF27ZtcenSJTqv6ggICEB4eLjCslatWiEjIwOAZVyzKAAyA0dHR3Tq1Am7du1SWL5r1y5ER0cLlCvhhYaGwt/fX+G4lJWVITk5WX5cOnXqBAcHB4U02dnZOH36tDxNVFQU8vLycPjwYXmaQ4cOIS8vz2KOL2MMU6dOxYYNG7B3716EhoYqfE7HSjvGGEpLS+lY1dK3b1+cOnUKaWlp8lfnzp3x/PPPIy0tDU2aNKFjpUFpaSnOnTuHgIAAOq/q6N69u9JQHRcvXkRISAgAC7lmGdSEmvBW3Q1+1apV7OzZs+yNN95grq6u7Nq1a0JnzaQKCgpYamoqS01NZQDY119/zVJTU+Xd/z/77DPm6enJNmzYwE6dOsWeffZZld0kg4KC2O7du9nx48fZY489prKbZLt27VhKSgpLSUlhbdu2tahupa+99hrz9PRkSUlJCl1wHz58KE9Dx6pGbGws27dvH0tPT2cnT55kc+bMYRKJhO3cuZMxRsdKk9q9wBijY1XbW2+9xZKSktjVq1fZwYMH2dChQ5m7u7v8Ok3Hqsbhw4eZvb09++STT9ilS5fYb7/9xlxcXNivv/4qTyP240UBkBmtWLGChYSEMEdHR9axY0d5F2drlpiYyAAovSZMmMAYq+oqOW/ePObv78+kUinr1asXO3XqlMI2iouL2dSpU5m3tzdzdnZmQ4cOZRkZGQpp7t69y55//nnm7u7O3N3d2fPPP8/u379vpm9pOFXHCABbvXq1PA0dqxovvvii/LfUoEED1rdvX3nwwxgdK03qBkB0rGpUj1Pj4ODAAgMD2ZNPPsnOnDkj/5yOlaK//vqLtWnThkmlUhYWFsZ++OEHhc/Ffrw4xhgzrAyJEEIIIcSyUBsgQgghhNgcCoAIIYQQYnMoACKEEEKIzaEAiBBCCCE2hwIgQgghhNgcCoAIIYQQYnMoACKEEEKIzaEAiBBiNSZOnIiRI0cKsm+O47Bp0yZB9k0I0R0FQIQQUbp9+zYcHBzw8OFDVFRUwNXVVT7Roildu3YNHMchLS3N5PsihAiHAiBCiCilpKSgQ4cOcHFxwbFjx+Dt7Y1GjRoJnS1CiJWgAIgQIkoHDhxA9+7dAQD79++X/5+PBQsWwNfXFx4eHnj11VdRVlYm/2z79u3o0aMHvLy84OPjg6FDh+LKlSvyz0NDQwEAERER4DgOMTEx8s9+/vlntG7dGlKpFAEBAZg6darCfu/cuYMnnngCLi4uaN68ObZs2aLPVyeEmAEFQIQQ0cjIyICXlxe8vLzw9ddf4/vvv4eXlxfmzJmDTZs2wcvLC6+//rrGbezZswfnzp1DYmIi4uPjsXHjRixYsED+eVFREWbOnIkjR45gz549kEgkeOKJJyCTyQAAhw8fBgDs3r0b2dnZ2LBhAwBg5cqVmDJlCiZNmoRTp05hy5YtaNasmcK+FyxYgNGjR+PkyZMYMmQInn/+edy7d8+Yh4gQYiQ0GSohRDQqKipw48YN5Ofno3Pnzjhy5Ajc3NzQoUMH/PPPP2jUqBHc3NxQv359letPnDgRf/31FzIzM+Hi4gIA+O677/DOO+8gLy8PEonyM9/t27fh6+uLU6dOoU2bNrh27RpCQ0ORmpqKDh06yNM1bNgQL7zwAj7++GOV++Y4DnPnzsVHH30EoCrQcnd3x9atWzFo0CADjwwhxNioBIgQIhr29vZo3Lgxzp8/jy5duqB9+/bIycmBn58fevXqhcaNG6sNfqq1b99eHvwAQFRUFAoLC5GZmQkAuHLlCp577jk0adIEHh4e8iovTQ2sc3NzcfPmTfTt21fjvtu1ayf/v6urK9zd3ZGbm6v1exNCzM9e6AwQQki11q1b4/r16ygvL4dMJoObmxsqKipQUVEBNzc3hISE4MyZM3ptm+M4AMCwYcMQHByMH3/8EYGBgZDJZGjTpo1CO6G6nJ2dee3DwcFBaZ/VVWuEEHGhEiBCiGhs3boVaWlp8Pf3x6+//oq0tDS0adMGS5YsQVpaGrZu3ap1GydOnEBxcbH8/cGDB+Hm5oagoCDcvXsX586dw9y5c9G3b1+0atUK9+/fV1jf0dERAFBZWSlf5u7ujsaNG2PPnj1G+qaEEKFRCRAhRDRCQkKQk5ODW7duYcSIEZBIJDh79iyefPJJBAYG8tpGWVkZXnrpJcydOxfXr1/HvHnzMHXqVEgkEtSrVw8+Pj744YcfEBAQgIyMDMyePVthfV9fXzg7O2P79u0ICgqCk5MTPD09MX/+fEyePBm+vr4YPHgwCgoK8N9//2HatGmmOBSEEBOjEiBCiKgkJSWhS5cucHJywqFDh9CwYUPewQ8A9O3bF82bN0evXr0wevRoDBs2DPPnzwcASCQSrFu3DseOHUObNm3w5ptv4osvvlBY397eHsuWLcP333+PwMBAjBgxAgAwYcIELFmyBN9++y1at26NoUOH4tKlS0b73oQQ86JeYIQQQgixOVQCRAghhBCbQwEQIYQQQmwOBUCEEEIIsTkUABFCCCHE5lAARAghhBCbQwEQIYQQQmwOBUCEEEIIsTkUABFCCCHE5lAARAghhBCbQwEQIYQQQmwOBUCEEEIIsTkUABFCCCHE5vwf/xVNxE3gR4EAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(sum(training_loss, []))\n",
    "plt.plot(np.arange(1, n_epochs + 1) * len(dataloader_train), [np.mean(loss_epoch) for loss_epoch in training_loss], '-o', label = \"Loss moyenne sur l'epoch\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.legend()\n",
    "plt.xlabel('# batch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sum(test_loss, []))\n",
    "plt.plot(np.arange(1, n_epochs + 1) * len(dataloader_valid), [np.mean(loss_epoch) for loss_epoch in test_loss], '-o', label = \"Loss moyenne sur l'epoch\")\n",
    "plt.title(\"Test loss\")\n",
    "plt.legend()\n",
    "plt.xlabel('# batch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4994ac40",
   "metadata": {},
   "source": [
    "Vérifiez que vous avez bien enregistré votre modèle en fin d'entrainement. Chargez le avec la fonction \n",
    "```python\n",
    "modele = torch.load(...) \n",
    "```\n",
    "et vérifiez que vous pouvez l'utiliser sur des données du problème."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e164707e",
   "metadata": {},
   "source": [
    "## Entraînement de réseaux de neurones\n",
    "\n",
    "Dans cette partie vous définissez une ou plusieurs architecture de réseaux de neurones profonds et vous les réglez sur les données d'entrainement. \n",
    "Vous pouvez notamment utiliser des réseaux à base de convolutions et/ou de couches réurrentes. Vous pouvez vous inspirer de ce qui a été dit en cours sur la reconnaissance vocale.\n",
    "\n",
    "Vous pouvez si vous le souhaitez mettre en place des stratégies d'augmentation de données pour améliorer vos résultats. Pour mettre l'augmentation de données en pratique pouvez vous renseigner sur l'argument collate_fn du dataloader standard de Pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a914bb87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46810f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "878c3943",
   "metadata": {},
   "source": [
    "## Synthèse de résultats \n",
    "\n",
    "Une fois que votre ou vos réseaux sont entrainez vous comparez leurs performances selon les métriques définies en introduction sur l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbac3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dda3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
