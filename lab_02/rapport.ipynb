{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapport\n",
    "- Balthazar Neveu\n",
    "- balthazarneveu@gmail.com\n",
    "- [Lab 2 on github](https://github.com/balthazarneveu/MVA23_SIGNAL/tree/lab_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par une première passe d'homogénéisation du jeu de donnée. En effet, l'échantillonnage temporal n'est pas régulier... l'horodatage n'est pas le même pour tous les signaux.\n",
    "Afin de simplifier la visualisation et le travail sur les données, nous allons simplement ré-échantillonner le tout (timestamps réguliers, échantillonnage à 10HZ).\n",
    "Chaque signal aura 100 échantillons ce qui permet d'avoir un volume raisonnable et même de supperposer les différents signaux pour les comparer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import homogenize_dataset, get_hand_crafted_features, ALL_CLASSIFIERS, train_classifier, get_better_features, extract_peaks\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy\n",
    "\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import homogenize_dataset\n",
    "num_samples = 100\n",
    "df = df_train = homogenize_dataset(dataset=\"train\", num_samples=num_samples)\n",
    "df_test = homogenize_dataset(dataset=\"test\", num_samples=num_samples)\n",
    "extract_peaks(df_train, add_to_df=True)\n",
    "extract_peaks(df_test, add_to_df=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, labels_features = get_better_features(df_train)\n",
    "x_test, y_test, _ = get_better_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whiten=True\n",
    "whiten=False\n",
    "x_train, y_train, labels_features = get_better_features(df_train, whiten=whiten)\n",
    "x_test, y_test, _ = get_better_features(df_test,  whiten=whiten)\n",
    "\n",
    "from utilities import ALL_CLASSIFIERS, DECISION_TREE, RANDOM_FOREST, SVM, ADABOOST, XG_BOOST\n",
    "COLOR_LIST = \"rgbckyp\"\n",
    "best_accuracies_overall = []\n",
    "best_feature_dimensions = []\n",
    "# classifiers_list = ALL_CLASSIFIERS #[DECISION_TREE, RANDOM_FOREST, SVM, ADABOOST]\n",
    "# classifiers_list = [SVM]\n",
    "classifiers_list = [XG_BOOST,  DECISION_TREE, RANDOM_FOREST, ADABOOST, SVM]\n",
    "scanned_feature_dimension = list(range(1, len(labels_features)+1))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for classifier_index, classifier_type in enumerate(classifiers_list):\n",
    "    best_accuracies = []\n",
    "    feature_dimensions = []\n",
    "    color = COLOR_LIST[classifier_index%len(COLOR_LIST)]\n",
    "    for feature_dimension in scanned_feature_dimension:\n",
    "        accuracies, best_depth, _ = train_classifier(x_train, x_test, y_train, y_test, feature_dimension=feature_dimension, debug=False,  show=False, classifier=classifier_type)\n",
    "        # print(f\"#features={feature_dimension} Tree depth={best_depth} accuracy training {accuracies[0]*100:.1f}% | accuracy test {accuracies[1]*100:.1f}%\")\n",
    "        best_accuracies.append(accuracies)\n",
    "        feature_dimensions.append(feature_dimension)\n",
    "    # plt.plot(feature_dimensions, 100.*np.array(best_accuracies)[:, 0], color+\"--\", alpha=0.1) #label=f\"{classifier_type} accuracy training\")\n",
    "    plt.plot(feature_dimensions, 100.*np.array(best_accuracies)[:, 1], color+\"-\") #label=f\"{classifier_type} accuracy validation\")\n",
    "    best_index = np.argmax(np.array(best_accuracies)[:, 1])\n",
    "    best_accuracy = best_accuracies[best_index][1]\n",
    "    best_feature_dimension = feature_dimensions[best_index]\n",
    "    plt.plot(feature_dimensions[best_index], 100.*best_accuracy, color+\"o\", label=f\"{classifier_type} Accuracy {100*best_accuracy:.1f}% - {best_feature_dimension} features\")\n",
    "    plt.legend()\n",
    "    best_accuracies_overall.append(best_accuracy)\n",
    "    best_feature_dimensions.append(best_feature_dimension)\n",
    "best_classifier_index = np.argmax(np.array(best_accuracies_overall))\n",
    "plt.title(f\"Best accuracy {100*best_accuracies_overall[best_classifier_index]:.1f} %\\n\"+ \n",
    "          f\" obtained with {classifiers_list[best_classifier_index]}\" +\n",
    "          f\" on {best_feature_dimensions[best_classifier_index]} features\")\n",
    "new_labels = labels_features[:len(scanned_feature_dimension)]\n",
    "new_labels = [f\"{label}\\n{feature}\" for label, feature in zip(new_labels, scanned_feature_dimension)]\n",
    "plt.xticks(scanned_feature_dimension, new_labels, rotation=80)\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(r\"Accuracy (%)\")\n",
    "plt.ylim(70, 100)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Phase exploratoire\n",
    "On commence par chercher des critères simples (statistiques basiques des signaux)  pour classifier les données.\n",
    "\n",
    "### Min puissance\n",
    "- Si on fait défiler les signaux de puissance en figeant l'échelle verticale, nous remarquons clairement un \"paquet\" au dessus des autres.\n",
    "- Un descripteur très simple pour discriminer la menace correspond au mimimum de la puissance. On sait déjà qu'il n'est pas suffisant (au vu du gros \"paquet\" vert et rouge)\n",
    "\n",
    "![](figures/superposition_signaux_puissance.png)\n",
    "\n",
    "\n",
    "### Moyenne et puissance des fréquences\n",
    "\n",
    "- L'étude de l'évolution temporelle des fréquences ne révèle pas de fluctuations perceptibles (signaux stationnaires).\n",
    "\n",
    "![](figures/superposition_signaux_frequence.png)\n",
    "\n",
    "- On peut simplement évaluer les moments et ainsi tracer la distribution moyenne des fréquences ainsi que l'écart type\n",
    "\n",
    "\n",
    "\n",
    "En poussant un peu l'étude manuelle des caractéristiques, on commence à arriver à distinguer quelques groupes:\n",
    "- écart type(1/fréquence)/moyenne(1/fréquence)\n",
    "- min(puissance)\n",
    "\n",
    "![](figures/x=freq_avg_y=freq_std_div_freq_avg.png)\n",
    "\n",
    "\n",
    "Un arbre de décision devrait permettre de classifier les données (partitionner l'espace en plusieurs rectangles).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification basique\n",
    "Une approche rationnelle consiste à aggréger les caractéristiques et entraîner un classifieur simple (arbre de décisions) en jouant sur sa profondeur.\n",
    "Plus on ajoute de caractéristiques, plus on espère que la précision du modèle augmente\n",
    "\n",
    "\n",
    "![decision_tree](figures/decision_tree.png)\n",
    "\n",
    "- Avec une unique caractéristique (bien choisie) et un unique seuil, nous atteignons 70% de précision.\n",
    "- En augmentant la profondeur de l'arbre de décision jusqu'à 4, nous atteignons une précision de 75%\n",
    "- Avec 2 caractéristiques, nous remarquons qu'il est nécessaire d'avoir une profondeur de 4 sur l'arbre de décision afin d'atteindre la précision optimale.\n",
    "- Nous remarquons qu'au delà d'une profondeur de 6, l'arbre de décision est à la limite de \"sur-apprentissage\" (la précision sur la base d'entraînement augmente alors que la précision de validation stagne ou commence à diminuer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training and testing sets by extracting the features from the dataframes.\n",
    "x_train, y_train = get_hand_crafted_features(df_train)\n",
    "x_test, y_test = get_hand_crafted_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plots = True\n",
    "plt.figure(figsize=(10, 10))\n",
    "for feature_dimension in range(1, 5):\n",
    "    accuracies, best_depth, _ = train_classifier(x_train, x_test, y_train, y_test, feature_dimension=feature_dimension, debug=True,  show=not compare_plots)\n",
    "    print(f\"#features={feature_dimension} Tree depth={best_depth} accuracy training {accuracies[0]*100:.1f}% | accuracy test {accuracies[1]*100:.1f}%\")\n",
    "if compare_plots:\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first retieve some remaining \"difficult\" signals to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = 2\n",
    "accuracies, best_depth, classifier = train_classifier(x_train, x_test, y_train, y_test, feature_dimension=features_to_keep, debug=False,  show=False, forced_depth=4)\n",
    "y_train_pred = classifier.predict(x_train[:, :features_to_keep])\n",
    "missclassified_indexes = np.where(y_train_pred!=y_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indexes = df.index.values\n",
    "# Get indexes that are not in missclassified_indexes\n",
    "not_missclassified_indexes = np.setdiff1d(all_indexes, missclassified_indexes)\n",
    "\n",
    "# Generate\n",
    "num_indexes_to_choose = 100  # specify the number of indexes you want\n",
    "random_indexes = np.random.choice(not_missclassified_indexes, num_indexes_to_choose, replace=False)\n",
    "df_random = df.take(random_indexes).copy().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hard = df.take(missclassified_indexes).copy()\n",
    "df_hard.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hard.menace.sum()/len(df_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array([df_hard.puissance[idx] for idx in range(len(df_hard)) if df_hard.menace[idx]]).T, \"r\", alpha=0.05, label=\"menace\")\n",
    "plt.plot(np.array([df_hard.puissance[idx] for idx in range(len(df_hard)) if not df_hard.menace[idx]]).T, \"g\", alpha=0.05, label=\"safe\")\n",
    "plt.plot(np.array([df_random.puissance[idx] for idx in range(len(df_random)) if not df_random.menace[idx]]).T, \"g\", alpha=0.1, label=\"safe\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_min, _props =  scipy.signal.find_peaks(-df_hard.puissance[0])\n",
    "peaks, props =  scipy.signal.find_peaks(df_hard.puissance[0])\n",
    "plt.plot(df_hard.puissance[0], \"k-\", label=\"power\")\n",
    "plt.plot(peaks, df_hard.puissance[0][peaks], \"xr\", label=\"extracted peaks\")\n",
    "plt.plot(peaks_min, df_hard.puissance[0][peaks_min], \"xb\", label=\"min extracted peaks\")\n",
    "plt.grid()\n",
    "plt.title(\"Extraction of peaks from the power signal\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# np.average(peaks[1:] - peaks[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks, peak_vals = extract_peaks(df, add_to_df=True)\n",
    "plt.hist([len(el) for idx, el in enumerate(df.peaks_loc) if df.menace[idx]], color=\"r\", bins=40, label=\"menace\", density=True)\n",
    "plt.hist([len(el) for idx, el in enumerate(df.peaks_loc) if not df.menace[idx]], color=\"g\", bins=40, alpha=0.5, label=\"safe\", density=True)\n",
    "plt.title(\"Histogram of number of peaks per signal\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([el for idx, el in enumerate(df.impulsion_freq) if df.menace[idx]], color=\"r\", bins=100, label=\"menace\", density=True)\n",
    "plt.hist([el for idx, el in enumerate(df.impulsion_freq) if not df.menace[idx]], color=\"g\", bins=100, alpha=0.5, label=\"safe\", density=True)\n",
    "plt.title(\"Histogram of impulsion periods\")\n",
    "plt.grid()\n",
    "plt.xlim(0, 7500)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"r\" if el else \"g\" for el in df[\"menace\"]]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(x_train[:, 2], x_train[:, 0], x_train[:, 1], color=colors)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
